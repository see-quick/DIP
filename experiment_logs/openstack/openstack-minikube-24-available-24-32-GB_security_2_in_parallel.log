[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;33mWARNING[m] The artifact xml-apis:xml-apis:jar:2.0.2 has been relocated to xml-apis:xml-apis:jar:1.0.b2
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.11.3 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.11.3 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.10.5 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.26 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.25 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.11.3 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] kubernetes-model-coordination-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-events-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, automaton-1.11-8.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] kubernetes-model-metrics-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] jackson-dataformat-yaml-2.10.5.jar, crd-generator-0.29.0-SNAPSHOT.jar define 15 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.YAMLException
[[1;33mWARNING[m]   - 5 more...
[[1;33mWARNING[m] kubernetes-model-apiextensions-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] generex-1.0.2.jar, crd-generator-0.29.0-SNAPSHOT.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, zjsonpatch-0.3.0.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-admissionregistration-5.12.0.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] kubernetes-model-certificates-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-policy-5.12.0.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] jackson-datatype-jsr310-2.13.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] kubernetes-model-flowcontrol-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, crd-annotations-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] kubernetes-model-discovery-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.11.3.jar define 68 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 58 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-node-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.25.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-core-2.11.3.jar define 117 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 107 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] snakeyaml-1.26.jar, crd-generator-0.29.0-SNAPSHOT.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-storageclass-5.12.0.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-databind-2.11.3.jar define 657 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 647 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 31 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;31mERROR[m] Feb 22, 2022 3:48:50 PM org.junit.platform.launcher.core.LauncherConfigurationParameters loadClasspathResource
[[1;31mERROR[m] INFO: Loading JUnit Platform configuration parameters from classpath resource [file:/home/cloud-user/strimzi-kafka-operator/systemtest/target/test-classes/junit-platform.properties].
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/cloud-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-02-22T15-48-21_995-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-02-22 20:48:53 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:220] Used environment variables:
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:221] CONFIG: /home/cloud-user/strimzi-kafka-operator/systemtest/config.json
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] STRIMZI_RBAC_SCOPE: CLUSTER
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_CLIENTS_VERSION: 0.2.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] SKIP_TEARDOWN: false
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] LB_FINALIZERS: false
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] DOCKER_ORG: strimzi
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_LOG_DIR: /home/cloud-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] DOCKER_REGISTRY: quay.io
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] DOCKER_TAG: latest
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OLM_SOURCE_NAME: community-operators
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] STRIMZI_FEATURE_GATES: 
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] CLIENTS_KAFKA_VERSION: 3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] STRIMZI_LOG_LEVEL: DEBUG
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] ST_KAFKA_VERSION: 3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OPERATOR_IMAGE_PULL_POLICY: Always
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] BRIDGE_IMAGE: latest-released
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-02-22 20:48:54 [ForkJoinPool-1-worker-3] [32mINFO [m [Environment:222] OLM_OPERATOR_VERSION: 0.26.1
2022-02-22 20:48:55 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-02-22 20:48:55 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-02-22 20:48:55 [ForkJoinPool-1-worker-3] [32mINFO [m [SetupClusterOperator:195] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@2bebbf4f
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-02-22 20:48:55 [ForkJoinPool-1-worker-3] [32mINFO [m [SetupClusterOperator:252] Install ClusterOperator via Yaml bundle
2022-02-22 20:48:57 [ForkJoinPool-1-worker-3] [33mWARN [m [KubeClusterResource:151] Namespace infra-namespace is already created, going to delete it
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-02-22 20:49:03 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-02-22 20:49:04 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:04 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-02-22 20:49:05 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-02-22 20:49:06 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-02-22 20:49:35 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-02-22 20:49:35 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCustomClusterCACertRenew-STARTED
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewCaCertsTriggerByExpiredCertificate-STARTED
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-0 for test case:testAutoRenewCaCertsTriggerByExpiredCertificate
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-1 for test case:testCustomClusterCACertRenew
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:70] Creating secret my-cluster-586352071-cluster-ca-cert
2022-02-22 20:49:45 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 20:49:45 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-02-22 20:49:45 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClientsCaKeysTriggeredByAnno-STARTED
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-02-22 20:49:45 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1828] Generating custom RootCA, IntermediateCA, and ClusterCA, ClientsCA for Strimzi and PEM bundles.
2022-02-22 20:49:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-586352071 in namespace namespace-1
2022-02-22 20:49:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-0
2022-02-22 20:49:46 [ForkJoinPool-1-worker-1] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-02-22 20:49:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-586352071 will have desired state: Ready
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1844] Deploy all certificates and keys as secrets.
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-967799640-cluster-ca-cert to be deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-967799640-cluster-ca-cert successfully deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-967799640-cluster-ca-cert
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:50] Secret my-cluster-967799640-cluster-ca-cert created
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-967799640-cluster-ca to be deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-967799640-cluster-ca successfully deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-967799640-clients-ca-cert to be deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-967799640-clients-ca-cert successfully deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-967799640-clients-ca-cert
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:50] Secret my-cluster-967799640-clients-ca-cert created
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-967799640-clients-ca to be deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-967799640-clients-ca successfully deleted
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1805] Check ClusterCA and ClientsCA certificates.
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-967799640 in namespace namespace-1
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-1
2022-02-22 20:49:49 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-967799640 will have desired state: Ready
2022-02-22 20:52:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-586352071 is in desired state: Ready
2022-02-22 20:52:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-894272712-1808647155 in namespace namespace-1
2022-02-22 20:52:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-0
2022-02-22 20:52:20 [ForkJoinPool-1-worker-1] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-02-22 20:52:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-894272712-1808647155 will have desired state: Ready
2022-02-22 20:52:21 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-894272712-1808647155 is in desired state: Ready
2022-02-22 20:52:21 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1734938666-93643192 in namespace namespace-1
2022-02-22 20:52:21 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-0
2022-02-22 20:52:21 [ForkJoinPool-1-worker-1] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-02-22 20:52:21 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1734938666-93643192 will have desired state: Ready
2022-02-22 20:52:22 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1734938666-93643192 is in desired state: Ready
2022-02-22 20:52:22 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-586352071-kafka-clients in namespace namespace-1
2022-02-22 20:52:22 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-0
2022-02-22 20:52:22 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-586352071-kafka-clients will be ready
2022-02-22 20:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-586352071-kafka-clients is ready
2022-02-22 20:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 20:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:674] Checking produced and consumed messages to pod:my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4
2022-02-22 20:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@635c3924, messages=[], arguments=[--topic, my-topic-1734938666-93643192, --bootstrap-server, my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4', podNamespace='namespace-0', bootstrapServer='my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092', topicName='my-topic-1734938666-93643192', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2479d3cb}
2022-02-22 20:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092:my-topic-1734938666-93643192 from pod my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4
2022-02-22 20:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4 -n namespace-0 -- /opt/kafka/producer.sh --topic my-topic-1734938666-93643192 --bootstrap-server my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092 --max-messages 100
2022-02-22 20:52:32 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 20:52:32 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 20:52:32 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2134b36b, messages=[], arguments=[--group-instance-id, instance570836469, --topic, my-topic-1734938666-93643192, --group-id, my-consumer-group-201268625, --bootstrap-server, my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4', podNamespace='namespace-0', bootstrapServer='my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092', topicName='my-topic-1734938666-93643192', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-201268625', consumerInstanceId='instance570836469', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68df7716}
2022-02-22 20:52:32 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092#my-topic-1734938666-93643192 from pod my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4
2022-02-22 20:52:32 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4 -n namespace-0 -- /opt/kafka/consumer.sh --group-instance-id instance570836469 --topic my-topic-1734938666-93643192 --group-id my-consumer-group-201268625 --bootstrap-server my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092 --max-messages 100
2022-02-22 20:52:39 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 20:52:39 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 20:52:39 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:131] Waiting for Secret my-cluster-586352071-cluster-ca-cert certificate change
2022-02-22 20:52:39 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:138] Certificate in Secret my-cluster-586352071-cluster-ca-cert has changed, was -----BEGIN CERTIFICATE-----
MIIDKjCCAhKgAwIBAgIJAJBt0u1FFTG+MA0GCSqGSIb3DQEBCwUAMCoxEzARBgNV
BAoMCnN0cmltemkuaW8xEzARBgNVBAMMCmNsdXN0ZXItY2EwHhcNMTgxMDIzMTAw
MTExWhcNMTgxMDI0MTAwMTExWjAqMRMwEQYDVQQKDApzdHJpbXppLmlvMRMwEQYD
VQQDDApjbHVzdGVyLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
xpuYrNXYHqw3ajwd12aAeuTlAX4rVwVdPuIex6A4NL8J3d2DV+ngXgNTH//RhiF5
If5KRWSsLei5BUIrwuQutOUNCQwyACmri9+yrx6+tevligiokAUwhHxcDHZpwC3T
+2dzk/BkI++vbSuvjFmBKGQi9gfyoTnStTEQ85KVJUS170hzwDjzaEiJsKpOPx/G
+KTdkAopLucoxr4sxhYeO4mQ2PkT0QL+R8Ohs6LD6v/bqalFP+rS8vibolfxjMNm
lXQCOd8UfXy8OEOaNoNCvhnn/cT/hbEG/ARbV3hHmUh9COV+TSV5dhsbmS5h4MKw
LzP449nGQBmLSkZMu984DQIDAQABo1MwUTAdBgNVHQ4EFgQUOXubcHBZJ7vSzjpi
pfXdFSP3dsEwHwYDVR0jBBgwFoAUOXubcHBZJ7vSzjpipfXdFSP3dsEwDwYDVR0T
AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAFFPGykbzUREDMzh+33i3a8TF
UTQnPMN/SuVbpLfQdpkpLO+aVWjVvJP6qMrI7jRO5zhj4MecAf7YKpe+dyRTTz9B
Dy9BZcujHYjCKdcKBnBFQ7B1xQm9tL1bw+a3ABzSTlhLiBcCxhJEawlWy1Gh18ab
3x/Kqnz0mk/jt5+n9HwlKHuBQVIxRCsfHWwq+WvIfoxM+N//akV8/29hLUf5TlYH
F5CX4G2pA5sSdaDHQ4ekQWuqM6tfvsGLl2KOmEFEgVR4GfaWI4BsUCzlpBNcCGWv
V7klZvBxQPG7MVy0cB8yzTDtUpR0jUFUkZOSp5Pr3yWcwPWEgWkaXYfO+FWETA==
-----END CERTIFICATE-----
, is now -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUI5DIyj7TONMJ/R5B5ruRP0k4Q7cwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjAyMjIyMDQ5NDhaFw0yMzAyMjIyMDQ5NDhaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQCoWp3qFvCmSCChEuscariEHYhCRS4ZRs/IRg+CSVD/
rNmQopuMbygHZnlkn0uGIkXMMOBVsY5x/eY6gOoJ8DWGKj3vvG7lyjfCyYC+tpAY
rI9gt0a38ciG0CC74r8epHoVBfyS9FNrJDog2Y4Lse87SeJPeCM0JlZME0TVF9Ap
zcCaJSDuwgbc5Dt8x1Gtco31UWAgK9omHPNqUPazl6BGlWxM+PxFN3YZR+IN7pW9
qBf/7qkS3f4KFcvFjmokqDLCmwK1CKVWWzVjFI+LxxMI0ttla5+RY0apZj2kA9TP
ZKOrPEP8PY9St50lzxx+eoI+3E84OGOsTUe3+nQaU9YluDkxoDnPrnjj1uyi9eeT
qarek8pMmBSjXebHNGbW4VFa3s5nnKBeJsRyVO+7D8+IJFJnln/N/az1rJJpaXiJ
9SgS3wTWmqzd9NeqYTL3Nu9mzfm4Z0mEf6Napj/CNbwXFmLoKIJ17dmg7ANn4Wvg
VHXtwOZ4zz8scCvuk2ArPd/zmNiZEu4wFmv+d7nxkeXTlZ5iK97l0Svtwkgz5wIF
KRFMu0gjR492Q5qtuZDjqll9KIomQNJh0/RTRSFVKiz4nDM7Z/Ce2kRzik2+aSzg
sgy5cLi7/kKqyrKT3lUxBqDeoxRWSmr6pflq4tdVig0HmDZ1TlGzGL9U2IL7VQOL
YwIDAQABo0UwQzAdBgNVHQ4EFgQUWwlGW+oN8KjbgzyayXmKSKPjbuAwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
ABKMa1dM9BPs3uq47T+v6s/BLptTPboYJ5pYgbzdon0MfFR8q+BsX/L9CbBbbYdi
GXMrLCmAKinL5Hahf6aPAOLdWGUR3YtT2XpAMc5geqcf6H60W7ncq0/Hv3Qtc7SA
xR0bpnZFIiNRCK4YM2rCkTTDW/zJ/pUKMq6OzCTYKijgtF6uporZDbaZe4y9mE3t
H2aCEanvpr0M3PCUdf17rBjvNl7hAHoAv0G6XnVbhacWhFIYfY5cYj4nvptjQ8hB
WWf8/J+OzxMYis3YtOCMYb+oQCdEquD7LS1IwzPBimeemqqwilcipktKOgUtT7Ew
c2WFEF0o1Ul7N7nNc4kh+IUbj7lK4TmA36Aq9NNH+Ds0NS1wwF939CTpfl69tKX2
mxM/RQuzqqkz8TM4GbGCOcOcECOzvlyS+40PuDH7+vcQcAsG+REDEVxLpUIpnLcC
WZfWajfWznVDx9pBqpDxZoXlyXrNBNGif+4Tc5CQPe1MoyS37eGf80xAJlkdLivn
fz9xHmZXpWfj7VK1KEVIE3jGzobxHRktMYYC9KjIP3FkEN1B55C861ZlNoeh9zUS
zpDOH+CGbfnHsg0tIRA1x/G0fkXPKu8P/EUyz2eJ3drvQxzOohsAkw74wt9XzrDb
r5+gPHV/fORGPgDbG+9QWs15iPnOfZObjVULQTDCLgoc
-----END CERTIFICATE-----

2022-02-22 20:52:39 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaUtils:178] Waiting for cluster stability
2022-02-22 20:53:00 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-967799640 is in desired state: Ready
2022-02-22 20:53:00 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1562] Change of kafka validity and renewal days - reconciliation should start.
2022-02-22 20:53:00 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-967799640-kafka rolling update
2022-02-22 20:53:43 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaUtils:206] Kafka cluster is stable after 61 polls.
2022-02-22 20:53:43 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:686] Checking produced and consumed messages to pod:my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4
2022-02-22 20:53:43 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@205284d, messages=[], arguments=[--topic, my-topic-1734938666-93643192, --bootstrap-server, my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4', podNamespace='namespace-0', bootstrapServer='my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092', topicName='my-topic-1734938666-93643192', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1ac9cd11}
2022-02-22 20:53:43 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092:my-topic-1734938666-93643192 from pod my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4
2022-02-22 20:53:43 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4 -n namespace-0 -- /opt/kafka/producer.sh --topic my-topic-1734938666-93643192 --bootstrap-server my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092 --max-messages 100
2022-02-22 20:53:46 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 20:53:46 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 20:53:46 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@760e1e5c, messages=[], arguments=[--group-instance-id, instance2026327588, --topic, my-topic-1734938666-93643192, --group-id, my-consumer-group-201268625, --bootstrap-server, my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4', podNamespace='namespace-0', bootstrapServer='my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092', topicName='my-topic-1734938666-93643192', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-201268625', consumerInstanceId='instance2026327588', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@227bfe47}
2022-02-22 20:53:46 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092#my-topic-1734938666-93643192 from pod my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4
2022-02-22 20:53:46 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-586352071-kafka-clients-75f46b75cf-bn9n4 -n namespace-0 -- /opt/kafka/consumer.sh --group-instance-id instance2026327588 --topic my-topic-1734938666-93643192 --group-id my-consumer-group-201268625 --bootstrap-server my-cluster-586352071-kafka-bootstrap.namespace-0.svc:9092 --max-messages 100
2022-02-22 20:53:54 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 20:53:54 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 20:53:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 20:53:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoRenewCaCertsTriggerByExpiredCertificate
2022-02-22 20:53:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1734938666-93643192 in namespace namespace-0
2022-02-22 20:54:04 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-586352071-kafka-clients in namespace namespace-0
2022-02-22 20:54:44 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-894272712-1808647155 in namespace namespace-0
2022-02-22 20:54:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-586352071 in namespace namespace-0
2022-02-22 20:54:54 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-586352071
2022-02-22 20:55:04 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 20:55:04 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-0 for test case:testAutoRenewCaCertsTriggerByExpiredCertificate
2022-02-22 20:55:48 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewCaCertsTriggerByExpiredCertificate-FINISHED
2022-02-22 20:55:48 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 20:55:48 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 20:55:48 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAclWithSuperUser-STARTED
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-2 for test case:testAutoReplaceClientsCaKeysTriggeredByAnno
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-02-22 20:55:50 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-967799640-kafka has been successfully rolled
2022-02-22 20:55:50 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-967799640-kafka to be ready
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1889416559 in namespace namespace-2
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-2
2022-02-22 20:55:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1889416559 will have desired state: Ready
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-967799640 will have desired state: Ready
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-967799640 is in desired state: Ready
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-967799640 is ready
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1592] Initial ClusterCA cert dates: Mon Feb 21 15:49:48 EST 2022 --> Wed Mar 23 16:49:48 EDT 2022
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1593] Changed ClusterCA cert dates: Mon Feb 21 15:49:48 EST 2022 --> Wed Mar 23 16:49:48 EDT 2022
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1594] KafkaBroker cert creation dates: Tue Feb 22 15:50:31 EST 2022 --> Mon Mar 14 16:50:31 EDT 2022
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1595] KafkaBroker cert changed dates:  Tue Feb 22 15:54:32 EST 2022 --> Sat Sep 10 16:54:32 EDT 2022
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1596] Zookeeper cert creation dates: Tue Feb 22 15:49:53 EST 2022 --> Mon Mar 14 16:49:53 EDT 2022
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1597] Zookeeper cert changed dates:  Tue Feb 22 15:53:05 EST 2022 --> Sat Sep 10 16:53:05 EDT 2022
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testCustomClusterCACertRenew
2022-02-22 20:56:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-967799640 in namespace namespace-1
2022-02-22 20:56:36 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 20:56:36 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-1 for test case:testCustomClusterCACertRenew
2022-02-22 20:57:04 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCustomClusterCACertRenew-FINISHED
2022-02-22 20:57:04 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 20:57:04 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 20:57:04 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertificates-STARTED
2022-02-22 20:57:08 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 20:57:08 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-3 for test case:testAclWithSuperUser
2022-02-22 20:57:08 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-02-22 20:57:08 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-02-22 20:57:08 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-02-22 20:57:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1331644180 in namespace namespace-3
2022-02-22 20:57:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-3
2022-02-22 20:57:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1331644180 will have desired state: Ready
2022-02-22 20:58:01 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1889416559 is in desired state: Ready
2022-02-22 20:58:01 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-178553838-1829137060 in namespace namespace-3
2022-02-22 20:58:01 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-2
2022-02-22 20:58:01 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-178553838-1829137060 will have desired state: Ready
2022-02-22 20:58:02 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-178553838-1829137060 is in desired state: Ready
2022-02-22 20:58:02 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-860168507-341766020 in namespace namespace-3
2022-02-22 20:58:02 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-2
2022-02-22 20:58:02 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-860168507-341766020 will have desired state: Ready
2022-02-22 20:58:03 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-860168507-341766020 is in desired state: Ready
2022-02-22 20:58:03 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1889416559-kafka-clients in namespace namespace-3
2022-02-22 20:58:03 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-2
2022-02-22 20:58:03 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1889416559-kafka-clients will be ready
2022-02-22 20:58:05 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1889416559-kafka-clients is ready
2022-02-22 20:58:05 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 20:58:05 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:467] Checking produced and consumed messages to pod:my-cluster-1889416559-kafka-clients-56549b745b-btjl8
2022-02-22 20:58:05 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@53e9c622, messages=[], arguments=[--topic, my-topic-860168507-341766020, --bootstrap-server, my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1889416559-kafka-clients-56549b745b-btjl8', podNamespace='namespace-2', bootstrapServer='my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-860168507-341766020', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@295da9b6}
2022-02-22 20:58:05 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092:my-topic-860168507-341766020 from pod my-cluster-1889416559-kafka-clients-56549b745b-btjl8
2022-02-22 20:58:05 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1889416559-kafka-clients-56549b745b-btjl8 -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-860168507-341766020 --bootstrap-server my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092 --max-messages 100
2022-02-22 20:58:09 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 20:58:09 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 20:58:09 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@8d3406b, messages=[], arguments=[--group-instance-id, instance1565518384, --topic, my-topic-860168507-341766020, --group-id, my-consumer-group-906150974, --bootstrap-server, my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1889416559-kafka-clients-56549b745b-btjl8', podNamespace='namespace-2', bootstrapServer='my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-860168507-341766020', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-906150974', consumerInstanceId='instance1565518384', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52c9d79}
2022-02-22 20:58:09 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092#my-topic-860168507-341766020 from pod my-cluster-1889416559-kafka-clients-56549b745b-btjl8
2022-02-22 20:58:09 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1889416559-kafka-clients-56549b745b-btjl8 -n namespace-2 -- /opt/kafka/consumer.sh --group-instance-id instance1565518384 --topic my-topic-860168507-341766020 --group-id my-consumer-group-906150974 --bootstrap-server my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092 --max-messages 100
2022-02-22 20:58:16 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 20:58:16 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 20:58:16 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:481] Triggering CA cert renewal by adding the annotation
2022-02-22 20:58:16 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:493] Patching secret my-cluster-1889416559-clients-ca with strimzi.io/force-replace
2022-02-22 20:58:16 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:503] Wait for kafka to rolling restart (1)...
2022-02-22 20:58:16 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1889416559-kafka rolling update
2022-02-22 21:01:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1331644180 is in desired state: Ready
2022-02-22 21:01:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-171979183-870129389 in namespace namespace-3
2022-02-22 21:01:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-3
2022-02-22 21:01:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-171979183-870129389 will have desired state: Ready
2022-02-22 21:01:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-171979183-870129389 is in desired state: Ready
2022-02-22 21:01:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1808606737-1709396356 in namespace namespace-3
2022-02-22 21:01:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-3
2022-02-22 21:01:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1808606737-1709396356 will have desired state: Ready
2022-02-22 21:01:51 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1808606737-1709396356 is in desired state: Ready
2022-02-22 21:01:51 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1108] Checking kafka super user:my-user-1808606737-1709396356 that is able to send messages to topic:my-topic-171979183-870129389
2022-02-22 21:01:51 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 21:01:52 [ForkJoinPool-1-worker-1] [32mINFO [m [ProducerConfig:376] ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.49.2:30811]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1043683700
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 6000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties18102554639746739831.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties10552995004359952346.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-02-22 21:01:52 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 21:01:52 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 21:01:52 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645563712395
2022-02-22 21:01:53 [kafka-producer-network-thread | producer-1043683700] [32mINFO [m [Metadata:402] [Producer clientId=producer-1043683700] Resetting the last seen epoch of partition my-topic-171979183-870129389-0 to 0 since the associated topicId changed from null to iNSjY0XSSjqXdvwSKXClBw
2022-02-22 21:01:53 [kafka-producer-network-thread | producer-1043683700] [32mINFO [m [Metadata:287] [Producer clientId=producer-1043683700] Cluster ID: Vlmz0Ty0RfaMdyNCFU3qKw
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ExternalKafkaClient:182] Sent 100 messages.
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaProducer:1228] [Producer clientId=producer-1043683700] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:659] Metrics scheduler closed
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:663] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:669] Metrics reporters closed
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:83] App info kafka.producer for producer-1043683700 unregistered
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1122] Checking kafka super user:my-user-1808606737-1709396356 that is able to read messages to topic:my-topic-171979183-870129389 regardless that we configured Acls with only write operation
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerConfig:376] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.49.2:30811]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-1132912768
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-consumer-group-27068787
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties13614856642315683017.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties1378674458448961785.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645563714508
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaConsumer:966] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Subscribed to topic(s): my-topic-171979183-870129389
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:402] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Resetting the last seen epoch of partition my-topic-171979183-870129389-0 to 0 since the associated topicId changed from null to iNSjY0XSSjqXdvwSKXClBw
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:287] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Cluster ID: Vlmz0Ty0RfaMdyNCFU3qKw
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:853] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Discovered group coordinator 192.168.49.2:32368 (id: 2147483645 rack: null)
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:535] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] (Re-)joining group
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1000] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Request joining group due to: need to re-join with the given member-id
2022-02-22 21:01:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:535] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] (Re-)joining group
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:595] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Successfully joined group with generation Generation{generationId=1, memberId='consumer-1132912768-c94a6f41-0ead-497f-9011-9b0b5cfcc0ee', protocol='range'}
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:652] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Finished assignment for group at generation 1: {consumer-1132912768-c94a6f41-0ead-497f-9011-9b0b5cfcc0ee=Assignment(partitions=[my-topic-171979183-870129389-0])}
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:761] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Successfully synced group in generation Generation{generationId=1, memberId='consumer-1132912768-c94a6f41-0ead-497f-9011-9b0b5cfcc0ee', protocol='range'}
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:279] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Notifying assignor about the new Assignment(partitions=[my-topic-171979183-870129389-0])
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:291] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Adding newly assigned partitions: my-topic-171979183-870129389-0
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1388] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Found no committed offset for partition my-topic-171979183-870129389-0
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [SubscriptionState:398] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Resetting offset for partition my-topic-171979183-870129389-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.49.2:31629 (id: 1 rack: null)], epoch=0}}.
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ExternalKafkaClient:224] Received 100 messages.
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:310] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Revoke previously assigned partitions my-topic-171979183-870129389-0
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1060] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Member consumer-1132912768-c94a6f41-0ead-497f-9011-9b0b5cfcc0ee sending LeaveGroup request to coordinator 192.168.49.2:32368 (id: 2147483645 rack: null) due to the consumer is being closed
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:972] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Resetting generation due to: consumer pro-actively leaving the group
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1000] [Consumer clientId=consumer-1132912768, groupId=my-consumer-group-27068787] Request joining group due to: consumer pro-actively leaving the group
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:659] Metrics scheduler closed
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:663] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:669] Metrics reporters closed
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:83] App info kafka.consumer for consumer-1132912768 unregistered
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1808606737-1709396356-non-super-user in namespace namespace-3
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-3
2022-02-22 21:01:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1808606737-1709396356-non-super-user will have desired state: Ready
2022-02-22 21:01:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1808606737-1709396356-non-super-user is in desired state: Ready
2022-02-22 21:01:59 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1148] Checking kafka super user:my-user-1808606737-1709396356-non-super-user that is able to send messages to topic:my-topic-171979183-870129389
2022-02-22 21:01:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ProducerConfig:376] ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.49.2:30811]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-806254253
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 6000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties9818389755079350597.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties15855898966723023962.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-02-22 21:01:59 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 21:01:59 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 21:01:59 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645563719569
2022-02-22 21:01:59 [kafka-producer-network-thread | producer-806254253] [32mINFO [m [Metadata:402] [Producer clientId=producer-806254253] Resetting the last seen epoch of partition my-topic-171979183-870129389-0 to 0 since the associated topicId changed from null to iNSjY0XSSjqXdvwSKXClBw
2022-02-22 21:01:59 [kafka-producer-network-thread | producer-806254253] [32mINFO [m [Metadata:287] [Producer clientId=producer-806254253] Cluster ID: Vlmz0Ty0RfaMdyNCFU3qKw
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ExternalKafkaClient:182] Sent 100 messages.
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaProducer:1228] [Producer clientId=producer-806254253] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:659] Metrics scheduler closed
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:663] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:669] Metrics reporters closed
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:83] App info kafka.producer for producer-806254253 unregistered
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1156] Checking kafka super user:my-user-1808606737-1709396356-non-super-user that is not able to read messages to topic:my-topic-171979183-870129389 because of defined ACLs on only write operation
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerConfig:376] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.49.2:30811]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-333072534
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-consumer-group-1011525874
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties1679828347790999465.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties9013978514220306676.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645563720531
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaConsumer:966] [Consumer clientId=consumer-333072534, groupId=my-consumer-group-1011525874] Subscribed to topic(s): my-topic-171979183-870129389
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:402] [Consumer clientId=consumer-333072534, groupId=my-consumer-group-1011525874] Resetting the last seen epoch of partition my-topic-171979183-870129389-0 to 0 since the associated topicId changed from null to iNSjY0XSSjqXdvwSKXClBw
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:287] [Consumer clientId=consumer-333072534, groupId=my-consumer-group-1011525874] Cluster ID: Vlmz0Ty0RfaMdyNCFU3qKw
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:261] [Consumer clientId=consumer-333072534, groupId=my-consumer-group-1011525874] FindCoordinator request hit fatal exception
org.apache.kafka.common.errors.GroupAuthorizationException: Not authorized to access group: my-consumer-group-1011525874
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testAclWithSuperUser
2022-02-22 21:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1808606737-1709396356 in namespace namespace-3
2022-02-22 21:02:10 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1808606737-1709396356-non-super-user in namespace namespace-3
2022-02-22 21:02:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-171979183-870129389 in namespace namespace-3
2022-02-22 21:02:30 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1331644180 in namespace namespace-3
2022-02-22 21:02:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:02:40 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-3 for test case:testAclWithSuperUser
2022-02-22 21:02:42 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1889416559-kafka has been successfully rolled
2022-02-22 21:02:42 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:524] Wait for kafka to rolling restart (2)...
2022-02-22 21:02:42 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1889416559-kafka rolling update
2022-02-22 21:03:24 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAclWithSuperUser-FINISHED
2022-02-22 21:03:24 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:03:24 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:03:24 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCustomClientsCACertRenew-STARTED
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-4 for test case:testCertificates
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:127] Running testCertificates my-cluster-161642661
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-161642661 in namespace namespace-4
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-4
2022-02-22 21:03:29 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-161642661 will have desired state: Ready
2022-02-22 21:09:29 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-161642661 is in desired state: Ready
2022-02-22 21:09:29 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:137] Check Kafka bootstrap certificate
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-kafka-bootstrap:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-kafka-bootstrap
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:140] OPENSSL OUTPUT: 

CONNECTED(00000003)
---
Certificate chain
 0 s:O = io.strimzi, CN = my-cluster-161642661-kafka
   i:O = io.strimzi, CN = cluster-ca v0
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
 1 s:O = io.strimzi, CN = cluster-ca v0
   i:O = io.strimzi, CN = cluster-ca v0
-----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUINXxPww5bzqPpgIVyx3QsB8wCF4wDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjAyMjIyMTAzMjlaFw0yMzAyMjIyMTAzMjlaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQCv+K1AuDzvTreA1gbQxGaAnbHVGKyQqzYi4svlyFon
rI9YbVNncVgK3lWYNvV1JIZc74pZxGRF7ztTGb06kvICgkpw+bvGvmIFGmC+kZe1
I/0v8UtKI+8WTVVUaGxE1B3CvpdeDrD9O12ihldDIPrOPjtmGQYiPFZNO12KtTku
N/yo4pwdtdcZJBAD3cL9j9XiGbP/CqYIo94R7Bufh9WhsWgpbQjDh5dze0WFS/fO
7INzkMLuOKvaUKFSqBYmL6byBs7qT3r9cCOyGUBPCR4sf60bZRjukJT8Rk0FA1aJ
gFglR/0hWY7EQ6Rq7W7nmTsQqXxqGo4Il570FkM3IeCR0XgO+ublk9vAI0v3Y53Z
lXdB3DU3RtQo7kviIk5zQNknqMQUzp3Q/30o8y5N9O2FBTWBGG5IcfmA2vVWPxAY
12erJYAyxfDloSHOxMuXsh4WCdBwtkkAAGwd8VblOmZ/yQmR/VPs2P4G9O5iZQpS
nx1PrALTWZUa3PG+gP8OuJPKfW6O6alD/KKZzjwdK6M7dLif22+ZdCb7lzdK2tid
FNCKVWXTvwIOK5FJtv2fwhZKRCHzwax46kbO8nq8JFgrvI8d5FdLTqNwG+dTmUhc
LN3ZRIsu6BR5XimhJ3zUfHwd4fjOu+ZVyqjcd81yqbbfzPZfwH0b2MD/52RyTNMa
hQIDAQABo0UwQzAdBgNVHQ4EFgQUzfBynJtggaq+25aBuW4EpQUbepAwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AF2SCf/YgIUgLCLxmjpWiwrX+L67H5TImMKktfSfVSpNhI9wMo79RWTd/xe6YOsQ
BvA+7vhV+6LdFn8oSZxTWrRMNAYKjLykxszs6rPBV0UZpS3T67wO3e8/1vnOktZI
VxWNrWhRL3qIFeKEnf2IJ/dskCpD9/u3LYwTFmqQkZHhQOFTvKc1ZAsdil8qp31K
RicmicoM3Nk+0OcrefAM0FXUkVuWzA1XYeCC0onF0EC4rc9jhswu/9r3+Ywq0rLU
Tr5NyKNJOjVNMZIOaIeE32JJUipyOzcCAM/ZAO1h6gXeTFsfA5QKPrEwEwtjsrZm
EBeKjsbNbr6e/6rXm1RIvRbQL6x1486bHXN9gHkbBJcvTesH4nGbEp9tnJhXZf9i
fsPpaj/oT7oIXtJibC57wbxKwbfjnbmlP+sfbdl1sCLSUbEu11zzOuHRPOxz7FsK
zEAYUau/NxPdM58Es2KeKdbTPFjzvLgXBp4MxIKG/jc6XejIc1UTVEE3SUOOBTNy
YJfnb/rdmREqt60JnTGXIs9W8qHgwgBtTzgm7xr2phhIVckfNVKWjhdtWHTGtdvh
95/emOOEPUwA6anZ35vouXderFRc1xYyBlghYvWlfm3EhGaMaV8YmDHeJsGyPtqN
hrAbULnP814FTIs2y7+sSXoOj1Vk29lF+ZClaOHjFNPg
-----END CERTIFICATE-----
---
Server certificate
subject=O = io.strimzi, CN = my-cluster-161642661-kafka

issuer=O = io.strimzi, CN = cluster-ca v0

---
No client certificate CA names sent
Peer signing digest: SHA256
Peer signature type: RSA-PSS
Server Temp Key: X25519, 253 bits
---
SSL handshake has read 3494 bytes and written 369 bytes
Verification: OK
Verified peername: my-cluster-161642661-kafka-bootstrap
---
New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384
Server public key is 2048 bit
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---



2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:143] Check zookeeper client certificate
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-zookeeper-client:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-zookeeper-client -cert /opt/kafka/broker-certs/my-cluster-161642661-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-161642661-kafka-0.key
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:154] Checking certificates for podId 0
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:156] Check kafka certificate for port 9091
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-kafka-0.my-cluster-161642661-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-kafka-0.my-cluster-161642661-kafka-brokers.namespace-4.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-161642661-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-161642661-kafka-0.key
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:156] Check kafka certificate for port 9093
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-kafka-0.my-cluster-161642661-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-kafka-0.my-cluster-161642661-kafka-brokers.namespace-4.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-161642661-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-161642661-kafka-0.key
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:163] Check zookeeper certificate for port 2181
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-zookeeper-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-zookeeper-0.my-cluster-161642661-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-zookeeper-0.my-cluster-161642661-zookeeper-nodes.namespace-4.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-0.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-0.key
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:163] Check zookeeper certificate for port 3888
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-zookeeper-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-zookeeper-0.my-cluster-161642661-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-zookeeper-0.my-cluster-161642661-zookeeper-nodes.namespace-4.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-0.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-0.key
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:154] Checking certificates for podId 1
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:156] Check kafka certificate for port 9091
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-kafka-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-kafka-1.my-cluster-161642661-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-kafka-1.my-cluster-161642661-kafka-brokers.namespace-4.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-161642661-kafka-1.crt -key /opt/kafka/broker-certs/my-cluster-161642661-kafka-1.key
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:31 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:156] Check kafka certificate for port 9093
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-kafka-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-kafka-1.my-cluster-161642661-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-kafka-1.my-cluster-161642661-kafka-brokers.namespace-4.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-161642661-kafka-1.crt -key /opt/kafka/broker-certs/my-cluster-161642661-kafka-1.key
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:163] Check zookeeper certificate for port 2181
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-zookeeper-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-zookeeper-1.my-cluster-161642661-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-zookeeper-1.my-cluster-161642661-zookeeper-nodes.namespace-4.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-1.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-1.key
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:163] Check zookeeper certificate for port 3888
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-161642661-zookeeper-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-161642661-zookeeper-1.my-cluster-161642661-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-161642661-zookeeper-1.my-cluster-161642661-zookeeper-nodes.namespace-4.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-1.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-161642661-zookeeper-1.key
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testCertificates
2022-02-22 21:09:32 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-161642661 in namespace namespace-4
2022-02-22 21:09:42 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:09:42 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-4 for test case:testCertificates
2022-02-22 21:09:42 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1889416559-kafka has been successfully rolled
2022-02-22 21:09:42 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1889416559-kafka to be ready
2022-02-22 21:10:27 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertificates-FINISHED
2022-02-22 21:10:27 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:10:27 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:10:27 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testClientsCACertRenew-STARTED
2022-02-22 21:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-5 for test case:testCustomClientsCACertRenew
2022-02-22 21:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-02-22 21:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-02-22 21:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-02-22 21:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1828] Generating custom RootCA, IntermediateCA, and ClusterCA, ClientsCA for Strimzi and PEM bundles.
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1844] Deploy all certificates and keys as secrets.
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-95840612-cluster-ca-cert to be deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:154] Secret: my-cluster-95840612-cluster-ca-cert successfully deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-95840612-cluster-ca-cert
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:50] Secret my-cluster-95840612-cluster-ca-cert created
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-95840612-cluster-ca to be deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:154] Secret: my-cluster-95840612-cluster-ca successfully deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-95840612-clients-ca-cert to be deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:154] Secret: my-cluster-95840612-clients-ca-cert successfully deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-95840612-clients-ca-cert
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:50] Secret my-cluster-95840612-clients-ca-cert created
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-95840612-clients-ca to be deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:154] Secret: my-cluster-95840612-clients-ca successfully deleted
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1805] Check ClusterCA and ClientsCA certificates.
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-95840612 in namespace namespace-5
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-5
2022-02-22 21:10:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-95840612 will have desired state: Ready
2022-02-22 21:10:35 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:539] Checking the certificates have been replaced
2022-02-22 21:10:35 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:550] Checking consumed messages to pod:my-cluster-1889416559-kafka-clients-56549b745b-btjl8
2022-02-22 21:10:35 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1a4b6adb, messages=[], arguments=[--group-instance-id, instance1782785203, --topic, my-topic-860168507-341766020, --group-id, my-consumer-group-77147821, --bootstrap-server, my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1889416559-kafka-clients-56549b745b-btjl8', podNamespace='namespace-2', bootstrapServer='my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-860168507-341766020', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-77147821', consumerInstanceId='instance1782785203', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@605e1caa}
2022-02-22 21:10:35 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092#my-topic-860168507-341766020 from pod my-cluster-1889416559-kafka-clients-56549b745b-btjl8
2022-02-22 21:10:35 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1889416559-kafka-clients-56549b745b-btjl8 -n namespace-2 -- /opt/kafka/consumer.sh --group-instance-id instance1782785203 --topic my-topic-860168507-341766020 --group-id my-consumer-group-77147821 --bootstrap-server my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092 --max-messages 100
2022-02-22 21:10:42 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 21:10:42 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 21:10:42 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1897353754-403295759 in namespace namespace-5
2022-02-22 21:10:42 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-2
2022-02-22 21:10:42 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1897353754-403295759 will have desired state: Ready
2022-02-22 21:10:43 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1897353754-403295759 is in desired state: Ready
2022-02-22 21:10:43 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1889416559-kafka-clients-tls in namespace namespace-5
2022-02-22 21:10:43 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-2
2022-02-22 21:10:43 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1889416559-kafka-clients-tls will be ready
2022-02-22 21:10:45 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1889416559-kafka-clients-tls is ready
2022-02-22 21:10:45 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:575] Checking consumed messages to pod:my-cluster-1889416559-kafka-clients-tls-58bd95cf4c-n9w2r
2022-02-22 21:10:45 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3e1699a, messages=[], arguments=[--group-instance-id, instance508559555, --topic, my-topic-860168507-341766020, --group-id, my-consumer-group-1737933035, --bootstrap-server, my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1889416559-kafka-clients-tls-58bd95cf4c-n9w2r', podNamespace='namespace-2', bootstrapServer='my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-860168507-341766020', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1737933035', consumerInstanceId='instance508559555', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@42bddea4}
2022-02-22 21:10:45 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092#my-topic-860168507-341766020 from pod my-cluster-1889416559-kafka-clients-tls-58bd95cf4c-n9w2r
2022-02-22 21:10:45 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1889416559-kafka-clients-tls-58bd95cf4c-n9w2r -n namespace-2 -- /opt/kafka/consumer.sh --group-instance-id instance508559555 --topic my-topic-860168507-341766020 --group-id my-consumer-group-1737933035 --bootstrap-server my-cluster-1889416559-kafka-bootstrap.namespace-2.svc:9092 --max-messages 100
2022-02-22 21:10:52 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 21:10:52 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 21:10:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:10:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoReplaceClientsCaKeysTriggeredByAnno
2022-02-22 21:10:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1889416559-kafka-clients in namespace namespace-2
2022-02-22 21:12:12 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1889416559-kafka-clients-tls in namespace namespace-2
2022-02-22 21:12:12 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1897353754-403295759 in namespace namespace-2
2022-02-22 21:12:22 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-178553838-1829137060 in namespace namespace-2
2022-02-22 21:12:32 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-860168507-341766020 in namespace namespace-2
2022-02-22 21:12:42 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1889416559 in namespace namespace-2
2022-02-22 21:12:42 [ForkJoinPool-1-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-1889416559
2022-02-22 21:12:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:12:52 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-2 for test case:testAutoReplaceClientsCaKeysTriggeredByAnno
2022-02-22 21:13:36 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClientsCaKeysTriggeredByAnno-FINISHED
2022-02-22 21:13:36 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:13:36 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:13:36 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithMirrorMaker-STARTED
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-6 for test case:testClientsCACertRenew
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-522824700 in namespace namespace-6
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-6
2022-02-22 21:13:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-522824700 will have desired state: Ready
2022-02-22 21:22:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-522824700 is in desired state: Ready
2022-02-22 21:22:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser strimzi-tls-user-727907852 in namespace namespace-6
2022-02-22 21:22:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-6
2022-02-22 21:22:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: strimzi-tls-user-727907852 will have desired state: Ready
2022-02-22 21:23:00 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaUser: strimzi-tls-user-727907852 is in desired state: Ready
2022-02-22 21:23:00 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1669] Change of kafka validity and renewal days - reconciliation should start.
2022-02-22 21:23:00 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-522824700-entity-operator rolling update
2022-02-22 21:24:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-95840612 is in desired state: Ready
2022-02-22 21:24:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser strimzi-tls-user-1762124366 in namespace namespace-6
2022-02-22 21:24:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-5
2022-02-22 21:24:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: strimzi-tls-user-1762124366 will have desired state: Ready
2022-02-22 21:24:32 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: strimzi-tls-user-1762124366 is in desired state: Ready
2022-02-22 21:24:32 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1669] Change of kafka validity and renewal days - reconciliation should start.
2022-02-22 21:24:32 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-95840612-entity-operator rolling update
2022-02-22 21:24:42 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-95840612-entity-operator will be ready
2022-02-22 21:27:15 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-95840612-entity-operator is ready
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-95840612-entity-operator rolling update finished
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1691] Initial ClientsCA cert dates: Mon Feb 21 16:10:31 EST 2022 --> Wed Mar 23 17:10:31 EDT 2022
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1692] Changed ClientsCA cert dates: Mon Feb 21 16:10:31 EST 2022 --> Wed Mar 23 17:10:31 EDT 2022
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1693] Initial userCert dates: Tue Feb 22 16:24:32 EST 2022 --> Mon Mar 14 17:24:32 EDT 2022
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1694] Changed userCert dates: Tue Feb 22 16:24:45 EST 2022 --> Sat Sep 10 17:24:45 EDT 2022
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testCustomClientsCACertRenew
2022-02-22 21:27:25 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser strimzi-tls-user-1762124366 in namespace namespace-5
2022-02-22 21:27:30 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-522824700-entity-operator will be ready
2022-02-22 21:27:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-95840612 in namespace namespace-5
2022-02-22 21:27:45 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:27:45 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-5 for test case:testCustomClientsCACertRenew
2022-02-22 21:28:08 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-522824700-entity-operator is ready
2022-02-22 21:28:13 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCustomClientsCACertRenew-FINISHED
2022-02-22 21:28:13 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:28:13 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:28:13 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewClusterCaCertsTriggeredByAnno-STARTED
2022-02-22 21:28:16 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:28:16 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-7 for test case:testTlsHostnameVerificationWithMirrorMaker
2022-02-22 21:28:16 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-02-22 21:28:16 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-02-22 21:28:16 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-02-22 21:28:17 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-413910147-source in namespace namespace-7
2022-02-22 21:28:17 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-7
2022-02-22 21:28:17 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-413910147-source will have desired state: Ready
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-522824700-entity-operator rolling update finished
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1691] Initial ClientsCA cert dates: Tue Feb 22 16:13:37 EST 2022 --> Mon Mar 14 17:13:37 EDT 2022
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1692] Changed ClientsCA cert dates: Tue Feb 22 16:23:02 EST 2022 --> Sat Sep 10 17:23:02 EDT 2022
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1693] Initial userCert dates: Tue Feb 22 16:22:58 EST 2022 --> Mon Mar 14 17:22:58 EDT 2022
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1694] Changed userCert dates: Tue Feb 22 16:27:35 EST 2022 --> Sat Sep 10 17:27:35 EDT 2022
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testClientsCACertRenew
2022-02-22 21:28:18 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of KafkaUser strimzi-tls-user-727907852 in namespace namespace-6
2022-02-22 21:28:28 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-522824700 in namespace namespace-6
2022-02-22 21:28:38 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:28:38 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-6 for test case:testClientsCACertRenew
2022-02-22 21:29:05 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testClientsCACertRenew-FINISHED
2022-02-22 21:29:05 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:29:05 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:29:05 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCustomClusterCAClientsCA-STARTED
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-8 for test case:testAutoRenewClusterCaCertsTriggeredByAnno
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-996164972 in namespace namespace-8
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-8
2022-02-22 21:29:08 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-996164972 will have desired state: Ready
2022-02-22 21:29:32 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-413910147-source is in desired state: Ready
2022-02-22 21:29:32 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-413910147-target in namespace namespace-8
2022-02-22 21:29:32 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-7
2022-02-22 21:29:32 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-413910147-target will have desired state: Ready
2022-02-22 21:34:46 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-413910147-target is in desired state: Ready
2022-02-22 21:34:46 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:905] Getting IP of the source bootstrap service for consumer
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:908] Getting IP of the target bootstrap service for producer
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:911] KafkaMirrorMaker without config ssl.endpoint.identification.algorithm will not connect to consumer with address 10.100.208.188:9093
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:912] KafkaMirrorMaker without config ssl.endpoint.identification.algorithm will not connect to producer with address 10.104.37.183:9093
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaMirrorMaker my-cluster-413910147 in namespace namespace-8
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-7
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-02-22 21:34:47 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:245] Wait until Pod my-cluster-413910147-mirror-maker is present
2022-02-22 21:34:48 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:249] Pod my-cluster-413910147-mirror-maker is present
2022-02-22 21:34:48 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:236] Wait until Pod my-cluster-413910147-mirror-maker-6874fb699c-fkzlv is in CrashLoopBackOff state
2022-02-22 21:35:10 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:241] Pod my-cluster-413910147-mirror-maker-6874fb699c-fkzlv is in CrashLoopBackOff state
2022-02-22 21:35:10 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:947] KafkaMirrorMaker with config ssl.endpoint.identification.algorithm will connect to consumer with address 10.100.208.188:9093
2022-02-22 21:35:10 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:948] KafkaMirrorMaker with config ssl.endpoint.identification.algorithm will connect to producer with address 10.104.37.183:9093
2022-02-22 21:35:10 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:950] Adding configuration ssl.endpoint.identification.algorithm to the mirror maker...
2022-02-22 21:35:10 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaMirrorMaker: my-cluster-413910147 will have desired state: Ready
2022-02-22 21:36:45 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-996164972 is in desired state: Ready
2022-02-22 21:36:45 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-991723349-149151380 in namespace namespace-8
2022-02-22 21:36:45 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-8
2022-02-22 21:36:45 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-991723349-149151380 will have desired state: Ready
2022-02-22 21:36:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-991723349-149151380 is in desired state: Ready
2022-02-22 21:36:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1918818806-942373908 in namespace namespace-8
2022-02-22 21:36:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-8
2022-02-22 21:36:46 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1918818806-942373908 will have desired state: Ready
2022-02-22 21:36:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1918818806-942373908 is in desired state: Ready
2022-02-22 21:36:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-996164972-kafka-clients in namespace namespace-8
2022-02-22 21:36:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-8
2022-02-22 21:36:47 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996164972-kafka-clients will be ready
2022-02-22 21:36:49 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996164972-kafka-clients is ready
2022-02-22 21:36:49 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 21:36:49 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:274] Checking produced and consumed messages to pod:my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j
2022-02-22 21:36:49 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@21cb8f5a, messages=[], arguments=[--topic, my-topic-1918818806-942373908, --bootstrap-server, my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j', podNamespace='namespace-8', bootstrapServer='my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092', topicName='my-topic-1918818806-942373908', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b5681ec}
2022-02-22 21:36:49 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092:my-topic-1918818806-942373908 from pod my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j
2022-02-22 21:36:49 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j -n namespace-8 -- /opt/kafka/producer.sh --topic my-topic-1918818806-942373908 --bootstrap-server my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092 --max-messages 100
2022-02-22 21:36:53 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 21:36:53 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 21:36:53 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@75100e19, messages=[], arguments=[--group-instance-id, instance1268950539, --topic, my-topic-1918818806-942373908, --group-id, my-consumer-group-218522551, --bootstrap-server, my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j', podNamespace='namespace-8', bootstrapServer='my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092', topicName='my-topic-1918818806-942373908', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-218522551', consumerInstanceId='instance1268950539', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2a16b356}
2022-02-22 21:36:53 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092#my-topic-1918818806-942373908 from pod my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j
2022-02-22 21:36:53 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j -n namespace-8 -- /opt/kafka/consumer.sh --group-instance-id instance1268950539 --topic my-topic-1918818806-942373908 --group-id my-consumer-group-218522551 --bootstrap-server my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092 --max-messages 100
2022-02-22 21:37:00 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 21:37:00 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 21:37:00 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:288] Triggering CA cert renewal by adding the annotation
2022-02-22 21:37:00 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:300] Patching secret my-cluster-996164972-cluster-ca-cert with strimzi.io/force-renew
2022-02-22 21:37:00 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:305] Wait for zk to rolling restart ...
2022-02-22 21:37:00 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-996164972-zookeeper rolling update
2022-02-22 21:38:40 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-996164972-zookeeper has been successfully rolled
2022-02-22 21:38:40 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-996164972-zookeeper to be ready
2022-02-22 21:39:12 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:309] Wait for kafka to rolling restart ...
2022-02-22 21:39:12 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-996164972-kafka rolling update
2022-02-22 21:40:12 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-996164972-kafka has been successfully rolled
2022-02-22 21:40:12 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-996164972-kafka to be ready
2022-02-22 21:40:40 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:313] Wait for EO to rolling restart ...
2022-02-22 21:40:40 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996164972-entity-operator rolling update
2022-02-22 21:40:40 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996164972-entity-operator will be ready
2022-02-22 21:40:58 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaMirrorMaker: my-cluster-413910147 is in desired state: Ready
2022-02-22 21:40:58 [ForkJoinPool-1-worker-5] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-413910147-mirror-maker is not deleted yet! Triggering force delete by cmd client!
2022-02-22 21:41:03 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:41:03 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:309] Delete all resources for testTlsHostnameVerificationWithMirrorMaker
2022-02-22 21:41:03 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-413910147-target in namespace namespace-7
2022-02-22 21:41:13 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaMirrorMaker my-cluster-413910147 in namespace namespace-7
2022-02-22 21:41:13 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-413910147-source in namespace namespace-7
2022-02-22 21:41:23 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:41:23 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-7 for test case:testTlsHostnameVerificationWithMirrorMaker
2022-02-22 21:41:34 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithMirrorMaker-FINISHED
2022-02-22 21:41:34 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:41:34 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:41:34 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCaRenewalBreakInMiddle-STARTED
2022-02-22 21:41:35 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:41:35 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-9 for test case:testCustomClusterCAClientsCA
2022-02-22 21:41:35 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-9
2022-02-22 21:41:35 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-9
2022-02-22 21:41:35 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-02-22 21:41:35 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1828] Generating custom RootCA, IntermediateCA, and ClusterCA, ClientsCA for Strimzi and PEM bundles.
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1844] Deploy all certificates and keys as secrets.
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-201959760-cluster-ca-cert to be deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-201959760-cluster-ca-cert successfully deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-201959760-cluster-ca-cert
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:50] Secret my-cluster-201959760-cluster-ca-cert created
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-201959760-cluster-ca to be deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-201959760-cluster-ca successfully deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-201959760-clients-ca-cert to be deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-201959760-clients-ca-cert successfully deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-201959760-clients-ca-cert
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:50] Secret my-cluster-201959760-clients-ca-cert created
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:150] Waiting for Secret: my-cluster-201959760-clients-ca to be deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecretUtils:154] Secret: my-cluster-201959760-clients-ca successfully deleted
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1805] Check ClusterCA and ClientsCA certificates.
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1720]  Deploy kafka with new certs/secrets.
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-201959760 in namespace namespace-9
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-9
2022-02-22 21:41:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-201959760 will have desired state: Ready
2022-02-22 21:42:10 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996164972-entity-operator is ready
2022-02-22 21:42:20 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996164972-entity-operator rolling update finished
2022-02-22 21:42:20 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:317] Wait for CC and KE to rolling restart ...
2022-02-22 21:42:20 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996164972-kafka-exporter rolling update
2022-02-22 21:43:20 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996164972-kafka-exporter will be ready
2022-02-22 21:43:20 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996164972-kafka-exporter is ready
2022-02-22 21:43:30 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996164972-kafka-exporter rolling update finished
2022-02-22 21:43:30 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996164972-cruise-control rolling update
2022-02-22 21:43:30 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996164972-cruise-control will be ready
2022-02-22 21:43:30 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996164972-cruise-control is ready
2022-02-22 21:43:41 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996164972-cruise-control rolling update finished
2022-02-22 21:43:41 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:322] Checking the certificates have been replaced
2022-02-22 21:43:41 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:336] Checking consumed messages to pod:my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j
2022-02-22 21:43:41 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@51120489, messages=[], arguments=[--group-instance-id, instance961004254, --topic, my-topic-1918818806-942373908, --group-id, my-consumer-group-1626893028, --bootstrap-server, my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j', podNamespace='namespace-8', bootstrapServer='my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092', topicName='my-topic-1918818806-942373908', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1626893028', consumerInstanceId='instance961004254', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5305eda9}
2022-02-22 21:43:41 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092#my-topic-1918818806-942373908 from pod my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j
2022-02-22 21:43:41 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996164972-kafka-clients-7d8d85764d-tzf7j -n namespace-8 -- /opt/kafka/consumer.sh --group-instance-id instance961004254 --topic my-topic-1918818806-942373908 --group-id my-consumer-group-1626893028 --bootstrap-server my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9092 --max-messages 100
2022-02-22 21:43:47 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 21:43:47 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 21:43:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser bob-my-cluster-996164972 in namespace namespace-9
2022-02-22 21:43:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-8
2022-02-22 21:43:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: bob-my-cluster-996164972 will have desired state: Ready
2022-02-22 21:43:48 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: bob-my-cluster-996164972 is in desired state: Ready
2022-02-22 21:43:48 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-996164972-kafka-clients-tls in namespace namespace-8
2022-02-22 21:43:48 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-8
2022-02-22 21:43:48 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996164972-kafka-clients-tls will be ready
2022-02-22 21:43:50 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996164972-kafka-clients-tls is ready
2022-02-22 21:43:50 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:360] Checking consumed messages to pod:my-cluster-996164972-kafka-clients-tls-77f7c8bd8c-lqnfx
2022-02-22 21:43:50 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@57a80630, messages=[], arguments=[--group-instance-id, instance1673971549, --topic, my-topic-1918818806-942373908, --group-id, my-consumer-group-630764268, --bootstrap-server, my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9093, --max-messages, 100, USER=bob_my_cluster_996164972], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-996164972-kafka-clients-tls-77f7c8bd8c-lqnfx', podNamespace='namespace-8', bootstrapServer='my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9093', topicName='my-topic-1918818806-942373908', maxMessages=100, kafkaUsername='bob-my-cluster-996164972', consumerGroupName='my-consumer-group-630764268', consumerInstanceId='instance1673971549', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11a384c}
2022-02-22 21:43:50 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9093#my-topic-1918818806-942373908 from pod my-cluster-996164972-kafka-clients-tls-77f7c8bd8c-lqnfx
2022-02-22 21:43:50 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996164972-kafka-clients-tls-77f7c8bd8c-lqnfx -n namespace-8 -- /opt/kafka/consumer.sh --group-instance-id instance1673971549 --topic my-topic-1918818806-942373908 --group-id my-consumer-group-630764268 --bootstrap-server my-cluster-996164972-kafka-bootstrap.namespace-8.svc:9093 --max-messages 100 USER=bob_my_cluster_996164972
2022-02-22 21:43:59 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 21:43:59 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 21:43:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:43:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoRenewClusterCaCertsTriggeredByAnno
2022-02-22 21:43:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-996164972-kafka-clients in namespace namespace-8
2022-02-22 21:44:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-201959760 is in desired state: Ready
2022-02-22 21:44:58 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1748] Check Kafka(s) and Zookeeper(s) certificates.
2022-02-22 21:44:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1530449081-202736110 in namespace namespace-9
2022-02-22 21:44:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-9
2022-02-22 21:44:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1530449081-202736110 will have desired state: Ready
2022-02-22 21:44:59 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1530449081-202736110 is in desired state: Ready
2022-02-22 21:44:59 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1759] Check KafkaUser certificate.
2022-02-22 21:44:59 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-107323692-930112573 in namespace namespace-9
2022-02-22 21:44:59 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-9
2022-02-22 21:44:59 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-107323692-930112573 will have desired state: Ready
2022-02-22 21:45:00 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-107323692-930112573 is in desired state: Ready
2022-02-22 21:45:00 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1766] Send and receive messages over TLS.
2022-02-22 21:45:00 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-201959760-kafka-clients in namespace namespace-9
2022-02-22 21:45:00 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-9
2022-02-22 21:45:00 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-201959760-kafka-clients will be ready
2022-02-22 21:45:02 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-201959760-kafka-clients is ready
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1780] Check for certificates used within kafka pod internal clients (producer/consumer)
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-9 exec my-cluster-201959760-kafka-clients-767566b7bb-xnwrf -- /bin/bash -c openssl x509 -in /opt/kafka/user-secret-my-user-107323692-930112573/ca.crt -noout -nameopt RFC2253 -issuer
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-9 exec my-cluster-201959760-kafka-clients-767566b7bb-xnwrf -- /bin/bash -c openssl x509 -in /opt/kafka/user-secret-my-user-107323692-930112573/ca.crt -noout -nameopt RFC2253 -subject
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-9 exec my-cluster-201959760-kafka-clients-767566b7bb-xnwrf -- /bin/bash -c openssl x509 -in /opt/kafka/cluster-ca-my-user-107323692-930112573/ca.crt -noout -nameopt RFC2253 -issuer
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-9 exec my-cluster-201959760-kafka-clients-767566b7bb-xnwrf -- /bin/bash -c openssl x509 -in /opt/kafka/cluster-ca-my-user-107323692-930112573/ca.crt -noout -nameopt RFC2253 -subject
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1797] Checking produced and consumed messages via TLS to pod:my-cluster-201959760-kafka-clients-767566b7bb-xnwrf
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2e947273, messages=[], arguments=[--topic, my-topic-1530449081-202736110, --bootstrap-server, my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093, --max-messages, 100, USER=my_user_107323692_930112573], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-201959760-kafka-clients-767566b7bb-xnwrf', podNamespace='namespace-9', bootstrapServer='my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093', topicName='my-topic-1530449081-202736110', maxMessages=100, kafkaUsername='my-user-107323692-930112573', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@498d3292}
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093:my-topic-1530449081-202736110 from pod my-cluster-201959760-kafka-clients-767566b7bb-xnwrf
2022-02-22 21:45:03 [ForkJoinPool-1-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-201959760-kafka-clients-767566b7bb-xnwrf -n namespace-9 -- /opt/kafka/producer.sh --topic my-topic-1530449081-202736110 --bootstrap-server my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093 --max-messages 100 USER=my_user_107323692_930112573
2022-02-22 21:45:08 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-02-22 21:45:08 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-02-22 21:45:08 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4068fa3b, messages=[], arguments=[--group-instance-id, instance1214155373, --topic, my-topic-1530449081-202736110, --group-id, my-consumer-group-1548334312, --bootstrap-server, my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093, --max-messages, 100, USER=my_user_107323692_930112573], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-201959760-kafka-clients-767566b7bb-xnwrf', podNamespace='namespace-9', bootstrapServer='my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093', topicName='my-topic-1530449081-202736110', maxMessages=100, kafkaUsername='my-user-107323692-930112573', consumerGroupName='my-consumer-group-1548334312', consumerInstanceId='instance1214155373', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@339eb106}
2022-02-22 21:45:08 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093:my-topic-1530449081-202736110 from pod my-cluster-201959760-kafka-clients-767566b7bb-xnwrf
2022-02-22 21:45:08 [ForkJoinPool-1-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-201959760-kafka-clients-767566b7bb-xnwrf -n namespace-9 -- /opt/kafka/consumer.sh --group-instance-id instance1214155373 --topic my-topic-1530449081-202736110 --group-id my-consumer-group-1548334312 --bootstrap-server my-cluster-201959760-kafka-bootstrap.namespace-9.svc:9093 --max-messages 100 USER=my_user_107323692_930112573
2022-02-22 21:45:17 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 21:45:17 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 21:45:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 21:45:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testCustomClusterCAClientsCA
2022-02-22 21:45:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-107323692-930112573 in namespace namespace-9
2022-02-22 21:45:27 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-201959760-kafka-clients in namespace namespace-9
2022-02-22 21:45:39 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-996164972-kafka-clients-tls in namespace namespace-8
2022-02-22 21:45:39 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser bob-my-cluster-996164972 in namespace namespace-8
2022-02-22 21:45:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-991723349-149151380 in namespace namespace-8
2022-02-22 21:45:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1918818806-942373908 in namespace namespace-8
2022-02-22 21:46:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-996164972 in namespace namespace-8
2022-02-22 21:46:09 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-8, for cruise control Kafka cluster my-cluster-996164972
2022-02-22 21:46:19 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:46:19 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-8 for test case:testAutoRenewClusterCaCertsTriggeredByAnno
2022-02-22 21:46:27 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1530449081-202736110 in namespace namespace-9
2022-02-22 21:46:37 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-201959760 in namespace namespace-9
2022-02-22 21:46:47 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 21:46:47 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-9 for test case:testCustomClusterCAClientsCA
2022-02-22 21:47:03 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewClusterCaCertsTriggeredByAnno-FINISHED
2022-02-22 21:47:03 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:47:03 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:47:03 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-10 for test case:testCaRenewalBreakInMiddle
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-10
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-10
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-10
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1515898600 in namespace namespace-10
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-10
2022-02-22 21:47:04 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1515898600 will have desired state: Ready
2022-02-22 21:47:31 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCustomClusterCAClientsCA-FINISHED
2022-02-22 21:47:31 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 21:47:31 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 21:47:31 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithKafkaConnect-STARTED
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-11 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-11
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-11
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-11
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1899155392 in namespace namespace-11
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-11
2022-02-22 21:47:33 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1899155392 will have desired state: Ready
2022-02-22 21:49:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1899155392 is in desired state: Ready
2022-02-22 21:49:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-439330681-417500319 in namespace namespace-11
2022-02-22 21:49:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-11
2022-02-22 21:49:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-439330681-417500319 will have desired state: Ready
2022-02-22 21:49:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-439330681-417500319 is in desired state: Ready
2022-02-22 21:49:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1375846644-1137378235 in namespace namespace-11
2022-02-22 21:49:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-11
2022-02-22 21:49:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1375846644-1137378235 will have desired state: Ready
2022-02-22 21:49:38 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1375846644-1137378235 is in desired state: Ready
2022-02-22 21:49:38 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1899155392-kafka-clients in namespace namespace-11
2022-02-22 21:49:38 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-11
2022-02-22 21:49:38 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1899155392-kafka-clients will be ready
2022-02-22 21:49:40 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1899155392-kafka-clients is ready
2022-02-22 21:49:40 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 21:49:40 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:274] Checking produced and consumed messages to pod:my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt
2022-02-22 21:49:40 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6900daf6, messages=[], arguments=[--topic, my-topic-1375846644-1137378235, --bootstrap-server, my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt', podNamespace='namespace-11', bootstrapServer='my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092', topicName='my-topic-1375846644-1137378235', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@12a0f814}
2022-02-22 21:49:40 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092:my-topic-1375846644-1137378235 from pod my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt
2022-02-22 21:49:40 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt -n namespace-11 -- /opt/kafka/producer.sh --topic my-topic-1375846644-1137378235 --bootstrap-server my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092 --max-messages 100
2022-02-22 21:49:43 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 21:49:43 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 21:49:43 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7fe7e37, messages=[], arguments=[--group-instance-id, instance1856218629, --topic, my-topic-1375846644-1137378235, --group-id, my-consumer-group-669558206, --bootstrap-server, my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt', podNamespace='namespace-11', bootstrapServer='my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092', topicName='my-topic-1375846644-1137378235', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-669558206', consumerInstanceId='instance1856218629', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@120b543a}
2022-02-22 21:49:43 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092#my-topic-1375846644-1137378235 from pod my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt
2022-02-22 21:49:43 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt -n namespace-11 -- /opt/kafka/consumer.sh --group-instance-id instance1856218629 --topic my-topic-1375846644-1137378235 --group-id my-consumer-group-669558206 --bootstrap-server my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092 --max-messages 100
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:288] Triggering CA cert renewal by adding the annotation
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:300] Patching secret my-cluster-1899155392-cluster-ca-cert with strimzi.io/force-renew
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:300] Patching secret my-cluster-1899155392-clients-ca-cert with strimzi.io/force-renew
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:305] Wait for zk to rolling restart ...
2022-02-22 21:49:49 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1899155392-zookeeper rolling update
2022-02-22 21:50:49 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1899155392-zookeeper has been successfully rolled
2022-02-22 21:50:49 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1899155392-zookeeper to be ready
2022-02-22 21:51:21 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:309] Wait for kafka to rolling restart ...
2022-02-22 21:51:21 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1899155392-kafka rolling update
2022-02-22 21:51:57 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1515898600 is in desired state: Ready
2022-02-22 21:51:57 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-841108270-1695381205 in namespace namespace-11
2022-02-22 21:51:57 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-10
2022-02-22 21:51:57 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-841108270-1695381205 will have desired state: Ready
2022-02-22 21:51:58 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-841108270-1695381205 is in desired state: Ready
2022-02-22 21:51:58 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1004066807-1569203280 in namespace namespace-11
2022-02-22 21:51:58 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-10
2022-02-22 21:51:58 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1004066807-1569203280 will have desired state: Ready
2022-02-22 21:51:59 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1004066807-1569203280 is in desired state: Ready
2022-02-22 21:51:59 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1515898600-kafka-clients in namespace namespace-11
2022-02-22 21:51:59 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-10
2022-02-22 21:51:59 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1515898600-kafka-clients will be ready
2022-02-22 21:52:02 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1515898600-kafka-clients is ready
2022-02-22 21:52:02 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 21:52:02 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2ca3123d, messages=[], arguments=[--topic, my-topic-1004066807-1569203280, --bootstrap-server, my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093, --max-messages, 100, USER=my_user_841108270_1695381205], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt', podNamespace='namespace-10', bootstrapServer='my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093', topicName='my-topic-1004066807-1569203280', maxMessages=100, kafkaUsername='my-user-841108270-1695381205', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e39e5d5}
2022-02-22 21:52:02 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093:my-topic-1004066807-1569203280 from pod my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 21:52:02 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt -n namespace-10 -- /opt/kafka/producer.sh --topic my-topic-1004066807-1569203280 --bootstrap-server my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093 --max-messages 100 USER=my_user_841108270_1695381205
2022-02-22 21:52:07 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-02-22 21:52:07 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-02-22 21:52:07 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@434ca150, messages=[], arguments=[--group-instance-id, instance1703613286, --topic, my-topic-1004066807-1569203280, --group-id, my-consumer-group-743799728, --bootstrap-server, my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093, --max-messages, 100, USER=my_user_841108270_1695381205], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt', podNamespace='namespace-10', bootstrapServer='my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093', topicName='my-topic-1004066807-1569203280', maxMessages=100, kafkaUsername='my-user-841108270-1695381205', consumerGroupName='my-consumer-group-743799728', consumerInstanceId='instance1703613286', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52caf191}
2022-02-22 21:52:07 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093:my-topic-1004066807-1569203280 from pod my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 21:52:07 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt -n namespace-10 -- /opt/kafka/consumer.sh --group-instance-id instance1703613286 --topic my-topic-1004066807-1569203280 --group-id my-consumer-group-743799728 --bootstrap-server my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093 --max-messages 100 USER=my_user_841108270_1695381205
2022-02-22 21:52:15 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 21:52:15 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 21:52:15 [ForkJoinPool-1-worker-5] [32mINFO [m [SecretUtils:70] Creating secret my-cluster-1515898600-cluster-ca-cert
2022-02-22 21:52:15 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:16 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:17 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:18 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:19 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:20 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:21 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:22 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:23 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:24 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:25 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:26 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:27 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:28 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:29 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1238] No pods of my-cluster-1515898600-zookeeper are in desired state
2022-02-22 21:52:30 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1241] Pod in 'Pending' state: my-cluster-1515898600-zookeeper-0
2022-02-22 21:52:30 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5d631d7a, messages=[], arguments=[--group-instance-id, instance197145072, --topic, my-topic-1004066807-1569203280, --group-id, my-consumer-group-416789958, --bootstrap-server, my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093, --max-messages, 100, USER=my_user_841108270_1695381205], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt', podNamespace='namespace-10', bootstrapServer='my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093', topicName='my-topic-1004066807-1569203280', maxMessages=100, kafkaUsername='my-user-841108270-1695381205', consumerGroupName='my-consumer-group-416789958', consumerInstanceId='instance197145072', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3a7ec4ac}
2022-02-22 21:52:30 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093:my-topic-1004066807-1569203280 from pod my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 21:52:30 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt -n namespace-10 -- /opt/kafka/consumer.sh --group-instance-id instance197145072 --topic my-topic-1004066807-1569203280 --group-id my-consumer-group-416789958 --bootstrap-server my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093 --max-messages 100 USER=my_user_841108270_1695381205
2022-02-22 21:52:38 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 21:52:38 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 21:52:38 [ForkJoinPool-1-worker-5] [32mINFO [m [SecretUtils:131] Waiting for Secret my-cluster-1515898600-cluster-ca-cert certificate change
2022-02-22 21:52:38 [ForkJoinPool-1-worker-5] [32mINFO [m [SecretUtils:138] Certificate in Secret my-cluster-1515898600-cluster-ca-cert has changed, was -----BEGIN CERTIFICATE-----
MIIDKjCCAhKgAwIBAgIJAJBt0u1FFTG+MA0GCSqGSIb3DQEBCwUAMCoxEzARBgNV
BAoMCnN0cmltemkuaW8xEzARBgNVBAMMCmNsdXN0ZXItY2EwHhcNMTgxMDIzMTAw
MTExWhcNMTgxMDI0MTAwMTExWjAqMRMwEQYDVQQKDApzdHJpbXppLmlvMRMwEQYD
VQQDDApjbHVzdGVyLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
xpuYrNXYHqw3ajwd12aAeuTlAX4rVwVdPuIex6A4NL8J3d2DV+ngXgNTH//RhiF5
If5KRWSsLei5BUIrwuQutOUNCQwyACmri9+yrx6+tevligiokAUwhHxcDHZpwC3T
+2dzk/BkI++vbSuvjFmBKGQi9gfyoTnStTEQ85KVJUS170hzwDjzaEiJsKpOPx/G
+KTdkAopLucoxr4sxhYeO4mQ2PkT0QL+R8Ohs6LD6v/bqalFP+rS8vibolfxjMNm
lXQCOd8UfXy8OEOaNoNCvhnn/cT/hbEG/ARbV3hHmUh9COV+TSV5dhsbmS5h4MKw
LzP449nGQBmLSkZMu984DQIDAQABo1MwUTAdBgNVHQ4EFgQUOXubcHBZJ7vSzjpi
pfXdFSP3dsEwHwYDVR0jBBgwFoAUOXubcHBZJ7vSzjpipfXdFSP3dsEwDwYDVR0T
AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAFFPGykbzUREDMzh+33i3a8TF
UTQnPMN/SuVbpLfQdpkpLO+aVWjVvJP6qMrI7jRO5zhj4MecAf7YKpe+dyRTTz9B
Dy9BZcujHYjCKdcKBnBFQ7B1xQm9tL1bw+a3ABzSTlhLiBcCxhJEawlWy1Gh18ab
3x/Kqnz0mk/jt5+n9HwlKHuBQVIxRCsfHWwq+WvIfoxM+N//akV8/29hLUf5TlYH
F5CX4G2pA5sSdaDHQ4ekQWuqM6tfvsGLl2KOmEFEgVR4GfaWI4BsUCzlpBNcCGWv
V7klZvBxQPG7MVy0cB8yzTDtUpR0jUFUkZOSp5Pr3yWcwPWEgWkaXYfO+FWETA==
-----END CERTIFICATE-----
, is now -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUJA1G7b6Fzzwyj/9aLO5ZjEmSlZMwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjAyMjIyMTUyMTVaFw0yMjAzMDEyMTUyMTVaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQDSTLIukufiA3HBx+GoR+N0OSTt4mBUA0NaxN4dSyFR
y8BHCQiWcDU7VGn15tSyKcH5oDZknwQwFvgJqvtWQ5DI2v+Ro6UsGPrWGvi8JMgu
Zrx7eDbpnomcvKGUsFXJr8uvWHhK1hn7hg715yZfGDYwGX9kuQgNpbwd/KAoTzv4
gOOHXYycHF7M/hLOFnZFvz/PpaIf8mDWZU5IeAjGI+pla07lhEbqO0nn6xW8GVPR
K2I/LE2dUaex0NsHUjLcBrHgQqs/oZVXKqe5Ig1t2TJQ3f006+Vs8ZRQqhQCUNSH
szzsZICBNSwq2s/J+FgJRG7o21d8w5tpcyQ4dmXYOGQBqsjhCNXy2wmEoGpbywqo
m+4PDN3rqhMNZbUFPwudxVId56PwqtfvVgw0diIpNgj06FjjWdrTu7Z46zcxBG3o
BhmsBU1Wsv3qDgD3ISro+B/JQogSB+Egn/+Wr5M8YqcoCSV9BxKh1N+lkEftzG4V
Sq3hDPMMD5SpAqCk6rMYmLkHhniI8IQswO6n0XybeLr0ZdGQOuHX03A6huS3vogI
7lTg46kzJm/Fx7FwUEgYsp4Jrw5PXMJ3hQMZEjKH5GmkDJlu4/wH0AFHKs4Bpohi
ixG+dyZzfxTpArGg9IfPc1D3biBgHJv8x/28Df9l7/BLYuPB2CZRjqg05yGtmuVI
KQIDAQABo0UwQzAdBgNVHQ4EFgQUEJcTt7nJpfQYkkcMpPHcBppoCzswEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
ADI6H1szqBvSgt1R8fv59RRvoo/zSkQrQx362DL1C0qUF4TubOolenYbLzbG+kET
e0z77tRvBLNCmt3kHFAXF+RFZBavmWffp+Ov37cBJwy/JxG3qWyE60VkR+WqZ42A
XZ9YQZNzBSejgjSF0Sn0ba+5bhk2SAe4hi3MHeXzTFH2Bmob2gT2F77UDcot31MR
7u+g48Xkuy1s5dUSbW83IWu6kFSz0klcOus5Kc/wTRp+EY2VanR47cMXBrKH9UOf
nIPlA1JelL1ugOFWBTm96sJMByQsy6X+4fCeoWGf9nW4ofpRiudFh2SxfkcqTWoF
o4hn/NR1oP/PAd6ZaGMytT8Wlm5sUHYGZgtNQVE8LMshG/Uu7aRrXJXAuoqdqmkR
4o+oaO03LuCBhcDYG1GnQ2x/XChREKvJC6bFIUZEnijrppwATsnR6vhGcvCSzDaT
Ht3XTcNPP/OUUXOeDRVYvFSq5t4V7eCoKNi61rKHlISC0VG+SfnE9EH4LA3DpjkE
DMVq6MU8Cx+uVUV/CmsmqpmxOXVhgLXnXzfohgxAxI0jtudbHWnJROv6oQHNOBTI
tYBfedEbB+6neS8vPS8HKi/y93cFG9NfwivJRK8hv2A4wJ9JMWURxlWZWjMhRF8e
MsawLzmcazDw/jY4cLmPOXhLqmcL+1zESQaB9SrVuKnj
-----END CERTIFICATE-----

2022-02-22 21:52:38 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1515898600-zookeeper rolling update
2022-02-22 21:54:22 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1899155392-kafka has been successfully rolled
2022-02-22 21:54:22 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1899155392-kafka to be ready
2022-02-22 21:55:11 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:313] Wait for EO to rolling restart ...
2022-02-22 21:55:11 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1899155392-entity-operator rolling update
2022-02-22 21:55:11 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1899155392-entity-operator will be ready
2022-02-22 21:59:04 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1515898600-zookeeper has been successfully rolled
2022-02-22 21:59:04 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1515898600-zookeeper to be ready
2022-02-22 21:59:07 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1899155392-entity-operator is ready
2022-02-22 21:59:17 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1899155392-entity-operator rolling update finished
2022-02-22 21:59:17 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:317] Wait for CC and KE to rolling restart ...
2022-02-22 21:59:17 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1899155392-kafka-exporter rolling update
2022-02-22 21:59:35 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1515898600-kafka rolling update
2022-02-22 21:59:52 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1899155392-kafka-exporter will be ready
2022-02-22 21:59:52 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1899155392-kafka-exporter is ready
2022-02-22 22:00:02 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1899155392-kafka-exporter rolling update finished
2022-02-22 22:00:02 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1899155392-cruise-control rolling update
2022-02-22 22:00:02 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1899155392-cruise-control will be ready
2022-02-22 22:00:02 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1899155392-cruise-control is ready
2022-02-22 22:00:12 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1899155392-cruise-control rolling update finished
2022-02-22 22:00:12 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:322] Checking the certificates have been replaced
2022-02-22 22:00:12 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:336] Checking consumed messages to pod:my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt
2022-02-22 22:00:12 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4c46bd2c, messages=[], arguments=[--group-instance-id, instance1580900899, --topic, my-topic-1375846644-1137378235, --group-id, my-consumer-group-20217561, --bootstrap-server, my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt', podNamespace='namespace-11', bootstrapServer='my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092', topicName='my-topic-1375846644-1137378235', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-20217561', consumerInstanceId='instance1580900899', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@25c3cbf}
2022-02-22 22:00:12 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092#my-topic-1375846644-1137378235 from pod my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt
2022-02-22 22:00:12 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1899155392-kafka-clients-7b4b666bbd-ldpqt -n namespace-11 -- /opt/kafka/consumer.sh --group-instance-id instance1580900899 --topic my-topic-1375846644-1137378235 --group-id my-consumer-group-20217561 --bootstrap-server my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9092 --max-messages 100
2022-02-22 22:00:19 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 22:00:19 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 22:00:19 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser bob-my-cluster-1899155392 in namespace namespace-11
2022-02-22 22:00:19 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-11
2022-02-22 22:00:19 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: bob-my-cluster-1899155392 will have desired state: Ready
2022-02-22 22:00:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: bob-my-cluster-1899155392 is in desired state: Ready
2022-02-22 22:00:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1899155392-kafka-clients-tls in namespace namespace-11
2022-02-22 22:00:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-11
2022-02-22 22:00:20 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1899155392-kafka-clients-tls will be ready
2022-02-22 22:00:22 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1899155392-kafka-clients-tls is ready
2022-02-22 22:00:22 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:360] Checking consumed messages to pod:my-cluster-1899155392-kafka-clients-tls-7bdbbdbd55-bct85
2022-02-22 22:00:22 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7cefa4e2, messages=[], arguments=[--group-instance-id, instance772878985, --topic, my-topic-1375846644-1137378235, --group-id, my-consumer-group-1800708743, --bootstrap-server, my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9093, --max-messages, 100, USER=bob_my_cluster_1899155392], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1899155392-kafka-clients-tls-7bdbbdbd55-bct85', podNamespace='namespace-11', bootstrapServer='my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9093', topicName='my-topic-1375846644-1137378235', maxMessages=100, kafkaUsername='bob-my-cluster-1899155392', consumerGroupName='my-consumer-group-1800708743', consumerInstanceId='instance772878985', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f35da7f}
2022-02-22 22:00:22 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9093#my-topic-1375846644-1137378235 from pod my-cluster-1899155392-kafka-clients-tls-7bdbbdbd55-bct85
2022-02-22 22:00:22 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1899155392-kafka-clients-tls-7bdbbdbd55-bct85 -n namespace-11 -- /opt/kafka/consumer.sh --group-instance-id instance772878985 --topic my-topic-1375846644-1137378235 --group-id my-consumer-group-1800708743 --bootstrap-server my-cluster-1899155392-kafka-bootstrap.namespace-11.svc:9093 --max-messages 100 USER=bob_my_cluster_1899155392
2022-02-22 22:00:30 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 22:00:30 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 22:00:30 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:00:30 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-02-22 22:00:30 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1899155392-kafka-clients in namespace namespace-11
2022-02-22 22:00:40 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1515898600-kafka has been successfully rolled
2022-02-22 22:00:40 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1515898600-kafka to be ready
2022-02-22 22:01:09 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1515898600-entity-operator rolling update
2022-02-22 22:01:14 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1515898600-entity-operator will be ready
2022-02-22 22:01:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1899155392-kafka-clients-tls in namespace namespace-11
2022-02-22 22:01:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser bob-my-cluster-1899155392 in namespace namespace-11
2022-02-22 22:02:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-439330681-417500319 in namespace namespace-11
2022-02-22 22:02:10 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1375846644-1137378235 in namespace namespace-11
2022-02-22 22:02:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1899155392 in namespace namespace-11
2022-02-22 22:02:20 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-11, for cruise control Kafka cluster my-cluster-1899155392
2022-02-22 22:02:30 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:02:30 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-11 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-02-22 22:02:42 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1515898600-entity-operator is ready
2022-02-22 22:02:52 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1515898600-entity-operator rolling update finished
2022-02-22 22:02:52 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1272] Checking produced and consumed messages to pod:my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 22:02:52 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@270dde86, messages=[], arguments=[--group-instance-id, instance696054337, --topic, my-topic-1004066807-1569203280, --group-id, my-consumer-group-1949062646, --bootstrap-server, my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093, --max-messages, 100, USER=my_user_841108270_1695381205], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt', podNamespace='namespace-10', bootstrapServer='my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093', topicName='my-topic-1004066807-1569203280', maxMessages=100, kafkaUsername='my-user-841108270-1695381205', consumerGroupName='my-consumer-group-1949062646', consumerInstanceId='instance696054337', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e6db535}
2022-02-22 22:02:52 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093:my-topic-1004066807-1569203280 from pod my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 22:02:52 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt -n namespace-10 -- /opt/kafka/consumer.sh --group-instance-id instance696054337 --topic my-topic-1004066807-1569203280 --group-id my-consumer-group-1949062646 --bootstrap-server my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093 --max-messages 100 USER=my_user_841108270_1695381205
2022-02-22 22:03:00 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 22:03:00 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 22:03:00 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1490917738-263261362 in namespace namespace-11
2022-02-22 22:03:00 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-10
2022-02-22 22:03:00 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1490917738-263261362 will have desired state: Ready
2022-02-22 22:03:14 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-02-22 22:03:14 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:03:14 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:03:14 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertRegeneratedAfterInternalCAisDeleted-STARTED
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-12 for test case:testTlsHostnameVerificationWithKafkaConnect
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-12
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-12
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-12
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1379373361 in namespace namespace-12
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-12
2022-02-22 22:03:16 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1379373361 will have desired state: Ready
2022-02-22 22:04:31 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1490917738-263261362 is in desired state: Ready
2022-02-22 22:04:31 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@71a0318d, messages=[], arguments=[--topic, my-topic-1490917738-263261362, --bootstrap-server, my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093, --max-messages, 100, USER=my_user_841108270_1695381205], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt', podNamespace='namespace-10', bootstrapServer='my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093', topicName='my-topic-1490917738-263261362', maxMessages=100, kafkaUsername='my-user-841108270-1695381205', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a100478}
2022-02-22 22:04:31 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093:my-topic-1490917738-263261362 from pod my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 22:04:31 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt -n namespace-10 -- /opt/kafka/producer.sh --topic my-topic-1490917738-263261362 --bootstrap-server my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093 --max-messages 100 USER=my_user_841108270_1695381205
2022-02-22 22:04:35 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-02-22 22:04:35 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-02-22 22:04:35 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7d7f94d8, messages=[], arguments=[--group-instance-id, instance1590002735, --topic, my-topic-1490917738-263261362, --group-id, my-consumer-group-1106945734, --bootstrap-server, my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093, --max-messages, 100, USER=my_user_841108270_1695381205], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt', podNamespace='namespace-10', bootstrapServer='my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093', topicName='my-topic-1490917738-263261362', maxMessages=100, kafkaUsername='my-user-841108270-1695381205', consumerGroupName='my-consumer-group-1106945734', consumerInstanceId='instance1590002735', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7367382}
2022-02-22 22:04:35 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093:my-topic-1490917738-263261362 from pod my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt
2022-02-22 22:04:35 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1515898600-kafka-clients-7c8c88dc75-249wt -n namespace-10 -- /opt/kafka/consumer.sh --group-instance-id instance1590002735 --topic my-topic-1490917738-263261362 --group-id my-consumer-group-1106945734 --bootstrap-server my-cluster-1515898600-kafka-bootstrap.namespace-10.svc:9093 --max-messages 100 USER=my_user_841108270_1695381205
2022-02-22 22:04:44 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 22:04:44 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 22:04:44 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:04:44 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:309] Delete all resources for testCaRenewalBreakInMiddle
2022-02-22 22:04:44 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1004066807-1569203280 in namespace namespace-10
2022-02-22 22:04:54 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1490917738-263261362 in namespace namespace-10
2022-02-22 22:05:01 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1379373361 is in desired state: Ready
2022-02-22 22:05:01 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:852] Getting IP of the bootstrap service
2022-02-22 22:05:01 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:856] KafkaConnect without config ssl.endpoint.identification.algorithm will not connect to 10.111.25.20:9093
2022-02-22 22:05:01 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1379373361-kafka-clients in namespace namespace-12
2022-02-22 22:05:01 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-12
2022-02-22 22:05:01 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1379373361-kafka-clients will be ready
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1379373361-kafka-clients is ready
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1379373361-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update NetworkPolicy my-cluster-1379373361-allow in namespace namespace-12
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-12
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaConnect my-cluster-1379373361 in namespace namespace-12
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-12
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-02-22 22:05:02 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:245] Wait until Pod my-cluster-1379373361-connect is present
2022-02-22 22:05:03 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:249] Pod my-cluster-1379373361-connect is present
2022-02-22 22:05:03 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:236] Wait until Pod my-cluster-1379373361-connect-6c67b78fc8-9m9pg is in CrashLoopBackOff state
2022-02-22 22:05:04 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1515898600-kafka-clients in namespace namespace-10
2022-02-22 22:05:30 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:241] Pod my-cluster-1379373361-connect-6c67b78fc8-9m9pg is in CrashLoopBackOff state
2022-02-22 22:05:30 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:886] KafkaConnect with config ssl.endpoint.identification.algorithm will connect to 10.111.25.20:9093
2022-02-22 22:05:30 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaConnect: my-cluster-1379373361 will have desired state: Ready
2022-02-22 22:05:44 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-841108270-1695381205 in namespace namespace-10
2022-02-22 22:05:54 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1515898600 in namespace namespace-10
2022-02-22 22:06:04 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:06:04 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-10 for test case:testCaRenewalBreakInMiddle
2022-02-22 22:06:45 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCaRenewalBreakInMiddle-FINISHED
2022-02-22 22:06:45 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:06:45 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:06:45 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectCipherSuites-STARTED
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-13 for test case:testCertRegeneratedAfterInternalCAisDeleted
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-13
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-13
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-13
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-245280911 in namespace namespace-13
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-13
2022-02-22 22:06:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-245280911 will have desired state: Ready
2022-02-22 22:08:52 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-245280911 is in desired state: Ready
2022-02-22 22:08:52 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1107644071-772844135 in namespace namespace-13
2022-02-22 22:08:52 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-13
2022-02-22 22:08:52 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1107644071-772844135 will have desired state: Ready
2022-02-22 22:08:53 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1107644071-772844135 is in desired state: Ready
2022-02-22 22:08:53 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1090506053-1151329525 in namespace namespace-13
2022-02-22 22:08:53 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-13
2022-02-22 22:08:53 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1090506053-1151329525 will have desired state: Ready
2022-02-22 22:08:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1090506053-1151329525 is in desired state: Ready
2022-02-22 22:08:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-245280911-kafka-clients in namespace namespace-13
2022-02-22 22:08:54 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-13
2022-02-22 22:08:54 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-245280911-kafka-clients will be ready
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-245280911-kafka-clients is ready
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:811] Verifying that secret Secret(apiVersion=v1, data={ca.crt=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, ca.p12=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, ca.password=a0RPWGYyRlBqaXc5}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations={strimzi.io/ca-cert-generation=0}, clusterName=null, creationTimestamp=2022-02-22T22:06:53Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-245280911, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-245280911, strimzi.io/cluster=my-cluster-245280911, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testCertRegeneratedAfterInternalCAisDeleted}, managedFields=[], name=my-cluster-245280911-clients-ca-cert, namespace=namespace-13, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-245280911, uid=3a4952b0-755e-4a15-8c9b-ab76cb2b1d0f, additionalProperties={})], resourceVersion=15215, selfLink=/api/v1/namespaces/namespace-13/secrets/my-cluster-245280911-clients-ca-cert, uid=1333dc23-4718-4708-97d8-b2347dc7d51f, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) with name my-cluster-245280911-clients-ca-cert is present
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:811] Verifying that secret Secret(apiVersion=v1, data={ca.crt=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, ca.p12=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, ca.password=QjFTWmJCM0l3N2lH}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations={strimzi.io/ca-cert-generation=0}, clusterName=null, creationTimestamp=2022-02-22T22:06:53Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-245280911, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-245280911, strimzi.io/cluster=my-cluster-245280911, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testCertRegeneratedAfterInternalCAisDeleted}, managedFields=[], name=my-cluster-245280911-cluster-ca-cert, namespace=namespace-13, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-245280911, uid=3a4952b0-755e-4a15-8c9b-ab76cb2b1d0f, additionalProperties={})], resourceVersion=15214, selfLink=/api/v1/namespaces/namespace-13/secrets/my-cluster-245280911-cluster-ca-cert, uid=cd012f14-cbf3-4b55-ba66-c2c25e34237e, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) with name my-cluster-245280911-cluster-ca-cert is present
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:816] Deleting secret my-cluster-245280911-clients-ca-cert
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:816] Deleting secret my-cluster-245280911-cluster-ca-cert
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-245280911-kafka are stable
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:08:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:08:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:08:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:08:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:08:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:08:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:08:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:08:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:08:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:08:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:08:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:08:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:08:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:09:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:09:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:09:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:09:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:09:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:09:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:09:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:09:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:09:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:09:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:09:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:09:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:09:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:09:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:09:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:09:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:09:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:09:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:09:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:09:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:09:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:09:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:09:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:09:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:09:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:09:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:09:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:09:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:09:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:09:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:09:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:09:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:09:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:09:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:09:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:09:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:09:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:09:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:09:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:09:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:09:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:09:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:09:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:09:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:09:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:09:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:09:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:09:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:09:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:09:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:09:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:09:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:09:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:09:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:09:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:09:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:09:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:09:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:09:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:09:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:09:31 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:09:31 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:09:31 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:09:31 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:09:33 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:09:33 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:09:33 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:09:33 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:09:34 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:09:34 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:09:34 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:09:34 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:09:35 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:09:35 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:09:35 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:09:35 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:09:36 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:09:36 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:09:36 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:09:36 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:09:37 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:09:37 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:09:37 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:09:37 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:09:38 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:09:38 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:09:38 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:09:38 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:09:39 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:09:39 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:09:39 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:09:39 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:09:40 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:326] Pod my-cluster-245280911-kafka-0 is not stable in phase following phase Pending reset the stability counter from 43 to 0
2022-02-22 22:09:41 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:09:41 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:09:41 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:09:41 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:09:42 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:09:42 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:09:42 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:09:42 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:09:43 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:09:43 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:09:43 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:09:43 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:09:44 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:09:44 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:09:44 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:09:44 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:09:45 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:45 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:45 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:45 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:09:46 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:46 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:46 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:46 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:09:47 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:47 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:47 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:47 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:09:48 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:48 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:48 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:48 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:09:49 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:49 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:49 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:49 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:09:50 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:50 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:50 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:50 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:09:51 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:51 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:51 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:51 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:09:52 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:52 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:52 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:52 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:09:53 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:53 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:53 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:53 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:09:54 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:54 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:54 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:54 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:09:55 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:55 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:55 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:55 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:09:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:56 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:09:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:57 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:09:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:58 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:09:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:09:59 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:10:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:10:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:10:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:10:00 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:10:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:10:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:10:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:10:01 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:10:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:10:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:10:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:10:02 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:10:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:10:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:10:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:10:03 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:10:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:10:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:10:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:10:04 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:10:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:10:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:10:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:10:05 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:10:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:10:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:10:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:10:06 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:10:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:10:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:10:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:10:07 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:10:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:10:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:10:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:10:08 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:10:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:10:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:10:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:10:09 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:10:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:10:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:10:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:10:10 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:10:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:10:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:10:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:10:11 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:10:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:10:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:10:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:10:12 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:10:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:10:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:10:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:10:13 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:10:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:10:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:10:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:10:14 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:10:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:10:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:10:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:10:15 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:10:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:10:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:10:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:10:16 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:10:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:10:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:10:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:10:17 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:10:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:10:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:10:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:10:18 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:10:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:10:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:10:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:10:19 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:10:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:10:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:10:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:10:20 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:10:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:10:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:10:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:10:21 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:10:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:10:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:10:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:10:22 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:10:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:10:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:10:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:10:23 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:10:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-02-22 22:10:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-02-22 22:10:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-02-22 22:10:24 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 7
2022-02-22 22:10:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-02-22 22:10:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-02-22 22:10:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-02-22 22:10:25 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 6
2022-02-22 22:10:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-02-22 22:10:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-02-22 22:10:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-02-22 22:10:26 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 5
2022-02-22 22:10:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-02-22 22:10:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-02-22 22:10:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-02-22 22:10:27 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 4
2022-02-22 22:10:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-02-22 22:10:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-02-22 22:10:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-02-22 22:10:28 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 3
2022-02-22 22:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-02-22 22:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-02-22 22:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-02-22 22:10:29 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 2
2022-02-22 22:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-02-22 22:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-02-22 22:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-02-22 22:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf is in the Running state. Remaining seconds pod to be stable 1
2022-02-22 22:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-245280911-kafka-0 ,my-cluster-245280911-kafka-1 ,my-cluster-245280911-kafka-2 ,my-cluster-245280911-kafka-clients-69c765f67c-bw8sf
2022-02-22 22:10:30 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-245280911-kafka rolling update
2022-02-22 22:11:12 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaConnect: my-cluster-1379373361 is in desired state: Ready
2022-02-22 22:11:12 [ForkJoinPool-1-worker-3] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-1379373361-connect is not deleted yet! Triggering force delete by cmd client!
2022-02-22 22:11:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:11:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testTlsHostnameVerificationWithKafkaConnect
2022-02-22 22:11:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of NetworkPolicy my-cluster-1379373361-allow in namespace namespace-12
2022-02-22 22:11:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of KafkaConnect my-cluster-1379373361 in namespace namespace-12
2022-02-22 22:11:27 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1379373361-kafka-clients in namespace namespace-12
2022-02-22 22:12:10 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-245280911-kafka has been successfully rolled
2022-02-22 22:12:10 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-245280911-kafka to be ready
2022-02-22 22:12:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1379373361 in namespace namespace-12
2022-02-22 22:12:27 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:12:27 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-12 for test case:testTlsHostnameVerificationWithKafkaConnect
2022-02-22 22:13:11 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithKafkaConnect-FINISHED
2022-02-22 22:13:11 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:13:11 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:13:11 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectTlsVersion-STARTED
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-14 for test case:testKafkaAndKafkaConnectCipherSuites
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-14
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-14
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-14
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1385] Deploying Kafka cluster with the support TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 cipher algorithms
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1648506282 in namespace namespace-14
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-14
2022-02-22 22:13:15 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1648506282 will have desired state: Ready
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-245280911 will have desired state: Ready
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-245280911 is in desired state: Ready
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-245280911 is ready
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-245280911-clients-ca-cert
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:50] Secret my-cluster-245280911-clients-ca-cert created
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-245280911-cluster-ca-cert
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [SecretUtils:50] Secret my-cluster-245280911-cluster-ca-cert created
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:835] Checking consumed messages to pod:my-cluster-245280911-kafka-clients-69c765f67c-bw8sf
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@bfde451, messages=[], arguments=[--topic, my-topic-1090506053-1151329525, --bootstrap-server, my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093, --max-messages, 100, USER=my_user_1107644071_772844135], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-245280911-kafka-clients-69c765f67c-bw8sf', podNamespace='namespace-13', bootstrapServer='my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093', topicName='my-topic-1090506053-1151329525', maxMessages=100, kafkaUsername='my-user-1107644071-772844135', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2e5b72d9}
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093:my-topic-1090506053-1151329525 from pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf
2022-02-22 22:13:29 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-245280911-kafka-clients-69c765f67c-bw8sf -n namespace-13 -- /opt/kafka/producer.sh --topic my-topic-1090506053-1151329525 --bootstrap-server my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093 --max-messages 100 USER=my_user_1107644071_772844135
2022-02-22 22:13:33 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-02-22 22:13:33 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-02-22 22:13:33 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@315655a8, messages=[], arguments=[--group-instance-id, instance648809928, --topic, my-topic-1090506053-1151329525, --group-id, my-consumer-group-2050734270, --bootstrap-server, my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093, --max-messages, 100, USER=my_user_1107644071_772844135], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-245280911-kafka-clients-69c765f67c-bw8sf', podNamespace='namespace-13', bootstrapServer='my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093', topicName='my-topic-1090506053-1151329525', maxMessages=100, kafkaUsername='my-user-1107644071-772844135', consumerGroupName='my-consumer-group-2050734270', consumerInstanceId='instance648809928', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@67591a3e}
2022-02-22 22:13:33 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093:my-topic-1090506053-1151329525 from pod my-cluster-245280911-kafka-clients-69c765f67c-bw8sf
2022-02-22 22:13:33 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-245280911-kafka-clients-69c765f67c-bw8sf -n namespace-13 -- /opt/kafka/consumer.sh --group-instance-id instance648809928 --topic my-topic-1090506053-1151329525 --group-id my-consumer-group-2050734270 --bootstrap-server my-cluster-245280911-kafka-bootstrap.namespace-13.svc:9093 --max-messages 100 USER=my_user_1107644071_772844135
2022-02-22 22:13:41 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 22:13:41 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 22:13:41 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:13:41 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testCertRegeneratedAfterInternalCAisDeleted
2022-02-22 22:13:41 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1090506053-1151329525 in namespace namespace-13
2022-02-22 22:13:51 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-245280911-kafka-clients in namespace namespace-13
2022-02-22 22:14:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1107644071-772844135 in namespace namespace-13
2022-02-22 22:14:41 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-245280911 in namespace namespace-13
2022-02-22 22:14:51 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:14:51 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-13 for test case:testCertRegeneratedAfterInternalCAisDeleted
2022-02-22 22:15:18 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertRegeneratedAfterInternalCAisDeleted-FINISHED
2022-02-22 22:15:18 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:15:18 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:15:18 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAclRuleReadAndWrite-STARTED
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-15 for test case:testKafkaAndKafkaConnectTlsVersion
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-15
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-15
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-15
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1307] Deploying Kafka cluster with the support TLSv1.2 TLS
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-987044843 in namespace namespace-15
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-15
2022-02-22 22:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-987044843 will have desired state: Ready
2022-02-22 22:18:27 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1648506282 is in desired state: Ready
2022-02-22 22:18:27 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1397] Verifying that Kafka Connect has the accepted configuration:
 ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2022-02-22 22:18:27 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1648506282-kafka-clients in namespace namespace-15
2022-02-22 22:18:27 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-14
2022-02-22 22:18:27 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1648506282-kafka-clients will be ready
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1648506282-kafka-clients is ready
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1648506282-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update NetworkPolicy my-cluster-1648506282-allow in namespace namespace-14
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-14
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaConnect my-cluster-1648506282 in namespace namespace-15
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-14
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1414] Verifying that Kafka Connect status is NotReady because of different cipher suites complexity of algorithm
2022-02-22 22:18:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaConnect: my-cluster-1648506282 will have desired state: NotReady
2022-02-22 22:22:24 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-987044843 is in desired state: Ready
2022-02-22 22:22:24 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1319] Verifying that Kafka cluster has the accepted configuration:
ssl.enabled.protocols -> TLSv1.2
ssl.protocol -> TLSv1.3
2022-02-22 22:22:24 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-987044843-kafka-clients in namespace namespace-15
2022-02-22 22:22:24 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-15
2022-02-22 22:22:24 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-987044843-kafka-clients will be ready
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-987044843-kafka-clients is ready
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-987044843-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update NetworkPolicy my-cluster-987044843-allow in namespace namespace-15
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-15
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaConnect my-cluster-987044843 in namespace namespace-15
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-15
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1342] Verifying that Kafka Connect status is NotReady because of different TLS version
2022-02-22 22:22:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaConnect: my-cluster-987044843 will have desired state: NotReady
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaConnect: my-cluster-1648506282 is in desired state: NotReady
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1418] Replacing Kafka Connect config to the cipher suites same as the Kafka broker has.
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1422] Verifying that Kafka Connect has the accepted configuration:
 ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 change
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 change
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1427] Verifying that Kafka Connect is stable
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-1648506282-connect are stable
2022-02-22 22:23:31 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:23:32 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:23:33 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:23:34 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:23:35 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:23:36 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:23:37 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:23:38 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:23:39 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:23:40 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:23:41 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:23:42 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:23:43 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:23:44 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:23:45 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:23:46 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:23:47 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:23:48 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:23:49 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:23:50 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:23:51 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:23:52 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:23:53 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:23:54 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:23:55 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:23:56 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:23:57 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:23:58 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:23:59 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:24:00 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:24:01 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:24:02 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:24:03 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:24:04 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:24:05 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:24:06 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:24:07 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:24:08 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:24:09 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:24:10 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:24:11 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:24:12 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:24:13 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:24:14 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 7
2022-02-22 22:24:15 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 6
2022-02-22 22:24:16 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 5
2022-02-22 22:24:17 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 4
2022-02-22 22:24:18 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 3
2022-02-22 22:24:19 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 2
2022-02-22 22:24:20 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-1648506282-connect-5cb48f59f7-m72bf is in the Running state. Remaining seconds pod to be stable 1
2022-02-22 22:24:20 [ForkJoinPool-1-worker-5] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-1648506282-connect-5cb48f59f7-m72bf
2022-02-22 22:24:20 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:1431] Verifying that Kafka Connect status is Ready because of the same cipher suites complexity of algorithm
2022-02-22 22:24:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaConnect: my-cluster-1648506282 will have desired state: Ready
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaConnect: my-cluster-987044843 is in desired state: NotReady
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1346] Replacing Kafka Connect config to the newest(TLSv1.2) one same as the Kafka broker has.
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1350] Verifying that Kafka Connect has the accepted configuration:
 ssl.enabled.protocols -> TLSv1.2
 ssl.protocol -> TLSv1.3
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.enabled.protocols -> TLSv1.2 change
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.enabled.protocols -> TLSv1.2 change
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.protocol -> TLSv1.3 change
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.protocol -> TLSv1.3 change
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1359] Verifying that Kafka Connect is stable
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-987044843-connect are stable
2022-02-22 22:27:28 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 50
2022-02-22 22:27:29 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 49
2022-02-22 22:27:30 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 48
2022-02-22 22:27:31 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 47
2022-02-22 22:27:32 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 46
2022-02-22 22:27:33 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 45
2022-02-22 22:27:34 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 44
2022-02-22 22:27:35 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 43
2022-02-22 22:27:36 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 42
2022-02-22 22:27:37 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 41
2022-02-22 22:27:38 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 40
2022-02-22 22:27:39 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 39
2022-02-22 22:27:40 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 38
2022-02-22 22:27:41 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 37
2022-02-22 22:27:42 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 36
2022-02-22 22:27:43 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 35
2022-02-22 22:27:44 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 34
2022-02-22 22:27:45 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 33
2022-02-22 22:27:46 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 32
2022-02-22 22:27:47 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 31
2022-02-22 22:27:48 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 30
2022-02-22 22:27:49 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 29
2022-02-22 22:27:50 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 28
2022-02-22 22:27:51 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 27
2022-02-22 22:27:52 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 26
2022-02-22 22:27:53 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 25
2022-02-22 22:27:54 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 24
2022-02-22 22:27:55 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 23
2022-02-22 22:27:56 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 22
2022-02-22 22:27:57 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 21
2022-02-22 22:27:58 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 20
2022-02-22 22:27:59 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 19
2022-02-22 22:28:00 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 18
2022-02-22 22:28:01 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 17
2022-02-22 22:28:02 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 16
2022-02-22 22:28:03 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 15
2022-02-22 22:28:04 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 14
2022-02-22 22:28:05 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 13
2022-02-22 22:28:06 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 12
2022-02-22 22:28:07 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 11
2022-02-22 22:28:08 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 10
2022-02-22 22:28:09 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 9
2022-02-22 22:28:10 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 8
2022-02-22 22:28:11 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 7
2022-02-22 22:28:12 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 6
2022-02-22 22:28:13 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 5
2022-02-22 22:28:14 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 4
2022-02-22 22:28:15 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 3
2022-02-22 22:28:16 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 2
2022-02-22 22:28:17 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-987044843-connect-58664566f8-lth7m is in the Running state. Remaining seconds pod to be stable 1
2022-02-22 22:28:17 [ForkJoinPool-1-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-987044843-connect-58664566f8-lth7m
2022-02-22 22:28:17 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1363] Verifying that Kafka Connect status is Ready because of same TLS version
2022-02-22 22:28:17 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaConnect: my-cluster-987044843 will have desired state: Ready
2022-02-22 22:29:36 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaConnect: my-cluster-1648506282 is in desired state: Ready
2022-02-22 22:29:36 [ForkJoinPool-1-worker-5] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-1648506282-connect is not deleted yet! Triggering force delete by cmd client!
2022-02-22 22:29:41 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:29:41 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:309] Delete all resources for testKafkaAndKafkaConnectCipherSuites
2022-02-22 22:29:41 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of NetworkPolicy my-cluster-1648506282-allow in namespace namespace-14
2022-02-22 22:29:41 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaConnect my-cluster-1648506282 in namespace namespace-14
2022-02-22 22:29:41 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1648506282-kafka-clients in namespace namespace-14
2022-02-22 22:30:21 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1648506282 in namespace namespace-14
2022-02-22 22:30:31 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:30:31 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-14 for test case:testKafkaAndKafkaConnectCipherSuites
2022-02-22 22:31:14 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectCipherSuites-FINISHED
2022-02-22 22:31:14 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:31:14 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:31:14 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertRenewalInMaintenanceWindow-STARTED
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-16 for test case:testAclRuleReadAndWrite
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-16
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-16
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-16
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-712645836 in namespace namespace-16
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-16
2022-02-22 22:31:18 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-712645836 will have desired state: Ready
2022-02-22 22:32:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-712645836 is in desired state: Ready
2022-02-22 22:32:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1886738408-1545852810 in namespace namespace-16
2022-02-22 22:32:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-16
2022-02-22 22:32:31 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1886738408-1545852810 will have desired state: Ready
2022-02-22 22:32:32 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1886738408-1545852810 is in desired state: Ready
2022-02-22 22:32:32 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser kafka-user-write in namespace namespace-16
2022-02-22 22:32:32 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-16
2022-02-22 22:32:32 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: kafka-user-write will have desired state: Ready
2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: kafka-user-write is in desired state: Ready
2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1010] Checking KafkaUser kafka-user-write that is able to send messages to topic 'my-topic-1886738408-1545852810'
2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [ProducerConfig:376] ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.49.2:30450]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-6343108
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 6000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties12397303774181759830.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties4604448476464401632.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 22:32:33 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645569153368
2022-02-22 22:32:33 [kafka-producer-network-thread | producer-6343108] [32mINFO [m [Metadata:402] [Producer clientId=producer-6343108] Resetting the last seen epoch of partition my-topic-1886738408-1545852810-0 to 0 since the associated topicId changed from null to 6N8M4yaQRZmDuvqck4xSiA
2022-02-22 22:32:33 [kafka-producer-network-thread | producer-6343108] [32mINFO [m [Metadata:287] [Producer clientId=producer-6343108] Cluster ID: rYUXokZPTpitG7L3NSuLRw
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ExternalKafkaClient:182] Sent 500 messages.
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaProducer:1228] [Producer clientId=producer-6343108] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:659] Metrics scheduler closed
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:663] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:669] Metrics reporters closed
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:83] App info kafka.producer for producer-6343108 unregistered
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerConfig:376] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.49.2:30450]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-1537958660
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-consumer-group-450488709
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties7431514895963658150.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties5236032450428975766.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645569155525
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaConsumer:966] [Consumer clientId=consumer-1537958660, groupId=my-consumer-group-450488709] Subscribed to topic(s): my-topic-1886738408-1545852810
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:402] [Consumer clientId=consumer-1537958660, groupId=my-consumer-group-450488709] Resetting the last seen epoch of partition my-topic-1886738408-1545852810-0 to 0 since the associated topicId changed from null to 6N8M4yaQRZmDuvqck4xSiA
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:287] [Consumer clientId=consumer-1537958660, groupId=my-consumer-group-450488709] Cluster ID: rYUXokZPTpitG7L3NSuLRw
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:261] [Consumer clientId=consumer-1537958660, groupId=my-consumer-group-450488709] FindCoordinator request hit fatal exception
org.apache.kafka.common.errors.GroupAuthorizationException: Not authorized to access group: my-consumer-group-450488709
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser kafka-user-read in namespace namespace-16
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-16
2022-02-22 22:32:35 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: kafka-user-read will have desired state: Ready
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: kafka-user-read is in desired state: Ready
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerConfig:376] ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [192.168.49.2:30450]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-823744454
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = consumer-group-name-1
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties626161450354369018.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties18445218187775260174.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645569156712
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaConsumer:966] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Subscribed to topic(s): my-topic-1886738408-1545852810
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:402] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Resetting the last seen epoch of partition my-topic-1886738408-1545852810-0 to 0 since the associated topicId changed from null to 6N8M4yaQRZmDuvqck4xSiA
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [Metadata:287] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Cluster ID: rYUXokZPTpitG7L3NSuLRw
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:853] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Discovered group coordinator 192.168.49.2:30217 (id: 2147483645 rack: null)
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:535] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] (Re-)joining group
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1000] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Request joining group due to: need to re-join with the given member-id
2022-02-22 22:32:36 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:535] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] (Re-)joining group
2022-02-22 22:32:39 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:595] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Successfully joined group with generation Generation{generationId=1, memberId='consumer-823744454-4692aa06-9341-46b9-a180-d2989efedb34', protocol='range'}
2022-02-22 22:32:39 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:652] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Finished assignment for group at generation 1: {consumer-823744454-4692aa06-9341-46b9-a180-d2989efedb34=Assignment(partitions=[my-topic-1886738408-1545852810-0])}
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:761] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Successfully synced group in generation Generation{generationId=1, memberId='consumer-823744454-4692aa06-9341-46b9-a180-d2989efedb34', protocol='range'}
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:279] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Notifying assignor about the new Assignment(partitions=[my-topic-1886738408-1545852810-0])
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:291] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Adding newly assigned partitions: my-topic-1886738408-1545852810-0
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1388] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Found no committed offset for partition my-topic-1886738408-1545852810-0
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [SubscriptionState:398] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Resetting offset for partition my-topic-1886738408-1545852810-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[192.168.49.2:30146 (id: 1 rack: null)], epoch=0}}.
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ExternalKafkaClient:224] Received 500 messages.
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:310] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Revoke previously assigned partitions my-topic-1886738408-1545852810-0
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1060] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Member consumer-823744454-4692aa06-9341-46b9-a180-d2989efedb34 sending LeaveGroup request to coordinator 192.168.49.2:30217 (id: 2147483645 rack: null) due to the consumer is being closed
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:972] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Resetting generation due to: consumer pro-actively leaving the group
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ConsumerCoordinator:1000] [Consumer clientId=consumer-823744454, groupId=consumer-group-name-1] Request joining group due to: consumer pro-actively leaving the group
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:659] Metrics scheduler closed
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:663] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:669] Metrics reporters closed
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:83] App info kafka.consumer for consumer-823744454 unregistered
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1058] Checking KafkaUser kafka-user-read that is not able to send messages to topic 'my-topic-1886738408-1545852810'
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ProducerConfig:376] ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [192.168.49.2:30450]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-750545934
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 6000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = 
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = 
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties9525475362441847532.keystore
	ssl.keystore.password = [hidden]
	ssl.keystore.type = PKCS12
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /tmp/io.strimzi.systemtest.kafkaclients.clientproperties.AbstractKafkaClientProperties13283889953206658147.truststore
	ssl.truststore.password = [hidden]
	ssl.truststore.type = PKCS12
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:119] Kafka version: 3.1.0
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:120] Kafka commitId: 37edeed0777bacb3
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:121] Kafka startTimeMs: 1645569160382
2022-02-22 22:32:40 [kafka-producer-network-thread | producer-750545934] [32mINFO [m [Metadata:402] [Producer clientId=producer-750545934] Resetting the last seen epoch of partition my-topic-1886738408-1545852810-0 to 0 since the associated topicId changed from null to 6N8M4yaQRZmDuvqck4xSiA
2022-02-22 22:32:40 [kafka-producer-network-thread | producer-750545934] [32mINFO [m [Metadata:287] [Producer clientId=producer-750545934] Cluster ID: rYUXokZPTpitG7L3NSuLRw
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [1;31mERROR[m [ExternalKafkaClient:171] Error sending message 0 - org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [my-topic-1886738408-1545852810]
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaProducer:1228] [Producer clientId=producer-750545934] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:659] Metrics scheduler closed
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:663] Closing reporter org.apache.kafka.common.metrics.JmxReporter
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [Metrics:669] Metrics reporters closed
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [AppInfoParser:83] App info kafka.producer for producer-750545934 unregistered
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testAclRuleReadAndWrite
2022-02-22 22:32:40 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser kafka-user-write in namespace namespace-16
2022-02-22 22:32:50 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser kafka-user-read in namespace namespace-16
2022-02-22 22:33:00 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1886738408-1545852810 in namespace namespace-16
2022-02-22 22:33:10 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-712645836 in namespace namespace-16
2022-02-22 22:33:20 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:33:20 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-16 for test case:testAclRuleReadAndWrite
2022-02-22 22:33:40 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaConnect: my-cluster-987044843 is in desired state: Ready
2022-02-22 22:33:40 [ForkJoinPool-1-worker-3] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-987044843-connect is not deleted yet! Triggering force delete by cmd client!
2022-02-22 22:33:45 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:33:45 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testKafkaAndKafkaConnectTlsVersion
2022-02-22 22:33:45 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of NetworkPolicy my-cluster-987044843-allow in namespace namespace-15
2022-02-22 22:33:45 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of KafkaConnect my-cluster-987044843 in namespace namespace-15
2022-02-22 22:33:45 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-987044843-kafka-clients in namespace namespace-15
2022-02-22 22:33:47 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAclRuleReadAndWrite-FINISHED
2022-02-22 22:33:47 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:33:47 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:33:47 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testOwnerReferenceOfCASecrets-STARTED
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-17 for test case:testCertRenewalInMaintenanceWindow
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-17
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-17
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-17
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:712] Maintenance window is: * 38-52 * * * ? *
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1850082713 in namespace namespace-17
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-17
2022-02-22 22:33:49 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1850082713 will have desired state: Ready
2022-02-22 22:34:25 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-987044843 in namespace namespace-15
2022-02-22 22:34:35 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:34:35 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-15 for test case:testKafkaAndKafkaConnectTlsVersion
2022-02-22 22:35:18 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectTlsVersion-FINISHED
2022-02-22 22:35:18 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:35:18 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:35:18 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testClusterCACertRenew-STARTED
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-18 for test case:testOwnerReferenceOfCASecrets
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-18
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-18
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-18
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-70927646 in namespace namespace-18
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-18
2022-02-22 22:35:22 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-70927646 will have desired state: Ready
2022-02-22 22:36:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1850082713 is in desired state: Ready
2022-02-22 22:36:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1216088496-516279421 in namespace namespace-18
2022-02-22 22:36:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-17
2022-02-22 22:36:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1216088496-516279421 will have desired state: Ready
2022-02-22 22:36:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1216088496-516279421 is in desired state: Ready
2022-02-22 22:36:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1092609313-2099003628 in namespace namespace-18
2022-02-22 22:36:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-17
2022-02-22 22:36:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1092609313-2099003628 will have desired state: Ready
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1092609313-2099003628 is in desired state: Ready
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1092609313-2099003628 in namespace namespace-18
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-17
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1092609313-2099003628 will have desired state: Ready
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1092609313-2099003628 is in desired state: Ready
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1850082713-kafka-clients in namespace namespace-18
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-17
2022-02-22 22:36:20 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1850082713-kafka-clients will be ready
2022-02-22 22:36:22 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1850082713-kafka-clients is ready
2022-02-22 22:36:22 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 22:36:22 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:742] Annotate secret my-cluster-1850082713-cluster-ca-cert with secret force-renew annotation
2022-02-22 22:36:22 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:749] Wait until maintenance windows starts
2022-02-22 22:38:01 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:755] Maintenance window starts
2022-02-22 22:38:01 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:759] Wait until rolling update is triggered during maintenance window
2022-02-22 22:38:01 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1850082713-kafka rolling update
2022-02-22 22:42:14 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-70927646 is in desired state: Ready
2022-02-22 22:42:14 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1458] Listing all cluster CAs for my-cluster-70927646
2022-02-22 22:42:14 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1462] Deleting Kafka:my-cluster-70927646
2022-02-22 22:42:14 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaUtils:397] Waiting for deletion of Kafka:my-cluster-70927646
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1466] Checking actual secrets after Kafka deletion
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1469] Checking that my-cluster-70927646-clients-ca secret is still present
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1472] Deleting secret: my-cluster-70927646-clients-ca
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1469] Checking that my-cluster-70927646-clients-ca-cert secret is still present
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1472] Deleting secret: my-cluster-70927646-clients-ca-cert
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1469] Checking that my-cluster-70927646-cluster-ca secret is still present
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1472] Deleting secret: my-cluster-70927646-cluster-ca
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1469] Checking that my-cluster-70927646-cluster-ca-cert secret is still present
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1472] Deleting secret: my-cluster-70927646-cluster-ca-cert
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1476] Deploying Kafka with generateSecretOwnerReference set to true
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-second-cluster-my-cluster-70927646 in namespace namespace-18
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-18
2022-02-22 22:42:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-second-cluster-my-cluster-70927646 will have desired state: Ready
2022-02-22 22:42:41 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1850082713-kafka has been successfully rolled
2022-02-22 22:42:41 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-1850082713-kafka to be ready
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1850082713 will have desired state: Ready
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1850082713 is in desired state: Ready
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1850082713 is ready
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:764] Checking consumed messages to pod:my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@63369e90, messages=[], arguments=[--topic, my-topic-1092609313-2099003628, --bootstrap-server, my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093, --max-messages, 100, USER=my_user_1216088496_516279421], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4', podNamespace='namespace-17', bootstrapServer='my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093', topicName='my-topic-1092609313-2099003628', maxMessages=100, kafkaUsername='my-user-1216088496-516279421', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@70358ebf}
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093:my-topic-1092609313-2099003628 from pod my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4
2022-02-22 22:43:50 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4 -n namespace-17 -- /opt/kafka/producer.sh --topic my-topic-1092609313-2099003628 --bootstrap-server my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093 --max-messages 100 USER=my_user_1216088496_516279421
2022-02-22 22:43:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-02-22 22:43:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-02-22 22:43:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@62942e79, messages=[], arguments=[--group-instance-id, instance900187597, --topic, my-topic-1092609313-2099003628, --group-id, my-consumer-group-2128842921, --bootstrap-server, my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093, --max-messages, 100, USER=my_user_1216088496_516279421], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4', podNamespace='namespace-17', bootstrapServer='my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093', topicName='my-topic-1092609313-2099003628', maxMessages=100, kafkaUsername='my-user-1216088496-516279421', consumerGroupName='my-consumer-group-2128842921', consumerInstanceId='instance900187597', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3e881f61}
2022-02-22 22:43:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093:my-topic-1092609313-2099003628 from pod my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4
2022-02-22 22:43:57 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1850082713-kafka-clients-7574bd8f66-m8dw4 -n namespace-17 -- /opt/kafka/consumer.sh --group-instance-id instance900187597 --topic my-topic-1092609313-2099003628 --group-id my-consumer-group-2128842921 --bootstrap-server my-cluster-1850082713-kafka-bootstrap.namespace-17.svc:9093 --max-messages 100 USER=my_user_1216088496_516279421
2022-02-22 22:44:05 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-02-22 22:44:05 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-02-22 22:44:05 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:44:05 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:309] Delete all resources for testCertRenewalInMaintenanceWindow
2022-02-22 22:44:05 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1092609313-2099003628 in namespace namespace-17
2022-02-22 22:44:15 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1850082713-kafka-clients in namespace namespace-17
2022-02-22 22:44:55 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1092609313-2099003628 in namespace namespace-17
2022-02-22 22:44:55 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1216088496-516279421 in namespace namespace-17
2022-02-22 22:45:05 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1850082713 in namespace namespace-17
2022-02-22 22:45:15 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:45:15 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-17 for test case:testCertRenewalInMaintenanceWindow
2022-02-22 22:45:42 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertRenewalInMaintenanceWindow-FINISHED
2022-02-22 22:45:42 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:45:42 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:45:42 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewClientsCaCertsTriggeredByAnno-STARTED
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-19 for test case:testClusterCACertRenew
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-19
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-19
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-19
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1708401754 in namespace namespace-19
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-19
2022-02-22 22:45:43 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1708401754 will have desired state: Ready
2022-02-22 22:52:25 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-second-cluster-my-cluster-70927646 is in desired state: Ready
2022-02-22 22:52:25 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1491] Deleting Kafka:my-second-cluster-my-cluster-70927646
2022-02-22 22:52:25 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaUtils:397] Waiting for deletion of Kafka:my-second-cluster-my-cluster-70927646
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1495] Checking actual secrets after Kafka deletion
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1498] Checking that my-second-cluster-my-cluster-70927646-clients-ca secret is deleted
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1498] Checking that my-second-cluster-my-cluster-70927646-clients-ca-cert secret is deleted
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1498] Checking that my-second-cluster-my-cluster-70927646-cluster-ca secret is deleted
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:1498] Checking that my-second-cluster-my-cluster-70927646-cluster-ca-cert secret is deleted
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testOwnerReferenceOfCASecrets
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-second-cluster-my-cluster-70927646 in namespace namespace-18
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-70927646 in namespace namespace-18
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 22:52:27 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-18 for test case:testOwnerReferenceOfCASecrets
2022-02-22 22:52:57 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1708401754 is in desired state: Ready
2022-02-22 22:52:57 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1562] Change of kafka validity and renewal days - reconciliation should start.
2022-02-22 22:52:57 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1708401754-kafka rolling update
2022-02-22 22:53:11 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testOwnerReferenceOfCASecrets-FINISHED
2022-02-22 22:53:11 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 22:53:11 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 22:53:11 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClusterCaKeysTriggeredByAnno-STARTED
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-20 for test case:testAutoRenewClientsCaCertsTriggeredByAnno
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-20
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-20
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-20
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-850111446 in namespace namespace-20
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-20
2022-02-22 22:53:12 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-850111446 will have desired state: Ready
2022-02-22 22:59:52 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1708401754-kafka has been successfully rolled
2022-02-22 22:59:52 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-1708401754-kafka to be ready
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1708401754 will have desired state: Ready
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1708401754 is in desired state: Ready
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1708401754 is ready
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1592] Initial ClusterCA cert dates: Tue Feb 22 17:45:43 EST 2022 --> Mon Mar 14 18:45:43 EDT 2022
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1593] Changed ClusterCA cert dates: Tue Feb 22 17:52:59 EST 2022 --> Sat Sep 10 18:52:59 EDT 2022
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1594] KafkaBroker cert creation dates: Tue Feb 22 17:46:15 EST 2022 --> Mon Mar 14 18:46:15 EDT 2022
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1595] KafkaBroker cert changed dates:  Tue Feb 22 17:54:19 EST 2022 --> Sat Sep 10 18:54:19 EDT 2022
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1596] Zookeeper cert creation dates: Tue Feb 22 17:45:47 EST 2022 --> Mon Mar 14 18:45:47 EDT 2022
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:1597] Zookeeper cert changed dates:  Tue Feb 22 17:52:59 EST 2022 --> Sat Sep 10 18:52:59 EDT 2022
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testClusterCACertRenew
2022-02-22 23:00:23 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1708401754 in namespace namespace-19
2022-02-22 23:00:33 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 23:00:33 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-19 for test case:testClusterCACertRenew
2022-02-22 23:00:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-850111446 is in desired state: Ready
2022-02-22 23:00:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1924425433-1846548893 in namespace namespace-20
2022-02-22 23:00:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-20
2022-02-22 23:00:50 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1924425433-1846548893 will have desired state: Ready
2022-02-22 23:00:51 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1924425433-1846548893 is in desired state: Ready
2022-02-22 23:00:51 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-404657524-365820804 in namespace namespace-20
2022-02-22 23:00:51 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-20
2022-02-22 23:00:51 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-404657524-365820804 will have desired state: Ready
2022-02-22 23:00:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-404657524-365820804 is in desired state: Ready
2022-02-22 23:00:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-850111446-kafka-clients in namespace namespace-20
2022-02-22 23:00:52 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-20
2022-02-22 23:00:52 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-850111446-kafka-clients will be ready
2022-02-22 23:00:54 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-850111446-kafka-clients is ready
2022-02-22 23:00:54 [ForkJoinPool-1-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 23:00:54 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:274] Checking produced and consumed messages to pod:my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7
2022-02-22 23:00:54 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@729c6793, messages=[], arguments=[--topic, my-topic-404657524-365820804, --bootstrap-server, my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7', podNamespace='namespace-20', bootstrapServer='my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092', topicName='my-topic-404657524-365820804', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@48524cea}
2022-02-22 23:00:54 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092:my-topic-404657524-365820804 from pod my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7
2022-02-22 23:00:54 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7 -n namespace-20 -- /opt/kafka/producer.sh --topic my-topic-404657524-365820804 --bootstrap-server my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092 --max-messages 100
2022-02-22 23:00:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 23:00:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 23:00:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@67520b74, messages=[], arguments=[--group-instance-id, instance833164648, --topic, my-topic-404657524-365820804, --group-id, my-consumer-group-1877840895, --bootstrap-server, my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7', podNamespace='namespace-20', bootstrapServer='my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092', topicName='my-topic-404657524-365820804', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1877840895', consumerInstanceId='instance833164648', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5efd1234}
2022-02-22 23:00:57 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092#my-topic-404657524-365820804 from pod my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7
2022-02-22 23:00:57 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7 -n namespace-20 -- /opt/kafka/consumer.sh --group-instance-id instance833164648 --topic my-topic-404657524-365820804 --group-id my-consumer-group-1877840895 --bootstrap-server my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092 --max-messages 100
2022-02-22 23:01:03 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:01:03 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:01:03 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:288] Triggering CA cert renewal by adding the annotation
2022-02-22 23:01:03 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:300] Patching secret my-cluster-850111446-clients-ca-cert with strimzi.io/force-renew
2022-02-22 23:01:03 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:309] Wait for kafka to rolling restart ...
2022-02-22 23:01:03 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-850111446-kafka rolling update
2022-02-22 23:01:16 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testClusterCACertRenew-FINISHED
2022-02-22 23:01:16 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 23:01:16 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-02-22 23:01:16 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceAllCaKeysTriggeredByAnno-STARTED
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-21 for test case:testAutoReplaceClusterCaKeysTriggeredByAnno
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-21
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-21
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-21
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-523411585 in namespace namespace-21
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-21
2022-02-22 23:01:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-523411585 will have desired state: Ready
2022-02-22 23:04:23 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-850111446-kafka has been successfully rolled
2022-02-22 23:04:23 [ForkJoinPool-1-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-850111446-kafka to be ready
2022-02-22 23:07:11 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:322] Checking the certificates have been replaced
2022-02-22 23:07:11 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:336] Checking consumed messages to pod:my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7
2022-02-22 23:07:11 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5db0b7ee, messages=[], arguments=[--group-instance-id, instance337424616, --topic, my-topic-404657524-365820804, --group-id, my-consumer-group-2020445092, --bootstrap-server, my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7', podNamespace='namespace-20', bootstrapServer='my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092', topicName='my-topic-404657524-365820804', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2020445092', consumerInstanceId='instance337424616', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@760bf325}
2022-02-22 23:07:11 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092#my-topic-404657524-365820804 from pod my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7
2022-02-22 23:07:11 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-850111446-kafka-clients-79b4559bcd-dhsv7 -n namespace-20 -- /opt/kafka/consumer.sh --group-instance-id instance337424616 --topic my-topic-404657524-365820804 --group-id my-consumer-group-2020445092 --bootstrap-server my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9092 --max-messages 100
2022-02-22 23:07:18 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:07:18 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:07:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser bob-my-cluster-850111446 in namespace namespace-21
2022-02-22 23:07:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-20
2022-02-22 23:07:18 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: bob-my-cluster-850111446 will have desired state: Ready
2022-02-22 23:07:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:405] KafkaUser: bob-my-cluster-850111446 is in desired state: Ready
2022-02-22 23:07:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-850111446-kafka-clients-tls in namespace namespace-20
2022-02-22 23:07:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-20
2022-02-22 23:07:19 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-850111446-kafka-clients-tls will be ready
2022-02-22 23:07:21 [ForkJoinPool-1-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-850111446-kafka-clients-tls is ready
2022-02-22 23:07:21 [ForkJoinPool-1-worker-5] [32mINFO [m [SecurityST:360] Checking consumed messages to pod:my-cluster-850111446-kafka-clients-tls-6bf99cbb95-b2lf8
2022-02-22 23:07:21 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2b0c120a, messages=[], arguments=[--group-instance-id, instance1170923725, --topic, my-topic-404657524-365820804, --group-id, my-consumer-group-506244776, --bootstrap-server, my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9093, --max-messages, 100, USER=bob_my_cluster_850111446], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-850111446-kafka-clients-tls-6bf99cbb95-b2lf8', podNamespace='namespace-20', bootstrapServer='my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9093', topicName='my-topic-404657524-365820804', maxMessages=100, kafkaUsername='bob-my-cluster-850111446', consumerGroupName='my-consumer-group-506244776', consumerInstanceId='instance1170923725', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2b75ba2}
2022-02-22 23:07:21 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9093#my-topic-404657524-365820804 from pod my-cluster-850111446-kafka-clients-tls-6bf99cbb95-b2lf8
2022-02-22 23:07:21 [ForkJoinPool-1-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-850111446-kafka-clients-tls-6bf99cbb95-b2lf8 -n namespace-20 -- /opt/kafka/consumer.sh --group-instance-id instance1170923725 --topic my-topic-404657524-365820804 --group-id my-consumer-group-506244776 --bootstrap-server my-cluster-850111446-kafka-bootstrap.namespace-20.svc:9093 --max-messages 100 USER=bob_my_cluster_850111446
2022-02-22 23:07:29 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:07:29 [ForkJoinPool-1-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:07:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 23:07:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoRenewClientsCaCertsTriggeredByAnno
2022-02-22 23:07:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-850111446-kafka-clients in namespace namespace-20
2022-02-22 23:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-523411585 is in desired state: Ready
2022-02-22 23:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1387635120-1751408050 in namespace namespace-21
2022-02-22 23:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-21
2022-02-22 23:09:04 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1387635120-1751408050 will have desired state: Ready
2022-02-22 23:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1387635120-1751408050 is in desired state: Ready
2022-02-22 23:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-526714916-689400629 in namespace namespace-21
2022-02-22 23:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-21
2022-02-22 23:09:05 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-526714916-689400629 will have desired state: Ready
2022-02-22 23:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-526714916-689400629 is in desired state: Ready
2022-02-22 23:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-523411585-kafka-clients in namespace namespace-21
2022-02-22 23:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-21
2022-02-22 23:09:06 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-kafka-clients will be ready
2022-02-22 23:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-kafka-clients is ready
2022-02-22 23:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 23:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:467] Checking produced and consumed messages to pod:my-cluster-523411585-kafka-clients-56c65469c4-gr7df
2022-02-22 23:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@b1cc22b, messages=[], arguments=[--topic, my-topic-526714916-689400629, --bootstrap-server, my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-523411585-kafka-clients-56c65469c4-gr7df', podNamespace='namespace-21', bootstrapServer='my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092', topicName='my-topic-526714916-689400629', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@79089a7b}
2022-02-22 23:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092:my-topic-526714916-689400629 from pod my-cluster-523411585-kafka-clients-56c65469c4-gr7df
2022-02-22 23:09:08 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-523411585-kafka-clients-56c65469c4-gr7df -n namespace-21 -- /opt/kafka/producer.sh --topic my-topic-526714916-689400629 --bootstrap-server my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092 --max-messages 100
2022-02-22 23:09:09 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-850111446-kafka-clients-tls in namespace namespace-20
2022-02-22 23:09:09 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaUser bob-my-cluster-850111446 in namespace namespace-20
2022-02-22 23:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 23:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 23:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@312bae2e, messages=[], arguments=[--group-instance-id, instance1820043773, --topic, my-topic-526714916-689400629, --group-id, my-consumer-group-937902988, --bootstrap-server, my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-523411585-kafka-clients-56c65469c4-gr7df', podNamespace='namespace-21', bootstrapServer='my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092', topicName='my-topic-526714916-689400629', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-937902988', consumerInstanceId='instance1820043773', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ccd268e}
2022-02-22 23:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092#my-topic-526714916-689400629 from pod my-cluster-523411585-kafka-clients-56c65469c4-gr7df
2022-02-22 23:09:11 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-523411585-kafka-clients-56c65469c4-gr7df -n namespace-21 -- /opt/kafka/consumer.sh --group-instance-id instance1820043773 --topic my-topic-526714916-689400629 --group-id my-consumer-group-937902988 --bootstrap-server my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092 --max-messages 100
2022-02-22 23:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:481] Triggering CA cert renewal by adding the annotation
2022-02-22 23:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:493] Patching secret my-cluster-523411585-cluster-ca with strimzi.io/force-replace
2022-02-22 23:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:498] Wait for zk to rolling restart (1)...
2022-02-22 23:09:17 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-523411585-zookeeper rolling update
2022-02-22 23:09:19 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1924425433-1846548893 in namespace namespace-20
2022-02-22 23:09:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-404657524-365820804 in namespace namespace-20
2022-02-22 23:09:29 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-850111446 in namespace namespace-20
2022-02-22 23:09:29 [ForkJoinPool-1-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-20, for cruise control Kafka cluster my-cluster-850111446
2022-02-22 23:09:39 [ForkJoinPool-1-worker-5] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 23:09:39 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-20 for test case:testAutoRenewClientsCaCertsTriggeredByAnno
2022-02-22 23:10:23 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewClientsCaCertsTriggeredByAnno-FINISHED
2022-02-22 23:10:23 [ForkJoinPool-1-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractST:596] Not first test we are gonna generate cluster name
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:160] Creating namespace:namespace-22 for test case:testAutoReplaceAllCaKeysTriggeredByAnno
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-22
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-22
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-22
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:601] Creating a cluster
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Kafka my-cluster-1101004062 in namespace namespace-22
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-22
2022-02-22 23:10:26 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for Kafka: my-cluster-1101004062 will have desired state: Ready
2022-02-22 23:11:02 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-523411585-zookeeper has been successfully rolled
2022-02-22 23:11:02 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:503] Wait for kafka to rolling restart (1)...
2022-02-22 23:11:02 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-523411585-kafka rolling update
2022-02-22 23:14:47 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-523411585-kafka has been successfully rolled
2022-02-22 23:14:47 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:508] Wait for EO to rolling restart (1)...
2022-02-22 23:14:47 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-523411585-entity-operator rolling update
2022-02-22 23:15:20 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] Kafka: my-cluster-1101004062 is in desired state: Ready
2022-02-22 23:15:20 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-957061716-500383839 in namespace namespace-22
2022-02-22 23:15:20 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-22
2022-02-22 23:15:20 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-957061716-500383839 will have desired state: Ready
2022-02-22 23:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-957061716-500383839 is in desired state: Ready
2022-02-22 23:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaTopic my-topic-1907523586-297846632 in namespace namespace-22
2022-02-22 23:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-22
2022-02-22 23:15:21 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaTopic: my-topic-1907523586-297846632 will have desired state: Ready
2022-02-22 23:15:22 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaTopic: my-topic-1907523586-297846632 is in desired state: Ready
2022-02-22 23:15:22 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1101004062-kafka-clients in namespace namespace-22
2022-02-22 23:15:22 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-22
2022-02-22 23:15:22 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-kafka-clients will be ready
2022-02-22 23:15:24 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-kafka-clients is ready
2022-02-22 23:15:24 [ForkJoinPool-1-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-02-22 23:15:24 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:467] Checking produced and consumed messages to pod:my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46
2022-02-22 23:15:24 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1a9e3af8, messages=[], arguments=[--topic, my-topic-1907523586-297846632, --bootstrap-server, my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092, --max-messages, 100], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46', podNamespace='namespace-22', bootstrapServer='my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092', topicName='my-topic-1907523586-297846632', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@251c625c}
2022-02-22 23:15:24 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092:my-topic-1907523586-297846632 from pod my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46
2022-02-22 23:15:24 [ForkJoinPool-1-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46 -n namespace-22 -- /opt/kafka/producer.sh --topic my-topic-1907523586-297846632 --bootstrap-server my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092 --max-messages 100
2022-02-22 23:15:27 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-02-22 23:15:27 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-02-22 23:15:27 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@8b2b14d, messages=[], arguments=[--group-instance-id, instance1759426819, --topic, my-topic-1907523586-297846632, --group-id, my-consumer-group-38592734, --bootstrap-server, my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46', podNamespace='namespace-22', bootstrapServer='my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092', topicName='my-topic-1907523586-297846632', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-38592734', consumerInstanceId='instance1759426819', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4e7a345c}
2022-02-22 23:15:27 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092#my-topic-1907523586-297846632 from pod my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46
2022-02-22 23:15:27 [ForkJoinPool-1-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46 -n namespace-22 -- /opt/kafka/consumer.sh --group-instance-id instance1759426819 --topic my-topic-1907523586-297846632 --group-id my-consumer-group-38592734 --bootstrap-server my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092 --max-messages 100
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:481] Triggering CA cert renewal by adding the annotation
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:493] Patching secret my-cluster-1101004062-cluster-ca with strimzi.io/force-replace
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:493] Patching secret my-cluster-1101004062-clients-ca with strimzi.io/force-replace
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:498] Wait for zk to rolling restart (1)...
2022-02-22 23:15:33 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1101004062-zookeeper rolling update
2022-02-22 23:16:13 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-entity-operator will be ready
2022-02-22 23:17:03 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1101004062-zookeeper has been successfully rolled
2022-02-22 23:17:03 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:503] Wait for kafka to rolling restart (1)...
2022-02-22 23:17:03 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1101004062-kafka rolling update
2022-02-22 23:18:43 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1101004062-kafka has been successfully rolled
2022-02-22 23:18:43 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:508] Wait for EO to rolling restart (1)...
2022-02-22 23:18:43 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1101004062-entity-operator rolling update
2022-02-22 23:19:13 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-entity-operator will be ready
2022-02-22 23:19:41 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-entity-operator is ready
2022-02-22 23:19:51 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-523411585-entity-operator rolling update finished
2022-02-22 23:19:51 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:513] Wait for KafkaExporter and CruiseControl to rolling restart (1)...
2022-02-22 23:19:51 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-523411585-kafka-exporter rolling update
2022-02-22 23:19:51 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-kafka-exporter will be ready
2022-02-22 23:19:55 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-entity-operator is ready
2022-02-22 23:20:05 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1101004062-entity-operator rolling update finished
2022-02-22 23:20:05 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:513] Wait for KafkaExporter and CruiseControl to rolling restart (1)...
2022-02-22 23:20:05 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1101004062-kafka-exporter rolling update
2022-02-22 23:20:05 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-kafka-exporter will be ready
2022-02-22 23:20:19 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-kafka-exporter is ready
2022-02-22 23:20:29 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-523411585-kafka-exporter rolling update finished
2022-02-22 23:20:29 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-523411585-cruise-control rolling update
2022-02-22 23:20:29 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-cruise-control will be ready
2022-02-22 23:20:36 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-kafka-exporter is ready
2022-02-22 23:20:37 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-cruise-control is ready
2022-02-22 23:20:46 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1101004062-kafka-exporter rolling update finished
2022-02-22 23:20:46 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1101004062-cruise-control rolling update
2022-02-22 23:20:46 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-cruise-control will be ready
2022-02-22 23:20:47 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-523411585-cruise-control rolling update finished
2022-02-22 23:20:47 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:519] Wait for zk to rolling restart (2)...
2022-02-22 23:20:47 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-523411585-zookeeper rolling update
2022-02-22 23:20:56 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-cruise-control is ready
2022-02-22 23:21:06 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1101004062-cruise-control rolling update finished
2022-02-22 23:21:06 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:519] Wait for zk to rolling restart (2)...
2022-02-22 23:21:06 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1101004062-zookeeper rolling update
2022-02-22 23:21:42 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-523411585-zookeeper has been successfully rolled
2022-02-22 23:21:42 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-523411585-zookeeper to be ready
2022-02-22 23:22:07 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:524] Wait for kafka to rolling restart (2)...
2022-02-22 23:22:07 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-523411585-kafka rolling update
2022-02-22 23:22:51 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1101004062-zookeeper has been successfully rolled
2022-02-22 23:22:51 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1101004062-zookeeper to be ready
2022-02-22 23:22:57 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-523411585-kafka has been successfully rolled
2022-02-22 23:22:57 [ForkJoinPool-1-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-523411585-kafka to be ready
2022-02-22 23:23:22 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:524] Wait for kafka to rolling restart (2)...
2022-02-22 23:23:22 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1101004062-kafka rolling update
2022-02-22 23:23:25 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:529] Wait for EO to rolling restart (2)...
2022-02-22 23:23:25 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-523411585-entity-operator rolling update
2022-02-22 23:23:30 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-entity-operator will be ready
2022-02-22 23:25:37 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-entity-operator is ready
2022-02-22 23:25:47 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-523411585-entity-operator rolling update finished
2022-02-22 23:25:47 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:534] Wait for KafkaExporter and CruiseControl to rolling restart (2)...
2022-02-22 23:25:47 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-523411585-kafka-exporter rolling update
2022-02-22 23:25:57 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1101004062-kafka has been successfully rolled
2022-02-22 23:25:58 [ForkJoinPool-1-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1101004062-kafka to be ready
2022-02-22 23:26:32 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-kafka-exporter will be ready
2022-02-22 23:26:32 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-kafka-exporter is ready
2022-02-22 23:26:42 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-523411585-kafka-exporter rolling update finished
2022-02-22 23:26:42 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-523411585-cruise-control rolling update
2022-02-22 23:26:42 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-cruise-control will be ready
2022-02-22 23:26:42 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-cruise-control is ready
2022-02-22 23:26:49 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:529] Wait for EO to rolling restart (2)...
2022-02-22 23:26:49 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1101004062-entity-operator rolling update
2022-02-22 23:26:49 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-entity-operator will be ready
2022-02-22 23:26:52 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-523411585-cruise-control rolling update finished
2022-02-22 23:26:52 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:539] Checking the certificates have been replaced
2022-02-22 23:26:52 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:550] Checking consumed messages to pod:my-cluster-523411585-kafka-clients-56c65469c4-gr7df
2022-02-22 23:26:52 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@239d6ec8, messages=[], arguments=[--group-instance-id, instance1099670453, --topic, my-topic-526714916-689400629, --group-id, my-consumer-group-934156086, --bootstrap-server, my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-523411585-kafka-clients-56c65469c4-gr7df', podNamespace='namespace-21', bootstrapServer='my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092', topicName='my-topic-526714916-689400629', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-934156086', consumerInstanceId='instance1099670453', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@359d1faf}
2022-02-22 23:26:52 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092#my-topic-526714916-689400629 from pod my-cluster-523411585-kafka-clients-56c65469c4-gr7df
2022-02-22 23:26:52 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-523411585-kafka-clients-56c65469c4-gr7df -n namespace-21 -- /opt/kafka/consumer.sh --group-instance-id instance1099670453 --topic my-topic-526714916-689400629 --group-id my-consumer-group-934156086 --bootstrap-server my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092 --max-messages 100
2022-02-22 23:26:58 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:26:58 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:26:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-33813835-286867381 in namespace namespace-22
2022-02-22 23:26:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-21
2022-02-22 23:26:58 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-33813835-286867381 will have desired state: Ready
2022-02-22 23:26:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-33813835-286867381 is in desired state: Ready
2022-02-22 23:26:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-523411585-kafka-clients-tls in namespace namespace-22
2022-02-22 23:26:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-21
2022-02-22 23:26:59 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-523411585-kafka-clients-tls will be ready
2022-02-22 23:27:01 [ForkJoinPool-1-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-523411585-kafka-clients-tls is ready
2022-02-22 23:27:01 [ForkJoinPool-1-worker-1] [32mINFO [m [SecurityST:575] Checking consumed messages to pod:my-cluster-523411585-kafka-clients-tls-6cff575568-lxwhs
2022-02-22 23:27:01 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@31d7b597, messages=[], arguments=[--group-instance-id, instance13307708, --topic, my-topic-526714916-689400629, --group-id, my-consumer-group-1883623811, --bootstrap-server, my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-523411585-kafka-clients-tls-6cff575568-lxwhs', podNamespace='namespace-21', bootstrapServer='my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092', topicName='my-topic-526714916-689400629', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1883623811', consumerInstanceId='instance13307708', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6c6aafd2}
2022-02-22 23:27:01 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092#my-topic-526714916-689400629 from pod my-cluster-523411585-kafka-clients-tls-6cff575568-lxwhs
2022-02-22 23:27:01 [ForkJoinPool-1-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-523411585-kafka-clients-tls-6cff575568-lxwhs -n namespace-21 -- /opt/kafka/consumer.sh --group-instance-id instance13307708 --topic my-topic-526714916-689400629 --group-id my-consumer-group-1883623811 --bootstrap-server my-cluster-523411585-kafka-bootstrap.namespace-21.svc:9092 --max-messages 100
2022-02-22 23:27:07 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:27:07 [ForkJoinPool-1-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:27:07 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 23:27:07 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoReplaceClusterCaKeysTriggeredByAnno
2022-02-22 23:27:07 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-523411585-kafka-clients in namespace namespace-21
2022-02-22 23:28:37 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-523411585-kafka-clients-tls in namespace namespace-21
2022-02-22 23:28:37 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-33813835-286867381 in namespace namespace-21
2022-02-22 23:28:47 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1387635120-1751408050 in namespace namespace-21
2022-02-22 23:28:57 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-526714916-689400629 in namespace namespace-21
2022-02-22 23:29:07 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-523411585 in namespace namespace-21
2022-02-22 23:29:07 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-21, for cruise control Kafka cluster my-cluster-523411585
2022-02-22 23:29:17 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 23:29:17 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-21 for test case:testAutoReplaceClusterCaKeysTriggeredByAnno
2022-02-22 23:30:01 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClusterCaKeysTriggeredByAnno-FINISHED
2022-02-22 23:30:01 [ForkJoinPool-1-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 23:31:36 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-entity-operator is ready
2022-02-22 23:31:46 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1101004062-entity-operator rolling update finished
2022-02-22 23:31:46 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:534] Wait for KafkaExporter and CruiseControl to rolling restart (2)...
2022-02-22 23:31:46 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1101004062-kafka-exporter rolling update
2022-02-22 23:32:31 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-kafka-exporter will be ready
2022-02-22 23:32:31 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-kafka-exporter is ready
2022-02-22 23:32:41 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1101004062-kafka-exporter rolling update finished
2022-02-22 23:32:41 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1101004062-cruise-control rolling update
2022-02-22 23:32:41 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-cruise-control will be ready
2022-02-22 23:32:41 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-cruise-control is ready
2022-02-22 23:32:51 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1101004062-cruise-control rolling update finished
2022-02-22 23:32:51 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:539] Checking the certificates have been replaced
2022-02-22 23:32:51 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:550] Checking consumed messages to pod:my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46
2022-02-22 23:32:51 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@27d8b4ef, messages=[], arguments=[--group-instance-id, instance2056412439, --topic, my-topic-1907523586-297846632, --group-id, my-consumer-group-1299435540, --bootstrap-server, my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46', podNamespace='namespace-22', bootstrapServer='my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092', topicName='my-topic-1907523586-297846632', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1299435540', consumerInstanceId='instance2056412439', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@67cb25e3}
2022-02-22 23:32:51 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092#my-topic-1907523586-297846632 from pod my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46
2022-02-22 23:32:51 [ForkJoinPool-1-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1101004062-kafka-clients-7d5bb5465b-7zc46 -n namespace-22 -- /opt/kafka/consumer.sh --group-instance-id instance2056412439 --topic my-topic-1907523586-297846632 --group-id my-consumer-group-1299435540 --bootstrap-server my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092 --max-messages 100
2022-02-22 23:32:57 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:32:57 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:32:57 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update KafkaUser my-user-1021492324-1740743509 in namespace namespace-22
2022-02-22 23:32:57 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-22
2022-02-22 23:32:57 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:394] Wait for KafkaUser: my-user-1021492324-1740743509 will have desired state: Ready
2022-02-22 23:32:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:405] KafkaUser: my-user-1021492324-1740743509 is in desired state: Ready
2022-02-22 23:32:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:152] Create/Update Deployment my-cluster-1101004062-kafka-clients-tls in namespace namespace-22
2022-02-22 23:32:58 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:161] Using Namespace: namespace-22
2022-02-22 23:32:58 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1101004062-kafka-clients-tls will be ready
2022-02-22 23:33:00 [ForkJoinPool-1-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1101004062-kafka-clients-tls is ready
2022-02-22 23:33:00 [ForkJoinPool-1-worker-3] [32mINFO [m [SecurityST:575] Checking consumed messages to pod:my-cluster-1101004062-kafka-clients-tls-6f7674875b-dsbsn
2022-02-22 23:33:00 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2a7b86d3, messages=[], arguments=[--group-instance-id, instance1127087929, --topic, my-topic-1907523586-297846632, --group-id, my-consumer-group-1972361835, --bootstrap-server, my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092, --max-messages, 100], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1101004062-kafka-clients-tls-6f7674875b-dsbsn', podNamespace='namespace-22', bootstrapServer='my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092', topicName='my-topic-1907523586-297846632', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1972361835', consumerInstanceId='instance1127087929', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14a15d99}
2022-02-22 23:33:00 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092#my-topic-1907523586-297846632 from pod my-cluster-1101004062-kafka-clients-tls-6f7674875b-dsbsn
2022-02-22 23:33:00 [ForkJoinPool-1-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1101004062-kafka-clients-tls-6f7674875b-dsbsn -n namespace-22 -- /opt/kafka/consumer.sh --group-instance-id instance1127087929 --topic my-topic-1907523586-297846632 --group-id my-consumer-group-1972361835 --bootstrap-server my-cluster-1101004062-kafka-bootstrap.namespace-22.svc:9092 --max-messages 100
2022-02-22 23:33:06 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-02-22 23:33:06 [ForkJoinPool-1-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-02-22 23:33:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 23:33:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for testAutoReplaceAllCaKeysTriggeredByAnno
2022-02-22 23:33:06 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1101004062-kafka-clients in namespace namespace-22
2022-02-22 23:33:06 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-957061716-500383839 in namespace namespace-22
2022-02-22 23:33:16 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of KafkaTopic my-topic-1907523586-297846632 in namespace namespace-22
2022-02-22 23:33:26 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Kafka my-cluster-1101004062 in namespace namespace-22
2022-02-22 23:33:26 [ForkJoinPool-1-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-22, for cruise control Kafka cluster my-cluster-1101004062
2022-02-22 23:33:37 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of Deployment my-cluster-1101004062-kafka-clients-tls in namespace namespace-22
2022-02-22 23:34:27 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of KafkaUser my-user-1021492324-1740743509 in namespace namespace-22
2022-02-22 23:34:27 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 23:34:27 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSuiteNamespaceManager:197] Deleting namespace:namespace-22 for test case:testAutoReplaceAllCaKeysTriggeredByAnno
2022-02-22 23:34:33 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceAllCaKeysTriggeredByAnno-FINISHED
2022-02-22 23:34:33 [ForkJoinPool-1-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-02-22 23:34:33 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 23:34:33 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:307] In context SecurityST is everything deleted.
2022-02-22 23:34:33 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
[[1;34mINFO[m] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9,944.969 s - in io.strimzi.systemtest.security.SecurityST
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [SetupClusterOperator:586] ============================================================================
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [SetupClusterOperator:587] Un-installing cluster operator from infra-namespace namespace
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [SetupClusterOperator:588] ============================================================================
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:305] ############################################################################
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:309] Delete all resources for JUnit Jupiter
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-02-22 23:34:39 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-02-22 23:34:39 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-02-22 23:34:49 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-02-22 23:34:49 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-02-22 23:34:59 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-02-22 23:34:59 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-02-22 23:35:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-02-22 23:35:09 [ForkJoinPool-1-worker-1] [32mINFO [m [ResourceManager:231] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-02-22 23:35:19 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-02-22 23:35:19 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:231] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-02-22 23:35:19 [ForkJoinPool-1-worker-3] [32mINFO [m [ResourceManager:330] ############################################################################
2022-02-22 23:35:29 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-02-22 23:35:29 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-02-22 23:35:29 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-02-22 23:35:29 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-02-22 23:35:29 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift . [1;32mSUCCESS[m [  5.349 s]
[[1;34mINFO[m] test ............................................... [1;32mSUCCESS[m [  2.021 s]
[[1;34mINFO[m] crd-annotations .................................... [1;32mSUCCESS[m [  1.923 s]
[[1;34mINFO[m] crd-generator ...................................... [1;32mSUCCESS[m [  4.227 s]
[[1;34mINFO[m] api ................................................ [1;32mSUCCESS[m [ 11.821 s]
[[1;34mINFO[m] mockkube ........................................... [1;32mSUCCESS[m [  2.040 s]
[[1;34mINFO[m] config-model ....................................... [1;32mSUCCESS[m [  1.366 s]
[[1;34mINFO[m] certificate-manager ................................ [1;32mSUCCESS[m [  1.539 s]
[[1;34mINFO[m] operator-common .................................... [1;32mSUCCESS[m [  3.042 s]
[[1;34mINFO[m] systemtest ......................................... [1;32mSUCCESS[m [  02:46 h]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  02:47 h
[[1;34mINFO[m] Finished at: 2022-02-22T18:35:30-05:00
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
