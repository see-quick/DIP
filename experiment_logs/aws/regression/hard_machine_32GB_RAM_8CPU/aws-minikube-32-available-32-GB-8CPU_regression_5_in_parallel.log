[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6.1 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, generex-1.0.2.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 61 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] kubernetes-model-storageclass-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-apiextensions-5.12.0.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-discovery-5.12.0.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-metrics-5.12.0.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-flowcontrol-5.12.0.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-events-5.12.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] automaton-1.11-8.jar, crd-generator-0.29.0-SNAPSHOT.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 114 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-coordination-5.12.0.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] zjsonpatch-0.3.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] kubernetes-model-admissionregistration-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-certificates-5.12.0.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-datatype-jsr310-2.13.1.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] crd-annotations-0.29.0-SNAPSHOT.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[[1;33mWARNING[m]   - 7 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.36.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-node-5.12.0.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] jackson-databind-2.12.6.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 700 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 690 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, snakeyaml-1.27.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] kubernetes-model-policy-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 32 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Changes detected - recompiling the module!
[[1;34mINFO[m] Compiling 68 source files to /home/ec2-user/strimzi-kafka-operator/systemtest/target/test-classes
[[1;34mINFO[m] /home/ec2-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/security/oauth/OauthPlainIsolatedST.java: Some input files use or override a deprecated API.
[[1;34mINFO[m] /home/ec2-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/security/oauth/OauthPlainIsolatedST.java: Recompile with -Xlint:deprecation for details.
[[1;34mINFO[m] /home/ec2-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java: /home/ec2-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java uses unchecked or unsafe operations.
[[1;34mINFO[m] /home/ec2-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java: Recompile with -Xlint:unchecked for details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/ec2-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-04-09T08-27-13_674-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.tracing.TracingST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.ListenersST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.MultipleListenersST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.KafkaST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.ConfigProviderST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.custom.CustomAuthorizerST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.OpaIntegrationST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.log.LoggingChangeST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.log.LogSettingST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.FeatureGatesIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.RecoveryIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.HelmChartIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.SpecificIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.DrainCleanerIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectBuilderIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.JmxIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.dump.LogDumpScriptIsolatedST
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-04-09 08:27:42 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.user.UserST
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:219] Used environment variables:
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:220] CONFIG: /home/ec2-user/strimzi-kafka-operator/systemtest/config.json
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SKIP_TEARDOWN: false
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] LB_FINALIZERS: false
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_ORG: strimzi
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_LOG_DIR: /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_REGISTRY: quay.io
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_TAG: latest
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAME: community-operators
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_FEATURE_GATES: 
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] BRIDGE_IMAGE: latest-released
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_VERSION: 
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-04-09 08:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 08:27:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 08:27:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 08:27:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 08:27:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 08:28:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 08:28:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: user-st
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: user-st
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-04-09 08:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-04-09 08:29:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-04-09 08:29:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-04-09 08:29:37 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:37 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:37 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-04-09 08:29:37 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-04-09 08:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-04-09 08:29:37 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:37 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:37 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testTlsExternalUser
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-04-09 08:29:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1670489752-672801536 in namespace user-st
2022-04-09 08:29:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-04-09 08:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-202202601-624474910 in namespace user-st
2022-04-09 08:29:37 [ForkJoinPool-3-worker-13] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-04-09 08:29:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-659671699-506853401 in namespace user-st
2022-04-09 08:29:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1670489752-672801536 will have desired state: Ready
2022-04-09 08:29:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-04-09 08:29:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-659671699-506853401 will have desired state: Ready
2022-04-09 08:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-202202601-624474910 will have desired state: Ready
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-04-09 08:29:37 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-04-09 08:29:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-49088747 in namespace namespace-0
2022-04-09 08:29:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-04-09 08:29:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-49088747 will have desired state: Ready
2022-04-09 08:29:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1670489752-672801536 is in desired state: Ready
2022-04-09 08:29:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-04-09 08:29:38 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-04-09 08:29:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-202202601-624474910 is in desired state: Ready
2022-04-09 08:29:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-659671699-506853401 is in desired state: Ready
2022-04-09 08:29:38 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-04-09 08:29:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-04-09 08:29:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:29:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testUserTemplate
2022-04-09 08:29:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-659671699-506853401 in namespace user-st
2022-04-09 08:29:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-04-09 08:29:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-04-09 08:29:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-04-09 08:29:40 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-1670489752-672801536
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [SecretUtils:50] Secret my-user-1670489752-672801536 created
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1670489752-672801536 will have desired state: Ready
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1670489752-672801536 is in desired state: Ready
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1670489752-672801536
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-1670489752-672801536 deleted
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateUser
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1670489752-672801536 in namespace user-st
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-04-09 08:29:40 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:41 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-04-09 08:29:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:29:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-04-09 08:29:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-04-09 08:29:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-202202601-624474910
2022-04-09 08:29:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:29:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-202202601-624474910
2022-04-09 08:29:41 [ForkJoinPool-3-worker-5] [33mWARN [m [KafkaUserUtils:68] KafkaUser my-user-202202601-624474910 is not deleted yet! Triggering force delete by cmd client!
2022-04-09 08:29:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-202202601-624474910 deleted
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-202202601-624474910
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:29:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-202202601-624474910 in namespace user-st
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-04-09 08:29:45 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:45 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-04-09 08:29:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b52286f3 in namespace namespace-1
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-09 08:29:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b52286f3 will have desired state: Ready
2022-04-09 08:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-04-09 08:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-04-09 08:29:49 [ForkJoinPool-3-worker-9] [33mWARN [m [KafkaUserUtils:68] KafkaUser encrypted-arnost is not deleted yet! Triggering force delete by cmd client!
2022-04-09 08:29:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-04-09 08:29:50 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-04-09 08:29:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:29:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-04-09 08:29:53 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:29:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-04-09 08:29:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-04-09 08:29:57 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-04-09 08:29:57 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:29:57 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-04-09 08:29:57 [ForkJoinPool-3-worker-5] [33mWARN [m [KafkaUserUtils:68] KafkaUser scramed-arnost is not deleted yet! Triggering force delete by cmd client!
2022-04-09 08:29:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-04-09 08:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-04-09 08:30:01 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-04-09 08:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:30:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-49088747 is in desired state: Ready
2022-04-09 08:30:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1791205572-713602131 in namespace namespace-0
2022-04-09 08:30:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-04-09 08:30:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1791205572-713602131 will have desired state: Ready
2022-04-09 08:30:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1791205572-713602131 is in desired state: Ready
2022-04-09 08:30:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1791205572-713602131 will have desired state: Ready
2022-04-09 08:30:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1791205572-713602131 is in desired state: Ready
2022-04-09 08:30:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:30:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-04-09 08:30:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1791205572-713602131 in namespace namespace-0
2022-04-09 08:30:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-49088747 in namespace namespace-0
2022-04-09 08:31:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:31:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testTlsExternalUser
2022-04-09 08:31:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b52286f3 is in desired state: Ready
2022-04-09 08:31:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1548811771-641032517 in namespace namespace-1
2022-04-09 08:31:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-09 08:31:10 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-04-09 08:31:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1548811771-641032517 will have desired state: Ready
2022-04-09 08:31:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1548811771-641032517 is in desired state: Ready
2022-04-09 08:31:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-1
2022-04-09 08:31:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-09 08:31:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-04-09 08:31:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-04-09 08:31:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-1
2022-04-09 08:31:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-09 08:31:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-04-09 08:31:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-04-09 08:31:13 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:346] Deploying KafkaClients pod for TLS listener
2022-04-09 08:31:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b52286f3-tls-kafka-clients in namespace namespace-1
2022-04-09 08:31:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-09 08:31:13 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b52286f3-tls-kafka-clients will be ready
2022-04-09 08:31:14 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-04-09 08:31:14 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b52286f3-tls-kafka-clients is ready
2022-04-09 08:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-04-09 08:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b52286f3-plain-kafka-clients in namespace namespace-1
2022-04-09 08:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-09 08:31:15 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b52286f3-plain-kafka-clients will be ready
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b52286f3-plain-kafka-clients is ready
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:357] Checking if user secrets with secret prefixes exists
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:373] Checking if TLS user is able to send messages
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4994bf49, messages=[], arguments=[--max-messages, 100, USER=top_secret_encrypted_leopold, --bootstrap-server, my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093, --topic, my-topic-83383454-4042747], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b52286f3-tls-kafka-clients-5bb9dc84c9-4j2l7', podNamespace='namespace-1', bootstrapServer='my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@31578839}
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093:my-topic-83383454-4042747 from pod my-cluster-b52286f3-tls-kafka-clients-5bb9dc84c9-4j2l7
2022-04-09 08:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b52286f3-tls-kafka-clients-5bb9dc84c9-4j2l7 -n namespace-1 -- /opt/kafka/producer.sh --max-messages 100 USER=top_secret_encrypted_leopold --bootstrap-server my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093 --topic my-topic-83383454-4042747
2022-04-09 08:31:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 08:31:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 08:31:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@60544990, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-165212564, --group-instance-id, instance1625951773, USER=top_secret_encrypted_leopold, --bootstrap-server, my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093, --topic, my-topic-83383454-4042747], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b52286f3-tls-kafka-clients-5bb9dc84c9-4j2l7', podNamespace='namespace-1', bootstrapServer='my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-165212564', consumerInstanceId='instance1625951773', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7896bee2}
2022-04-09 08:31:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093:my-topic-83383454-4042747 from pod my-cluster-b52286f3-tls-kafka-clients-5bb9dc84c9-4j2l7
2022-04-09 08:31:21 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b52286f3-tls-kafka-clients-5bb9dc84c9-4j2l7 -n namespace-1 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-165212564 --group-instance-id instance1625951773 USER=top_secret_encrypted_leopold --bootstrap-server my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9093 --topic my-topic-83383454-4042747
2022-04-09 08:31:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 08:31:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 08:31:28 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-04-09 08:31:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@56f384a0, messages=[], arguments=[--max-messages, 100, USER=top_secret_scramed_leopold, --bootstrap-server, my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092, --topic, my-topic-83383454-4042747], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b52286f3-plain-kafka-clients-866fdc7fb6-ts29h', podNamespace='namespace-1', bootstrapServer='my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@224d296e}
2022-04-09 08:31:28 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092:my-topic-83383454-4042747 from pod my-cluster-b52286f3-plain-kafka-clients-866fdc7fb6-ts29h
2022-04-09 08:31:28 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b52286f3-plain-kafka-clients-866fdc7fb6-ts29h -n namespace-1 -- /opt/kafka/producer.sh --max-messages 100 USER=top_secret_scramed_leopold --bootstrap-server my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092 --topic my-topic-83383454-4042747
2022-04-09 08:31:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 08:31:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 08:31:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@59859310, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-165212564, --group-instance-id, instance517139199, USER=top_secret_scramed_leopold, --bootstrap-server, my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092, --topic, my-topic-83383454-4042747], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b52286f3-plain-kafka-clients-866fdc7fb6-ts29h', podNamespace='namespace-1', bootstrapServer='my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-165212564', consumerInstanceId='instance517139199', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1bdf5132}
2022-04-09 08:31:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092#my-topic-83383454-4042747 from pod my-cluster-b52286f3-plain-kafka-clients-866fdc7fb6-ts29h
2022-04-09 08:31:31 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b52286f3-plain-kafka-clients-866fdc7fb6-ts29h -n namespace-1 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-165212564 --group-instance-id instance517139199 USER=top_secret_scramed_leopold --bootstrap-server my-cluster-b52286f3-kafka-bootstrap.namespace-1.svc:9092 --topic my-topic-83383454-4042747
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:398] Deleting KafkaUser:scramed-leopold
2022-04-09 08:31:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-04-09 08:31:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-04-09 08:31:59 [ForkJoinPool-3-worker-7] [32mINFO [m [UserST:402] Checking if secrets are deleted
2022-04-09 08:31:59 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-1
2022-04-09 08:32:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1548811771-641032517 in namespace namespace-1
2022-04-09 08:32:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b52286f3-tls-kafka-clients in namespace namespace-1
2022-04-09 08:32:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b52286f3-plain-kafka-clients in namespace namespace-1
2022-04-09 08:32:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b52286f3 in namespace namespace-1
2022-04-09 08:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-1
2022-04-09 08:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:32:50 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-04-09 08:32:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-04-09 08:32:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:32:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:32:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for UserST
2022-04-09 08:32:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-04-09 08:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 12, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 333.922 s - in io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-04-09 08:33:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-04-09 08:33:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-04-09 08:33:16 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-04-09 08:33:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-04-09 08:33:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-04-09 08:33:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-04-09 08:34:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-04-09 08:34:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:34:30 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:34:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:34:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-04-09 08:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-04-09 08:34:30 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-04-09 08:34:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-04-09 08:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:34:30 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:34:30 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2039494316-2029760590 in namespace throttling-quota-st
2022-04-09 08:34:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-864396078-1723431563 in namespace throttling-quota-st
2022-04-09 08:34:30 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:34:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1396494281-1035703219 in namespace throttling-quota-st
2022-04-09 08:34:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-124070705-1111413178 in namespace throttling-quota-st
2022-04-09 08:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2039494316-2029760590 will have desired state: Ready
2022-04-09 08:34:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1396494281-1035703219 will have desired state: Ready
2022-04-09 08:34:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-124070705-1111413178 will have desired state: Ready
2022-04-09 08:34:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-864396078-1723431563 will have desired state: Ready
2022-04-09 08:34:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2039494316-2029760590 is in desired state: Ready
2022-04-09 08:34:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1396494281-1035703219 is in desired state: Ready
2022-04-09 08:34:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-124070705-1111413178 is in desired state: Ready
2022-04-09 08:34:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:34:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-864396078-1723431563 is in desired state: Ready
2022-04-09 08:34:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-7eb0959b-kafka-clients in namespace throttling-quota-st
2022-04-09 08:34:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-e435e330-kafka-clients in namespace throttling-quota-st
2022-04-09 08:34:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-04-09 08:34:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:34:31 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-65375b7c-kafka-clients will be in active state
2022-04-09 08:34:31 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:34:31 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-7eb0959b-kafka-clients will be in active state
2022-04-09 08:34:31 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-e435e330-kafka-clients will be in active state
2022-04-09 08:34:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:34:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:34:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:34:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:36:03 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-65375b7c-kafka-clients-xq6wl log
2022-04-09 08:36:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:36:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-0d06aa56-kafka-clients-r8b8x log
2022-04-09 08:36:08 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job list-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:36:08 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: list-admin-my-cluster-65375b7c-kafka-clients will be in active state
2022-04-09 08:36:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-04-09 08:36:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:36:08 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:36:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:36:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:36:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-65375b7c-kafka-clients-p2286 log
2022-04-09 08:36:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:36:26 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-65375b7c-kafka-clients will be in active state
2022-04-09 08:36:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:37:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:37:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-0d06aa56-kafka-clients-s8hm4 log
2022-04-09 08:37:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-04-09 08:37:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:37:48 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:37:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:37:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-65375b7c-kafka-clients-nzjff log
2022-04-09 08:38:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job list-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:38:02 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: list-admin-my-cluster-65375b7c-kafka-clients will be in active state
2022-04-09 08:38:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-65375b7c-kafka-clients to finished
2022-04-09 08:38:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:38:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-04-09 08:38:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job list-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:38:06 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:38:06 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2039494316-2029760590 in namespace throttling-quota-st
2022-04-09 08:38:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job list-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:38:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-65375b7c-kafka-clients in namespace throttling-quota-st
2022-04-09 08:38:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:38:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-04-09 08:38:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:38:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-e435e330-kafka-clients-6kstq log
2022-04-09 08:38:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-7eb0959b-kafka-clients-scs8k log
2022-04-09 08:38:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-7eb0959b-kafka-clients in namespace throttling-quota-st
2022-04-09 08:38:40 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-7eb0959b-kafka-clients will be in active state
2022-04-09 08:38:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-04-09 08:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-e435e330-kafka-clients in namespace throttling-quota-st
2022-04-09 08:39:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-124070705-1111413178 in namespace throttling-quota-st
2022-04-09 08:39:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:39:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-04-09 08:39:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:39:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:39:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-0d06aa56-kafka-clients-v6fjd log
2022-04-09 08:39:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-04-09 08:39:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:39:28 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:39:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:39:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-7eb0959b-kafka-clients-tbf8k log
2022-04-09 08:39:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job alter-admin-my-cluster-7eb0959b-kafka-clients in namespace throttling-quota-st
2022-04-09 08:39:58 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: alter-admin-my-cluster-7eb0959b-kafka-clients will be in active state
2022-04-09 08:39:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:41:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:41:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-0d06aa56-kafka-clients-pvlvv log
2022-04-09 08:41:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-04-09 08:41:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:41:09 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:41:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:42:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:42:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-0d06aa56-kafka-clients-shzvd log
2022-04-09 08:42:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:42:49 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:42:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 08:44:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-7eb0959b-kafka-clients-r7frb log
2022-04-09 08:44:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-04-09 08:44:15 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-04-09 08:44:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-04-09 08:45:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:45:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-04-09 08:45:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-7eb0959b-kafka-clients in namespace throttling-quota-st
2022-04-09 08:45:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-7eb0959b-kafka-clients in namespace throttling-quota-st
2022-04-09 08:45:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1396494281-1035703219 in namespace throttling-quota-st
2022-04-09 08:45:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-04-09 08:45:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job alter-admin-my-cluster-7eb0959b-kafka-clients in namespace throttling-quota-st
2022-04-09 08:45:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:45:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-04-09 08:45:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:46:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-0d06aa56-kafka-clients-r2kn4 log
2022-04-09 08:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-0d06aa56-kafka-clients.
2022-04-09 08:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:48:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-0d06aa56-kafka-clients.
2022-04-09 08:48:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:48:04 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:48:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-0d06aa56-kafka-clients.
2022-04-09 08:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:49:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:50:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-0d06aa56-kafka-clients.
2022-04-09 08:50:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:50:21 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:50:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:51:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-0d06aa56-kafka-clients.
2022-04-09 08:51:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:51:30 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-0d06aa56-kafka-clients will be in active state
2022-04-09 08:51:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-0d06aa56-kafka-clients to finished
2022-04-09 08:52:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:52:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-04-09 08:52:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-864396078-1723431563 in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-0d06aa56-kafka-clients in namespace throttling-quota-st
2022-04-09 08:52:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:52:49 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-04-09 08:52:49 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:52:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ThrottlingQuotaST:353] Tearing down resources after all test
2022-04-09 08:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-04-09 08:53:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-04-09 08:53:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,249.102 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.topic.TopicST
2022-04-09 08:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: topic-st
2022-04-09 08:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: topic-st
2022-04-09 08:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-04-09 08:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:494] Deploying shared Kafka across all test cases in topic-st namespace
2022-04-09 08:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-04-09 08:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-04-09 08:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-04-09 08:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-04-09 08:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-be81141d-isolated in namespace topic-st
2022-04-09 08:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-be81141d-isolated will have desired state: Ready
2022-04-09 08:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-be81141d-isolated is in desired state: Ready
2022-04-09 08:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-be81141d-isolated-kafka-clients in namespace topic-st
2022-04-09 08:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-be81141d-isolated-kafka-clients will be ready
2022-04-09 08:56:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-be81141d-isolated-kafka-clients is ready
2022-04-09 08:56:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1090432372-2100406675 in namespace topic-st
2022-04-09 08:56:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1090432372-2100406675 will have desired state: Ready
2022-04-09 08:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1090432372-2100406675 is in desired state: Ready
2022-04-09 08:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 08:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3e50d93a, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1090432372-2100406675], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-be81141d-isolated-kafka-clients-694c6f694b-gcm5v', podNamespace='topic-st', bootstrapServer='my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1090432372-2100406675', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6ffb7d67}
2022-04-09 08:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-1090432372-2100406675 from pod my-cluster-be81141d-isolated-kafka-clients-694c6f694b-gcm5v
2022-04-09 08:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be81141d-isolated-kafka-clients-694c6f694b-gcm5v -n topic-st -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1090432372-2100406675
2022-04-09 08:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 08:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 08:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:398] Deleting KafkaTopic: my-topic-1090432372-2100406675
2022-04-09 08:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:400] KafkaTopic my-topic-1090432372-2100406675 deleted
2022-04-09 08:58:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:404] Wait KafkaTopic my-topic-1090432372-2100406675 recreation
2022-04-09 08:58:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1090432372-2100406675 creation 
2022-04-09 08:58:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:406] KafkaTopic my-topic-1090432372-2100406675 recreated
2022-04-09 08:58:21 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1646e5c2, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-330890902, --group-instance-id, instance534016480, --bootstrap-server, my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1090432372-2100406675], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-be81141d-isolated-kafka-clients-694c6f694b-gcm5v', podNamespace='topic-st', bootstrapServer='my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1090432372-2100406675', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-330890902', consumerInstanceId='instance534016480', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4d57506d}
2022-04-09 08:58:21 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-1090432372-2100406675 from pod my-cluster-be81141d-isolated-kafka-clients-694c6f694b-gcm5v
2022-04-09 08:58:21 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be81141d-isolated-kafka-clients-694c6f694b-gcm5v -n topic-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-330890902 --group-instance-id instance534016480 --bootstrap-server my-cluster-be81141d-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1090432372-2100406675
2022-04-09 08:58:26 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 08:58:26 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 08:58:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:58:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-04-09 08:58:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-be81141d-isolated-kafka-clients in namespace topic-st
2022-04-09 08:58:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1090432372-2100406675 in namespace topic-st
2022-04-09 08:58:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-be81141d-isolated in namespace topic-st
2022-04-09 08:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-04-09 08:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-04-09 08:59:06 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-04-09 08:59:06 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 08:59:06 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-04-09 08:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-04-09 08:59:06 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-04-09 08:59:06 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:59:06 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:59:06 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-04-09 08:59:06 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 08:59:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-04-09 08:59:06 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1366434234-562723657 in namespace topic-st
2022-04-09 08:59:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-511344029-1032895516 in namespace topic-st
2022-04-09 08:59:06 [ForkJoinPool-3-worker-11] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic my-topic-1366434234-562723657 exists
2022-04-09 08:59:06 [ForkJoinPool-3-worker-11] [32mINFO [m [TopicST:459] Checking topic my-topic-1366434234-562723657 in Kafka
2022-04-09 08:59:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-511344029-1032895516 will have desired state: Ready
2022-04-09 08:59:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-04-09 08:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-04-09 08:59:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-511344029-1032895516 is in desired state: Ready
2022-04-09 08:59:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-04-09 08:59:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-04-09 08:59:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-511344029-1032895516 will have desired state: NotReady
2022-04-09 08:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-04-09 08:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 08:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:323] Checking if my-topic-1006185193-62011103 is on topic list
2022-04-09 08:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:459] Checking topic my-topic-1006185193-62011103 in Kafka
2022-04-09 08:59:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-511344029-1032895516 is in desired state: NotReady
2022-04-09 08:59:08 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-04-09 08:59:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-04-09 08:59:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-511344029-1032895516 deletion
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-511344029-1032895516 in namespace topic-st
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-04-09 08:59:09 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:59:09 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-1097727481-1343210572 --replication-factor 3 --partitions 3
2022-04-09 08:59:09 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:09 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1097727481-1343210572 creation 
2022-04-09 08:59:09 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:09 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1366434234-562723657 will have desired state: NotReady
2022-04-09 08:59:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1366434234-562723657 is in desired state: NotReady
2022-04-09 08:59:09 [ForkJoinPool-3-worker-11] [32mINFO [m [TopicST:91] Delete topic my-topic-1366434234-562723657
2022-04-09 08:59:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-04-09 08:59:10 [ForkJoinPool-3-worker-13] [32mINFO [m [TopicST:459] Checking topic topic-with-replication-to-change in Kafka
2022-04-09 08:59:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-1366434234-562723657 deletion
2022-04-09 08:59:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-04-09 08:59:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-04-09 08:59:10 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:485] Checking in KafkaTopic CR that topic my-topic-1097727481-1343210572 was created with expected settings
2022-04-09 08:59:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-04-09 08:59:11 [ForkJoinPool-3-worker-11] [32mINFO [m [TopicST:459] Checking topic topic-example-new in Kafka
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:326] Topic with name my-topic-1006185193-62011103 is not created yet
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:328] Trying to send messages to non-existing topic my-topic-1006185193-62011103
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6cf79e0e, messages=[], arguments=[--max-messages, 100, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1006185193-62011103], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-7dd7cb68d4-f82ls', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1006185193-62011103', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@183290e2}
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-1006185193-62011103 from pod topic-cluster-name-kafka-clients-7dd7cb68d4-f82ls
2022-04-09 08:59:12 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec topic-cluster-name-kafka-clients-7dd7cb68d4-f82ls -n topic-st -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1006185193-62011103
2022-04-09 08:59:12 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:12 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:12 [ForkJoinPool-3-worker-13] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-04-09 08:59:12 [ForkJoinPool-3-worker-13] [32mINFO [m [TopicST:459] Checking topic another-topic in Kafka
2022-04-09 08:59:14 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:14 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:14 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:122] Editing topic via Kafka, settings to partitions 5
2022-04-09 08:59:14 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:14 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:14 [ForkJoinPool-3-worker-11] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic topic-example-new exists
2022-04-09 08:59:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:59:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-04-09 08:59:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-04-09 08:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1366434234-562723657 in namespace topic-st
2022-04-09 08:59:14 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 08:59:14 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 08:59:14 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@fc20fe4, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-104784551, --group-instance-id, instance1769641721, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1006185193-62011103], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-7dd7cb68d4-f82ls', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1006185193-62011103', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-104784551', consumerInstanceId='instance1769641721', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1429c32b}
2022-04-09 08:59:14 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-1006185193-62011103 from pod topic-cluster-name-kafka-clients-7dd7cb68d4-f82ls
2022-04-09 08:59:14 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec topic-cluster-name-kafka-clients-7dd7cb68d4-f82ls -n topic-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-104784551 --group-instance-id instance1769641721 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1006185193-62011103
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic another-topic exists
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-04-09 08:59:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-04-09 08:59:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:59:16 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-1097727481-1343210572 --partitions 5
2022-04-09 08:59:16 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:16 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-1097727481-1343210572
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-1097727481-1343210572
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:473] Checking topic my-topic-1097727481-1343210572 in Kafka topic-cluster-name
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-04-09 08:59:19 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 08:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 08:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 08:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:344] Checking if my-topic-1006185193-62011103 is on topic list
2022-04-09 08:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:459] Checking topic my-topic-1006185193-62011103 in Kafka
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1006185193-62011103 creation 
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [TopicST:356] Topic successfully created
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-04-09 08:59:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-04-09 08:59:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 08:59:24 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-04-09 08:59:24 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:00:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:00:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-04-09 09:00:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for TopicST
2022-04-09 09:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-04-09 09:00:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 411.263 s - in io.strimzi.systemtest.operators.topic.TopicST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.ReconciliationST
2022-04-09 09:00:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1e585fe0 in namespace namespace-2
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-04-09 09:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1e585fe0 will have desired state: Ready
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b1461f65 in namespace namespace-3
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-09 09:00:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b1461f65 will have desired state: Ready
2022-04-09 09:02:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b1461f65 is in desired state: Ready
2022-04-09 09:02:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-04-09 09:02:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-04-09 09:02:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b1461f65 will have desired state: ReconciliationPaused
2022-04-09 09:02:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b1461f65 is in desired state: ReconciliationPaused
2022-04-09 09:02:52 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-b1461f65-kafka will have stable 3 replicas
2022-04-09 09:02:52 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-09 09:02:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-09 09:02:54 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-09 09:02:55 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-09 09:02:56 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-09 09:02:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-09 09:02:58 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-09 09:02:59 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-09 09:03:00 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-09 09:03:01 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-09 09:03:02 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-09 09:03:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-09 09:03:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-09 09:03:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-09 09:03:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-09 09:03:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-09 09:03:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-09 09:03:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-09 09:03:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-09 09:03:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-09 09:03:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:228] Pod my-cluster-b1461f65-kafka has 3 replicas
2022-04-09 09:03:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-04-09 09:03:11 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-b1461f65-kafka to be ready
2022-04-09 09:03:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1e585fe0 is in desired state: Ready
2022-04-09 09:03:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2143669430-1585115070 in namespace namespace-3
2022-04-09 09:03:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-04-09 09:03:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2143669430-1585115070 will have desired state: Ready
2022-04-09 09:03:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2143669430-1585115070 is in desired state: Ready
2022-04-09 09:03:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-04-09 09:03:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2143669430-1585115070 will have desired state: ReconciliationPaused
2022-04-09 09:03:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2143669430-1585115070 is in desired state: ReconciliationPaused
2022-04-09 09:03:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:03:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:03:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:03:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:03:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-04-09 09:03:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:03:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:03:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-04-09 09:03:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:03:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:03:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-04-09 09:04:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-04-09 09:04:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-04-09 09:04:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-04-09 09:04:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-04-09 09:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-04-09 09:04:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-04-09 09:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-04-09 09:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-04-09 09:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-04-09 09:04:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-04-09 09:04:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-04-09 09:04:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-04-09 09:04:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-04-09 09:04:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:04:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:04:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-04-09 09:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:05:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-04-09 09:05:04 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:05:04 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:05:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-1e585fe0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2143669430-1585115070 --describe --bootstrap-server my-cluster-1e585fe0-kafka-bootstrap:9092
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-2143669430-1585115070
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-1e585fe0 in namespace namespace-3
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-04-09 09:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1e585fe0 will have desired state: PendingProposal
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b1461f65 will have desired state: Ready
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b1461f65 is in desired state: Ready
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-b1461f65 is ready
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b1461f65-kafka-clients in namespace namespace-3
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-09 09:05:09 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1461f65-kafka-clients will be ready
2022-04-09 09:05:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1e585fe0 is in desired state: PendingProposal
2022-04-09 09:05:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:164] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-04-09 09:05:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1e585fe0 will have desired state: ProposalReady
2022-04-09 09:05:11 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1461f65-kafka-clients is ready
2022-04-09 09:05:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b1461f65-scraper in namespace namespace-3
2022-04-09 09:05:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-09 09:05:11 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1461f65-scraper will be ready
2022-04-09 09:05:13 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1461f65-scraper is ready
2022-04-09 09:05:13 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-b1461f65-scraper to be ready
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-b1461f65-scraper is ready
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-b1461f65-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-b1461f65-allow in namespace namespace-3
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-b1461f65 in namespace namespace-3
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-04-09 09:05:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-b1461f65 will have desired state: ReconciliationPaused
2022-04-09 09:05:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-b1461f65 is in desired state: ReconciliationPaused
2022-04-09 09:05:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-b1461f65-connect will have stable 0 replicas
2022-04-09 09:05:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-09 09:05:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-09 09:05:26 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-09 09:05:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-09 09:05:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-09 09:05:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-09 09:05:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-09 09:05:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-09 09:05:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-09 09:05:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-09 09:05:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-09 09:05:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-09 09:05:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-09 09:05:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-09 09:05:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-09 09:05:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-09 09:05:40 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-09 09:05:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-09 09:05:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-09 09:05:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-09 09:05:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:228] Pod my-cluster-b1461f65-connect has 0 replicas
2022-04-09 09:05:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-04-09 09:05:43 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1461f65-connect will be ready
2022-04-09 09:06:47 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1461f65-connect is ready
2022-04-09 09:06:47 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-b1461f65-connect to be ready
2022-04-09 09:06:57 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-b1461f65-connect is ready
2022-04-09 09:06:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-b1461f65 in namespace namespace-3
2022-04-09 09:06:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-09 09:06:57 [ForkJoinPool-3-worker-11] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-04-09 09:06:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-b1461f65 will have desired state: Ready
2022-04-09 09:06:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-b1461f65 is in desired state: Ready
2022-04-09 09:06:58 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:06:58 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:06:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-04-09 09:06:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-b1461f65 will have desired state: ReconciliationPaused
2022-04-09 09:06:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-b1461f65 is in desired state: ReconciliationPaused
2022-04-09 09:06:59 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:06:59 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:06:59 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-04-09 09:07:00 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:00 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:00 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-04-09 09:07:02 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:02 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:02 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-04-09 09:07:03 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:03 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:03 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-04-09 09:07:04 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:04 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:04 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-04-09 09:07:05 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:05 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:05 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-04-09 09:07:06 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:06 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-04-09 09:07:08 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:08 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:08 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-04-09 09:07:09 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:09 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:09 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-04-09 09:07:10 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:10 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-04-09 09:07:11 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:11 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:11 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-04-09 09:07:12 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:12 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:12 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-04-09 09:07:14 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:14 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:14 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-04-09 09:07:15 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:15 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:15 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-04-09 09:07:16 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:16 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:16 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-04-09 09:07:17 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:17 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:17 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-04-09 09:07:19 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:19 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:19 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-04-09 09:07:20 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:20 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:20 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-04-09 09:07:21 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:21 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-04-09 09:07:22 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65
2022-04-09 09:07:22 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:22 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-04-09 09:07:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-04-09 09:07:22 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65/config
2022-04-09 09:07:22 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:23 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-b1461f65-connect-7b594bb46f-fxlrq -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-b1461f65/config
2022-04-09 09:07:23 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:07:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:07:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-04-09 09:07:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-b1461f65-allow in namespace namespace-3
2022-04-09 09:07:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-b1461f65 in namespace namespace-3
2022-04-09 09:07:23 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-b1461f65 in namespace namespace-3
2022-04-09 09:07:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b1461f65-kafka-clients in namespace namespace-3
2022-04-09 09:07:23 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b1461f65 in namespace namespace-3
2022-04-09 09:07:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b1461f65-scraper in namespace namespace-3
2022-04-09 09:08:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:08:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-04-09 09:08:19 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-04-09 09:08:19 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:11:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1e585fe0 is in desired state: ProposalReady
2022-04-09 09:11:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1e585fe0 will have desired state: ReconciliationPaused
2022-04-09 09:11:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1e585fe0 is in desired state: ReconciliationPaused
2022-04-09 09:11:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): Annotating KafkaRebalance:my-cluster-1e585fe0 with annotation approve
2022-04-09 09:11:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 19 polls
2022-04-09 09:11:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 18 polls
2022-04-09 09:11:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 17 polls
2022-04-09 09:11:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 16 polls
2022-04-09 09:11:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 15 polls
2022-04-09 09:11:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 14 polls
2022-04-09 09:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 13 polls
2022-04-09 09:11:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 12 polls
2022-04-09 09:11:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 11 polls
2022-04-09 09:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 10 polls
2022-04-09 09:11:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 9 polls
2022-04-09 09:11:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 8 polls
2022-04-09 09:11:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 7 polls
2022-04-09 09:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 6 polls
2022-04-09 09:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 5 polls
2022-04-09 09:11:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 4 polls
2022-04-09 09:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 3 polls
2022-04-09 09:11:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 2 polls
2022-04-09 09:11:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status gonna be stable in 1 polls
2022-04-09 09:11:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): KafkaRebalance status is stable for 20 polls intervals
2022-04-09 09:11:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:178] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-04-09 09:11:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1e585fe0 will have desired state: ProposalReady
2022-04-09 09:11:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1e585fe0 is in desired state: ProposalReady
2022-04-09 09:11:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-2/my-cluster-1e585fe0): Annotating KafkaRebalance:my-cluster-1e585fe0 with annotation approve
2022-04-09 09:11:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1e585fe0 will have desired state: Ready
2022-04-09 09:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1e585fe0 is in desired state: Ready
2022-04-09 09:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-04-09 09:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2143669430-1585115070 in namespace namespace-2
2022-04-09 09:13:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1e585fe0 in namespace namespace-2
2022-04-09 09:13:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-1e585fe0 in namespace namespace-2
2022-04-09 09:13:21 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-1e585fe0
2022-04-09 09:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-04-09 09:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-04-09 09:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-04-09 09:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 803.382 s - in io.strimzi.systemtest.operators.ReconciliationST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-04-09 09:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-04-09 09:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-04-09 09:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-04-09 09:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-04-09 09:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-09 09:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-04-09 09:15:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-04-09 09:15:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1577833048-1701092451 in namespace http-bridge-scram-sha-st
2022-04-09 09:15:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1577833048-1701092451 will have desired state: Ready
2022-04-09 09:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1577833048-1701092451 is in desired state: Ready
2022-04-09 09:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-04-09 09:15:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-04-09 09:15:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-04-09 09:15:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-09 09:15:35 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-04-09 09:15:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:15:53 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-04-09 09:15:53 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:15:53 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-83383454-4042747 in namespace http-bridge-scram-sha-st
2022-04-09 09:15:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-865838194-1435404572 in namespace http-bridge-scram-sha-st
2022-04-09 09:15:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-83383454-4042747 will have desired state: Ready
2022-04-09 09:15:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-865838194-1435404572 will have desired state: Ready
2022-04-09 09:15:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-83383454-4042747 is in desired state: Ready
2022-04-09 09:15:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-960534831 in namespace http-bridge-scram-sha-st
2022-04-09 09:15:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-865838194-1435404572 is in desired state: Ready
2022-04-09 09:15:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job producer-814667262 in namespace http-bridge-scram-sha-st
2022-04-09 09:15:54 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: producer-814667262 will be in active state
2022-04-09 09:15:54 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-960534831 will be in active state
2022-04-09 09:15:55 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-814667262 to finished
2022-04-09 09:15:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:15:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-480337915 in namespace http-bridge-scram-sha-st
2022-04-09 09:15:55 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-480337915 will be in active state
2022-04-09 09:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer producer-480337915 and consumer consumer-960534831 finish
2022-04-09 09:16:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:16:06 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-643419923 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:06 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: consumer-643419923 will be in active state
2022-04-09 09:16:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-643419923 to finished
2022-04-09 09:16:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:16:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-04-09 09:16:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-960534831 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job producer-480337915 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-83383454-4042747 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:16:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-04-09 09:16:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job producer-814667262 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-643419923 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-865838194-1435404572 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-04-09 09:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:16:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:16:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-04-09 09:16:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:16:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:16:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-04-09 09:16:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-04-09 09:16:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1577833048-1701092451 in namespace http-bridge-scram-sha-st
2022-04-09 09:16:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-09 09:16:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-09 09:17:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 183.256 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-04-09 09:17:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-04-09 09:17:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-04-09 09:17:23 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-04-09 09:17:23 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-04-09 09:17:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-09 09:17:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-04-09 09:18:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-04-09 09:18:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-500273208-2144414253 in namespace http-bridge-tls-st
2022-04-09 09:18:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-500273208-2144414253 will have desired state: Ready
2022-04-09 09:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-500273208-2144414253 is in desired state: Ready
2022-04-09 09:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-04-09 09:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-04-09 09:18:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-04-09 09:18:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-09 09:18:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-04-09 09:18:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:18:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:18:57 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:18:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1311533978-766566670 in namespace http-bridge-tls-st
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1553662353-581772021 in namespace http-bridge-tls-st
2022-04-09 09:18:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1311533978-766566670 will have desired state: Ready
2022-04-09 09:18:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1553662353-581772021 will have desired state: Ready
2022-04-09 09:18:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1311533978-766566670 is in desired state: Ready
2022-04-09 09:18:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-558758748 in namespace http-bridge-tls-st
2022-04-09 09:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1553662353-581772021 is in desired state: Ready
2022-04-09 09:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1812049737 in namespace http-bridge-tls-st
2022-04-09 09:18:58 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-1812049737 will be in active state
2022-04-09 09:18:58 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: consumer-558758748 will be in active state
2022-04-09 09:18:58 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:18:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job producer-366889396 in namespace http-bridge-tls-st
2022-04-09 09:18:58 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: producer-366889396 will be in active state
2022-04-09 09:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-1812049737 to finished
2022-04-09 09:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ClientUtils:61] Waiting till producer producer-366889396 and consumer consumer-558758748 finish
2022-04-09 09:19:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:19:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-932143372 in namespace http-bridge-tls-st
2022-04-09 09:19:08 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-932143372 will be in active state
2022-04-09 09:19:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-932143372 to finished
2022-04-09 09:19:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:19:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-04-09 09:19:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job consumer-558758748 in namespace http-bridge-tls-st
2022-04-09 09:19:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1311533978-766566670 in namespace http-bridge-tls-st
2022-04-09 09:19:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job producer-366889396 in namespace http-bridge-tls-st
2022-04-09 09:19:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:19:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-04-09 09:19:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-1812049737 in namespace http-bridge-tls-st
2022-04-09 09:19:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1553662353-581772021 in namespace http-bridge-tls-st
2022-04-09 09:19:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job consumer-932143372 in namespace http-bridge-tls-st
2022-04-09 09:19:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:19:25 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-04-09 09:19:25 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-04-09 09:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-04-09 09:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-04-09 09:19:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-09 09:19:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-09 09:19:30 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-500273208-2144414253 in namespace http-bridge-tls-st
2022-04-09 09:20:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 182.213 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.tracing.TracingST
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: tracing-st
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: tracing-st
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: tracing-st
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:497] === Applying jaeger operator install files ===
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-cluster-role-binding.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-cluster-role-binding.yaml
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
kind: "ClusterRoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
  namespace: "tracing-st"
roleRef:
  kind: "ClusterRole"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-cluster-role.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-cluster-role.yaml
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRole"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "console.openshift.io"
  resources:
  - "consolelinks"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "namespaces"
  verbs:
  - "get"
  - "list"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  verbs:
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
  - "clusterrolebindings"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-crd.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-crd.yaml
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-operator.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-operator.yaml
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: "jaeger-operator"
  template:
    metadata:
      labels:
        name: "jaeger-operator"
    spec:
      serviceAccountName: "jaeger-operator"
      containers:
      - name: "jaeger-operator"
        image: "jaegertracing/jaeger-operator:1.20.0"
        ports:
        - containerPort: 8383
          name: "http-metrics"
        - containerPort: 8686
          name: "cr-metrics"
        args:
        - "start"
        - "--kafka-provision=no"
        imagePullPolicy: "Always"
        env:
        - name: "WATCH_NAMESPACE"
          value: ""
        - name: "POD_NAME"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.name"
        - name: "POD_NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.namespace"
        - name: "OPERATOR_NAME"
          value: "jaeger-operator"
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-role-binding.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-role-binding.yaml
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
kind: "RoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
roleRef:
  kind: "Role"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-role.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-role.yaml
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "Role"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "image.openshift.io"
  resources:
  - "imagestreams"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-service-account.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-service-account.yaml
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "v1"
kind: "ServiceAccount"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: jaeger-operator will be ready
2022-04-09 09:20:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: jaeger-operator is ready
2022-04-09 09:20:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment jaeger-operator to be ready
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment jaeger-operator is ready
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy jaeger-allow in namespace tracing-st
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:524] Network policy for jaeger successfully created
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMaker2Service-STARTED
2022-04-09 09:20:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:20:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:20:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testKafkaBridgeService-STARTED
2022-04-09 09:20:40 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:20:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsService-STARTED
2022-04-09 09:20:40 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsConnectService-STARTED
2022-04-09 09:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMakerService-STARTED
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testProducerConsumerMirrorMaker2Service
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-04-09 09:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testProducerConsumerMirrorMakerService
2022-04-09 09:20:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-09 09:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testProducerConsumerStreamsConnectService
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testProducerConsumerStreamsService
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testKafkaBridgeService
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 apply -f -
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 apply -f -
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-04-09 09:20:41 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 apply -f -
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:41 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-09 09:20:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 apply -f -
2022-04-09 09:20:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:20:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 apply -f -
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-09 09:20:47 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-09 09:20:47 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-09 09:20:53 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-09 09:20:53 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-09 09:20:57 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-09 09:20:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-07e790c1-kafka-clients in namespace namespace-6
2022-04-09 09:20:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:20:57 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-07e790c1-kafka-clients will be ready
2022-04-09 09:20:59 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-07e790c1-kafka-clients is ready
2022-04-09 09:20:59 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:20:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-07e790c1 in namespace namespace-8
2022-04-09 09:20:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:20:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-07e790c1 will have desired state: Ready
2022-04-09 09:20:59 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-09 09:20:59 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-09 09:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-09 09:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-09 09:21:03 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-09 09:21:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-cb75436a-kafka-clients in namespace namespace-5
2022-04-09 09:21:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:21:03 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-cb75436a-kafka-clients will be ready
2022-04-09 09:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-cb75436a-kafka-clients is ready
2022-04-09 09:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cb75436a in namespace namespace-8
2022-04-09 09:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cb75436a will have desired state: Ready
2022-04-09 09:21:09 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-09 09:21:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d8eb42b0-kafka-clients in namespace namespace-7
2022-04-09 09:21:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:21:09 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d8eb42b0-kafka-clients will be ready
2022-04-09 09:21:11 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d8eb42b0-kafka-clients is ready
2022-04-09 09:21:11 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:21:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d8eb42b0 in namespace namespace-8
2022-04-09 09:21:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:21:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d8eb42b0 will have desired state: Ready
2022-04-09 09:21:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-09 09:21:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5b40f397-kafka-clients in namespace namespace-4
2022-04-09 09:21:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:21:12 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-09 09:21:12 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-09 09:21:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5b40f397-kafka-clients will be ready
2022-04-09 09:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5b40f397-kafka-clients is ready
2022-04-09 09:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5b40f397 in namespace namespace-8
2022-04-09 09:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5b40f397 will have desired state: Ready
2022-04-09 09:21:22 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-09 09:21:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-73df0f01-kafka-clients in namespace namespace-8
2022-04-09 09:21:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-09 09:21:22 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-73df0f01-kafka-clients will be ready
2022-04-09 09:21:24 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-73df0f01-kafka-clients is ready
2022-04-09 09:21:24 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:21:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-73df0f01 in namespace namespace-8
2022-04-09 09:21:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-09 09:21:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-73df0f01 will have desired state: Ready
2022-04-09 09:22:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-07e790c1 is in desired state: Ready
2022-04-09 09:22:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1774801084-245395019 in namespace namespace-8
2022-04-09 09:22:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:22:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1774801084-245395019 will have desired state: Ready
2022-04-09 09:22:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1774801084-245395019 is in desired state: Ready
2022-04-09 09:22:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1695587602-963161027 in namespace namespace-8
2022-04-09 09:22:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:22:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1695587602-963161027 will have desired state: Ready
2022-04-09 09:22:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1695587602-963161027 is in desired state: Ready
2022-04-09 09:22:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-07e790c1-scraper in namespace namespace-6
2022-04-09 09:22:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:22:26 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-07e790c1-scraper will be ready
2022-04-09 09:22:29 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-07e790c1-scraper is ready
2022-04-09 09:22:29 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-07e790c1-scraper to be ready
2022-04-09 09:22:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cb75436a is in desired state: Ready
2022-04-09 09:22:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cb75436a-target in namespace namespace-8
2022-04-09 09:22:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:22:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cb75436a-target will have desired state: Ready
2022-04-09 09:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5b40f397 is in desired state: Ready
2022-04-09 09:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5b40f397-target in namespace namespace-8
2022-04-09 09:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5b40f397-target will have desired state: Ready
2022-04-09 09:22:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d8eb42b0 is in desired state: Ready
2022-04-09 09:22:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1980161605-1909463840 in namespace namespace-8
2022-04-09 09:22:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:22:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1980161605-1909463840 will have desired state: Ready
2022-04-09 09:22:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1980161605-1909463840 is in desired state: Ready
2022-04-09 09:22:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-232841048-2125363277 in namespace namespace-8
2022-04-09 09:22:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:22:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-232841048-2125363277 will have desired state: Ready
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-07e790c1-scraper is ready
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-07e790c1-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-07e790c1-allow in namespace namespace-6
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-07e790c1 in namespace namespace-8
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:22:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-07e790c1 will have desired state: Ready
2022-04-09 09:22:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-232841048-2125363277 is in desired state: Ready
2022-04-09 09:22:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-d8eb42b0-hello-world-producer in namespace namespace-7
2022-04-09 09:22:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:22:40 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-d8eb42b0-hello-world-producer will be in active state
2022-04-09 09:22:41 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:41 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:42 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:42 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:42 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-producer is not present. Present services are ["jaeger-query"].
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-d8eb42b0-hello-world-consumer in namespace namespace-7
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:22:44 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-d8eb42b0-hello-world-consumer will be in active state
2022-04-09 09:22:45 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:45 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:45 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-09 09:22:46 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:46 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:46 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["hello-world-producer","jaeger-query"].
2022-04-09 09:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-09 09:22:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:49 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-09 09:22:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-73df0f01 is in desired state: Ready
2022-04-09 09:22:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-73df0f01 in namespace namespace-8
2022-04-09 09:22:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-09 09:22:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-73df0f01 will have desired state: Ready
2022-04-09 09:22:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:50 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-d8eb42b0-kafka-clients-696b5f8f84-8bl6f -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-streams in namespace namespace-8
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-09 09:22:52 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-streams will be in active state
2022-04-09 09:22:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:22:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerStreamsService
2022-04-09 09:22:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-d8eb42b0-hello-world-producer in namespace namespace-7
2022-04-09 09:22:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-232841048-2125363277 in namespace namespace-7
2022-04-09 09:23:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-streams in namespace namespace-7
2022-04-09 09:23:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-d8eb42b0-hello-world-consumer in namespace namespace-7
2022-04-09 09:23:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d8eb42b0 in namespace namespace-7
2022-04-09 09:23:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1980161605-1909463840 in namespace namespace-7
2022-04-09 09:23:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-73df0f01 is in desired state: Ready
2022-04-09 09:23:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-68932309-675462550 in namespace namespace-8
2022-04-09 09:23:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-09 09:23:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-68932309-675462550 will have desired state: Ready
2022-04-09 09:23:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-68932309-675462550 is in desired state: Ready
2022-04-09 09:23:16 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 09:23:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace namespace-8
2022-04-09 09:23:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-09 09:23:16 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-04-09 09:23:17 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-73df0f01-hello-world-consumer in namespace namespace-8
2022-04-09 09:23:17 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-09 09:23:17 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-73df0f01-hello-world-consumer will be in active state
2022-04-09 09:23:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:bridge-producer to finished
2022-04-09 09:23:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d8eb42b0-kafka-clients in namespace namespace-7
2022-04-09 09:23:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-07e790c1 is in desired state: Ready
2022-04-09 09:23:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-07e790c1 in namespace namespace-8
2022-04-09 09:23:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:23:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-07e790c1 will have desired state: Ready
2022-04-09 09:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-07e790c1 is in desired state: Ready
2022-04-09 09:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-07e790c1-hello-world-producer in namespace namespace-6
2022-04-09 09:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-07e790c1-hello-world-producer will be in active state
2022-04-09 09:23:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-07e790c1-hello-world-consumer in namespace namespace-6
2022-04-09 09:23:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-09 09:23:49 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-07e790c1-hello-world-consumer will be in active state
2022-04-09 09:23:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-07e790c1-hello-world-producer and consumer my-cluster-07e790c1-hello-world-consumer finish
2022-04-09 09:23:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cb75436a-target is in desired state: Ready
2022-04-09 09:23:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2147098649-274767098 in namespace namespace-8
2022-04-09 09:23:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:23:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2147098649-274767098 will have desired state: Ready
2022-04-09 09:23:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2147098649-274767098 is in desired state: Ready
2022-04-09 09:23:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2147098649-274767098-target in namespace namespace-8
2022-04-09 09:23:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:23:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2147098649-274767098-target will have desired state: Ready
2022-04-09 09:23:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2147098649-274767098-target is in desired state: Ready
2022-04-09 09:23:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingST:267] Setting for kafka source plain bootstrap:my-cluster-cb75436a-kafka-bootstrap:9092
2022-04-09 09:23:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-cb75436a-hello-world-producer in namespace namespace-5
2022-04-09 09:23:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:23:54 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-cb75436a-hello-world-producer will be in active state
2022-04-09 09:23:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingST:276] Setting for kafka target plain bootstrap:my-cluster-cb75436a-target-kafka-bootstrap:9092
2022-04-09 09:23:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-cb75436a-hello-world-consumer in namespace namespace-5
2022-04-09 09:23:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:23:55 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-cb75436a-hello-world-consumer will be in active state
2022-04-09 09:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5b40f397-target is in desired state: Ready
2022-04-09 09:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1961908832-835018770 in namespace namespace-8
2022-04-09 09:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1961908832-835018770 will have desired state: Ready
2022-04-09 09:23:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-cb75436a in namespace namespace-8
2022-04-09 09:23:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-09 09:23:56 [ForkJoinPool-3-worker-7] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-04-09 09:23:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-cb75436a will have desired state: Ready
2022-04-09 09:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1961908832-835018770 is in desired state: Ready
2022-04-09 09:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-cluster-5b40f397.my-topic-1961908832-835018770 in namespace namespace-8
2022-04-09 09:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-cluster-5b40f397.my-topic-1961908832-835018770 will have desired state: Ready
2022-04-09 09:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-cluster-5b40f397.my-topic-1961908832-835018770 is in desired state: Ready
2022-04-09 09:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:177] Setting for kafka source plain bootstrap:my-cluster-5b40f397-kafka-bootstrap:9092
2022-04-09 09:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-5b40f397-hello-world-producer in namespace namespace-4
2022-04-09 09:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-5b40f397-hello-world-producer will be in active state
2022-04-09 09:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:186] Setting for kafka target plain bootstrap:my-cluster-5b40f397-target-kafka-bootstrap:9092
2022-04-09 09:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-5b40f397-hello-world-consumer in namespace namespace-4
2022-04-09 09:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-5b40f397-hello-world-consumer will be in active state
2022-04-09 09:24:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-5b40f397 in namespace namespace-8
2022-04-09 09:24:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-09 09:24:02 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2'
2022-04-09 09:24:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-5b40f397 will have desired state: Ready
2022-04-09 09:24:03 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 delete -f -
2022-04-09 09:24:03 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:24:03 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:24:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testProducerConsumerStreamsService
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-07e790c1-kafka-clients-85cc56f95d-hrqkl -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-07e790c1-kafka-clients-85cc56f95d-hrqkl -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer&operation=To_my-topic-1774801084-245395019
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-07e790c1-kafka-clients-85cc56f95d-hrqkl -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:04 [ForkJoinPool-3-worker-13] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-09 09:24:05 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-07e790c1-kafka-clients-85cc56f95d-hrqkl -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer&operation=From_my-topic-1774801084-245395019
2022-04-09 09:24:05 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:05 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-07e790c1-kafka-clients-85cc56f95d-hrqkl -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:24:05 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:05 [ForkJoinPool-3-worker-13] [32mINFO [m [TracingUtils:47] Jaeger service my-connect is present
2022-04-09 09:24:06 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-07e790c1-kafka-clients-85cc56f95d-hrqkl -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-connect&operation=From_my-topic-1774801084-245395019
2022-04-09 09:24:06 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:24:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:24:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerStreamsConnectService
2022-04-09 09:24:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-07e790c1-allow in namespace namespace-6
2022-04-09 09:24:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-07e790c1 in namespace namespace-6
2022-04-09 09:24:09 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsService-FINISHED
2022-04-09 09:24:09 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:24:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-07e790c1 in namespace namespace-6
2022-04-09 09:24:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-07e790c1-scraper in namespace namespace-6
2022-04-09 09:24:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1695587602-963161027 in namespace namespace-6
2022-04-09 09:24:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1774801084-245395019 in namespace namespace-6
2022-04-09 09:24:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-07e790c1-kafka-clients in namespace namespace-6
2022-04-09 09:24:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-07e790c1-hello-world-producer in namespace namespace-6
2022-04-09 09:24:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-07e790c1-hello-world-consumer in namespace namespace-6
2022-04-09 09:24:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-07e790c1 in namespace namespace-6
2022-04-09 09:25:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-cb75436a is in desired state: Ready
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer&operation=To_my-topic-2147098649-274767098
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 delete -f -
2022-04-09 09:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:04 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer&operation=From_my-topic-2147098649-274767098
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker is present
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker&operation=From_my-topic-2147098649-274767098
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker is present
2022-04-09 09:25:06 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-cb75436a-kafka-clients-55c58f898d-jxxgf -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker&operation=To_my-topic-2147098649-274767098
2022-04-09 09:25:06 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:25:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMakerService
2022-04-09 09:25:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2147098649-274767098-target in namespace namespace-5
2022-04-09 09:25:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cb75436a in namespace namespace-5
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-73df0f01-kafka-clients-5646789fbd-ctxr5 -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TracingUtils:47] Jaeger service my-kafka-bridge is present
2022-04-09 09:25:08 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-cb75436a-kafka-clients in namespace namespace-5
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-73df0f01-kafka-clients-5646789fbd-ctxr5 -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-kafka-bridge
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeService
2022-04-09 09:25:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-68932309-675462550 in namespace namespace-8
2022-04-09 09:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-5b40f397 is in desired state: Ready
2022-04-09 09:25:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2147098649-274767098 in namespace namespace-5
2022-04-09 09:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-09 09:25:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cb75436a-target in namespace namespace-5
2022-04-09 09:25:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-cb75436a-hello-world-consumer in namespace namespace-5
2022-04-09 09:25:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-cb75436a in namespace namespace-5
2022-04-09 09:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer&operation=To_my-topic-1961908832-835018770
2022-04-09 09:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer&operation=From_my-cluster-5b40f397.my-topic-1961908832-835018770
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker2 is present
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker2&operation=From_my-topic-1961908832-835018770
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker2 is present
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-5b40f397-kafka-clients-56fb975754-bmqml -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker2&operation=To_my-cluster-5b40f397.my-topic-1961908832-835018770
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMaker2Service
2022-04-09 09:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-cluster-5b40f397.my-topic-1961908832-835018770 in namespace namespace-4
2022-04-09 09:25:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-73df0f01 in namespace namespace-8
2022-04-09 09:25:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-cb75436a-hello-world-producer in namespace namespace-5
2022-04-09 09:25:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:25:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testProducerConsumerStreamsConnectService
2022-04-09 09:25:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5b40f397 in namespace namespace-4
2022-04-09 09:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsConnectService-FINISHED
2022-04-09 09:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-5b40f397-hello-world-consumer in namespace namespace-4
2022-04-09 09:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-5b40f397 in namespace namespace-4
2022-04-09 09:25:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1961908832-835018770 in namespace namespace-4
2022-04-09 09:25:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 delete -f -
2022-04-09 09:25:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:25:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-73df0f01-kafka-clients in namespace namespace-8
2022-04-09 09:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-73df0f01-hello-world-consumer in namespace namespace-8
2022-04-09 09:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-5b40f397-hello-world-producer in namespace namespace-4
2022-04-09 09:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5b40f397-kafka-clients in namespace namespace-4
2022-04-09 09:25:29 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 delete -f -
2022-04-09 09:25:29 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:25:29 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-73df0f01 in namespace namespace-8
2022-04-09 09:25:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5b40f397-target in namespace namespace-4
2022-04-09 09:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace namespace-8
2022-04-09 09:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 delete -f -
2022-04-09 09:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-09 09:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:25:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:25:58 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testProducerConsumerMirrorMakerService
2022-04-09 09:26:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMakerService-FINISHED
2022-04-09 09:26:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:26:27 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:26:27 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testKafkaBridgeService
2022-04-09 09:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testProducerConsumerMirrorMaker2Service
2022-04-09 09:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMaker2Service-FINISHED
2022-04-09 09:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:26:37 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testKafkaBridgeService-FINISHED
2022-04-09 09:26:37 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:26:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:26:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for TracingST
2022-04-09 09:26:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy jaeger-allow in namespace tracing-st
2022-04-09 09:26:37 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:37 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
apiVersion: "v1"
kind: "ServiceAccount"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
2022-04-09 09:26:37 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "Role"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "image.openshift.io"
  resources:
  - "imagestreams"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
kind: "RoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
roleRef:
  kind: "Role"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: "jaeger-operator"
  template:
    metadata:
      labels:
        name: "jaeger-operator"
    spec:
      serviceAccountName: "jaeger-operator"
      containers:
      - name: "jaeger-operator"
        image: "jaegertracing/jaeger-operator:1.20.0"
        ports:
        - containerPort: 8383
          name: "http-metrics"
        - containerPort: 8686
          name: "cr-metrics"
        args:
        - "start"
        - "--kafka-provision=no"
        imagePullPolicy: "Always"
        env:
        - name: "WATCH_NAMESPACE"
          value: ""
        - name: "POD_NAME"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.name"
        - name: "POD_NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.namespace"
        - name: "OPERATOR_NAME"
          value: "jaeger-operator"
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRole"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "console.openshift.io"
  resources:
  - "consolelinks"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "namespaces"
  verbs:
  - "get"
  - "list"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  verbs:
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
  - "clusterrolebindings"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-09 09:26:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:39 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-09 09:26:39 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Input: ---
kind: "ClusterRoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
  namespace: "tracing-st"
roleRef:
  kind: "ClusterRole"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-09 09:26:39 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 378.658 s - in io.strimzi.systemtest.tracing.TracingST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-api-st
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-api-st
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-api-st
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-STARTED
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-STARTED
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testCruiseControlBasicAPIRequests
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-9
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-9
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-10 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-10
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-325ebe50 in namespace namespace-9
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-9
2022-04-09 09:26:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-325ebe50 will have desired state: Ready
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-10
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-10
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka cruise-control-api-cluster-name in namespace namespace-10
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-10
2022-04-09 09:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-04-09 09:28:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-325ebe50 is in desired state: Ready
2022-04-09 09:28:22 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:48] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-04-09 09:28:22 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:58] Verifying that Cruise Control REST API is available
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: cruise-control-api-cluster-name is in desired state: Ready
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlApiST:153] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlApiST:157] Verifying that Cruise Control REST API is available using HTTP request without credentials
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka cruise-control-api-cluster-name in namespace namespace-10
2022-04-09 09:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-10, for cruise control Kafka cluster cruise-control-api-cluster-name
2022-04-09 09:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-10 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-04-09 09:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-FINISHED
2022-04-09 09:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:29:53 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:66] ----> KAFKA REBALANCE <----
2022-04-09 09:29:54 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:73] Waiting for CC will have for enough metrics to be recorded to make a proposal 
2022-04-09 09:31:13 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:97] ----> EXECUTION OF STOP PROPOSAL <----
2022-04-09 09:31:14 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:108] ----> USER TASKS <----
2022-04-09 09:31:14 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:126] Verifying that Cruise Control REST API doesn't allow HTTP requests
2022-04-09 09:31:14 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlApiST:132] Verifying that Cruise Control REST API doesn't allow unauthenticated requests
2022-04-09 09:31:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:31:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequests
2022-04-09 09:31:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-325ebe50 in namespace namespace-9
2022-04-09 09:31:15 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-9, for cruise control Kafka cluster my-cluster-325ebe50
2022-04-09 09:31:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:31:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testCruiseControlBasicAPIRequests
2022-04-09 09:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-FINISHED
2022-04-09 09:32:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:32:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:32:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlApiST is everything deleted.
2022-04-09 09:32:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 329.39 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-04-09 09:32:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:32:13 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:32:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:32:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:32:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-04-09 09:32:13 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-04-09 09:32:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-04-09 09:32:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-11 for test case:testCapacityFile
2022-04-09 09:32:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-11
2022-04-09 09:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-11
2022-04-09 09:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-11
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-12 for test case:testConfigurationReflection
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-12
2022-04-09 09:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-edff2009 in namespace namespace-11
2022-04-09 09:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-11
2022-04-09 09:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-edff2009 will have desired state: Ready
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-12
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-12
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-13 for test case:testDeployAndUnDeployCruiseControl
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-13
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cbdf4d00 in namespace namespace-12
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-12
2022-04-09 09:32:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cbdf4d00 will have desired state: Ready
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-13
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-13
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-14 for test case:testConfigurationFileIsCreated
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-14
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f6c915fc in namespace namespace-13
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-13
2022-04-09 09:32:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f6c915fc will have desired state: Ready
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-14
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-14
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-15 for test case:testConfigurationPerformanceOptions
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-15
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-364b8638 in namespace namespace-14
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-14
2022-04-09 09:32:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-364b8638 will have desired state: Ready
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-15
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-15
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2f036c14 in namespace namespace-15
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-15
2022-04-09 09:32:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2f036c14 will have desired state: Ready
2022-04-09 09:35:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f6c915fc is in desired state: Ready
2022-04-09 09:35:05 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-04-09 09:35:05 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f6c915fc-kafka rolling update
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-edff2009 is in desired state: Ready
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-11 exec my-cluster-edff2009-cruise-control-c4778b88f-6zl5l -- /bin/bash -c cat /tmp/capacity.json
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCapacityFile
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-edff2009 in namespace namespace-11
2022-04-09 09:35:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-11, for cruise control Kafka cluster my-cluster-edff2009
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-364b8638 is in desired state: Ready
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-14 exec my-cluster-364b8638-cruise-control-6d58566d4c-v2784 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-364b8638 in namespace namespace-14
2022-04-09 09:35:10 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-14, for cruise control Kafka cluster my-cluster-364b8638
2022-04-09 09:35:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:35:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-11 for test case:testCapacityFile
2022-04-09 09:35:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:35:18 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-04-09 09:35:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cbdf4d00 is in desired state: Ready
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-16 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-16
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-16
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-16
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-bb084efe in namespace namespace-16
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-16
2022-04-09 09:35:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bb084efe will have desired state: Ready
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-12 exec my-cluster-cbdf4d00-cruise-control-5fff9577b6-56dfm -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cbdf4d00 in namespace namespace-12
2022-04-09 09:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-12, for cruise control Kafka cluster my-cluster-cbdf4d00
2022-04-09 09:35:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2f036c14 is in desired state: Ready
2022-04-09 09:35:25 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-04-09 09:35:25 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-04-09 09:35:25 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-2f036c14-cruise-control rolling update
2022-04-09 09:35:30 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:35:30 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-14 for test case:testConfigurationFileIsCreated
2022-04-09 09:35:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:35:34 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-12 for test case:testConfigurationReflection
2022-04-09 09:36:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-04-09 09:36:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:36:14 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-04-09 09:36:14 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:36:15 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2f036c14-cruise-control will be ready
2022-04-09 09:36:15 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2f036c14-cruise-control is ready
2022-04-09 09:36:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-04-09 09:36:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:36:25 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-2f036c14-cruise-control rolling update finished
2022-04-09 09:36:25 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-04-09 09:36:25 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 09:36:26 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 09:36:27 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 09:36:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 09:36:29 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 09:36:30 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 09:36:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 09:36:32 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 09:36:33 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 09:36:34 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 09:36:35 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 09:36:36 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 09:36:37 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 09:36:38 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 09:36:39 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 09:36:40 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 09:36:41 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 09:36:42 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 09:36:43 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 09:36:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 09:36:45 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 09:36:46 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 09:36:47 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 09:36:48 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 09:36:49 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 09:36:50 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 09:36:51 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 09:36:52 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 09:36:53 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 09:36:54 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 09:36:55 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 09:36:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 09:36:57 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 09:36:58 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 09:36:59 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 09:37:00 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 09:37:01 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 09:37:02 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 09:37:03 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 09:37:04 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 09:37:05 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 09:37:06 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 09:37:07 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 09:37:08 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 09:37:09 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 09:37:10 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 09:37:11 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 09:37:12 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 09:37:13 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 09:37:14 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 09:37:15 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f6c915fc-kafka has been successfully rolled
2022-04-09 09:37:15 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f6c915fc-kafka to be ready
2022-04-09 09:37:15 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-2f036c14-kafka-0=2f99d8a3-1f88-488c-9bdf-213288df0d2d, my-cluster-2f036c14-kafka-1=6afa8101-13b6-4d8c-a178-9b28e1657cd9, my-cluster-2f036c14-kafka-2=4408cad9-a573-48e1-ada8-f85ba4221ea1} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 09:37:15 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-04-09 09:37:16 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-04-09 09:37:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:37:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-04-09 09:37:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2f036c14 in namespace namespace-15
2022-04-09 09:37:16 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-15, for cruise control Kafka cluster my-cluster-2f036c14
2022-04-09 09:37:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:37:26 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-15 for test case:testConfigurationPerformanceOptions
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f6c915fc will have desired state: Ready
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f6c915fc is in desired state: Ready
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f6c915fc is ready
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:120] Verifying that my-cluster-f6c915fc-cruise-control- pod is not present
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-f6c915fc-cruise-control- will have stable 0 replicas
2022-04-09 09:37:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 09:37:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 09:37:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-09 09:37:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-09 09:37:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-09 09:37:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-09 09:37:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-09 09:37:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-09 09:37:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-09 09:37:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-09 09:37:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-09 09:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-09 09:37:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-09 09:37:54 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-09 09:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-09 09:37:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-09 09:37:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-09 09:37:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-09 09:37:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-09 09:38:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-09 09:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-09 09:38:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-09 09:38:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:228] Pod my-cluster-f6c915fc-cruise-control- has 0 replicas
2022-04-09 09:38:02 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-04-09 09:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-04-09 09:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:38:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bb084efe is in desired state: Ready
2022-04-09 09:38:10 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-04-09 09:38:10 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-04-09 09:38:10 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-bb084efe-cruise-control rolling update
2022-04-09 09:38:45 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bb084efe-cruise-control will be ready
2022-04-09 09:38:45 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bb084efe-cruise-control is ready
2022-04-09 09:38:55 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-bb084efe-cruise-control rolling update finished
2022-04-09 09:38:55 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-04-09 09:38:55 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 09:38:56 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 09:38:57 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 09:38:58 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 09:38:59 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 09:39:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 09:39:01 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 09:39:02 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 09:39:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 09:39:04 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 09:39:05 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 09:39:06 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 09:39:07 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 09:39:08 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 09:39:09 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 09:39:10 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 09:39:11 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 09:39:12 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 09:39:13 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 09:39:14 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 09:39:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 09:39:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 09:39:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 09:39:18 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 09:39:19 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 09:39:20 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 09:39:21 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 09:39:22 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 09:39:23 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 09:39:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 09:39:25 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 09:39:26 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 09:39:27 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 09:39:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 09:39:29 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 09:39:30 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 09:39:31 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 09:39:32 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 09:39:33 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 09:39:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 09:39:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 09:39:36 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 09:39:37 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 09:39:38 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 09:39:39 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 09:39:40 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 09:39:41 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 09:39:42 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 09:39:43 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 09:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 09:39:45 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-bb084efe-kafka-2=1307875e-c637-4cd3-8663-f7a58c9ecaee, my-cluster-bb084efe-kafka-0=39c36a0c-c6c2-4795-a2df-7744e5b7d312, my-cluster-bb084efe-kafka-1=fcae97a1-c416-48dd-a25d-c3d95e572724} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 09:39:45 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-04-09 09:39:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:39:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-04-09 09:39:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-bb084efe in namespace namespace-16
2022-04-09 09:39:45 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-16, for cruise control Kafka cluster my-cluster-bb084efe
2022-04-09 09:39:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:39:56 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-16 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-04-09 09:40:02 [ForkJoinPool-3-worker-9] [1;31mERROR[m [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-f6c915fc} has correct cruise control metric reporter properties, null
io.strimzi.test.WaitException: Timeout after 120000 ms waiting for Verify that kafka configuration {cluster-name=my-cluster-f6c915fc} has correct cruise control metric reporter properties
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.specific.CruiseControlUtils.verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent(CruiseControlUtils.java:83)
	at io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.lambda$testDeployAndUnDeployCruiseControl$1(CruiseControlConfigurationST.java:124)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3082)
	at io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl(CruiseControlConfigurationST.java:124)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 09:40:02 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-04-09 09:40:02 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-04-09 09:40:02 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f6c915fc-kafka rolling update
2022-04-09 09:40:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-04-09 09:40:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:41:37 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f6c915fc-kafka has been successfully rolled
2022-04-09 09:41:37 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f6c915fc-kafka to be ready
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f6c915fc will have desired state: Ready
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f6c915fc is in desired state: Ready
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f6c915fc is ready
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f6c915fc in namespace namespace-13
2022-04-09 09:41:59 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-13, for cruise control Kafka cluster my-cluster-f6c915fc
2022-04-09 09:42:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:42:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-13 for test case:testDeployAndUnDeployCruiseControl
2022-04-09 09:42:53 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-04-09 09:42:53 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:42:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:42:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-04-09 09:42:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 645.104 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-04-09 09:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-st
2022-04-09 09:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-st
2022-04-09 09:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-st
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlReplicaMovementStrategy-STARTED
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage-STARTED
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithSingleNodeKafka-STARTED
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlTopicExclusion-STARTED
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancing-STARTED
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-17 for test case:testCruiseControlReplicaMovementStrategy
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-17
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-17
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-17
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-18 for test case:testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-18
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4864b554 in namespace namespace-17
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-09 09:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4864b554 will have desired state: Ready
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-18
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-18
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-19 for test case:testCruiseControlTopicExclusion
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-19
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d283b31e in namespace namespace-18
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-18
2022-04-09 09:42:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d283b31e will have desired state: Ready
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-19
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-19
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-20 for test case:testCruiseControlWithSingleNodeKafka
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-20
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1445d5f8 in namespace namespace-19
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-09 09:42:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1445d5f8 will have desired state: Ready
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-20
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-20
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-21 for test case:testCruiseControlIntraBrokerBalancing
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlST:169] Deploying single node Kafka with CruiseControl
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-21
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8e2307da in namespace namespace-20
2022-04-09 09:42:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-20
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-21
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-21
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3cb65789 in namespace namespace-21
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-21
2022-04-09 09:42:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3cb65789 will have desired state: Ready
2022-04-09 09:43:10 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlST:178] Increasing Kafka nodes to 3
2022-04-09 09:43:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8e2307da will have desired state: Ready
2022-04-09 09:45:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4864b554 is in desired state: Ready
2022-04-09 09:45:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-4864b554-kafka-clients in namespace namespace-21
2022-04-09 09:45:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-09 09:45:32 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4864b554-kafka-clients will be ready
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4864b554-kafka-clients is ready
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlST:234] Check for default CruiseControl replicaMovementStrategy in pod configuration file.
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-17 exec my-cluster-4864b554-cruise-control-56bf67b4b7-7hjz4 -c cruise-control -- cat /tmp/cruisecontrol.properties
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlST:248] Set non-default CruiseControl replicaMovementStrategies to KafkaRebalance resource.
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlST:252] Verifying that CC pod is rolling, because of change size of disk
2022-04-09 09:45:35 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4864b554-cruise-control rolling update
2022-04-09 09:45:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8e2307da is in desired state: Ready
2022-04-09 09:45:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:45:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithSingleNodeKafka
2022-04-09 09:45:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8e2307da in namespace namespace-20
2022-04-09 09:45:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-20, for cruise control Kafka cluster my-cluster-8e2307da
2022-04-09 09:45:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d283b31e is in desired state: Ready
2022-04-09 09:45:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-d283b31e in namespace namespace-21
2022-04-09 09:45:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-18
2022-04-09 09:45:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-d283b31e will have desired state: NotReady
2022-04-09 09:45:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3cb65789 is in desired state: Ready
2022-04-09 09:45:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-3cb65789 in namespace namespace-21
2022-04-09 09:45:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-21
2022-04-09 09:45:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-3cb65789 will have desired state: PendingProposal
2022-04-09 09:45:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-d283b31e is in desired state: NotReady
2022-04-09 09:45:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:45:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage
2022-04-09 09:45:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-d283b31e in namespace namespace-18
2022-04-09 09:45:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-3cb65789 is in desired state: PendingProposal
2022-04-09 09:45:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-3cb65789 will have desired state: ProposalReady
2022-04-09 09:45:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:45:56 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-20 for test case:testCruiseControlWithSingleNodeKafka
2022-04-09 09:46:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d283b31e in namespace namespace-18
2022-04-09 09:46:03 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-18, for cruise control Kafka cluster my-cluster-d283b31e
2022-04-09 09:46:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1445d5f8 is in desired state: Ready
2022-04-09 09:46:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic excluded-topic-1 in namespace namespace-19
2022-04-09 09:46:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-09 09:46:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: excluded-topic-1 will have desired state: Ready
2022-04-09 09:46:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: excluded-topic-1 is in desired state: Ready
2022-04-09 09:46:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic excluded-topic-2 in namespace namespace-19
2022-04-09 09:46:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-09 09:46:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: excluded-topic-2 will have desired state: Ready
2022-04-09 09:46:10 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4864b554-cruise-control will be ready
2022-04-09 09:46:10 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4864b554-cruise-control is ready
2022-04-09 09:46:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: excluded-topic-2 is in desired state: Ready
2022-04-09 09:46:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic included-topic in namespace namespace-19
2022-04-09 09:46:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-09 09:46:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: included-topic will have desired state: Ready
2022-04-09 09:46:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: included-topic is in desired state: Ready
2022-04-09 09:46:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-1445d5f8 in namespace namespace-21
2022-04-09 09:46:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-09 09:46:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1445d5f8 will have desired state: PendingProposal
2022-04-09 09:46:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1445d5f8 is in desired state: PendingProposal
2022-04-09 09:46:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1445d5f8 will have desired state: ProposalReady
2022-04-09 09:46:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:46:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-18 for test case:testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage
2022-04-09 09:46:20 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4864b554-cruise-control rolling update finished
2022-04-09 09:46:20 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-17 exec my-cluster-4864b554-cruise-control-6958456dc5-d7d9j -c cruise-control -- cat /tmp/cruisecontrol.properties
2022-04-09 09:46:20 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 09:46:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:46:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlReplicaMovementStrategy
2022-04-09 09:46:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-4864b554-kafka-clients in namespace namespace-17
2022-04-09 09:46:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4864b554 in namespace namespace-17
2022-04-09 09:46:35 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-17, for cruise control Kafka cluster my-cluster-4864b554
2022-04-09 09:46:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithSingleNodeKafka-FINISHED
2022-04-09 09:46:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:46:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage-FINISHED
2022-04-09 09:46:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:47:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:47:00 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-17 for test case:testCruiseControlReplicaMovementStrategy
2022-04-09 09:47:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlReplicaMovementStrategy-FINISHED
2022-04-09 09:47:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:51:08 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1445d5f8 is in desired state: ProposalReady
2022-04-09 09:51:08 [ForkJoinPool-3-worker-9] [32mINFO [m [CruiseControlST:208] Checking status of KafkaRebalance
2022-04-09 09:51:08 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #4(test) KafkaRebalance(cruise-control-st/my-cluster-1445d5f8): Annotating KafkaRebalance:my-cluster-1445d5f8 with annotation approve
2022-04-09 09:51:08 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-1445d5f8 will have desired state: Ready
2022-04-09 09:51:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-3cb65789 is in desired state: ProposalReady
2022-04-09 09:51:56 [ForkJoinPool-3-worker-11] [32mINFO [m [CruiseControlST:292] Checking status of KafkaRebalance
2022-04-09 09:51:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:51:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlIntraBrokerBalancing
2022-04-09 09:51:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-3cb65789 in namespace namespace-21
2022-04-09 09:51:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3cb65789 in namespace namespace-21
2022-04-09 09:51:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-21, for cruise control Kafka cluster my-cluster-3cb65789
2022-04-09 09:52:06 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:52:06 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-21 for test case:testCruiseControlIntraBrokerBalancing
2022-04-09 09:52:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-1445d5f8 is in desired state: Ready
2022-04-09 09:52:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:52:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlTopicExclusion
2022-04-09 09:52:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic excluded-topic-2 in namespace namespace-19
2022-04-09 09:52:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-1445d5f8 in namespace namespace-19
2022-04-09 09:52:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic included-topic in namespace namespace-19
2022-04-09 09:52:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic excluded-topic-1 in namespace namespace-19
2022-04-09 09:52:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1445d5f8 in namespace namespace-19
2022-04-09 09:52:33 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-19, for cruise control Kafka cluster my-cluster-1445d5f8
2022-04-09 09:52:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:52:43 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-19 for test case:testCruiseControlTopicExclusion
2022-04-09 09:52:50 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancing-FINISHED
2022-04-09 09:52:50 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:53:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlTopicExclusion-FINISHED
2022-04-09 09:53:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:53:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:53:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testAutoCreationOfCruiseControlTopics-STARTED
2022-04-09 09:53:04 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:53:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-71e633b3 in namespace cruise-control-st
2022-04-09 09:53:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-71e633b3 will have desired state: Ready
2022-04-09 09:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-71e633b3 is in desired state: Ready
2022-04-09 09:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: strimzi.cruisecontrol.metrics will have desired state: Ready
2022-04-09 09:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: strimzi.cruisecontrol.metrics is in desired state: Ready
2022-04-09 09:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples will have desired state: Ready
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples is in desired state: Ready
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples will have desired state: Ready
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples is in desired state: Ready
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:96] Checking partitions and replicas for strimzi.cruisecontrol.metrics
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:100] Checking partitions and replicas for strimzi.cruisecontrol.modeltrainingsamples
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:104] Checking partitions and replicas for strimzi.cruisecontrol.partitionmetricsamples
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoCreationOfCruiseControlTopics
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-71e633b3 in namespace cruise-control-st
2022-04-09 09:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-71e633b3
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testAutoCreationOfCruiseControlTopics-FINISHED
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-STARTED
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9ec70b96 in namespace cruise-control-st
2022-04-09 09:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9ec70b96 will have desired state: Ready
2022-04-09 09:59:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9ec70b96 is in desired state: Ready
2022-04-09 09:59:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-9ec70b96 in namespace cruise-control-st
2022-04-09 09:59:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: PendingProposal
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: PendingProposal
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): PendingProposal
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Verifying that KafkaRebalance resource is in PendingProposal state
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: PendingProposal
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: PendingProposal
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:85] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Verifying that KafkaRebalance resource is in ProposalReady state
2022-04-09 09:59:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: ProposalReady
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: ProposalReady
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ProposalReady
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Annotating KafkaRebalance:my-cluster-9ec70b96 with annotation approve
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-9ec70b96 annotated
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Verifying that annotation triggers the Rebalancing state
2022-04-09 10:01:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: Rebalancing
2022-04-09 10:01:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: Rebalancing
2022-04-09 10:01:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Verifying that KafkaRebalance is in the Ready state
2022-04-09 10:01:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: Ready
2022-04-09 10:02:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: Ready
2022-04-09 10:02:38 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:152] Annotating KafkaRebalance: my-cluster-9ec70b96 with 'refresh' anno
2022-04-09 10:02:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #6(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Annotating KafkaRebalance:my-cluster-9ec70b96 with annotation refresh
2022-04-09 10:02:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: ProposalReady
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: ProposalReady
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:156] Trying rebalancing process again
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ProposalReady
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ProposalReady
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): ============================================================================
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Annotating KafkaRebalance:my-cluster-9ec70b96 with annotation approve
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-9ec70b96 annotated
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Verifying that annotation triggers the Rebalancing state
2022-04-09 10:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: Rebalancing
2022-04-09 10:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: Rebalancing
2022-04-09 10:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-9ec70b96): Verifying that KafkaRebalance is in the Ready state
2022-04-09 10:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-9ec70b96 will have desired state: Ready
2022-04-09 10:02:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-9ec70b96 is in desired state: Ready
2022-04-09 10:02:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:02:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithRebalanceResourceAndRefreshAnnotation
2022-04-09 10:02:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-9ec70b96 in namespace cruise-control-st
2022-04-09 10:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9ec70b96 in namespace cruise-control-st
2022-04-09 10:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-9ec70b96
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-FINISHED
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithApiSecurityDisabled-STARTED
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6f0b7cdf in namespace cruise-control-st
2022-04-09 10:02:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6f0b7cdf will have desired state: Ready
2022-04-09 10:05:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6f0b7cdf is in desired state: Ready
2022-04-09 10:05:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-6f0b7cdf in namespace cruise-control-st
2022-04-09 10:05:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-6f0b7cdf will have desired state: PendingProposal
2022-04-09 10:05:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-6f0b7cdf is in desired state: PendingProposal
2022-04-09 10:05:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-6f0b7cdf will have desired state: ProposalReady
2022-04-09 10:11:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-6f0b7cdf is in desired state: ProposalReady
2022-04-09 10:11:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:11:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithApiSecurityDisabled
2022-04-09 10:11:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-6f0b7cdf in namespace cruise-control-st
2022-04-09 10:11:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6f0b7cdf in namespace cruise-control-st
2022-04-09 10:11:29 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-6f0b7cdf
2022-04-09 10:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithApiSecurityDisabled-FINISHED
2022-04-09 10:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlST is everything deleted.
2022-04-09 10:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,764.84 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.ListenersST
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: listeners-st
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: listeners-st
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: listeners-st
2022-04-09 10:12:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataCrt-STARTED
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:12:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsAuthenticated-STARTED
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataKey-STARTED
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-STARTED
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testMessagesTlsScramShaWithPredefinedPassword-STARTED
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-22 for test case:testCertificateWithNonExistingDataCrt
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-22
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-22
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-22
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-23 for test case:testMessagesTlsScramShaWithPredefinedPassword
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-7edfbfbd-custom-certificate-server-1
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [SecretUtils:50] Secret my-cluster-7edfbfbd-custom-certificate-server-1 created
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7edfbfbd in namespace namespace-22
2022-04-09 10:12:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-22
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-24 for test case:testSendMessagesCustomListenerTlsScramSha
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-24
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-154b75ef in namespace namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1695537870-539868899 in namespace namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-628873857-1473410053 in namespace namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-09 10:12:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-154b75ef will have desired state: Ready
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-24
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-24
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-25 for test case:testCertificateWithNonExistingDataKey
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-25
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-17b7a4fe in namespace namespace-24
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-09 10:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-17b7a4fe will have desired state: Ready
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-25
2022-04-09 10:12:25 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-25
2022-04-09 10:12:25 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-26 for test case:testSendMessagesTlsAuthenticated
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-26
2022-04-09 10:12:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-1234584e-custom-certificate-server-1
2022-04-09 10:12:26 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:50] Secret my-cluster-1234584e-custom-certificate-server-1 created
2022-04-09 10:12:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1234584e in namespace namespace-25
2022-04-09 10:12:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-25
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-26
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-26
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-12cad540 in namespace namespace-26
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-26
2022-04-09 10:12:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-12cad540 will have desired state: Ready
2022-04-09 10:13:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:13:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCertificateWithNonExistingDataCrt
2022-04-09 10:13:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7edfbfbd in namespace namespace-22
2022-04-09 10:13:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:13:11 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-22 for test case:testCertificateWithNonExistingDataCrt
2022-04-09 10:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:13:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainAnonymous-STARTED
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-27 for test case:testSendMessagesPlainAnonymous
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-27
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-27
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-27
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0efccd7a in namespace namespace-27
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-27
2022-04-09 10:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0efccd7a will have desired state: Ready
2022-04-09 10:13:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataCrt-FINISHED
2022-04-09 10:13:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:13:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:13:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainScramSha-STARTED
2022-04-09 10:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCertificateWithNonExistingDataKey
2022-04-09 10:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1234584e in namespace namespace-25
2022-04-09 10:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:13:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-25 for test case:testCertificateWithNonExistingDataKey
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-28 for test case:testSendMessagesPlainScramSha
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-28
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-28
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-28
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e55f0aa0 in namespace namespace-28
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-09 10:13:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e55f0aa0 will have desired state: Ready
2022-04-09 10:13:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:13:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-STARTED
2022-04-09 10:13:47 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:13:47 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testNonExistingCustomCertificate-STARTED
2022-04-09 10:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataKey-FINISHED
2022-04-09 10:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:13:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testAdvertisedHostNamesAppearsInBrokerCerts-STARTED
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-29 for test case:testSendMessagesTlsScramSha
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-29
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-29
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-29
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dab184f8 in namespace namespace-29
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-29
2022-04-09 10:13:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dab184f8 will have desired state: Ready
2022-04-09 10:14:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-17b7a4fe is in desired state: Ready
2022-04-09 10:14:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-347473633-83678478 in namespace namespace-29
2022-04-09 10:14:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-09 10:14:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-347473633-83678478 will have desired state: Ready
2022-04-09 10:14:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-347473633-83678478 is in desired state: Ready
2022-04-09 10:14:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1785988425-2087529768 in namespace namespace-29
2022-04-09 10:14:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-09 10:14:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1785988425-2087529768 will have desired state: Ready
2022-04-09 10:14:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1785988425-2087529768 is in desired state: Ready
2022-04-09 10:14:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-17b7a4fe-kafka-clients in namespace namespace-24
2022-04-09 10:14:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-09 10:14:39 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-17b7a4fe-kafka-clients will be ready
2022-04-09 10:14:42 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-17b7a4fe-kafka-clients is ready
2022-04-09 10:14:42 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:14:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ListenersST:442] Checking produced and consumed messages to pod:my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7
2022-04-09 10:14:42 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@68861591, messages=[], arguments=[--max-messages, 100, USER=my_user_1785988425_2087529768, --bootstrap-server, my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122, --topic, my-topic-347473633-83678478], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7', podNamespace='namespace-24', bootstrapServer='my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122', topicName='my-topic-347473633-83678478', maxMessages=100, kafkaUsername='my-user-1785988425-2087529768', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2fb9b552}
2022-04-09 10:14:42 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122:my-topic-347473633-83678478 from pod my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7
2022-04-09 10:14:42 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7 -n namespace-24 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1785988425_2087529768 --bootstrap-server my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122 --topic my-topic-347473633-83678478
2022-04-09 10:14:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-12cad540 is in desired state: Ready
2022-04-09 10:14:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1430739663-405072959 in namespace namespace-29
2022-04-09 10:14:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-26
2022-04-09 10:14:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1430739663-405072959 will have desired state: Ready
2022-04-09 10:14:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 10:14:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 10:14:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@74e317a1, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1602593841, --group-instance-id, instance475633242, USER=my_user_1785988425_2087529768, --bootstrap-server, my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122, --topic, my-topic-347473633-83678478], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7', podNamespace='namespace-24', bootstrapServer='my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122', topicName='my-topic-347473633-83678478', maxMessages=100, kafkaUsername='my-user-1785988425-2087529768', consumerGroupName='my-consumer-group-1602593841', consumerInstanceId='instance475633242', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5915a7ed}
2022-04-09 10:14:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122:my-topic-347473633-83678478 from pod my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7
2022-04-09 10:14:49 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-17b7a4fe-kafka-clients-56cbbcb74c-zrvx7 -n namespace-24 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1602593841 --group-instance-id instance475633242 USER=my_user_1785988425_2087529768 --bootstrap-server my-cluster-17b7a4fe-kafka-bootstrap.namespace-24.svc:9122 --topic my-topic-347473633-83678478
2022-04-09 10:14:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1430739663-405072959 is in desired state: Ready
2022-04-09 10:14:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-781212080-1874019318 in namespace namespace-29
2022-04-09 10:14:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-26
2022-04-09 10:14:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-781212080-1874019318 will have desired state: Ready
2022-04-09 10:14:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-781212080-1874019318 is in desired state: Ready
2022-04-09 10:14:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-12cad540-kafka-clients in namespace namespace-29
2022-04-09 10:14:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-26
2022-04-09 10:14:52 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-12cad540-kafka-clients will be ready
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-154b75ef is in desired state: Ready
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1695537870-539868899 will have desired state: Ready
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1695537870-539868899 is in desired state: Ready
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-628873857-1473410053 will have desired state: Ready
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-628873857-1473410053 is in desired state: Ready
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-154b75ef-kafka-clients in namespace namespace-23
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-09 10:14:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-154b75ef-kafka-clients will be ready
2022-04-09 10:14:55 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-12cad540-kafka-clients is ready
2022-04-09 10:14:55 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:14:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:221] Checking produced and consumed messages to pod:my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd
2022-04-09 10:14:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1f5ac398, messages=[], arguments=[--max-messages, 100, USER=my_user_781212080_1874019318, --bootstrap-server, my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093, --topic, my-topic-1430739663-405072959], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd', podNamespace='namespace-26', bootstrapServer='my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093', topicName='my-topic-1430739663-405072959', maxMessages=100, kafkaUsername='my-user-781212080-1874019318', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7fee3b6b}
2022-04-09 10:14:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093:my-topic-1430739663-405072959 from pod my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd
2022-04-09 10:14:55 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd -n namespace-26 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_781212080_1874019318 --bootstrap-server my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093 --topic my-topic-1430739663-405072959
2022-04-09 10:14:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-154b75ef-kafka-clients is ready
2022-04-09 10:14:58 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:14:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2798f68c, messages=[], arguments=[--max-messages, 100, USER=my_user_1695537870_539868899, --bootstrap-server, my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096, --topic, my-topic-628873857-1473410053], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-154b75ef-kafka-clients-5b9cc44ccb-k47wd', podNamespace='namespace-23', bootstrapServer='my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096', topicName='my-topic-628873857-1473410053', maxMessages=100, kafkaUsername='my-user-1695537870-539868899', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b4bbf33}
2022-04-09 10:14:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096:my-topic-628873857-1473410053 from pod my-cluster-154b75ef-kafka-clients-5b9cc44ccb-k47wd
2022-04-09 10:14:58 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-154b75ef-kafka-clients-5b9cc44ccb-k47wd -n namespace-23 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1695537870_539868899 --bootstrap-server my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096 --topic my-topic-628873857-1473410053
2022-04-09 10:15:00 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 10:15:00 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 10:15:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:15:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesCustomListenerTlsScramSha
2022-04-09 10:15:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1785988425-2087529768 in namespace namespace-24
2022-04-09 10:15:02 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 10:15:02 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 10:15:02 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1e00f8d8, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-797870756, --group-instance-id, instance1370167536, USER=my_user_781212080_1874019318, --bootstrap-server, my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093, --topic, my-topic-1430739663-405072959], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd', podNamespace='namespace-26', bootstrapServer='my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093', topicName='my-topic-1430739663-405072959', maxMessages=100, kafkaUsername='my-user-781212080-1874019318', consumerGroupName='my-consumer-group-797870756', consumerInstanceId='instance1370167536', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@103bbcd7}
2022-04-09 10:15:02 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093:my-topic-1430739663-405072959 from pod my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd
2022-04-09 10:15:02 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-12cad540-kafka-clients-6c66b588b8-x9jkd -n namespace-26 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-797870756 --group-instance-id instance1370167536 USER=my_user_781212080_1874019318 --bootstrap-server my-cluster-12cad540-kafka-bootstrap.namespace-26.svc:9093 --topic my-topic-1430739663-405072959
2022-04-09 10:15:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 10:15:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 10:15:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@f3abc85, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-2100048360, --group-instance-id, instance2106233457, USER=my_user_1695537870_539868899, --bootstrap-server, my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096, --topic, my-topic-628873857-1473410053], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-154b75ef-kafka-clients-5b9cc44ccb-k47wd', podNamespace='namespace-23', bootstrapServer='my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096', topicName='my-topic-628873857-1473410053', maxMessages=100, kafkaUsername='my-user-1695537870-539868899', consumerGroupName='my-consumer-group-2100048360', consumerInstanceId='instance2106233457', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@191e3ab2}
2022-04-09 10:15:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096:my-topic-628873857-1473410053 from pod my-cluster-154b75ef-kafka-clients-5b9cc44ccb-k47wd
2022-04-09 10:15:04 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-154b75ef-kafka-clients-5b9cc44ccb-k47wd -n namespace-23 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-2100048360 --group-instance-id instance2106233457 USER=my_user_1695537870_539868899 --bootstrap-server my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096 --topic my-topic-628873857-1473410053
2022-04-09 10:15:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0efccd7a is in desired state: Ready
2022-04-09 10:15:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-961590338-44408827 in namespace namespace-29
2022-04-09 10:15:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-27
2022-04-09 10:15:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-961590338-44408827 will have desired state: Ready
2022-04-09 10:15:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-961590338-44408827 is in desired state: Ready
2022-04-09 10:15:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0efccd7a-kafka-clients in namespace namespace-29
2022-04-09 10:15:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-27
2022-04-09 10:15:09 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0efccd7a-kafka-clients will be ready
2022-04-09 10:15:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-17b7a4fe-kafka-clients in namespace namespace-24
2022-04-09 10:15:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 10:15:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 10:15:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:15:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsAuthenticated
2022-04-09 10:15:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-781212080-1874019318 in namespace namespace-26
2022-04-09 10:15:12 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0efccd7a-kafka-clients is ready
2022-04-09 10:15:12 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:15:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ListenersST:152] Checking produced and consumed messages to pod:my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth
2022-04-09 10:15:12 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@730f1a82, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092, --topic, my-topic-961590338-44408827], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth', podNamespace='namespace-27', bootstrapServer='my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092', topicName='my-topic-961590338-44408827', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7c8be0f9}
2022-04-09 10:15:12 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092:my-topic-961590338-44408827 from pod my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth
2022-04-09 10:15:12 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth -n namespace-27 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092 --topic my-topic-961590338-44408827
2022-04-09 10:15:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 10:15:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 10:15:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2213] Changing password in secret: my-cluster-154b75ef-secret, we should be able to send/receive messages
2022-04-09 10:15:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:171] Waiting for user password will be changed to Y29tcGxldGVseV9kaWZmZXJlbnRfc2VjcmV0X3Bhc3N3b3Jk in secret: my-user-1695537870-539868899
2022-04-09 10:15:18 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:15:18 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:15:18 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@50219a16, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-720917100, --group-instance-id, instance1544620077, --bootstrap-server, my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092, --topic, my-topic-961590338-44408827], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth', podNamespace='namespace-27', bootstrapServer='my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092', topicName='my-topic-961590338-44408827', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-720917100', consumerInstanceId='instance1544620077', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7d8d78c8}
2022-04-09 10:15:18 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092#my-topic-961590338-44408827 from pod my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth
2022-04-09 10:15:18 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0efccd7a-kafka-clients-7fd86dddf5-6vsth -n namespace-27 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-720917100 --group-instance-id instance1544620077 --bootstrap-server my-cluster-0efccd7a-kafka-bootstrap.namespace-27.svc:9092 --topic my-topic-961590338-44408827
2022-04-09 10:15:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-12cad540-kafka-clients in namespace namespace-26
2022-04-09 10:15:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:15:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:15:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:15:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesPlainAnonymous
2022-04-09 10:15:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-961590338-44408827 in namespace namespace-27
2022-04-09 10:15:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0efccd7a-kafka-clients in namespace namespace-27
2022-04-09 10:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dab184f8 is in desired state: Ready
2022-04-09 10:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2024678425-176840820 in namespace namespace-29
2022-04-09 10:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-29
2022-04-09 10:15:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2024678425-176840820 will have desired state: Ready
2022-04-09 10:15:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2024678425-176840820 is in desired state: Ready
2022-04-09 10:15:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1843135643-1109669691 in namespace namespace-29
2022-04-09 10:15:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-29
2022-04-09 10:15:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1843135643-1109669691 will have desired state: Ready
2022-04-09 10:15:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1843135643-1109669691 is in desired state: Ready
2022-04-09 10:15:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-dab184f8-kafka-clients in namespace namespace-29
2022-04-09 10:15:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-29
2022-04-09 10:15:39 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-dab184f8-kafka-clients will be ready
2022-04-09 10:15:41 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-dab184f8-kafka-clients is ready
2022-04-09 10:15:41 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:15:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ListenersST:370] Checking produced and consumed messages to pod:my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd
2022-04-09 10:15:41 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1519101a, messages=[], arguments=[--max-messages, 100, USER=my_user_1843135643_1109669691, --bootstrap-server, my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096, --topic, my-topic-2024678425-176840820], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd', podNamespace='namespace-29', bootstrapServer='my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096', topicName='my-topic-2024678425-176840820', maxMessages=100, kafkaUsername='my-user-1843135643-1109669691', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@304f8866}
2022-04-09 10:15:41 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096:my-topic-2024678425-176840820 from pod my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd
2022-04-09 10:15:41 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd -n namespace-29 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1843135643_1109669691 --bootstrap-server my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096 --topic my-topic-2024678425-176840820
2022-04-09 10:15:45 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 10:15:45 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 10:15:45 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6389ff27, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-973146169, --group-instance-id, instance783961473, USER=my_user_1843135643_1109669691, --bootstrap-server, my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096, --topic, my-topic-2024678425-176840820], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd', podNamespace='namespace-29', bootstrapServer='my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096', topicName='my-topic-2024678425-176840820', maxMessages=100, kafkaUsername='my-user-1843135643-1109669691', consumerGroupName='my-consumer-group-973146169', consumerInstanceId='instance783961473', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6996e590}
2022-04-09 10:15:45 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096:my-topic-2024678425-176840820 from pod my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd
2022-04-09 10:15:45 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-dab184f8-kafka-clients-5969d5dfc8-sw5sd -n namespace-29 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-973146169 --group-instance-id instance783961473 USER=my_user_1843135643_1109669691 --bootstrap-server my-cluster-dab184f8-kafka-bootstrap.namespace-29.svc:9096 --topic my-topic-2024678425-176840820
2022-04-09 10:15:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-347473633-83678478 in namespace namespace-24
2022-04-09 10:15:53 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 10:15:53 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 10:15:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ListenersST:377] Checking if generated password has 25 characters
2022-04-09 10:15:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:15:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsScramSha
2022-04-09 10:15:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1843135643-1109669691 in namespace namespace-29
2022-04-09 10:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e55f0aa0 is in desired state: Ready
2022-04-09 10:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-381681748-2043337426 in namespace namespace-29
2022-04-09 10:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-09 10:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-381681748-2043337426 will have desired state: Ready
2022-04-09 10:15:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-381681748-2043337426 is in desired state: Ready
2022-04-09 10:15:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-616263708-1176153914 in namespace namespace-29
2022-04-09 10:15:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-09 10:15:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-616263708-1176153914 will have desired state: Ready
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-616263708-1176153914 is in desired state: Ready
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,283 INFO Processing override for entityPath: users/my-user-616263708-1176153914 with config: HashMap(SCRAM-SHA-512 -> [hidden]) (kafka.server.DynamicConfigManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,302 INFO Removing PRODUCE quota for user my-user-616263708-1176153914 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,318 INFO Removing FETCH quota for user my-user-616263708-1176153914 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,318 INFO Removing REQUEST quota for user my-user-616263708-1176153914 (kafka.server.ClientRequestQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,318 INFO Removing CONTROLLER_MUTATION quota for user my-user-616263708-1176153914 (kafka.server.ControllerMutationQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,605 INFO Processing override for entityPath: users/my-user-616263708-1176153914 with config: HashMap(SCRAM-SHA-512 -> [hidden]) (kafka.server.DynamicConfigManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,605 INFO Removing PRODUCE quota for user my-user-616263708-1176153914 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,606 INFO Removing FETCH quota for user my-user-616263708-1176153914 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,606 INFO Removing REQUEST quota for user my-user-616263708-1176153914 (kafka.server.ClientRequestQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-616263708-1176153914: 2022-04-09 10:15:55,606 INFO Removing CONTROLLER_MUTATION quota for user my-user-616263708-1176153914 (kafka.server.ControllerMutationQuotaManager) [/config/changes-event-process-thread]
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e55f0aa0-kafka-clients in namespace namespace-28
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-09 10:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e55f0aa0-kafka-clients will be ready
2022-04-09 10:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e55f0aa0-kafka-clients is ready
2022-04-09 10:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ListenersST:296] Checking produced and consumed messages to pod:my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn
2022-04-09 10:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@14fec845, messages=[], arguments=[--max-messages, 100, USER=my_user_616263708_1176153914, --bootstrap-server, my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095, --topic, my-topic-381681748-2043337426], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn', podNamespace='namespace-28', bootstrapServer='my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095', topicName='my-topic-381681748-2043337426', maxMessages=100, kafkaUsername='my-user-616263708-1176153914', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@154762a0}
2022-04-09 10:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095:my-topic-381681748-2043337426 from pod my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn
2022-04-09 10:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn -n namespace-28 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_616263708_1176153914 --bootstrap-server my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095 --topic my-topic-381681748-2043337426
2022-04-09 10:16:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-17b7a4fe in namespace namespace-24
2022-04-09 10:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@519311ec, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1966003217, --group-instance-id, instance1056011658, USER=my_user_616263708_1176153914, --bootstrap-server, my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095, --topic, my-topic-381681748-2043337426], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn', podNamespace='namespace-28', bootstrapServer='my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095', topicName='my-topic-381681748-2043337426', maxMessages=100, kafkaUsername='my-user-616263708-1176153914', consumerGroupName='my-consumer-group-1966003217', consumerInstanceId='instance1056011658', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b29f105}
2022-04-09 10:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095#my-topic-381681748-2043337426 from pod my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn
2022-04-09 10:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e55f0aa0-kafka-clients-796559b5-6p8gn -n namespace-28 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1966003217 --group-instance-id instance1056011658 USER=my_user_616263708_1176153914 --bootstrap-server my-cluster-e55f0aa0-kafka-bootstrap.namespace-28.svc:9095 --topic my-topic-381681748-2043337426
2022-04-09 10:16:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1430739663-405072959 in namespace namespace-26
2022-04-09 10:16:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-dab184f8-kafka-clients in namespace namespace-29
2022-04-09 10:16:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0efccd7a in namespace namespace-27
2022-04-09 10:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesPlainScramSha
2022-04-09 10:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-616263708-1176153914 in namespace namespace-28
2022-04-09 10:16:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:16:10 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-24 for test case:testSendMessagesCustomListenerTlsScramSha
2022-04-09 10:16:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-12cad540 in namespace namespace-26
2022-04-09 10:16:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:16:15 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-27 for test case:testSendMessagesPlainAnonymous
2022-04-09 10:16:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e55f0aa0-kafka-clients in namespace namespace-28
2022-04-09 10:16:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:16:21 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-26 for test case:testSendMessagesTlsAuthenticated
2022-04-09 10:16:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2222] We need to recreate Kafka Clients deployment, so the correct password from secret will be taken
2022-04-09 10:16:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-154b75ef-kafka-clients in namespace namespace-23
2022-04-09 10:16:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2024678425-176840820 in namespace namespace-29
2022-04-09 10:16:54 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-FINISHED
2022-04-09 10:16:54 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:16:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-381681748-2043337426 in namespace namespace-28
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-30 for test case:testNonExistingCustomCertificate
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-30
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-30
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-30
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c134ad57 in namespace namespace-30
2022-04-09 10:16:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-30
2022-04-09 10:16:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainAnonymous-FINISHED
2022-04-09 10:16:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:16:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e55f0aa0 in namespace namespace-28
2022-04-09 10:17:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-dab184f8 in namespace namespace-29
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-31 for test case:testAdvertisedHostNamesAppearsInBrokerCerts
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-31
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-31
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-31
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-13f24d27 in namespace namespace-31
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-31
2022-04-09 10:17:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-13f24d27 will have desired state: Ready
2022-04-09 10:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsAuthenticated-FINISHED
2022-04-09 10:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:17:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:17:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-29 for test case:testSendMessagesTlsScramSha
2022-04-09 10:17:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:17:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-28 for test case:testSendMessagesPlainScramSha
2022-04-09 10:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-154b75ef-kafka-clients in namespace namespace-23
2022-04-09 10:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-09 10:17:21 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-154b75ef-kafka-clients will be ready
2022-04-09 10:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-154b75ef-kafka-clients is ready
2022-04-09 10:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2226] Receiving messages with new password
2022-04-09 10:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@56dea16f, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-892677635, --group-instance-id, instance448227054, USER=my_user_1695537870_539868899, --bootstrap-server, my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096, --topic, my-topic-628873857-1473410053], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-154b75ef-kafka-clients-8589558cd4-p5mnz', podNamespace='namespace-23', bootstrapServer='my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096', topicName='my-topic-628873857-1473410053', maxMessages=100, kafkaUsername='my-user-1695537870-539868899', consumerGroupName='my-consumer-group-892677635', consumerInstanceId='instance448227054', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3e7dc7b8}
2022-04-09 10:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096:my-topic-628873857-1473410053 from pod my-cluster-154b75ef-kafka-clients-8589558cd4-p5mnz
2022-04-09 10:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-154b75ef-kafka-clients-8589558cd4-p5mnz -n namespace-23 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-892677635 --group-instance-id instance448227054 USER=my_user_1695537870_539868899 --bootstrap-server my-cluster-154b75ef-kafka-bootstrap.namespace-23.svc:9096 --topic my-topic-628873857-1473410053
2022-04-09 10:17:32 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 10:17:32 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 10:17:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:17:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMessagesTlsScramShaWithPredefinedPassword
2022-04-09 10:17:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-628873857-1473410053 in namespace namespace-23
2022-04-09 10:17:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:17:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testNonExistingCustomCertificate
2022-04-09 10:17:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c134ad57 in namespace namespace-30
2022-04-09 10:17:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:17:34 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-30 for test case:testNonExistingCustomCertificate
2022-04-09 10:17:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1695537870-539868899 in namespace namespace-23
2022-04-09 10:17:39 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testNonExistingCustomCertificate-FINISHED
2022-04-09 10:17:39 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:17:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-154b75ef-kafka-clients in namespace namespace-23
2022-04-09 10:17:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-154b75ef in namespace namespace-23
2022-04-09 10:17:44 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainScramSha-FINISHED
2022-04-09 10:17:44 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:17:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-154b75ef-kafka-clients in namespace namespace-23
2022-04-09 10:17:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-FINISHED
2022-04-09 10:17:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:18:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:18:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-23 for test case:testMessagesTlsScramShaWithPredefinedPassword
2022-04-09 10:18:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testMessagesTlsScramShaWithPredefinedPassword-FINISHED
2022-04-09 10:18:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-13f24d27 is in desired state: Ready
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ListenersST:2338] Encoding my-cluster-13f24d27-kafka-0.crt
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ListenersST:2338] Encoding my-cluster-13f24d27-kafka-1.crt
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ListenersST:2338] Encoding my-cluster-13f24d27-kafka-2.crt
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testAdvertisedHostNamesAppearsInBrokerCerts
2022-04-09 10:18:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-13f24d27 in namespace namespace-31
2022-04-09 10:18:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:18:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-31 for test case:testAdvertisedHostNamesAppearsInBrokerCerts
2022-04-09 10:19:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testAdvertisedHostNamesAppearsInBrokerCerts-FINISHED
2022-04-09 10:19:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ListenersST is everything deleted.
2022-04-09 10:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 444.254 s - in io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.MultipleListenersST
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:294] Starting to generate test cases for multiple listeners
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating INTERNAL listener
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type INTERNAL -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@d507d834, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@235fe642]
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating ROUTE listener
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type ROUTE -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@39103478]
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating LOADBALANCER listener
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type LOADBALANCER -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@b26dc37f, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@f3c5af9b]
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating NODEPORT listener
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type NODEPORT -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@1926625c, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@677e706a, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@9bd63a94, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@ea2e48a2]
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating INGRESS listener
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type INGRESS -> []
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:367] Finished with generation of test cases for multiple listeners
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: multiple-listeners-st
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-listeners-st
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: multiple-listeners-st
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.MultipleListenersST.testMultipleInternal-STARTED
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:163] This is listeners [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@d507d834, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@235fe642], which will verified.
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2836bb5b in namespace multiple-listeners-st
2022-04-09 10:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2836bb5b will have desired state: Ready
2022-04-09 10:20:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2836bb5b is in desired state: Ready
2022-04-09 10:20:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1087784090-281879305 in namespace multiple-listeners-st
2022-04-09 10:20:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1087784090-281879305 will have desired state: Ready
2022-04-09 10:21:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1087784090-281879305 is in desired state: Ready
2022-04-09 10:21:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-463330357-1387454653 in namespace multiple-listeners-st
2022-04-09 10:21:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-463330357-1387454653 will have desired state: Ready
2022-04-09 10:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-463330357-1387454653 is in desired state: Ready
2022-04-09 10:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2836bb5b-kafka-clients-tls in namespace multiple-listeners-st
2022-04-09 10:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2836bb5b-kafka-clients-tls will be ready
2022-04-09 10:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2836bb5b-kafka-clients-tls is ready
2022-04-09 10:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:252] Checking produced and consumed messages to pod:my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz
2022-04-09 10:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1556071240-1199795575 in namespace multiple-listeners-st
2022-04-09 10:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1556071240-1199795575 will have desired state: Ready
2022-04-09 10:21:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1556071240-1199795575 is in desired state: Ready
2022-04-09 10:21:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:127] Sending messages to - topic my-topic-463330357-1387454653, cluster my-cluster-2836bb5b and message count of 100
2022-04-09 10:21:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@75f89ec5, messages=[], arguments=[--max-messages, 100, USER=my_user_1087784090_281879305, --bootstrap-server, my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900, --topic, my-topic-1556071240-1199795575], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900', topicName='my-topic-1556071240-1199795575', maxMessages=100, kafkaUsername='my-user-1087784090-281879305', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7fbd6069}
2022-04-09 10:21:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900:my-topic-1556071240-1199795575 from pod my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz
2022-04-09 10:21:03 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz -n multiple-listeners-st -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1087784090_281879305 --bootstrap-server my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900 --topic my-topic-1556071240-1199795575
2022-04-09 10:21:07 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 10:21:07 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 10:21:07 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@761dd479, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-222385484, --group-instance-id, instance891589211, USER=my_user_1087784090_281879305, --bootstrap-server, my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900, --topic, my-topic-1556071240-1199795575], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900', topicName='my-topic-1556071240-1199795575', maxMessages=100, kafkaUsername='my-user-1087784090-281879305', consumerGroupName='my-consumer-group-222385484', consumerInstanceId='instance891589211', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@62b75e6e}
2022-04-09 10:21:07 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900:my-topic-1556071240-1199795575 from pod my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz
2022-04-09 10:21:07 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2836bb5b-kafka-clients-tls-6d4dff7d45-ftchz -n multiple-listeners-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-222385484 --group-instance-id instance891589211 USER=my_user_1087784090_281879305 --bootstrap-server my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13900 --topic my-topic-1556071240-1199795575
2022-04-09 10:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 10:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 10:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:133] Sent 100 and received 100
2022-04-09 10:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1412792981-1096760433 in namespace multiple-listeners-st
2022-04-09 10:21:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1412792981-1096760433 will have desired state: Ready
2022-04-09 10:21:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1412792981-1096760433 is in desired state: Ready
2022-04-09 10:21:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2836bb5b-kafka-clients-plain in namespace multiple-listeners-st
2022-04-09 10:21:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2836bb5b-kafka-clients-plain will be ready
2022-04-09 10:21:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2836bb5b-kafka-clients-plain is ready
2022-04-09 10:21:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:21:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:274] Checking produced and consumed messages to pod:my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj
2022-04-09 10:21:17 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4e09348d, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901, --topic, my-topic-1412792981-1096760433], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901', topicName='my-topic-1412792981-1096760433', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@47a272c9}
2022-04-09 10:21:17 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901:my-topic-1412792981-1096760433 from pod my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj
2022-04-09 10:21:17 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj -n multiple-listeners-st -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901 --topic my-topic-1412792981-1096760433
2022-04-09 10:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@eb3f7ab, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-840500104, --group-instance-id, instance86387702, --bootstrap-server, my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901, --topic, my-topic-1412792981-1096760433], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901', topicName='my-topic-1412792981-1096760433', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-840500104', consumerInstanceId='instance86387702', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1615470c}
2022-04-09 10:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901#my-topic-1412792981-1096760433 from pod my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj
2022-04-09 10:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2836bb5b-kafka-clients-plain-7c954fd45c-rkltj -n multiple-listeners-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-840500104 --group-instance-id instance86387702 --bootstrap-server my-cluster-2836bb5b-kafka-bootstrap.multiple-listeners-st.svc:13901 --topic my-topic-1412792981-1096760433
2022-04-09 10:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMultipleInternal
2022-04-09 10:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1556071240-1199795575 in namespace multiple-listeners-st
2022-04-09 10:21:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2836bb5b-kafka-clients-plain in namespace multiple-listeners-st
2022-04-09 10:21:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1087784090-281879305 in namespace multiple-listeners-st
2022-04-09 10:21:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2836bb5b in namespace multiple-listeners-st
2022-04-09 10:21:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2836bb5b-kafka-clients-tls in namespace multiple-listeners-st
2022-04-09 10:21:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-463330357-1387454653 in namespace multiple-listeners-st
2022-04-09 10:21:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1412792981-1096760433 in namespace multiple-listeners-st
2022-04-09 10:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.MultipleListenersST.testMultipleInternal-FINISHED
2022-04-09 10:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MultipleListenersST is everything deleted.
2022-04-09 10:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 152.693 s - in io.strimzi.systemtest.kafka.listeners.MultipleListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: dynamic-conf-st
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: dynamic-conf-st
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: dynamic-conf-st
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testSimpleDynamicConfiguration-STARTED
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-18f0207b in namespace dynamic-conf-st
2022-04-09 10:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-18f0207b will have desired state: Ready
2022-04-09 10:23:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-18f0207b is in desired state: Ready
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-18f0207b-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-18f0207b-kafka are stable
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:23:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:23:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:23:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:23:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:23:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:23:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:23:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:23:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:23:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:23:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:23:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:23:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:23:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:23:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:23:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:23:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:23:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:23:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:23:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:23:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:23:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:23:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:23:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:23:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:23:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:24:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:24:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:24:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:24:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:24:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:24:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:24:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:24:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:24:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:24:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:24:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:24:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:24:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:24:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:24:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:24:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:24:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:24:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:24:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:24:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:24:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:24:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:24:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:24:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:24:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:24:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:24:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:24:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:24:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:24:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:24:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:24:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:24:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:24:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:24:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:24:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:24:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:24:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:24:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:24:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:24:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:24:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:24:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:24:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:24:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:24:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:24:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:24:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:24:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:24:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:24:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:24:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:24:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:24:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:24:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:24:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:24:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:24:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:24:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:24:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:24:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:24:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:24:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:24:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:24:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:24:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:24:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:24:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:24:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:24:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:24:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:24:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:24:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:24:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:24:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:24:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:24:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:24:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:24:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:24:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-18f0207b-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-18f0207b-kafka-0 ,my-cluster-18f0207b-kafka-1 ,my-cluster-18f0207b-kafka-2
2022-04-09 10:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-18f0207b-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:102] Verify values after update
2022-04-09 10:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSimpleDynamicConfiguration
2022-04-09 10:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-18f0207b in namespace dynamic-conf-st
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testSimpleDynamicConfiguration-FINISHED
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testUpdateToExternalListenerCausesRollingRestart-STARTED
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3248aed1 in namespace dynamic-conf-st
2022-04-09 10:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3248aed1 will have desired state: Ready
2022-04-09 10:26:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3248aed1 is in desired state: Ready
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-3248aed1-kafka are stable
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:26:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:26:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:26:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:26:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:26:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:26:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:26:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:26:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:26:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:26:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:26:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:26:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:26:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:26:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:26:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:26:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:26:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:26:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:26:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:26:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:26:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:26:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:26:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:26:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:26:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:26:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:26:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:26:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:26:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:26:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:26:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:26:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:26:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:26:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:26:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:26:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:26:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:26:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:26:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:26:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:26:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:26:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:26:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:26:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:26:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:26:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:26:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:26:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:26:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:26:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:26:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:26:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:26:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:26:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:26:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:26:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:26:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:26:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:26:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:26:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:26:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:26:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:26:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:26:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:26:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:26:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:26:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:26:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:26:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:26:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:26:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:26:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:26:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:26:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-3248aed1-kafka-0 ,my-cluster-3248aed1-kafka-1 ,my-cluster-3248aed1-kafka-2
2022-04-09 10:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:163] Updating listeners of Kafka cluster
2022-04-09 10:27:01 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-3248aed1-kafka rolling update
2022-04-09 10:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-3248aed1-kafka has been successfully rolled
2022-04-09 10:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-3248aed1-kafka to be ready
2022-04-09 10:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3248aed1 will have desired state: Ready
2022-04-09 10:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3248aed1 is in desired state: Ready
2022-04-09 10:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-3248aed1 is ready
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-3248aed1-kafka are stable
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:28:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:28:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:28:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:28:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:28:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:28:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:29:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:29:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:29:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:29:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:29:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:29:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:29:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:29:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:29:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:29:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:29:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:29:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:29:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:29:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:29:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:29:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:29:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:29:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:29:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:29:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:29:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:29:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:29:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:29:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:29:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:29:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:29:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:29:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:29:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:29:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:29:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:29:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:29:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-3248aed1-kafka-0 ,my-cluster-3248aed1-kafka-1 ,my-cluster-3248aed1-kafka-2
2022-04-09 10:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-3248aed1-kafka are stable
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:30:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:30:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:30:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:30:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:30:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:30:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:30:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:30:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:30:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:30:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:30:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:30:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:30:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:30:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:30:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:30:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:30:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:30:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:30:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:30:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:30:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:30:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:30:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:30:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:30:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:30:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:30:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:30:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:30:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:30:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:30:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-3248aed1-kafka-0 ,my-cluster-3248aed1-kafka-1 ,my-cluster-3248aed1-kafka-2
2022-04-09 10:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:214] Updating listeners of Kafka cluster
2022-04-09 10:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-3248aed1-kafka rolling update
2022-04-09 10:31:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-3248aed1-kafka has been successfully rolled
2022-04-09 10:31:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-3248aed1-kafka to be ready
2022-04-09 10:32:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3248aed1 will have desired state: Ready
2022-04-09 10:32:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3248aed1 is in desired state: Ready
2022-04-09 10:32:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-3248aed1 is ready
2022-04-09 10:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-09 10:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-3248aed1-kafka are stable
2022-04-09 10:32:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:32:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:32:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 10:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 10:32:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:32:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:32:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 10:32:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:32:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:32:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 10:32:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:32:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:32:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 10:32:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:32:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:32:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 10:32:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:32:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:32:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 10:32:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:32:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:32:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 10:32:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:32:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:32:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 10:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 10:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 10:32:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:32:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:32:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 10:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 10:32:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:32:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:32:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 10:32:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:32:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:32:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 10:32:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:32:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:32:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 10:32:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:32:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:32:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 10:32:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:32:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:32:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 10:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 10:32:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:32:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:32:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 10:32:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:32:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:32:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 10:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 10:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 10:32:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:32:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:32:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 10:32:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:32:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:32:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 10:32:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:32:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:32:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 10:32:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:32:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:32:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 10:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 10:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 10:32:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:32:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:32:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 10:32:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:32:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:32:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 10:32:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:32:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:32:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 10:32:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:32:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:32:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 10:32:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:32:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:32:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 10:32:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:32:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:32:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 10:33:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:33:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:33:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 10:33:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:33:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:33:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 10:33:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:33:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:33:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 10:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 10:33:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:33:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:33:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 10:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 10:33:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:33:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:33:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 10:33:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:33:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:33:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 10:33:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:33:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:33:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 10:33:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:33:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:33:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 10:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 10:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 10:33:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:33:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:33:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 10:33:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:33:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:33:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 10:33:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:33:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:33:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-3248aed1-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 10:33:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-3248aed1-kafka-0 ,my-cluster-3248aed1-kafka-1 ,my-cluster-3248aed1-kafka-2
2022-04-09 10:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-3248aed1-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateToExternalListenerCausesRollingRestart
2022-04-09 10:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3248aed1 in namespace dynamic-conf-st
2022-04-09 10:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testUpdateToExternalListenerCausesRollingRestart-FINISHED
2022-04-09 10:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context DynamicConfST is everything deleted.
2022-04-09 10:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 709.927 s - in io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST
2022-04-09 10:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: dynamic-conf-shared-st
2022-04-09 10:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: dynamic-conf-shared-st
2022-04-09 10:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: dynamic-conf-shared-st
2022-04-09 10:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfSharedST:218] Deploying shared Kafka across all test cases!
2022-04-09 10:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka dynamic-configuration-shared-cluster-name in namespace dynamic-conf-shared-st
2022-04-09 10:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: dynamic-configuration-shared-cluster-name will have desired state: Ready
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: dynamic-configuration-shared-cluster-name is in desired state: Ready
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST.testDynConfiguration-STARTED
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:334] Kafka config {advertised.listeners=io.strimzi.kafka.config.model.ConfigModel@6111eada, alter.config.policy.class.name=io.strimzi.kafka.config.model.ConfigModel@72503941, alter.log.dirs.replication.quota.window.num=io.strimzi.kafka.config.model.ConfigModel@712338f4, alter.log.dirs.replication.quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@3e193811, authorizer.class.name=io.strimzi.kafka.config.model.ConfigModel@50305232, auto.create.topics.enable=io.strimzi.kafka.config.model.ConfigModel@2359e344, auto.leader.rebalance.enable=io.strimzi.kafka.config.model.ConfigModel@4bd59b43, background.threads=io.strimzi.kafka.config.model.ConfigModel@3c7bfb98, broker.heartbeat.interval.ms=io.strimzi.kafka.config.model.ConfigModel@6e889fe7, broker.id=io.strimzi.kafka.config.model.ConfigModel@22712b1, broker.id.generation.enable=io.strimzi.kafka.config.model.ConfigModel@65cf8e35, broker.rack=io.strimzi.kafka.config.model.ConfigModel@68b1e2f, broker.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@2cd6debe, client.quota.callback.class=io.strimzi.kafka.config.model.ConfigModel@16dee30f, compression.type=io.strimzi.kafka.config.model.ConfigModel@f7979c7, connection.failed.authentication.delay.ms=io.strimzi.kafka.config.model.ConfigModel@51dd6461, connections.max.idle.ms=io.strimzi.kafka.config.model.ConfigModel@4be473ca, connections.max.reauth.ms=io.strimzi.kafka.config.model.ConfigModel@2ac33722, control.plane.listener.name=io.strimzi.kafka.config.model.ConfigModel@6eb0c4fc, controlled.shutdown.enable=io.strimzi.kafka.config.model.ConfigModel@54744b27, controlled.shutdown.max.retries=io.strimzi.kafka.config.model.ConfigModel@3ce60840, controlled.shutdown.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@faf52d9, controller.listener.names=io.strimzi.kafka.config.model.ConfigModel@6750e0f4, controller.quorum.append.linger.ms=io.strimzi.kafka.config.model.ConfigModel@110bb24a, controller.quorum.election.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@1da31c23, controller.quorum.election.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@68efbcd4, controller.quorum.fetch.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@858a67d, controller.quorum.request.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@448dccde, controller.quorum.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@41ca9268, controller.quota.window.num=io.strimzi.kafka.config.model.ConfigModel@54d69f99, controller.quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@19f0a030, controller.socket.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@33b164ec, create.topic.policy.class.name=io.strimzi.kafka.config.model.ConfigModel@267de920, default.replication.factor=io.strimzi.kafka.config.model.ConfigModel@46378021, delegation.token.expiry.check.interval.ms=io.strimzi.kafka.config.model.ConfigModel@5ada49f7, delegation.token.expiry.time.ms=io.strimzi.kafka.config.model.ConfigModel@62f4c044, delegation.token.master.key=io.strimzi.kafka.config.model.ConfigModel@2b8911d3, delegation.token.max.lifetime.ms=io.strimzi.kafka.config.model.ConfigModel@51c5aa0b, delegation.token.secret.key=io.strimzi.kafka.config.model.ConfigModel@5958c435, delete.records.purgatory.purge.interval.requests=io.strimzi.kafka.config.model.ConfigModel@1d6ad9ba, delete.topic.enable=io.strimzi.kafka.config.model.ConfigModel@7e0e444d, fetch.max.bytes=io.strimzi.kafka.config.model.ConfigModel@2e73c75, fetch.purgatory.purge.interval.requests=io.strimzi.kafka.config.model.ConfigModel@1d013970, group.initial.rebalance.delay.ms=io.strimzi.kafka.config.model.ConfigModel@566d9463, group.max.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@5acd8ecf, group.max.size=io.strimzi.kafka.config.model.ConfigModel@61721577, group.min.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@75ac7a90, initial.broker.registration.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@4fd826ca, inter.broker.listener.name=io.strimzi.kafka.config.model.ConfigModel@79556208, inter.broker.protocol.version=io.strimzi.kafka.config.model.ConfigModel@8fd19f8, kafka.metrics.polling.interval.secs=io.strimzi.kafka.config.model.ConfigModel@1e139a8d, kafka.metrics.reporters=io.strimzi.kafka.config.model.ConfigModel@107311f3, leader.imbalance.check.interval.seconds=io.strimzi.kafka.config.model.ConfigModel@42e474d7, leader.imbalance.per.broker.percentage=io.strimzi.kafka.config.model.ConfigModel@223687aa, listener.security.protocol.map=io.strimzi.kafka.config.model.ConfigModel@20f3b693, listeners=io.strimzi.kafka.config.model.ConfigModel@2887c7f7, log.cleaner.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@49245cfc, log.cleaner.dedupe.buffer.size=io.strimzi.kafka.config.model.ConfigModel@5fb989a5, log.cleaner.delete.retention.ms=io.strimzi.kafka.config.model.ConfigModel@6243fbef, log.cleaner.enable=io.strimzi.kafka.config.model.ConfigModel@746d302f, log.cleaner.io.buffer.load.factor=io.strimzi.kafka.config.model.ConfigModel@1fc551e2, log.cleaner.io.buffer.size=io.strimzi.kafka.config.model.ConfigModel@c22171e, log.cleaner.io.max.bytes.per.second=io.strimzi.kafka.config.model.ConfigModel@71c319f6, log.cleaner.max.compaction.lag.ms=io.strimzi.kafka.config.model.ConfigModel@6cb2a3a9, log.cleaner.min.cleanable.ratio=io.strimzi.kafka.config.model.ConfigModel@594da59c, log.cleaner.min.compaction.lag.ms=io.strimzi.kafka.config.model.ConfigModel@454bc34c, log.cleaner.threads=io.strimzi.kafka.config.model.ConfigModel@2cae6eba, log.cleanup.policy=io.strimzi.kafka.config.model.ConfigModel@55f83f6d, log.dir=io.strimzi.kafka.config.model.ConfigModel@612d110b, log.dirs=io.strimzi.kafka.config.model.ConfigModel@7a67ffc1, log.flush.interval.messages=io.strimzi.kafka.config.model.ConfigModel@25638589, log.flush.interval.ms=io.strimzi.kafka.config.model.ConfigModel@31f4d51f, log.flush.offset.checkpoint.interval.ms=io.strimzi.kafka.config.model.ConfigModel@38d82f63, log.flush.scheduler.interval.ms=io.strimzi.kafka.config.model.ConfigModel@7d84464e, log.flush.start.offset.checkpoint.interval.ms=io.strimzi.kafka.config.model.ConfigModel@44f2dbba, log.index.interval.bytes=io.strimzi.kafka.config.model.ConfigModel@53b9b216, log.index.size.max.bytes=io.strimzi.kafka.config.model.ConfigModel@2ae33c35, log.message.downconversion.enable=io.strimzi.kafka.config.model.ConfigModel@758ab911, log.message.format.version=io.strimzi.kafka.config.model.ConfigModel@27f78311, log.message.timestamp.difference.max.ms=io.strimzi.kafka.config.model.ConfigModel@39c0bcfb, log.message.timestamp.type=io.strimzi.kafka.config.model.ConfigModel@af13d62, log.preallocate=io.strimzi.kafka.config.model.ConfigModel@2a135f1c, log.retention.bytes=io.strimzi.kafka.config.model.ConfigModel@31e7be44, log.retention.check.interval.ms=io.strimzi.kafka.config.model.ConfigModel@d7cfbe7, log.retention.hours=io.strimzi.kafka.config.model.ConfigModel@e65dd07, log.retention.minutes=io.strimzi.kafka.config.model.ConfigModel@3e3e9868, log.retention.ms=io.strimzi.kafka.config.model.ConfigModel@32271475, log.roll.hours=io.strimzi.kafka.config.model.ConfigModel@58ee7215, log.roll.jitter.hours=io.strimzi.kafka.config.model.ConfigModel@78c311ea, log.roll.jitter.ms=io.strimzi.kafka.config.model.ConfigModel@32f42b42, log.roll.ms=io.strimzi.kafka.config.model.ConfigModel@387a15d5, log.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@132fb615, log.segment.delete.delay.ms=io.strimzi.kafka.config.model.ConfigModel@7383b9db, max.connection.creation.rate=io.strimzi.kafka.config.model.ConfigModel@60be32d2, max.connections=io.strimzi.kafka.config.model.ConfigModel@88257ad, max.connections.per.ip=io.strimzi.kafka.config.model.ConfigModel@6673c7b6, max.connections.per.ip.overrides=io.strimzi.kafka.config.model.ConfigModel@8d0f7c2, max.incremental.fetch.session.cache.slots=io.strimzi.kafka.config.model.ConfigModel@19697ca8, message.max.bytes=io.strimzi.kafka.config.model.ConfigModel@3cc6f17, metadata.log.dir=io.strimzi.kafka.config.model.ConfigModel@7bfa56af, metadata.log.max.record.bytes.between.snapshots=io.strimzi.kafka.config.model.ConfigModel@741bd0f5, metadata.log.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@26457ee2, metadata.log.segment.min.bytes=io.strimzi.kafka.config.model.ConfigModel@469119ea, metadata.log.segment.ms=io.strimzi.kafka.config.model.ConfigModel@6f7770b9, metadata.max.retention.bytes=io.strimzi.kafka.config.model.ConfigModel@45c189da, metadata.max.retention.ms=io.strimzi.kafka.config.model.ConfigModel@487e4dee, metric.reporters=io.strimzi.kafka.config.model.ConfigModel@6ee4342, metrics.num.samples=io.strimzi.kafka.config.model.ConfigModel@5248099e, metrics.recording.level=io.strimzi.kafka.config.model.ConfigModel@1b0ed554, metrics.sample.window.ms=io.strimzi.kafka.config.model.ConfigModel@2125eae6, min.insync.replicas=io.strimzi.kafka.config.model.ConfigModel@55a559d3, node.id=io.strimzi.kafka.config.model.ConfigModel@47b3be00, num.io.threads=io.strimzi.kafka.config.model.ConfigModel@2afa77d7, num.network.threads=io.strimzi.kafka.config.model.ConfigModel@2406dab4, num.partitions=io.strimzi.kafka.config.model.ConfigModel@5b926426, num.recovery.threads.per.data.dir=io.strimzi.kafka.config.model.ConfigModel@18b92ea, num.replica.alter.log.dirs.threads=io.strimzi.kafka.config.model.ConfigModel@783619c3, num.replica.fetchers=io.strimzi.kafka.config.model.ConfigModel@6a77af0b, offset.metadata.max.bytes=io.strimzi.kafka.config.model.ConfigModel@220c4537, offsets.commit.required.acks=io.strimzi.kafka.config.model.ConfigModel@7f4d8f5e, offsets.commit.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@37fd6975, offsets.load.buffer.size=io.strimzi.kafka.config.model.ConfigModel@60258776, offsets.retention.check.interval.ms=io.strimzi.kafka.config.model.ConfigModel@20c053b8, offsets.retention.minutes=io.strimzi.kafka.config.model.ConfigModel@33640f81, offsets.topic.compression.codec=io.strimzi.kafka.config.model.ConfigModel@6f8e7e59, offsets.topic.num.partitions=io.strimzi.kafka.config.model.ConfigModel@23dc5ae9, offsets.topic.replication.factor=io.strimzi.kafka.config.model.ConfigModel@12390b9, offsets.topic.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@12289f1b, password.encoder.cipher.algorithm=io.strimzi.kafka.config.model.ConfigModel@e4847b1, password.encoder.iterations=io.strimzi.kafka.config.model.ConfigModel@4f824f83, password.encoder.key.length=io.strimzi.kafka.config.model.ConfigModel@52b9b892, password.encoder.keyfactory.algorithm=io.strimzi.kafka.config.model.ConfigModel@4bf3599, password.encoder.old.secret=io.strimzi.kafka.config.model.ConfigModel@48068f70, password.encoder.secret=io.strimzi.kafka.config.model.ConfigModel@15a53fa4, principal.builder.class=io.strimzi.kafka.config.model.ConfigModel@ac778aa, process.roles=io.strimzi.kafka.config.model.ConfigModel@77298796, producer.purgatory.purge.interval.requests=io.strimzi.kafka.config.model.ConfigModel@ba8fb0c, queued.max.request.bytes=io.strimzi.kafka.config.model.ConfigModel@374cadc2, queued.max.requests=io.strimzi.kafka.config.model.ConfigModel@6ff81a9, quota.window.num=io.strimzi.kafka.config.model.ConfigModel@3dd1b3d7, quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@716d4ffa, remote.log.index.file.cache.total.size.bytes=io.strimzi.kafka.config.model.ConfigModel@231ddea8, remote.log.manager.task.interval.ms=io.strimzi.kafka.config.model.ConfigModel@5995e1ab, remote.log.manager.task.retry.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@69eaae45, remote.log.manager.task.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@72e32e1d, remote.log.manager.task.retry.jitter=io.strimzi.kafka.config.model.ConfigModel@3fc65677, remote.log.manager.thread.pool.size=io.strimzi.kafka.config.model.ConfigModel@49a8609d, remote.log.metadata.manager.class.name=io.strimzi.kafka.config.model.ConfigModel@750c776b, remote.log.metadata.manager.class.path=io.strimzi.kafka.config.model.ConfigModel@1ef42c11, remote.log.metadata.manager.impl.prefix=io.strimzi.kafka.config.model.ConfigModel@130cb456, remote.log.metadata.manager.listener.name=io.strimzi.kafka.config.model.ConfigModel@19aa8222, remote.log.reader.max.pending.tasks=io.strimzi.kafka.config.model.ConfigModel@f0b0953, remote.log.reader.threads=io.strimzi.kafka.config.model.ConfigModel@30202807, remote.log.storage.manager.class.name=io.strimzi.kafka.config.model.ConfigModel@46788451, remote.log.storage.manager.class.path=io.strimzi.kafka.config.model.ConfigModel@b0eb52e, remote.log.storage.manager.impl.prefix=io.strimzi.kafka.config.model.ConfigModel@5c98648d, remote.log.storage.system.enable=io.strimzi.kafka.config.model.ConfigModel@6dc90f09, replica.fetch.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@6f9144ed, replica.fetch.max.bytes=io.strimzi.kafka.config.model.ConfigModel@11483bf4, replica.fetch.min.bytes=io.strimzi.kafka.config.model.ConfigModel@eeb995c, replica.fetch.response.max.bytes=io.strimzi.kafka.config.model.ConfigModel@2bff7ecf, replica.fetch.wait.max.ms=io.strimzi.kafka.config.model.ConfigModel@4e2ce171, replica.high.watermark.checkpoint.interval.ms=io.strimzi.kafka.config.model.ConfigModel@1e6be4db, replica.lag.time.max.ms=io.strimzi.kafka.config.model.ConfigModel@4c98af33, replica.selector.class=io.strimzi.kafka.config.model.ConfigModel@1c41a969, replica.socket.receive.buffer.bytes=io.strimzi.kafka.config.model.ConfigModel@354ff4f9, replica.socket.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@74c393b7, replication.quota.window.num=io.strimzi.kafka.config.model.ConfigModel@7dab4b2b, replication.quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@6c30bf16, request.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@26126655, reserved.broker.max.id=io.strimzi.kafka.config.model.ConfigModel@4e24070b, sasl.client.callback.handler.class=io.strimzi.kafka.config.model.ConfigModel@6971fff7, sasl.enabled.mechanisms=io.strimzi.kafka.config.model.ConfigModel@5035c0, sasl.jaas.config=io.strimzi.kafka.config.model.ConfigModel@3545cb51, sasl.kerberos.kinit.cmd=io.strimzi.kafka.config.model.ConfigModel@5bb79ead, sasl.kerberos.min.time.before.relogin=io.strimzi.kafka.config.model.ConfigModel@3ddf5c6b, sasl.kerberos.principal.to.local.rules=io.strimzi.kafka.config.model.ConfigModel@3c208025, sasl.kerberos.service.name=io.strimzi.kafka.config.model.ConfigModel@27a4cbb3, sasl.kerberos.ticket.renew.jitter=io.strimzi.kafka.config.model.ConfigModel@40017b1c, sasl.kerberos.ticket.renew.window.factor=io.strimzi.kafka.config.model.ConfigModel@77a5b73, sasl.login.callback.handler.class=io.strimzi.kafka.config.model.ConfigModel@3fe3c211, sasl.login.class=io.strimzi.kafka.config.model.ConfigModel@3e1b6f0d, sasl.login.connect.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@75092a49, sasl.login.read.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@20fd5658, sasl.login.refresh.buffer.seconds=io.strimzi.kafka.config.model.ConfigModel@12ffb1c, sasl.login.refresh.min.period.seconds=io.strimzi.kafka.config.model.ConfigModel@64a98f33, sasl.login.refresh.window.factor=io.strimzi.kafka.config.model.ConfigModel@584eba8d, sasl.login.refresh.window.jitter=io.strimzi.kafka.config.model.ConfigModel@124b784a, sasl.login.retry.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@6c428604, sasl.login.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@44ba4ca8, sasl.mechanism.controller.protocol=io.strimzi.kafka.config.model.ConfigModel@698d0c60, sasl.mechanism.inter.broker.protocol=io.strimzi.kafka.config.model.ConfigModel@20cdb033, sasl.oauthbearer.clock.skew.seconds=io.strimzi.kafka.config.model.ConfigModel@58c76088, sasl.oauthbearer.expected.audience=io.strimzi.kafka.config.model.ConfigModel@4e8eb834, sasl.oauthbearer.expected.issuer=io.strimzi.kafka.config.model.ConfigModel@4fe14389, sasl.oauthbearer.jwks.endpoint.refresh.ms=io.strimzi.kafka.config.model.ConfigModel@2e02e6e, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@6140cd9a, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@5ab4969e, sasl.oauthbearer.jwks.endpoint.url=io.strimzi.kafka.config.model.ConfigModel@1f0c7b4a, sasl.oauthbearer.scope.claim.name=io.strimzi.kafka.config.model.ConfigModel@1c9ca59b, sasl.oauthbearer.sub.claim.name=io.strimzi.kafka.config.model.ConfigModel@7bd7878f, sasl.oauthbearer.token.endpoint.url=io.strimzi.kafka.config.model.ConfigModel@5a9fe0d, sasl.server.callback.handler.class=io.strimzi.kafka.config.model.ConfigModel@798313d9, security.inter.broker.protocol=io.strimzi.kafka.config.model.ConfigModel@4ad10f, security.providers=io.strimzi.kafka.config.model.ConfigModel@47e6c576, socket.connection.setup.timeout.max.ms=io.strimzi.kafka.config.model.ConfigModel@d750211, socket.connection.setup.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@6c09d5bd, socket.receive.buffer.bytes=io.strimzi.kafka.config.model.ConfigModel@3a9da39a, socket.request.max.bytes=io.strimzi.kafka.config.model.ConfigModel@4b210cca, socket.send.buffer.bytes=io.strimzi.kafka.config.model.ConfigModel@6e61fa35, ssl.cipher.suites=io.strimzi.kafka.config.model.ConfigModel@4c677e79, ssl.client.auth=io.strimzi.kafka.config.model.ConfigModel@1bb1851c, ssl.enabled.protocols=io.strimzi.kafka.config.model.ConfigModel@58359dfc, ssl.endpoint.identification.algorithm=io.strimzi.kafka.config.model.ConfigModel@3964649d, ssl.engine.factory.class=io.strimzi.kafka.config.model.ConfigModel@4e87417e, ssl.key.password=io.strimzi.kafka.config.model.ConfigModel@5989b4d7, ssl.keymanager.algorithm=io.strimzi.kafka.config.model.ConfigModel@2d270867, ssl.keystore.certificate.chain=io.strimzi.kafka.config.model.ConfigModel@377dd1d2, ssl.keystore.key=io.strimzi.kafka.config.model.ConfigModel@4385faf2, ssl.keystore.location=io.strimzi.kafka.config.model.ConfigModel@573ec159, ssl.keystore.password=io.strimzi.kafka.config.model.ConfigModel@42cf9fe9, ssl.keystore.type=io.strimzi.kafka.config.model.ConfigModel@1b653523, ssl.principal.mapping.rules=io.strimzi.kafka.config.model.ConfigModel@77db1ed7, ssl.protocol=io.strimzi.kafka.config.model.ConfigModel@564e2d7c, ssl.provider=io.strimzi.kafka.config.model.ConfigModel@68c0d08b, ssl.secure.random.implementation=io.strimzi.kafka.config.model.ConfigModel@5abe3c9d, ssl.trustmanager.algorithm=io.strimzi.kafka.config.model.ConfigModel@753cacbb, ssl.truststore.certificates=io.strimzi.kafka.config.model.ConfigModel@52be4aad, ssl.truststore.location=io.strimzi.kafka.config.model.ConfigModel@3c949cf9, ssl.truststore.password=io.strimzi.kafka.config.model.ConfigModel@17e50522, ssl.truststore.type=io.strimzi.kafka.config.model.ConfigModel@72c3a11, transaction.abort.timed.out.transaction.cleanup.interval.ms=io.strimzi.kafka.config.model.ConfigModel@27e7f37b, transaction.max.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@3f8cd02a, transaction.remove.expired.transaction.cleanup.interval.ms=io.strimzi.kafka.config.model.ConfigModel@ff0eec, transaction.state.log.load.buffer.size=io.strimzi.kafka.config.model.ConfigModel@66655050, transaction.state.log.min.isr=io.strimzi.kafka.config.model.ConfigModel@7e011bcc, transaction.state.log.num.partitions=io.strimzi.kafka.config.model.ConfigModel@275589a6, transaction.state.log.replication.factor=io.strimzi.kafka.config.model.ConfigModel@5fa57bce, transaction.state.log.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@138e57ba, transactional.id.expiration.ms=io.strimzi.kafka.config.model.ConfigModel@65a7f17c, unclean.leader.election.enable=io.strimzi.kafka.config.model.ConfigModel@6eb44cd7, zookeeper.clientCnxnSocket=io.strimzi.kafka.config.model.ConfigModel@1a3b21c4, zookeeper.connect=io.strimzi.kafka.config.model.ConfigModel@7b6996be, zookeeper.connection.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@219fa4be, zookeeper.max.in.flight.requests=io.strimzi.kafka.config.model.ConfigModel@6d0196ae, zookeeper.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@2ef7bfcb, zookeeper.set.acl=io.strimzi.kafka.config.model.ConfigModel@191c00cd, zookeeper.ssl.cipher.suites=io.strimzi.kafka.config.model.ConfigModel@45faed04, zookeeper.ssl.client.enable=io.strimzi.kafka.config.model.ConfigModel@24afb7f3, zookeeper.ssl.crl.enable=io.strimzi.kafka.config.model.ConfigModel@37354d4a, zookeeper.ssl.enabled.protocols=io.strimzi.kafka.config.model.ConfigModel@4ed63909, zookeeper.ssl.endpoint.identification.algorithm=io.strimzi.kafka.config.model.ConfigModel@5cd67f1a, zookeeper.ssl.keystore.location=io.strimzi.kafka.config.model.ConfigModel@51f4b5c, zookeeper.ssl.keystore.password=io.strimzi.kafka.config.model.ConfigModel@18dd19a8, zookeeper.ssl.keystore.type=io.strimzi.kafka.config.model.ConfigModel@67bc75cb, zookeeper.ssl.ocsp.enable=io.strimzi.kafka.config.model.ConfigModel@87df889, zookeeper.ssl.protocol=io.strimzi.kafka.config.model.ConfigModel@77470771, zookeeper.ssl.truststore.location=io.strimzi.kafka.config.model.ConfigModel@7a75068, zookeeper.ssl.truststore.password=io.strimzi.kafka.config.model.ConfigModel@3316f7a, zookeeper.ssl.truststore.type=io.strimzi.kafka.config.model.ConfigModel@64ef7625, zookeeper.sync.time.ms=io.strimzi.kafka.config.model.ConfigModel@2b5c9946}
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:336] Number of all kafka configs 261
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:356] Number of dynamic-configs 40
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:364] Number of forbidden-exception-configs 7
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:371] Size of dynamic-configs with forbidden-exception-configs 46
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] compression.type -> CLUSTER_WIDE:STRING
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.min.compaction.lag.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.retention.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connections.per.ip.overrides -> CLUSTER_WIDE:STRING
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] metric.reporters -> CLUSTER_WIDE:LIST
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.flush.interval.messages -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.message.timestamp.difference.max.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.flush.interval.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] principal.builder.class -> PER_BROKER:CLASS
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.delete.retention.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.io.buffer.size -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] min.insync.replicas -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] zookeeper.connection.timeout.ms -> READ_ONLY:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.threads -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.io.max.bytes.per.second -> CLUSTER_WIDE:DOUBLE
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.min.cleanable.ratio -> CLUSTER_WIDE:DOUBLE
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.recovery.threads.per.data.dir -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.retention.bytes -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.network.threads -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleanup.policy -> CLUSTER_WIDE:LIST
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.message.timestamp.type -> CLUSTER_WIDE:STRING
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.preallocate -> CLUSTER_WIDE:BOOLEAN
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.roll.jitter.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connections -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connections.per.ip -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] background.threads -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.message.downconversion.enable -> CLUSTER_WIDE:BOOLEAN
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] message.max.bytes -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] ssl.protocol -> PER_BROKER:STRING
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.partitions -> READ_ONLY:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.io.threads -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] ssl.enabled.protocols -> PER_BROKER:LIST
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connection.creation.rate -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.roll.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] ssl.cipher.suites -> PER_BROKER:LIST
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] unclean.leader.election.enable -> CLUSTER_WIDE:BOOLEAN
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.index.interval.bytes -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.backoff.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.segment.bytes -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.io.buffer.load.factor -> CLUSTER_WIDE:DOUBLE
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.index.size.max.bytes -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] zookeeper.connect -> READ_ONLY:STRING
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.segment.delete.delay.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.max.compaction.lag.ms -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.replica.fetchers -> CLUSTER_WIDE:INT
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.dedupe.buffer.size -> CLUSTER_WIDE:LONG
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:234] Kafka config before updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3}'
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:238] Kafka config after updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3, log.index.interval.bytes=22365}'
2022-04-09 10:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-09 10:36:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-09 10:36:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:262] Dynamic Configuration in Kafka CR is log.index.interval.bytes=22365 and expected is log.index.interval.bytes=22365
2022-04-09 10:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-1 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:37:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:37:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-2 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:37:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:37:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:234] Kafka config before updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.index.interval.bytes=22365, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3}'
2022-04-09 10:37:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:238] Kafka config after updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.index.interval.bytes=22365, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3, log.cleaner.delete.retention.ms=6559}'
2022-04-09 10:37:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-09 10:38:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-09 10:38:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:262] Dynamic Configuration in Kafka CR is log.cleaner.delete.retention.ms=6559 and expected is log.cleaner.delete.retention.ms=6559
2022-04-09 10:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:38:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-1 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:38:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:38:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-2 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:38:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:38:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:234] Kafka config before updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.cleaner.delete.retention.ms=6559, log.index.interval.bytes=22365, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3}'
2022-04-09 10:38:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:238] Kafka config after updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.cleaner.delete.retention.ms=6559, log.index.interval.bytes=22365, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3, log.cleaner.threads=2}'
2022-04-09 10:38:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-09 10:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-09 10:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:262] Dynamic Configuration in Kafka CR is log.cleaner.threads=2 and expected is log.cleaner.threads=2
2022-04-09 10:39:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:39:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:39:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-1 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:39:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-2 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testDynConfiguration is everything deleted.
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST.testDynConfiguration-FINISHED
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for DynamicConfSharedST
2022-04-09 10:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka dynamic-configuration-shared-cluster-name in namespace dynamic-conf-shared-st
2022-04-09 10:39:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 366.395 s - in io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.KafkaST
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: kafka-st
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: kafka-st
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: kafka-st
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaOffsetsReplicationFactorHigherThanReplicas-STARTED
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsFalse-STARTED
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testTopicWithoutLabels-STARTED
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutTopicOperator-STARTED
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserOperator-STARTED
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-32 for test case:testKafkaJBODDeleteClaimsFalse
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-32
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-32
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-32
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-33 for test case:testEntityOperatorWithoutUserOperator
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-33
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dd8ebd4a in namespace namespace-32
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-32
2022-04-09 10:40:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dd8ebd4a will have desired state: Ready
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-33
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-33
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:787] Deploying Kafka cluster without UO in EO
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-34 for test case:testTopicWithoutLabels
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-34
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9461c891 in namespace namespace-33
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-33
2022-04-09 10:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9461c891 will have desired state: Ready
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-34
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-34
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-35 for test case:testKafkaOffsetsReplicationFactorHigherThanReplicas
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-35
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6df71a69 in namespace namespace-34
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-34
2022-04-09 10:40:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6df71a69 will have desired state: Ready
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-35
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-35
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-36 for test case:testEntityOperatorWithoutTopicOperator
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-36
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a43129e0 in namespace namespace-35
2022-04-09 10:40:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-35
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-36
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-36
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:757] Deploying Kafka cluster without TO in EO
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f620c087 in namespace namespace-36
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-36
2022-04-09 10:40:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f620c087 will have desired state: Ready
2022-04-09 10:40:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:40:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaOffsetsReplicationFactorHigherThanReplicas
2022-04-09 10:40:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a43129e0 in namespace namespace-35
2022-04-09 10:40:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:40:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-35 for test case:testKafkaOffsetsReplicationFactorHigherThanReplicas
2022-04-09 10:40:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testRemoveTopicOperatorFromEntityOperator-STARTED
2022-04-09 10:40:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserAndTopicOperators-STARTED
2022-04-09 10:40:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaOffsetsReplicationFactorHigherThanReplicas-FINISHED
2022-04-09 10:40:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:40:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:40:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testConsumerOffsetFiles-STARTED
2022-04-09 10:40:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserAndTopicOperatorsFromEntityOperator-STARTED
2022-04-09 10:40:46 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:40:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-37 for test case:testRemoveTopicOperatorFromEntityOperator
2022-04-09 10:40:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-37
2022-04-09 10:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-37
2022-04-09 10:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-37
2022-04-09 10:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:623] Deploying Kafka cluster my-cluster-c0cb84d9
2022-04-09 10:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c0cb84d9 in namespace namespace-37
2022-04-09 10:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-37
2022-04-09 10:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c0cb84d9 will have desired state: Ready
2022-04-09 10:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6df71a69 is in desired state: Ready
2022-04-09 10:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic topic-without-labels in namespace namespace-37
2022-04-09 10:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-34
2022-04-09 10:42:29 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-34 exec my-cluster-6df71a69-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 10:42:29 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:42:29 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic topic-without-labels deletion
2022-04-09 10:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9461c891 is in desired state: Ready
2022-04-09 10:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 134 seconds
2022-04-09 10:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-09 10:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testEntityOperatorWithoutUserOperator
2022-04-09 10:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9461c891 in namespace namespace-33
2022-04-09 10:42:34 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-34 exec my-cluster-6df71a69-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 10:42:34 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:42:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:42:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicWithoutLabels
2022-04-09 10:42:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic topic-without-labels in namespace namespace-34
2022-04-09 10:42:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6df71a69 in namespace namespace-34
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dd8ebd4a is in desired state: Ready
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-dd8ebd4a-kafka-0
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-dd8ebd4a-kafka-1
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-dd8ebd4a-kafka-0
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-dd8ebd4a-kafka-1
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1717] Checking PVC names included in JBOD array
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1724] Checking PVC on Kafka pods
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 0
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 0
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 1
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 1
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:930] Deleting cluster
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:933] Waiting for PVC deletion
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaJBODDeleteClaimsFalse
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-dd8ebd4a in namespace namespace-32
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-32 for test case:testKafkaJBODDeleteClaimsFalse
2022-04-09 10:42:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:42:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-33 for test case:testEntityOperatorWithoutUserOperator
2022-04-09 10:42:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:42:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-34 for test case:testTopicWithoutLabels
2022-04-09 10:43:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f620c087 is in desired state: Ready
2022-04-09 10:43:06 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 168 seconds
2022-04-09 10:43:06 [ForkJoinPool-3-worker-13] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-09 10:43:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:43:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testEntityOperatorWithoutTopicOperator
2022-04-09 10:43:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f620c087 in namespace namespace-36
2022-04-09 10:43:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c0cb84d9 is in desired state: Ready
2022-04-09 10:43:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-c0cb84d9-entity-operator-7866f76f69-zxczd will be deleted
2022-04-09 10:43:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:43:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-36 for test case:testEntityOperatorWithoutTopicOperator
2022-04-09 10:43:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsFalse-FINISHED
2022-04-09 10:43:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:43:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:43:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrue-STARTED
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-38 for test case:testEntityOperatorWithoutUserAndTopicOperators
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-38
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-38
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-38
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:814] Deploying Kafka cluster without UO and TO in EO
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-06784f97 in namespace namespace-38
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-38
2022-04-09 10:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-06784f97 will have desired state: Ready
2022-04-09 10:43:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:171] Pod my-cluster-c0cb84d9-entity-operator-7866f76f69-zxczd deleted
2022-04-09 10:43:31 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c0cb84d9-entity-operator will be ready
2022-04-09 10:43:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testTopicWithoutLabels-FINISHED
2022-04-09 10:43:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:43:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:43:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testAppDomainLabels-STARTED
2022-04-09 10:43:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserOperator-FINISHED
2022-04-09 10:43:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:43:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:43:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserOperatorFromEntityOperator-STARTED
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-39 for test case:testKafkaJBODDeleteClaimsTrue
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-39
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-39
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-39
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-201c9435 in namespace namespace-39
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-39
2022-04-09 10:43:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-201c9435 will have desired state: Ready
2022-04-09 10:43:51 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c0cb84d9-entity-operator is ready
2022-04-09 10:43:51 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-c0cb84d9-entity-operator to be ready
2022-04-09 10:44:00 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutTopicOperator-FINISHED
2022-04-09 10:44:00 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:44:00 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:44:00 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testUOListeningOnlyUsersInSameCluster-STARTED
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-40 for test case:testConsumerOffsetFiles
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-40
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-40
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-40
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-375e5268 in namespace namespace-40
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-40
2022-04-09 10:44:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-375e5268 will have desired state: Ready
2022-04-09 10:44:01 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-c0cb84d9-entity-operator is ready
2022-04-09 10:44:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:201] Wait until Pod my-cluster-c0cb84d9-entity-operator will have 1 containers
2022-04-09 10:44:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:205] Pod my-cluster-c0cb84d9-entity-operator has 1 containers
2022-04-09 10:44:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-c0cb84d9-entity-operator-7dc5cdd4c5-5c5pz will be deleted
2022-04-09 10:44:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:171] Pod my-cluster-c0cb84d9-entity-operator-7dc5cdd4c5-5c5pz deleted
2022-04-09 10:44:16 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c0cb84d9-entity-operator will be ready
2022-04-09 10:44:39 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c0cb84d9-entity-operator is ready
2022-04-09 10:44:39 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-c0cb84d9-entity-operator to be ready
2022-04-09 10:44:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-06784f97 is in desired state: Ready
2022-04-09 10:44:47 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 87 seconds
2022-04-09 10:44:47 [ForkJoinPool-3-worker-1] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-09 10:44:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:44:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testEntityOperatorWithoutUserAndTopicOperators
2022-04-09 10:44:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-06784f97 in namespace namespace-38
2022-04-09 10:44:49 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-c0cb84d9-entity-operator is ready
2022-04-09 10:44:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:44:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testRemoveTopicOperatorFromEntityOperator
2022-04-09 10:44:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c0cb84d9 in namespace namespace-37
2022-04-09 10:44:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:44:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-38 for test case:testEntityOperatorWithoutUserAndTopicOperators
2022-04-09 10:44:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:44:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-37 for test case:testRemoveTopicOperatorFromEntityOperator
2022-04-09 10:45:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-375e5268 is in desired state: Ready
2022-04-09 10:45:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1159827134-1031785486 in namespace namespace-40
2022-04-09 10:45:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-40
2022-04-09 10:45:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1159827134-1031785486 will have desired state: Ready
2022-04-09 10:45:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1159827134-1031785486 is in desired state: Ready
2022-04-09 10:45:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-375e5268-kafka-clients in namespace namespace-40
2022-04-09 10:45:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-40
2022-04-09 10:45:20 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-375e5268-kafka-clients will be ready
2022-04-09 10:45:22 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-375e5268-kafka-clients is ready
2022-04-09 10:45:22 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:45:22 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1415] Executing command cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V in my-cluster-375e5268-kafka-0
2022-04-09 10:45:23 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-40 exec my-cluster-375e5268-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V
2022-04-09 10:45:23 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:45:23 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1422] Result: 
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99

2022-04-09 10:45:23 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1113e3d8, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092, --topic, my-topic-1159827134-1031785486], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-375e5268-kafka-clients-6999948f5d-5n9d5', podNamespace='namespace-40', bootstrapServer='my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092', topicName='my-topic-1159827134-1031785486', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2fd37dd4}
2022-04-09 10:45:23 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092:my-topic-1159827134-1031785486 from pod my-cluster-375e5268-kafka-clients-6999948f5d-5n9d5
2022-04-09 10:45:23 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-375e5268-kafka-clients-6999948f5d-5n9d5 -n namespace-40 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092 --topic my-topic-1159827134-1031785486
2022-04-09 10:45:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserAndTopicOperators-FINISHED
2022-04-09 10:45:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:45:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:45:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrueFalse-STARTED
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-41 for test case:testUOListeningOnlyUsersInSameCluster
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-41
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-41
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-41
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1 in namespace namespace-41
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-41
2022-04-09 10:45:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1 will have desired state: Ready
2022-04-09 10:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@15e40a15, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-74221648, --group-instance-id, instance283086528, --bootstrap-server, my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092, --topic, my-topic-1159827134-1031785486], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-375e5268-kafka-clients-6999948f5d-5n9d5', podNamespace='namespace-40', bootstrapServer='my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092', topicName='my-topic-1159827134-1031785486', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-74221648', consumerInstanceId='instance283086528', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@112a3f96}
2022-04-09 10:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092#my-topic-1159827134-1031785486 from pod my-cluster-375e5268-kafka-clients-6999948f5d-5n9d5
2022-04-09 10:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-375e5268-kafka-clients-6999948f5d-5n9d5 -n namespace-40 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-74221648 --group-instance-id instance283086528 --bootstrap-server my-cluster-375e5268-kafka-bootstrap.namespace-40.svc:9092 --topic my-topic-1159827134-1031785486
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1429] Executing command cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V in my-cluster-375e5268-kafka-0
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-40 exec my-cluster-375e5268-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testConsumerOffsetFiles
2022-04-09 10:45:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1159827134-1031785486 in namespace namespace-40
2022-04-09 10:45:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-375e5268-kafka-clients in namespace namespace-40
2022-04-09 10:45:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testRemoveTopicOperatorFromEntityOperator-FINISHED
2022-04-09 10:45:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:45:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:45:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testLabelsAndAnnotationForPVC-STARTED
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-42 for test case:testKafkaJBODDeleteClaimsTrueFalse
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-42
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-42
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-42
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-327aab7b in namespace namespace-42
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-42
2022-04-09 10:45:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-327aab7b will have desired state: Ready
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-201c9435 is in desired state: Ready
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-201c9435-kafka-0
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-201c9435-kafka-1
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-201c9435-kafka-0
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-201c9435-kafka-1
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1717] Checking PVC names included in JBOD array
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1724] Checking PVC on Kafka pods
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 0
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 0
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 1
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 1
2022-04-09 10:46:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:906] Deleting cluster
2022-04-09 10:46:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:909] Waiting for PVC deletion
2022-04-09 10:46:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-375e5268 in namespace namespace-40
2022-04-09 10:46:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:46:32 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-40 for test case:testConsumerOffsetFiles
2022-04-09 10:46:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:46:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaJBODDeleteClaimsTrue
2022-04-09 10:46:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-201c9435 in namespace namespace-39
2022-04-09 10:46:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:46:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-39 for test case:testKafkaJBODDeleteClaimsTrue
2022-04-09 10:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrue-FINISHED
2022-04-09 10:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testCustomAndUpdatedValues-STARTED
2022-04-09 10:46:42 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:46:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-43 for test case:testLabelsAndAnnotationForPVC
2022-04-09 10:46:42 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-43
2022-04-09 10:46:43 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-43
2022-04-09 10:46:43 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-43
2022-04-09 10:46:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b57d2ead in namespace namespace-43
2022-04-09 10:46:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-43
2022-04-09 10:46:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b57d2ead will have desired state: Ready
2022-04-09 10:46:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1 is in desired state: Ready
2022-04-09 10:46:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2 in namespace namespace-43
2022-04-09 10:46:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-41
2022-04-09 10:46:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2 will have desired state: Ready
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-327aab7b is in desired state: Ready
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-327aab7b-kafka-0
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-327aab7b-kafka-1
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-327aab7b-kafka-0
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-327aab7b-kafka-1
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1717] Checking PVC names included in JBOD array
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1724] Checking PVC on Kafka pods
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 0
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 0
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 1
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 1
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:882] Deleting cluster
2022-04-09 10:46:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:885] Waiting for PVC deletion
2022-04-09 10:46:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testConsumerOffsetFiles-FINISHED
2022-04-09 10:46:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:46:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:46:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testMessagesAreStoredInDisk-STARTED
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-44 for test case:testRemoveUserAndTopicOperatorsFromEntityOperator
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-44
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-44
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-44
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1b06be96 in namespace namespace-44
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-44
2022-04-09 10:47:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1b06be96 will have desired state: Ready
2022-04-09 10:47:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:47:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaJBODDeleteClaimsTrueFalse
2022-04-09 10:47:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-327aab7b in namespace namespace-42
2022-04-09 10:47:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:47:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-42 for test case:testKafkaJBODDeleteClaimsTrueFalse
2022-04-09 10:47:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrueFalse-FINISHED
2022-04-09 10:47:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:47:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:47:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testLabelModificationDoesNotBreakCluster-STARTED
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-45 for test case:testMessagesAreStoredInDisk
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-45
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-45
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-45
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-59de3f5d in namespace namespace-45
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-45
2022-04-09 10:47:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-59de3f5d will have desired state: Ready
2022-04-09 10:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2 is in desired state: Ready
2022-04-09 10:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-245460290-1269067899 in namespace namespace-45
2022-04-09 10:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-41
2022-04-09 10:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-245460290-1269067899 will have desired state: Ready
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-245460290-1269067899 is in desired state: Ready
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:1292] Verifying that user my-user-245460290-1269067899 in cluster my-cluster-1 is created
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:1297] Verifying that user my-user-245460290-1269067899 in cluster my-cluster-2 is not created
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:1302] Verifying that user belongs to my-cluster-1 cluster
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testUOListeningOnlyUsersInSameCluster
2022-04-09 10:48:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2 in namespace namespace-41
2022-04-09 10:48:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-245460290-1269067899 in namespace namespace-41
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b57d2ead is in desired state: Ready
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1510] Check if Kubernetes labels are applied
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1517] Kubernetes labels are correctly set and present
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-0-my-cluster-b57d2ead-kafka-0 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-0-my-cluster-b57d2ead-kafka-1 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-0-my-cluster-b57d2ead-kafka-2 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-1-my-cluster-b57d2ead-kafka-0 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-1-my-cluster-b57d2ead-kafka-1 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-1-my-cluster-b57d2ead-kafka-2 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-my-cluster-b57d2ead-zookeeper-0 - testValue = testValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1535] Replacing kafka && zookeeper labels and annotations from testKey to editedTestValue
2022-04-09 10:48:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PersistentVolumeClaimUtils:30] Wait until PVC labels will change {testKey=editedTestValue}
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PersistentVolumeClaimUtils:46] PVC labels has changed {testKey=editedTestValue}
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PersistentVolumeClaimUtils:50] Wait until PVC annotation will change {testKey=editedTestValue}
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PersistentVolumeClaimUtils:66] PVC annotation has changed {testKey=editedTestValue}
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b57d2ead will have desired state: Ready
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b57d2ead is in desired state: Ready
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1549] [PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:47:11Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-0-my-cluster-b57d2ead-kafka-0, namespace=namespace-43, ownerReferences=[], resourceVersion=803924, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-0-my-cluster-b57d2ead-kafka-0, uid=cd3016bd-a53c-47a5-a05b-1c78e5de5c24, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=20Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-cd3016bd-a53c-47a5-a05b-1c78e5de5c24, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=20Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:47:11Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-0-my-cluster-b57d2ead-kafka-1, namespace=namespace-43, ownerReferences=[], resourceVersion=803927, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-0-my-cluster-b57d2ead-kafka-1, uid=595fe9dc-078e-466d-bc1c-4cc59a356e52, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=20Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-595fe9dc-078e-466d-bc1c-4cc59a356e52, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=20Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:47:11Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-0-my-cluster-b57d2ead-kafka-2, namespace=namespace-43, ownerReferences=[], resourceVersion=803928, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-0-my-cluster-b57d2ead-kafka-2, uid=87c950e3-94a0-4394-96a5-68df091d7b60, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=20Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-87c950e3-94a0-4394-96a5-68df091d7b60, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=20Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=true, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:47:11Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-1-my-cluster-b57d2ead-kafka-0, namespace=namespace-43, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-b57d2ead, uid=2c61dc58-ca56-4839-8643-b2628bf2c347, additionalProperties={})], resourceVersion=803930, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-1-my-cluster-b57d2ead-kafka-0, uid=ce53062f-6162-4f38-985b-7e933c59a910, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=10Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-ce53062f-6162-4f38-985b-7e933c59a910, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=10Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=true, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:47:11Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-1-my-cluster-b57d2ead-kafka-1, namespace=namespace-43, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-b57d2ead, uid=2c61dc58-ca56-4839-8643-b2628bf2c347, additionalProperties={})], resourceVersion=803931, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-1-my-cluster-b57d2ead-kafka-1, uid=69f80f13-82f1-43bd-806e-f55275764b51, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=10Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-69f80f13-82f1-43bd-806e-f55275764b51, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=10Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=true, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:47:11Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-1-my-cluster-b57d2ead-kafka-2, namespace=namespace-43, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-b57d2ead, uid=2c61dc58-ca56-4839-8643-b2628bf2c347, additionalProperties={})], resourceVersion=803929, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-1-my-cluster-b57d2ead-kafka-2, uid=e4394458-16c1-497b-beb7-36b77a3bc295, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=10Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-e4394458-16c1-497b-beb7-36b77a3bc295, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=10Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-09T10:46:48Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-b57d2ead, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-b57d2ead, strimzi.io/cluster=my-cluster-b57d2ead, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b57d2ead-zookeeper, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-my-cluster-b57d2ead-zookeeper-0, namespace=namespace-43, ownerReferences=[], resourceVersion=803900, selfLink=/api/v1/namespaces/namespace-43/persistentvolumeclaims/data-my-cluster-b57d2ead-zookeeper-0, uid=e457c3b1-b6ac-4bea-a58e-f5d871775365, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=3Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-e457c3b1-b6ac-4bea-a58e-f5d871775365, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=3Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={})]
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-0-my-cluster-b57d2ead-kafka-0 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-0-my-cluster-b57d2ead-kafka-1 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-0-my-cluster-b57d2ead-kafka-2 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-1-my-cluster-b57d2ead-kafka-0 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-1-my-cluster-b57d2ead-kafka-1 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-1-my-cluster-b57d2ead-kafka-2 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-my-cluster-b57d2ead-zookeeper-0 - testValue = editedTestValue
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testLabelsAndAnnotationForPVC
2022-04-09 10:48:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b57d2ead in namespace namespace-43
2022-04-09 10:48:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1 in namespace namespace-41
2022-04-09 10:48:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:48:23 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-43 for test case:testLabelsAndAnnotationForPVC
2022-04-09 10:48:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:48:30 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-41 for test case:testUOListeningOnlyUsersInSameCluster
2022-04-09 10:48:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1b06be96 is in desired state: Ready
2022-04-09 10:48:51 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-1b06be96-entity-operator will have stable 0 replicas
2022-04-09 10:48:51 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:52 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:54 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:55 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:56 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:58 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:48:59 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:49:00 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:49:01 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:49:02 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:49:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 10:49:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-09 10:49:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-09 10:49:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-09 10:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-59de3f5d is in desired state: Ready
2022-04-09 10:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1156999200-17867505 in namespace namespace-45
2022-04-09 10:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-45
2022-04-09 10:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1156999200-17867505 will have desired state: Ready
2022-04-09 10:49:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-09 10:49:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testLabelsAndAnnotationForPVC-FINISHED
2022-04-09 10:49:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-46 for test case:testRemoveUserOperatorFromEntityOperator
2022-04-09 10:49:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:49:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEODeletion-STARTED
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-46
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-46
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-46
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:669] Deploying Kafka cluster my-cluster-df38f7b8
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-df38f7b8 in namespace namespace-46
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-46
2022-04-09 10:49:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-df38f7b8 will have desired state: Ready
2022-04-09 10:49:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1156999200-17867505 is in desired state: Ready
2022-04-09 10:49:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-59de3f5d-kafka-clients in namespace namespace-46
2022-04-09 10:49:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-45
2022-04-09 10:49:07 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-59de3f5d-kafka-clients will be ready
2022-04-09 10:49:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-09 10:49:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-09 10:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-59de3f5d-kafka-clients is ready
2022-04-09 10:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-45 exec my-cluster-59de3f5d-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0; ls -1
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:49:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-45 exec my-cluster-59de3f5d-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0; ls -1 | sed -n '/my-topic-1156999200-17867505/p'
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1344] Executing command cd /var/lib/kafka/data/kafka-log0/my-topic-1156999200-17867505-0
/;cat 00000000000000000000.log in my-cluster-59de3f5d-kafka-0
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-45 exec my-cluster-59de3f5d-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/my-topic-1156999200-17867505-0
/;cat 00000000000000000000.log
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1348] Topic my-topic-1156999200-17867505 is present in kafka broker my-cluster-59de3f5d-kafka-0 with no data
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@669cc861, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092, --topic, my-topic-1156999200-17867505], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq', podNamespace='namespace-45', bootstrapServer='my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092', topicName='my-topic-1156999200-17867505', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4dbbc3da}
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092:my-topic-1156999200-17867505 from pod my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq
2022-04-09 10:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq -n namespace-45 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092 --topic my-topic-1156999200-17867505
2022-04-09 10:49:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-09 10:49:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-09 10:49:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-09 10:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1a5e974d, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-66239002, --group-instance-id, instance1027501573, --bootstrap-server, my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092, --topic, my-topic-1156999200-17867505], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq', podNamespace='namespace-45', bootstrapServer='my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092', topicName='my-topic-1156999200-17867505', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-66239002', consumerInstanceId='instance1027501573', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@70dd563d}
2022-04-09 10:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092#my-topic-1156999200-17867505 from pod my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq
2022-04-09 10:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq -n namespace-45 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-66239002 --group-instance-id instance1027501573 --bootstrap-server my-cluster-59de3f5d-kafka-bootstrap.namespace-45.svc:9092 --topic my-topic-1156999200-17867505
2022-04-09 10:49:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-09 10:49:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testUOListeningOnlyUsersInSameCluster-FINISHED
2022-04-09 10:49:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:49:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:49:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testJvmAndResources-STARTED
2022-04-09 10:49:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-09 10:49:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-09 10:49:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-47 for test case:testAppDomainLabels
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-47
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-47
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-47
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5f9cbc35 in namespace namespace-47
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-47
2022-04-09 10:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5f9cbc35 will have desired state: Ready
2022-04-09 10:49:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-09 10:49:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1355] Executing command cd /var/lib/kafka/data/kafka-log0/my-topic-1156999200-17867505-0
/;cat 00000000000000000000.log in my-cluster-59de3f5d-kafka-0
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-45 exec my-cluster-59de3f5d-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/my-topic-1156999200-17867505-0
/;cat 00000000000000000000.log
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1364] Deleting kafka pod my-cluster-59de3f5d-kafka-0
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1364] Deleting kafka pod my-cluster-59de3f5d-kafka-clients-7779c56469-v62lq
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1368] Wait for kafka to rolling restart ...
2022-04-09 10:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-59de3f5d-kafka rolling update
2022-04-09 10:49:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-09 10:49:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-09 10:49:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-09 10:49:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-09 10:49:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:228] Pod my-cluster-1b06be96-entity-operator has 0 replicas
2022-04-09 10:49:23 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1b06be96-entity-operator will be ready
2022-04-09 10:49:34 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-59de3f5d-kafka has been successfully rolled
2022-04-09 10:49:34 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of my-cluster-59de3f5d-kafka to be ready
2022-04-09 10:49:52 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1b06be96-entity-operator is ready
2022-04-09 10:49:52 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 170 seconds
2022-04-09 10:49:52 [ForkJoinPool-3-worker-11] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-09 10:49:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:49:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testRemoveUserAndTopicOperatorsFromEntityOperator
2022-04-09 10:49:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1b06be96 in namespace namespace-44
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-59de3f5d will have desired state: Ready
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-59de3f5d is in desired state: Ready
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-59de3f5d is ready
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaST:1371] Executing command cd /var/lib/kafka/data/kafka-log0/my-topic-1156999200-17867505-0
/;cat 00000000000000000000.log in my-cluster-59de3f5d-kafka-0
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-45 exec my-cluster-59de3f5d-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/my-topic-1156999200-17867505-0
/;cat 00000000000000000000.log
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testMessagesAreStoredInDisk
2022-04-09 10:50:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1156999200-17867505 in namespace namespace-45
2022-04-09 10:50:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:50:02 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-44 for test case:testRemoveUserAndTopicOperatorsFromEntityOperator
2022-04-09 10:50:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-59de3f5d-kafka-clients in namespace namespace-45
2022-04-09 10:50:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-df38f7b8 is in desired state: Ready
2022-04-09 10:50:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-df38f7b8-entity-operator-7b6775884f-wz8xw will be deleted
2022-04-09 10:50:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5f9cbc35 is in desired state: Ready
2022-04-09 10:50:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1199893395-498815884 in namespace namespace-47
2022-04-09 10:50:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-47
2022-04-09 10:50:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1199893395-498815884 will have desired state: Ready
2022-04-09 10:50:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1199893395-498815884 is in desired state: Ready
2022-04-09 10:50:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5f9cbc35-kafka-clients in namespace namespace-47
2022-04-09 10:50:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-47
2022-04-09 10:50:34 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5f9cbc35-kafka-clients will be ready
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5f9cbc35-kafka-clients is ready
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1220] ---> PODS <---
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1232] ---> STATEFUL SETS <---
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1236] Getting labels from stateful set of kafka resource
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-kafka, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1241] Getting labels from stateful set of zookeeper resource
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-zookeeper, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1244] ---> SERVICES <---
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-5f9cbc35-kafka-bootstrap service
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/discovery=true, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-kafka, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-5f9cbc35-kafka-brokers service
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-kafka, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-5f9cbc35-zookeeper-client service
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-zookeeper-client, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-5f9cbc35-zookeeper-nodes service
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-zookeeper, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1255] ---> SECRETS <---
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-clients-ca secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-clients-ca-cert secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-cluster-ca secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-cluster-ca-cert secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-cluster-operator-certs secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-entity-topic-operator-certs secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-topic-operator, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-entity-user-operator-certs secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-user-operator, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-kafka-brokers secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-5f9cbc35-zookeeper-nodes secret
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1266] ---> CONFIG MAPS <---
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-5f9cbc35-entity-topic-operator-config config map
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-topic-operator, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-5f9cbc35-entity-user-operator-config config map
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-user-operator, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-5f9cbc35-kafka-config config map
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-5f9cbc35-kafka, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-5f9cbc35-zookeeper-config config map
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-5f9cbc35, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-5f9cbc35, strimzi.io/cluster=my-cluster-5f9cbc35, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7fb250c3, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092, --topic, my-topic-1199893395-498815884], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-5f9cbc35-kafka-clients-7c8c948d9b-6dc9r', podNamespace='namespace-47', bootstrapServer='my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092', topicName='my-topic-1199893395-498815884', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@44a7e3ce}
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092:my-topic-1199893395-498815884 from pod my-cluster-5f9cbc35-kafka-clients-7c8c948d9b-6dc9r
2022-04-09 10:50:36 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5f9cbc35-kafka-clients-7c8c948d9b-6dc9r -n namespace-47 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092 --topic my-topic-1199893395-498815884
2022-04-09 10:50:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:171] Pod my-cluster-df38f7b8-entity-operator-7b6775884f-wz8xw deleted
2022-04-09 10:50:37 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-df38f7b8-entity-operator will be ready
2022-04-09 10:50:39 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:50:39 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:50:39 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6eda4e8, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-465695892, --group-instance-id, instance1989522699, --bootstrap-server, my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092, --topic, my-topic-1199893395-498815884], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5f9cbc35-kafka-clients-7c8c948d9b-6dc9r', podNamespace='namespace-47', bootstrapServer='my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092', topicName='my-topic-1199893395-498815884', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-465695892', consumerInstanceId='instance1989522699', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6442c795}
2022-04-09 10:50:39 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092#my-topic-1199893395-498815884 from pod my-cluster-5f9cbc35-kafka-clients-7c8c948d9b-6dc9r
2022-04-09 10:50:39 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5f9cbc35-kafka-clients-7c8c948d9b-6dc9r -n namespace-47 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-465695892 --group-instance-id instance1989522699 --bootstrap-server my-cluster-5f9cbc35-kafka-bootstrap.namespace-47.svc:9092 --topic my-topic-1199893395-498815884
2022-04-09 10:50:45 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:50:45 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:50:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:50:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAppDomainLabels
2022-04-09 10:50:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1199893395-498815884 in namespace namespace-47
2022-04-09 10:50:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserAndTopicOperatorsFromEntityOperator-FINISHED
2022-04-09 10:50:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:50:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:50:46 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testPersistentStorageSize-STARTED
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-48 for test case:testEODeletion
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-48
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-48
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-48
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-25e3abda in namespace namespace-48
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-48
2022-04-09 10:50:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25e3abda will have desired state: Ready
2022-04-09 10:50:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5f9cbc35-kafka-clients in namespace namespace-47
2022-04-09 10:51:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-59de3f5d in namespace namespace-45
2022-04-09 10:51:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:51:11 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-45 for test case:testMessagesAreStoredInDisk
2022-04-09 10:51:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5f9cbc35 in namespace namespace-47
2022-04-09 10:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testMessagesAreStoredInDisk-FINISHED
2022-04-09 10:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testForTopicOperator-STARTED
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-49 for test case:testJvmAndResources
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-49
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-49
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-49
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ac2f5328 in namespace namespace-49
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-49
2022-04-09 10:51:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ac2f5328 will have desired state: Ready
2022-04-09 10:51:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:51:45 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-47 for test case:testAppDomainLabels
2022-04-09 10:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25e3abda is in desired state: Ready
2022-04-09 10:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:122] Setting entity operator to null
2022-04-09 10:52:06 [ForkJoinPool-3-worker-7] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-25e3abda-entity-operator is not deleted yet! Triggering force delete by cmd client!
2022-04-09 10:52:12 [ForkJoinPool-3-worker-7] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-25e3abda-entity-operator is not deleted yet! Triggering force delete by cmd client!
2022-04-09 10:52:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testAppDomainLabels-FINISHED
2022-04-09 10:52:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:52:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 10:52:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testReadOnlyRootFileSystem-STARTED
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-50 for test case:testLabelModificationDoesNotBreakCluster
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-50
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-50
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-50
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-985cb0bb in namespace namespace-50
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-50
2022-04-09 10:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-985cb0bb will have desired state: Ready
2022-04-09 10:52:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-25e3abda-entity-operator-86d69bc59-2v7tr will be deleted
2022-04-09 10:52:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:171] Pod my-cluster-25e3abda-entity-operator-86d69bc59-2v7tr deleted
2022-04-09 10:52:17 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:130] Entity operator was deleted
2022-04-09 10:52:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:52:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testEODeletion
2022-04-09 10:52:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-25e3abda in namespace namespace-48
2022-04-09 10:52:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:52:27 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-48 for test case:testEODeletion
2022-04-09 10:52:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEODeletion-FINISHED
2022-04-09 10:52:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-51 for test case:testPersistentStorageSize
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-51
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-51
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-51
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-18c9f0df in namespace namespace-51
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-51
2022-04-09 10:52:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-18c9f0df will have desired state: Ready
2022-04-09 10:53:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ac2f5328 is in desired state: Ready
2022-04-09 10:53:09 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-ac2f5328-kafka-0 -c kafka -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-09 10:53:09 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:53:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-ac2f5328-zookeeper-0 -c zookeeper -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-09 10:53:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:53:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng -c topic-operator -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-09 10:53:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:53:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng -c user-operator -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-09 10:53:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:53:11 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:533] Check if -D java options are present in topic-operator
2022-04-09 10:53:11 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:533] Check if -D java options are present in user-operator
2022-04-09 10:53:11 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaST:552] Checking no rolling update for Kafka cluster
2022-04-09 10:53:11 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 10:53:12 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 10:53:13 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 10:53:14 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 10:53:15 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 10:53:16 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 10:53:17 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 10:53:18 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 10:53:19 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 10:53:20 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 10:53:21 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 10:53:22 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 10:53:23 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 10:53:24 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 10:53:25 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 10:53:26 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 10:53:27 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 10:53:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-985cb0bb is in desired state: Ready
2022-04-09 10:53:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1400354105-1640400322 in namespace namespace-51
2022-04-09 10:53:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-50
2022-04-09 10:53:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1400354105-1640400322 will have desired state: Ready
2022-04-09 10:53:28 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 10:53:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1400354105-1640400322 is in desired state: Ready
2022-04-09 10:53:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-985cb0bb-kafka-clients in namespace namespace-51
2022-04-09 10:53:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-50
2022-04-09 10:53:29 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-985cb0bb-kafka-clients will be ready
2022-04-09 10:53:29 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 10:53:30 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-985cb0bb-kafka-clients is ready
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1078] Waiting for kafka stateful set labels changed {label-name-1=name-of-the-label-1, label-name-2=name-of-the-label-2}
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-1 -> name-of-the-label-1
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-2 -> name-of-the-label-2
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1081] Getting labels from stateful set resource
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1084] Verifying default labels in the Kafka CR
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1095] Setting new values of labels from name-of-the-label-1 to new-name-of-the-label-1 | from name-of-the-label-2 to new-name-of-the-label-2 and adding one label-name-3 with value name-of-the-label-3
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1098] Edit kafka labels in Kafka CR
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1109] Waiting for kafka service labels changed {label-name-1=new-name-of-the-label-1, label-name-2=new-name-of-the-label-2, label-name-3=name-of-the-label-3}
2022-04-09 10:53:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ServiceUtils:33] Waiting for Service label change label-name-1 -> new-name-of-the-label-1
2022-04-09 10:53:31 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 10:53:32 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 10:53:33 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 10:53:34 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 10:53:35 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 10:53:36 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 10:53:37 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 10:53:38 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 10:53:39 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 10:53:40 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 10:53:41 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 10:53:42 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 10:53:43 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 10:53:44 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 10:53:45 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 10:53:46 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 10:53:47 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 10:53:48 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 10:53:49 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 10:53:50 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 10:53:51 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 10:53:52 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 10:53:53 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 10:53:54 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 10:53:55 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ServiceUtils:33] Waiting for Service label change label-name-2 -> new-name-of-the-label-2
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ServiceUtils:33] Waiting for Service label change label-name-3 -> name-of-the-label-3
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1112] Verifying kafka labels via services
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1118] Waiting for Kafka ConfigMap my-cluster-985cb0bb-kafka-config in namespace namespace-50 to have new labels: {label-name-1=new-name-of-the-label-1, label-name-2=new-name-of-the-label-2, label-name-3=name-of-the-label-3}
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:42] Waiting for ConfigMap my-cluster-985cb0bb-kafka-config label change label-name-1 -> new-name-of-the-label-1
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:42] Waiting for ConfigMap my-cluster-985cb0bb-kafka-config label change label-name-2 -> new-name-of-the-label-2
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:42] Waiting for ConfigMap my-cluster-985cb0bb-kafka-config label change label-name-3 -> name-of-the-label-3
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1121] Verifying Kafka labels on ConfigMap my-cluster-985cb0bb-kafka-config in namespace namespace-50
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1127] Waiting for kafka stateful set labels changed {label-name-1=new-name-of-the-label-1, label-name-2=new-name-of-the-label-2, label-name-3=name-of-the-label-3}
2022-04-09 10:53:56 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-1 -> new-name-of-the-label-1
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-2 -> new-name-of-the-label-2
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-3 -> name-of-the-label-3
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1130] Verifying kafka labels via stateful set
2022-04-09 10:53:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-985cb0bb-kafka rolling update
2022-04-09 10:53:57 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 10:53:58 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 10:53:59 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 10:54:00 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 10:54:01 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-zookeeper-0=d0ee808e-a0a1-4a1f-b612-4f809a2b6338} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 10:54:01 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 10:54:02 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 10:54:03 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 10:54:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-18c9f0df is in desired state: Ready
2022-04-09 10:54:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-411652237-217768442 in namespace namespace-51
2022-04-09 10:54:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-51
2022-04-09 10:54:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-411652237-217768442 will have desired state: Ready
2022-04-09 10:54:04 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 10:54:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-411652237-217768442 is in desired state: Ready
2022-04-09 10:54:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-18c9f0df-kafka-clients in namespace namespace-51
2022-04-09 10:54:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-51
2022-04-09 10:54:04 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-18c9f0df-kafka-clients will be ready
2022-04-09 10:54:05 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 10:54:06 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-18c9f0df-kafka-clients is ready
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaST:1694] Checking volume data-0-my-cluster-18c9f0df-kafka-0 and size of storage 70Gi
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaST:1694] Checking volume data-0-my-cluster-18c9f0df-kafka-1 and size of storage 70Gi
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaST:1694] Checking volume data-1-my-cluster-18c9f0df-kafka-0 and size of storage 20Gi
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaST:1694] Checking volume data-1-my-cluster-18c9f0df-kafka-1 and size of storage 20Gi
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaST:983] Checking produced and consumed messages to pod:my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1cef8bf7, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092, --topic, my-topic-411652237-217768442], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq', podNamespace='namespace-51', bootstrapServer='my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092', topicName='my-topic-411652237-217768442', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@cb5925f}
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092:my-topic-411652237-217768442 from pod my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq
2022-04-09 10:54:06 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq -n namespace-51 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092 --topic my-topic-411652237-217768442
2022-04-09 10:54:07 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 10:54:08 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 10:54:09 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 10:54:09 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:54:09 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:54:09 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2e63c690, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-2025179826, --group-instance-id, instance242599433, --bootstrap-server, my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092, --topic, my-topic-411652237-217768442], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq', podNamespace='namespace-51', bootstrapServer='my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092', topicName='my-topic-411652237-217768442', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2025179826', consumerInstanceId='instance242599433', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6c1b45cb}
2022-04-09 10:54:09 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092#my-topic-411652237-217768442 from pod my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq
2022-04-09 10:54:09 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-18c9f0df-kafka-clients-69d4d457cc-4z5xq -n namespace-51 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-2025179826 --group-instance-id instance242599433 --bootstrap-server my-cluster-18c9f0df-kafka-bootstrap.namespace-51.svc:9092 --topic my-topic-411652237-217768442
2022-04-09 10:54:10 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 10:54:11 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 10:54:12 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 10:54:13 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 10:54:14 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 10:54:15 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:54:15 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:54:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:54:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testPersistentStorageSize
2022-04-09 10:54:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-411652237-217768442 in namespace namespace-51
2022-04-09 10:54:15 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 10:54:16 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 10:54:17 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 10:54:18 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 10:54:19 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 10:54:20 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 10:54:21 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 10:54:22 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 10:54:23 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 10:54:24 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 10:54:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-18c9f0df-kafka-clients in namespace namespace-51
2022-04-09 10:54:25 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 10:54:26 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 10:54:27 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 10:54:28 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 10:54:29 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 10:54:30 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 10:54:31 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 10:54:32 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 10:54:33 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 10:54:34 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 10:54:35 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 10:54:36 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 10:54:37 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 10:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 10:54:39 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 10:54:39 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-df38f7b8-entity-operator is ready
2022-04-09 10:54:39 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-df38f7b8-entity-operator to be ready
2022-04-09 10:54:40 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 10:54:41 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 10:54:42 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 10:54:43 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 10:54:44 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 10:54:45 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 10:54:46 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 10:54:47 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 10:54:48 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 10:54:49 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 10:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-df38f7b8-entity-operator is ready
2022-04-09 10:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:201] Wait until Pod my-cluster-df38f7b8-entity-operator will have 2 containers
2022-04-09 10:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:205] Pod my-cluster-df38f7b8-entity-operator has 2 containers
2022-04-09 10:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-df38f7b8-entity-operator-799f446f78-brnzb will be deleted
2022-04-09 10:54:50 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 10:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-ac2f5328-kafka-0=e6d904b6-8250-4cd4-b3ae-b961e1f60c4b} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 10:54:51 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 50
2022-04-09 10:54:52 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 49
2022-04-09 10:54:53 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 48
2022-04-09 10:54:54 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 47
2022-04-09 10:54:55 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 46
2022-04-09 10:54:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-985cb0bb-kafka has been successfully rolled
2022-04-09 10:54:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-985cb0bb-kafka to be ready
2022-04-09 10:54:56 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 45
2022-04-09 10:54:57 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 44
2022-04-09 10:54:58 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 43
2022-04-09 10:54:59 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 42
2022-04-09 10:54:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:171] Pod my-cluster-df38f7b8-entity-operator-799f446f78-brnzb deleted
2022-04-09 10:54:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-df38f7b8-entity-operator will be ready
2022-04-09 10:55:00 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 41
2022-04-09 10:55:01 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 40
2022-04-09 10:55:02 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 39
2022-04-09 10:55:03 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 38
2022-04-09 10:55:04 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 37
2022-04-09 10:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-18c9f0df in namespace namespace-51
2022-04-09 10:55:05 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 36
2022-04-09 10:55:06 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 35
2022-04-09 10:55:07 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 34
2022-04-09 10:55:08 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 33
2022-04-09 10:55:09 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 32
2022-04-09 10:55:10 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 31
2022-04-09 10:55:11 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 30
2022-04-09 10:55:12 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 29
2022-04-09 10:55:13 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 28
2022-04-09 10:55:14 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 27
2022-04-09 10:55:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:55:15 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-51 for test case:testPersistentStorageSize
2022-04-09 10:55:15 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 26
2022-04-09 10:55:16 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 25
2022-04-09 10:55:17 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 24
2022-04-09 10:55:18 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 23
2022-04-09 10:55:19 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 22
2022-04-09 10:55:20 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 21
2022-04-09 10:55:21 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 20
2022-04-09 10:55:22 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 19
2022-04-09 10:55:23 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 18
2022-04-09 10:55:24 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 17
2022-04-09 10:55:25 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 16
2022-04-09 10:55:26 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 15
2022-04-09 10:55:27 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 14
2022-04-09 10:55:28 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 13
2022-04-09 10:55:29 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 12
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-985cb0bb will have desired state: Ready
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-985cb0bb is in desired state: Ready
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-985cb0bb is ready
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1136] Verifying via kafka pods
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1143] Removing labels: label-name-1 -> new-name-of-the-label-1, label-name-2 -> new-name-of-the-label-2, label-name-3 -> name-of-the-label-3
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1155] Waiting for kafka service labels deletion {app.kubernetes.io/instance=my-cluster-985cb0bb, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-985cb0bb, controller-revision-hash=my-cluster-985cb0bb-kafka-b94dbf46, statefulset.kubernetes.io/pod-name=my-cluster-985cb0bb-kafka-0, strimzi.io/cluster=my-cluster-985cb0bb, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-985cb0bb-kafka, test.case=testLabelModificationDoesNotBreakCluster}
2022-04-09 10:55:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ServiceUtils:44] Service label label-name-1 change to null
2022-04-09 10:55:30 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 11
2022-04-09 10:55:31 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 10
2022-04-09 10:55:32 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 9
2022-04-09 10:55:33 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 8
2022-04-09 10:55:34 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 7
2022-04-09 10:55:35 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 6
2022-04-09 10:55:36 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 5
2022-04-09 10:55:37 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 4
2022-04-09 10:55:38 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-df38f7b8-entity-operator is ready
2022-04-09 10:55:38 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-df38f7b8-entity-operator to be ready
2022-04-09 10:55:38 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 3
2022-04-09 10:55:39 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 2
2022-04-09 10:55:40 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 1
2022-04-09 10:55:41 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:80] {my-cluster-ac2f5328-entity-operator-5846b6b884-zg6ng=4b9dd781-14b9-421b-b30b-e122ea6a8d80} pods not rolling waiting, remaining seconds for stability 0
2022-04-09 10:55:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:55:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testJvmAndResources
2022-04-09 10:55:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ac2f5328 in namespace namespace-49
2022-04-09 10:55:48 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-df38f7b8-entity-operator is ready
2022-04-09 10:55:48 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 400 seconds
2022-04-09 10:55:48 [ForkJoinPool-3-worker-5] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-09 10:55:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:55:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testRemoveUserOperatorFromEntityOperator
2022-04-09 10:55:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-df38f7b8 in namespace namespace-46
2022-04-09 10:55:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:55:52 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-49 for test case:testJvmAndResources
2022-04-09 10:55:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testJvmAndResources-FINISHED
2022-04-09 10:55:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-52 for test case:testCustomAndUpdatedValues
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-52
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-52
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-52
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-651b6740 in namespace namespace-52
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-52
2022-04-09 10:55:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-651b6740 will have desired state: Ready
2022-04-09 10:55:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:55:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-46 for test case:testRemoveUserOperatorFromEntityOperator
2022-04-09 10:55:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testPersistentStorageSize-FINISHED
2022-04-09 10:55:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:56:02 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:56:02 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-53 for test case:testReadOnlyRootFileSystem
2022-04-09 10:56:02 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-53
2022-04-09 10:56:02 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-53
2022-04-09 10:56:02 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-53
2022-04-09 10:56:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9a289a80 in namespace namespace-53
2022-04-09 10:56:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-53
2022-04-09 10:56:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a289a80 will have desired state: Ready
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-54 for test case:testForTopicOperator
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-54
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-54
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-54
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fa23086b in namespace namespace-54
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-54
2022-04-09 10:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa23086b will have desired state: Ready
2022-04-09 10:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserOperatorFromEntityOperator-FINISHED
2022-04-09 10:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ServiceUtils:44] Service label label-name-2 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ServiceUtils:44] Service label label-name-3 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1158] Verifying kafka labels via services
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1164] Waiting for Kafka ConfigMap my-cluster-985cb0bb-kafka-config in namespace namespace-50 to have labels removed: [label-name-1, label-name-2, label-name-3]
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:53] Waiting for ConfigMap my-cluster-985cb0bb-kafka-config label label-name-1 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:60] ConfigMap my-cluster-985cb0bb-kafka-config label label-name-1 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:53] Waiting for ConfigMap my-cluster-985cb0bb-kafka-config label label-name-2 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:60] ConfigMap my-cluster-985cb0bb-kafka-config label label-name-2 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:53] Waiting for ConfigMap my-cluster-985cb0bb-kafka-config label label-name-3 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ConfigMapUtils:60] ConfigMap my-cluster-985cb0bb-kafka-config label label-name-3 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1167] Verifying Kafka labels on ConfigMap my-cluster-985cb0bb-kafka-config in namespace namespace-50
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1173] Waiting for kafka stateful set labels changed {app.kubernetes.io/instance=my-cluster-985cb0bb, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-985cb0bb, controller-revision-hash=my-cluster-985cb0bb-kafka-b94dbf46, statefulset.kubernetes.io/pod-name=my-cluster-985cb0bb-kafka-0, strimzi.io/cluster=my-cluster-985cb0bb, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-985cb0bb-kafka, test.case=testLabelModificationDoesNotBreakCluster}
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:96] Waiting for StatefulSet label label-name-1 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:101] StatefulSet label label-name-1 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:96] Waiting for StatefulSet label label-name-2 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:101] StatefulSet label label-name-2 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:96] Waiting for StatefulSet label label-name-3 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [StatefulSetUtils:101] StatefulSet label label-name-3 change to null
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1176] Verifying kafka labels via stateful set
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-985cb0bb-kafka rolling update
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-985cb0bb-kafka has been successfully rolled
2022-04-09 10:57:32 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-985cb0bb-kafka to be ready
2022-04-09 10:57:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-985cb0bb will have desired state: Ready
2022-04-09 10:57:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-985cb0bb is in desired state: Ready
2022-04-09 10:57:42 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-985cb0bb is ready
2022-04-09 10:57:42 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1181] Waiting for kafka pod labels deletion {app.kubernetes.io/instance=my-cluster-985cb0bb, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-985cb0bb, controller-revision-hash=my-cluster-985cb0bb-kafka-b94dbf46, statefulset.kubernetes.io/pod-name=my-cluster-985cb0bb-kafka-0, strimzi.io/cluster=my-cluster-985cb0bb, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-985cb0bb-kafka, test.case=testLabelModificationDoesNotBreakCluster}
2022-04-09 10:57:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:262] Waiting for Pod label label-name-1 change to null
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:267] Pod label label-name-1 changed to null
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:262] Waiting for Pod label label-name-2 change to null
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:267] Pod label label-name-2 changed to null
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:262] Waiting for Pod label label-name-3 change to null
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:267] Pod label label-name-3 changed to null
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1186] Verifying via kafka pods
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7b8a19a1, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092, --topic, my-topic-1400354105-1640400322], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-985cb0bb-kafka-clients-779cc47975-hdgjk', podNamespace='namespace-50', bootstrapServer='my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092', topicName='my-topic-1400354105-1640400322', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@fa00e19}
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092:my-topic-1400354105-1640400322 from pod my-cluster-985cb0bb-kafka-clients-779cc47975-hdgjk
2022-04-09 10:57:53 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-985cb0bb-kafka-clients-779cc47975-hdgjk -n namespace-50 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092 --topic my-topic-1400354105-1640400322
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:199] CLI_KAFKA_VERIFIABLE_PRODUCER RETURN code: 1
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:201] ======STDOUT START=======
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [33mWARN [m [Exec:358] Executor log is too long. Going to strip it and print only first 20000 characters
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:202] /tmp/.properties
Starting Producer with configuration:

[2022-04-09 10:57:54,925] INFO ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 (ProducerConfig:376)
[2022-04-09 10:57:55,059] DEBUG [Producer clientId=producer-1] Starting Kafka producer I/O thread. (Sender:238)
[2022-04-09 10:57:55,065] DEBUG [Producer clientId=producer-1] Initialize connection to node my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092 (id: -1 rack: null) for sending metadata request (NetworkClient:1156)
[2022-04-09 10:57:55,069] INFO Kafka version: 3.1.0 (AppInfoParser:119)
[2022-04-09 10:57:55,069] INFO Kafka commitId: 37edeed0777bacb3 (AppInfoParser:120)
[2022-04-09 10:57:55,070] INFO Kafka startTimeMs: 1649501875058 (AppInfoParser:121)
[2022-04-09 10:57:55,071] DEBUG Resolved host my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc as 10.103.67.189 (ClientUtils:113)
[2022-04-09 10:57:55,073] DEBUG [Producer clientId=producer-1] Kafka producer started (KafkaProducer:437)
[2022-04-09 10:57:55,073] DEBUG [Producer clientId=producer-1] Initiating connection to node my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092 (id: -1 rack: null) using address my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc/10.103.67.189 (NetworkClient:985)
[2022-04-09 10:57:55,116] DEBUG [Producer clientId=producer-1] Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1 (Selector:531)
{"timestamp":1649501875306,"name":"startup_complete"}
[2022-04-09 10:57:55,420] DEBUG [Producer clientId=producer-1] Completed connection to node -1. Fetching API versions. (NetworkClient:952)
[2022-04-09 10:57:55,420] DEBUG [Producer clientId=producer-1] Initiating API versions fetch from node -1. (NetworkClient:966)
[2022-04-09 10:57:55,443] DEBUG [Producer clientId=producer-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=0) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.1.0') (NetworkClient:521)
[2022-04-09 10:57:55,480] DEBUG [Producer clientId=producer-1] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=0): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=5), ApiVersion(apiKey=5, minVersion=0, maxVersion=3), ApiVersion(apiKey=6, minVersion=0, maxVersion=7), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=7), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=4), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=2), ApiVersion(apiKey=30, minVersion=0, maxVersion=2), ApiVersion(apiKey=31, minVersion=0, maxVersion=2), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=2), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=2), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=2), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=0), ApiVersion(apiKey=57, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[]) (NetworkClient:879)
[2022-04-09 10:57:55,557] DEBUG [Producer clientId=producer-1] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 5 [usable: 5], StopReplica(5): 0 to 3 [usable: 3], UpdateMetadata(6): 0 to 7 [usable: 7], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 7 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 2 [usable: 2], CreateAcls(30): 0 to 2 [usable: 2], DeleteAcls(31): 0 to 2 [usable: 2], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 2 [usable: 2], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 2 [usable: 2], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], AlterIsr(56): 0 [usable: 0], UpdateFeatures(57): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0]). (NetworkClient:921)
[2022-04-09 10:57:55,559] DEBUG [Producer clientId=producer-1] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='my-topic-1400354105-1640400322')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092 (id: -1 rack: null) (NetworkClient:1139)
[2022-04-09 10:57:55,562] DEBUG [Producer clientId=producer-1] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-1, correlationId=1) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='my-topic-1400354105-1640400322')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) (NetworkClient:521)
[2022-04-09 10:57:55,568] DEBUG [Producer clientId=producer-1] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=producer-1, correlationId=1): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=2, host='my-cluster-985cb0bb-kafka-2.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc', port=9092, rack=null), MetadataResponseBroker(nodeId=1, host='my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc', port=9092, rack=null)], clusterId='maJz2TqtT4ajXp3YpFWjzQ', controllerId=1, topics=[MetadataResponseTopic(errorCode=0, name='my-topic-1400354105-1640400322', topicId=HgGndwVjTu2xrcVlN8bhFQ, isInternal=false, partitions=[MetadataResponsePartition(errorCode=5, partitionIndex=0, leaderId=-1, leaderEpoch=3, replicaNodes=[0], isrNodes=[0], offlineReplicas=[0])], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648) (NetworkClient:879)
[2022-04-09 10:57:55,576] INFO [Producer clientId=producer-1] Resetting the last seen epoch of partition my-topic-1400354105-1640400322-0 to 3 since the associated topicId changed from null to HgGndwVjTu2xrcVlN8bhFQ (Metadata:402)
[2022-04-09 10:57:55,577] DEBUG [Producer clientId=producer-1] Requesting metadata update for partition my-topic-1400354105-1640400322-0 due to error LEADER_NOT_AVAILABLE (Metadata:356)
[2022-04-09 10:57:55,583] INFO [Producer clientId=producer-1] Cluster ID: maJz2TqtT4ajXp3YpFWjzQ (Metadata:287)
[2022-04-09 10:57:55,584] DEBUG [Producer clientId=producer-1] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='maJz2TqtT4ajXp3YpFWjzQ', nodes={1=my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc:9092 (id: 1 rack: null), 2=my-cluster-985cb0bb-kafka-2.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc:9092 (id: 2 rack: null)}, partitions=[PartitionMetadata(error=LEADER_NOT_AVAILABLE, partition=my-topic-1400354105-1640400322-0, leader=Optional.empty, leaderEpoch=Optional[3], replicas=0, isr=0, offlineReplicas=0)], controller=my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc:9092 (id: 1 rack: null)} (Metadata:291)
[2022-04-09 10:57:55,607] DEBUG [Producer clientId=producer-1] Requesting metadata update due to unknown leader topics from the batched records: [my-topic-1400354105-1640400322] (Sender:344)
[2022-04-09 10:57:55,616] INFO [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (KafkaProducer:1228)
[2022-04-09 10:57:55,617] DEBUG [Producer clientId=producer-1] Beginning shutdown of Kafka producer I/O thread, sending remaining records. (Sender:249)
[2022-04-09 10:57:55,618] DEBUG [Producer clientId=producer-1] Requesting metadata update due to unknown leader topics from the batched records: [my-topic-1400354105-1640400322] (Sender:344)
[2022-04-09 10:57:55,667] DEBUG [Producer clientId=producer-1] Requesting metadata update due to unknown leader topics from the batched records: [my-topic-1400354105-1640400322] (Sender:344)
[2022-04-09 10:57:55,668] DEBUG [Producer clientId=producer-1] Initialize connection to node my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc:9092 (id: 1 rack: null) for sending metadata request (NetworkClient:1156)
[2022-04-09 10:57:55,669] DEBUG Resolved host my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc as 172.17.0.14 (ClientUtils:113)
[2022-04-09 10:57:55,669] DEBUG [Producer clientId=producer-1] Initiating connection to node my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc:9092 (id: 1 rack: null) using address my-cluster-985cb0bb-kafka-1.my-cluster-985cb0bb-kafka-brokers.namespace-50.svc/172.17.0.14 (NetworkClient:985)
[2022-04-09 10:57:55,672] DEBUG [Producer clientId=producer-1] Created socket with SO_RCVBUF = 32768, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node 1 (Selector:531)
[2022-04-09 10:57:55,672] DEBUG [Producer clientId=producer-1] Completed connection to node 1. Fetching API versions. (NetworkClient:952)
[2022-04-09 10:57:55,673] DEBUG [Producer clientId=producer-1] Initiating API versions fetch from node 1. (NetworkClient:966)
[2022-04-09 10:57:55,673] DEBUG [Producer clientId=producer-1] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=2) and timeout 30000 to node 1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.1.0') (NetworkClient:521)
[2022-04-09 10:57:55,673] DEBUG [Producer clientId=producer-1] Requesting metadata update due to unknown leader topics from the batched records: [my-topic-1400354105-1640400322] (Sender:344)
[2022-04-09 10:57:55,674] DEBUG [Producer clientId=producer-1] Requesting metadata update due to unknown leader topics from the batched records: [my-topic-1400354105-1640400322] (Sender:344)
[2022-04-09 10:57:55,689] DEBUG [Producer clientId=producer-1] Received API_VERSIONS response from node 1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=producer-1, correlationId=2): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=5), ApiVersion(apiKey=5, minVersion=0, maxVersion=3), ApiVersion(apiKey=6, minVersion=0, maxVersion=7), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=7), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=4), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=2), ApiVersion(apiKey=30, minVersion=0, maxVersion=2), ApiVersion(apiKey=31, minVersion=0, maxVersion=2), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=2), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=2), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=2), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=0), ApiVersion(apiKey=57, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, 
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:203] ======STDOUT END======
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: false
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced -1 messages
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7a784ca6, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-74179823, --group-instance-id, instance1328859051, --bootstrap-server, my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092, --topic, my-topic-1400354105-1640400322], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-985cb0bb-kafka-clients-779cc47975-hdgjk', podNamespace='namespace-50', bootstrapServer='my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092', topicName='my-topic-1400354105-1640400322', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-74179823', consumerInstanceId='instance1328859051', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@752754dd}
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092#my-topic-1400354105-1640400322 from pod my-cluster-985cb0bb-kafka-clients-779cc47975-hdgjk
2022-04-09 10:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-985cb0bb-kafka-clients-779cc47975-hdgjk -n namespace-50 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-74179823 --group-instance-id instance1328859051 --bootstrap-server my-cluster-985cb0bb-kafka-bootstrap.namespace-50.svc:9092 --topic my-topic-1400354105-1640400322
2022-04-09 10:58:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa23086b is in desired state: Ready
2022-04-09 10:58:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-669434046-646649826 in namespace namespace-54
2022-04-09 10:58:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-54
2022-04-09 10:58:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-669434046-646649826 will have desired state: Ready
2022-04-09 10:58:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-669434046-646649826 is in desired state: Ready
2022-04-09 10:58:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-669434046-646649826 will have desired state: Ready
2022-04-09 10:58:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-669434046-646649826 is in desired state: Ready
2022-04-09 10:58:27 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 10:58:27 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:58:30 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic topic-from-cli --replication-factor 1 --partitions 1
2022-04-09 10:58:30 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:58:30 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic topic-from-cli creation 
2022-04-09 10:58:32 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:58:32 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:58:32 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestExecutionWatcher:28] KafkaST - Exception Sent (-1) and receive (100) message count is not equal has been thrown in @Test. Going to collect logs from components.
2022-04-09 10:58:32 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-09 10:58:32 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-09 10:58:32 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:252] Collecting events in Namespace kafka-st
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace kafka-st
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace kafka-st
2022-04-09 10:58:38 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace kafka-st
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace kafka-st
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace kafka-st
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace kafka-st
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:252] Collecting events in Namespace namespace-50
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace namespace-50
2022-04-09 10:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-50
2022-04-09 10:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace namespace-50
2022-04-09 10:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace namespace-50
2022-04-09 10:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace namespace-50
2022-04-09 10:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace namespace-50
2022-04-09 10:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 10:58:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:58:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testLabelModificationDoesNotBreakCluster
2022-04-09 10:58:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1400354105-1640400322 in namespace namespace-50
2022-04-09 10:58:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-985cb0bb in namespace namespace-50
2022-04-09 10:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-651b6740 is in desired state: Ready
2022-04-09 10:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:290] Verify values before update
2022-04-09 10:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-kafka in pod name
2022-04-09 10:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container kafka
2022-04-09 10:58:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1661] Checking kafka configuration
2022-04-09 10:58:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-985cb0bb-kafka-clients in namespace namespace-50
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-0 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-1 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-2 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-kafka
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container kafka
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-0 -- cat /tmp/strimzi.properties
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:308] Testing Zookeepers
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-zookeeper in pod name
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container zookeeper
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-651b6740-zookeeper
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container zookeeper
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-zookeeper
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container zookeeper
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:315] Checking configuration of TO and UO
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-entity-operator in pod name
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container topic-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-entity-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container topic-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-entity-operator in pod name
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container user-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-entity-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container user-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-entity-operator in pod name
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container tls-sidecar
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-entity-operator
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container tls-sidecar
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:326] Updating configuration of Kafka cluster
2022-04-09 10:58:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-651b6740-zookeeper rolling update
2022-04-09 10:58:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a289a80 is in desired state: Ready
2022-04-09 10:58:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a289a80 will have desired state: Ready
2022-04-09 10:58:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a289a80 is in desired state: Ready
2022-04-09 10:58:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1518261711-1238432363 in namespace namespace-54
2022-04-09 10:58:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-53
2022-04-09 10:58:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1518261711-1238432363 will have desired state: Ready
2022-04-09 10:58:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1518261711-1238432363 is in desired state: Ready
2022-04-09 10:58:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9a289a80-kafka-clients in namespace namespace-54
2022-04-09 10:58:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-53
2022-04-09 10:58:54 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9a289a80-kafka-clients will be ready
2022-04-09 10:58:58 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9a289a80-kafka-clients is ready
2022-04-09 10:58:58 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 10:58:58 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaST:1652] Checking produced and consumed messages to pod:my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt
2022-04-09 10:58:58 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@727991dc, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092, --topic, my-topic-1518261711-1238432363], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt', podNamespace='namespace-53', bootstrapServer='my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092', topicName='my-topic-1518261711-1238432363', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55131023}
2022-04-09 10:58:58 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092:my-topic-1518261711-1238432363 from pod my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt
2022-04-09 10:58:58 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt -n namespace-53 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092 --topic my-topic-1518261711-1238432363
2022-04-09 10:59:01 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 10:59:01 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 10:59:01 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@48dc5342, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1401923588, --group-instance-id, instance1007970387, --bootstrap-server, my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092, --topic, my-topic-1518261711-1238432363], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt', podNamespace='namespace-53', bootstrapServer='my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092', topicName='my-topic-1518261711-1238432363', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1401923588', consumerInstanceId='instance1007970387', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@16f0289b}
2022-04-09 10:59:01 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092#my-topic-1518261711-1238432363 from pod my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt
2022-04-09 10:59:01 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-9a289a80-kafka-clients-78bffc684c-glsjt -n namespace-53 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1401923588 --group-instance-id instance1007970387 --bootstrap-server my-cluster-9a289a80-kafka-bootstrap.namespace-53.svc:9092 --topic my-topic-1518261711-1238432363
2022-04-09 10:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 10:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 10:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 10:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testReadOnlyRootFileSystem
2022-04-09 10:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1518261711-1238432363 in namespace namespace-53
2022-04-09 10:59:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9a289a80 in namespace namespace-53
2022-04-09 10:59:07 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-53, for cruise control Kafka cluster my-cluster-9a289a80
2022-04-09 10:59:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9a289a80-kafka-clients in namespace namespace-53
2022-04-09 10:59:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 10:59:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-50 for test case:testLabelModificationDoesNotBreakCluster
2022-04-09 10:59:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testLabelModificationDoesNotBreakCluster-FINISHED
2022-04-09 10:59:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:00:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:00:07 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-53 for test case:testReadOnlyRootFileSystem
2022-04-09 11:00:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testReadOnlyRootFileSystem-FINISHED
2022-04-09 11:00:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:00:14 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 11:00:14 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:00:17 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-669434046-646649826 --partitions 2
2022-04-09 11:00:17 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:00:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa23086b will have desired state: Ready
2022-04-09 11:00:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa23086b is in desired state: Ready
2022-04-09 11:00:20 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-669434046-646649826
2022-04-09 11:00:20 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:00:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa23086b will have desired state: Ready
2022-04-09 11:00:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa23086b is in desired state: Ready
2022-04-09 11:00:23 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic topic-from-cli
2022-04-09 11:00:23 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:00:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-651b6740-zookeeper has been successfully rolled
2022-04-09 11:00:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-651b6740-zookeeper to be ready
2022-04-09 11:00:27 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic my-topic-669434046-646649826
2022-04-09 11:00:27 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:00:27 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-669434046-646649826 deletion
2022-04-09 11:00:27 [ForkJoinPool-3-worker-9] [33mWARN [m [KafkaTopicUtils:110] KafkaTopic my-topic-669434046-646649826 is not deleted yet! Triggering force delete by cmd client!
2022-04-09 11:00:41 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-54 exec my-cluster-fa23086b-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 11:00:41 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:00:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:00:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testForTopicOperator
2022-04-09 11:00:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-669434046-646649826 in namespace namespace-54
2022-04-09 11:00:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fa23086b in namespace namespace-54
2022-04-09 11:00:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:00:51 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-54 for test case:testForTopicOperator
2022-04-09 11:01:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testForTopicOperator-FINISHED
2022-04-09 11:01:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:01:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-651b6740 will have desired state: Ready
2022-04-09 11:01:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-651b6740 is in desired state: Ready
2022-04-09 11:01:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-651b6740 is ready
2022-04-09 11:01:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-651b6740-kafka rolling update
2022-04-09 11:03:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-651b6740-kafka has been successfully rolled
2022-04-09 11:03:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-651b6740-kafka to be ready
2022-04-09 11:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-651b6740 will have desired state: Ready
2022-04-09 11:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-651b6740 is in desired state: Ready
2022-04-09 11:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-651b6740 is ready
2022-04-09 11:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-651b6740-entity-operator rolling update
2022-04-09 11:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-651b6740-entity-operator will be ready
2022-04-09 11:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-651b6740-entity-operator is ready
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-651b6740-entity-operator rolling update finished
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-651b6740 will have desired state: Ready
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-651b6740 is in desired state: Ready
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:386] Verify values after update
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-kafka in pod name
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container kafka
2022-04-09 11:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1661] Checking kafka configuration
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-0 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-1 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-2 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-kafka
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container kafka
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-52 exec my-cluster-651b6740-kafka-0 -- cat /tmp/strimzi.properties
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:404] Testing Zookeepers
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-zookeeper in pod name
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container zookeeper
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-651b6740-zookeeper
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container zookeeper
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-zookeeper
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container zookeeper
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:410] Getting entity operator to check configuration of TO and UO
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-entity-operator in pod name
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container topic-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-entity-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container topic-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-entity-operator in pod name
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container user-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-entity-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container user-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-651b6740-entity-operator in pod name
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container tls-sidecar
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-651b6740-entity-operator
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container tls-sidecar
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-09 11:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-651b6740 in namespace namespace-52
2022-04-09 11:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-52 for test case:testCustomAndUpdatedValues
2022-04-09 11:05:43 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testCustomAndUpdatedValues-FINISHED
2022-04-09 11:05:43 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:05:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:05:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context KafkaST is everything deleted.
2022-04-09 11:05:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 23, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1,531.486 s <<< FAILURE! - in io.strimzi.systemtest.kafka.KafkaST
[[1;31mERROR[m] io.strimzi.systemtest.kafka.KafkaST.testLabelModificationDoesNotBreakCluster(ExtensionContext)  Time elapsed: 701.261 s  <<< FAILURE!
java.lang.AssertionError: Sent (-1) and receive (100) message count is not equal
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:26)
	at io.strimzi.systemtest.kafkaclients.clients.InternalKafkaClient.assertSentAndReceivedMessages(InternalKafkaClient.java:254)
	at io.strimzi.systemtest.kafkaclients.clients.InternalKafkaClient.checkProducedAndConsumedMessages(InternalKafkaClient.java:245)
	at io.strimzi.systemtest.kafka.KafkaST.testLabelModificationDoesNotBreakCluster(KafkaST.java:1189)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.ConfigProviderST
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: config-provider-st
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: config-provider-st
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: config-provider-st
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.ConfigProviderST.testConnectWithConnectorUsingConfigAndEnvProvider-STARTED
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-55 for test case:testConnectWithConnectorUsingConfigAndEnvProvider
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-55
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-55
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-55
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-09165ab2 in namespace namespace-55
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-09 11:05:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-09165ab2 will have desired state: Ready
2022-04-09 11:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-09165ab2 is in desired state: Ready
2022-04-09 11:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-09165ab2 in namespace namespace-55
2022-04-09 11:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-09 11:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-09165ab2 will have desired state: Ready
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-09165ab2 is in desired state: Ready
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigProviderST:100] Creating needed RoleBinding and Role for Kubernetes Config Provider
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding connector-config-rb in namespace namespace-55
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-09165ab2 in namespace namespace-55
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-09 11:08:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-09165ab2 will have desired state: Ready
2022-04-09 11:08:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-09165ab2 is in desired state: Ready
2022-04-09 11:08:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 11:08:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-my-consumer-group-2094600520 in namespace namespace-55
2022-04-09 11:08:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-09 11:08:15 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-my-consumer-group-2094600520 will be in active state
2022-04-09 11:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-09165ab2-connect-749fd8789-vckdc
2022-04-09 11:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-09165ab2-connect-749fd8789-vckdc
2022-04-09 11:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testConnectWithConnectorUsingConfigAndEnvProvider
2022-04-09 11:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding connector-config-rb in namespace namespace-55
2022-04-09 11:08:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job producer-my-consumer-group-2094600520 in namespace namespace-55
2022-04-09 11:08:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-09165ab2 in namespace namespace-55
2022-04-09 11:08:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-09165ab2 in namespace namespace-55
2022-04-09 11:08:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-09165ab2 in namespace namespace-55
2022-04-09 11:08:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:08:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-55 for test case:testConnectWithConnectorUsingConfigAndEnvProvider
2022-04-09 11:08:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.ConfigProviderST.testConnectWithConnectorUsingConfigAndEnvProvider-FINISHED
2022-04-09 11:08:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:08:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:08:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ConfigProviderST is everything deleted.
2022-04-09 11:08:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 195.499 s - in io.strimzi.systemtest.kafka.ConfigProviderST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.custom.CustomAuthorizerST
2022-04-09 11:09:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: custom-authorizer-st
2022-04-09 11:09:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: custom-authorizer-st
2022-04-09 11:09:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: custom-authorizer-st
2022-04-09 11:09:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka custom-authorizer in namespace custom-authorizer-st
2022-04-09 11:09:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: custom-authorizer will have desired state: Ready
2022-04-09 11:10:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: custom-authorizer is in desired state: Ready
2022-04-09 11:10:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:10:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:10:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclRuleReadAndWrite-STARTED
2022-04-09 11:10:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclWithSuperUser-STARTED
2022-04-09 11:10:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:10:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:10:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1853610514-1859362023 in namespace custom-authorizer-st
2022-04-09 11:10:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1714127404-773097600 in namespace custom-authorizer-st
2022-04-09 11:10:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1853610514-1859362023 will have desired state: Ready
2022-04-09 11:10:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1714127404-773097600 will have desired state: Ready
2022-04-09 11:10:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1853610514-1859362023 is in desired state: Ready
2022-04-09 11:10:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1714127404-773097600 is in desired state: Ready
2022-04-09 11:10:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser sre-admin in namespace custom-authorizer-st
2022-04-09 11:10:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser kafka-user-write in namespace custom-authorizer-st
2022-04-09 11:10:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: sre-admin will have desired state: Ready
2022-04-09 11:10:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: kafka-user-write will have desired state: Ready
2022-04-09 11:10:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: sre-admin is in desired state: Ready
2022-04-09 11:10:19 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-75436fe5-kafka-clients in namespace custom-authorizer-st
2022-04-09 11:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: kafka-user-write is in desired state: Ready
2022-04-09 11:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser kafka-user-read in namespace custom-authorizer-st
2022-04-09 11:10:19 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-75436fe5-kafka-clients will be ready
2022-04-09 11:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: kafka-user-read will have desired state: Ready
2022-04-09 11:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: kafka-user-read is in desired state: Ready
2022-04-09 11:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [CustomAuthorizerST:105] Checking KafkaUser kafka-user-write that is able to send messages to topic 'my-topic-1853610514-1859362023'
2022-04-09 11:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a5435fef-kafka-clients in namespace custom-authorizer-st
2022-04-09 11:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a5435fef-kafka-clients will be ready
2022-04-09 11:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-75436fe5-kafka-clients is ready
2022-04-09 11:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [CustomAuthorizerST:173] Checking kafka super user:sre-admin that is able to send messages to topic:my-topic-1714127404-773097600
2022-04-09 11:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@641f6003, messages=[], arguments=[--max-messages, 100, USER=sre_admin, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1714127404-773097600], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-75436fe5-kafka-clients-69c77c8558-g8qm9', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1714127404-773097600', maxMessages=100, kafkaUsername='sre-admin', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5126e60f}
2022-04-09 11:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1714127404-773097600 from pod my-cluster-75436fe5-kafka-clients-69c77c8558-g8qm9
2022-04-09 11:10:21 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-75436fe5-kafka-clients-69c77c8558-g8qm9 -n custom-authorizer-st -- /opt/kafka/producer.sh --max-messages 100 USER=sre_admin --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1714127404-773097600
2022-04-09 11:10:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a5435fef-kafka-clients is ready
2022-04-09 11:10:22 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:10:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4887ee24, messages=[], arguments=[--max-messages, 500, USER=kafka_user_write, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1853610514-1859362023], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1853610514-1859362023', maxMessages=500, kafkaUsername='kafka-user-write', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3e60f275}
2022-04-09 11:10:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 500 messages to custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1853610514-1859362023 from pod my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz
2022-04-09 11:10:22 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz -n custom-authorizer-st -- /opt/kafka/producer.sh --max-messages 500 USER=kafka_user_write --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1853610514-1859362023
2022-04-09 11:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [CustomAuthorizerST:187] Checking kafka super user:sre-admin that is able to read messages to topic:my-topic-83383454-4042747 regardless that we configured Acls with only write operation
2022-04-09 11:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5ce37434, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1780386320, --group-instance-id, instance844677026, USER=sre_admin, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1714127404-773097600], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-75436fe5-kafka-clients-69c77c8558-g8qm9', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1714127404-773097600', maxMessages=100, kafkaUsername='sre-admin', consumerGroupName='my-consumer-group-1780386320', consumerInstanceId='instance844677026', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3a93fbf7}
2022-04-09 11:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1714127404-773097600 from pod my-cluster-75436fe5-kafka-clients-69c77c8558-g8qm9
2022-04-09 11:10:25 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-75436fe5-kafka-clients-69c77c8558-g8qm9 -n custom-authorizer-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1780386320 --group-instance-id instance844677026 USER=sre_admin --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1714127404-773097600
2022-04-09 11:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 500 messages
2022-04-09 11:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6b4f1ddf, messages=[], arguments=[--max-messages, 500, --group-id, my-consumer-group-1910930048, --group-instance-id, instance1893001405, USER=kafka_user_write, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1853610514-1859362023], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1853610514-1859362023', maxMessages=500, kafkaUsername='kafka-user-write', consumerGroupName='my-consumer-group-1910930048', consumerInstanceId='instance1893001405', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5285aea0}
2022-04-09 11:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 500 messages from custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1853610514-1859362023 from pod my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz
2022-04-09 11:10:26 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz -n custom-authorizer-st -- /opt/kafka/consumer.sh --max-messages 500 --group-id my-consumer-group-1910930048 --group-instance-id instance1893001405 USER=kafka_user_write --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1853610514-1859362023
2022-04-09 11:10:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:10:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 0 messages
2022-04-09 11:10:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@57f43daf, messages=[], arguments=[--max-messages, 500, --group-id, consumer-group-name-1, --group-instance-id, instance1932223263, USER=kafka_user_read, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1853610514-1859362023], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1853610514-1859362023', maxMessages=500, kafkaUsername='kafka-user-read', consumerGroupName='consumer-group-name-1', consumerInstanceId='instance1932223263', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1cce385c}
2022-04-09 11:10:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 500 messages from custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1853610514-1859362023 from pod my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz
2022-04-09 11:10:30 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz -n custom-authorizer-st -- /opt/kafka/consumer.sh --max-messages 500 --group-id consumer-group-name-1 --group-instance-id instance1932223263 USER=kafka_user_read --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1853610514-1859362023
2022-04-09 11:10:32 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:10:32 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:10:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:10:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testAclWithSuperUser
2022-04-09 11:10:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser sre-admin in namespace custom-authorizer-st
2022-04-09 11:10:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1714127404-773097600 in namespace custom-authorizer-st
2022-04-09 11:10:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-75436fe5-kafka-clients in namespace custom-authorizer-st
2022-04-09 11:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 500 messages
2022-04-09 11:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [CustomAuthorizerST:137] Checking KafkaUser kafka-user-read that is not able to send messages to topic 'my-topic-1853610514-1859362023'
2022-04-09 11:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3bc9387a, messages=[], arguments=[--max-messages, 500, USER=kafka_user_read, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1853610514-1859362023], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1853610514-1859362023', maxMessages=500, kafkaUsername='kafka-user-read', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@28fb795f}
2022-04-09 11:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 500 messages to custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1853610514-1859362023 from pod my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz
2022-04-09 11:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a5435fef-kafka-clients-5d7747f97b-6f7dz -n custom-authorizer-st -- /opt/kafka/producer.sh --max-messages 500 USER=kafka_user_read --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1853610514-1859362023
2022-04-09 11:10:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:10:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced -1 messages
2022-04-09 11:10:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:10:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAclRuleReadAndWrite
2022-04-09 11:10:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser kafka-user-read in namespace custom-authorizer-st
2022-04-09 11:10:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser kafka-user-write in namespace custom-authorizer-st
2022-04-09 11:10:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1853610514-1859362023 in namespace custom-authorizer-st
2022-04-09 11:10:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a5435fef-kafka-clients in namespace custom-authorizer-st
2022-04-09 11:11:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:11:22 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclWithSuperUser-FINISHED
2022-04-09 11:11:22 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclRuleReadAndWrite-FINISHED
2022-04-09 11:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for CustomAuthorizerST
2022-04-09 11:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka custom-authorizer in namespace custom-authorizer-st
2022-04-09 11:11:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 175.212 s - in io.strimzi.systemtest.security.custom.CustomAuthorizerST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.OpaIntegrationST
2022-04-09 11:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: opa-integration-st
2022-04-09 11:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: opa-integration-st
2022-04-09 11:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: opa-integration-st
2022-04-09 11:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka opa-cluster in namespace opa-integration-st
2022-04-09 11:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: opa-cluster will have desired state: Ready
2022-04-09 11:13:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: opa-cluster is in desired state: Ready
2022-04-09 11:13:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:13:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorization-STARTED
2022-04-09 11:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorizationSuperUser-STARTED
2022-04-09 11:13:11 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:13:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:13:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser good-user in namespace opa-integration-st
2022-04-09 11:13:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-237156269-220879411 in namespace opa-integration-st
2022-04-09 11:13:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-237156269-220879411 will have desired state: Ready
2022-04-09 11:13:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: good-user will have desired state: Ready
2022-04-09 11:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-237156269-220879411 is in desired state: Ready
2022-04-09 11:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser arnost in namespace opa-integration-st
2022-04-09 11:13:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: good-user is in desired state: Ready
2022-04-09 11:13:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bad-user in namespace opa-integration-st
2022-04-09 11:13:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: arnost will have desired state: Ready
2022-04-09 11:13:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bad-user will have desired state: Ready
2022-04-09 11:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: arnost is in desired state: Ready
2022-04-09 11:13:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: bad-user is in desired state: Ready
2022-04-09 11:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-fde5459a-kafka-clients in namespace opa-integration-st
2022-04-09 11:13:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2dc03fef-kafka-clients in namespace opa-integration-st
2022-04-09 11:13:14 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-fde5459a-kafka-clients will be ready
2022-04-09 11:13:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2dc03fef-kafka-clients will be ready
2022-04-09 11:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-fde5459a-kafka-clients is ready
2022-04-09 11:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [OpaIntegrationST:120] Checking KafkaUser good-user that is able to send and receive messages to/from topic 'my-topic-237156269-220879411'
2022-04-09 11:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@44e204c8, messages=[], arguments=[--max-messages, 100, USER=arnost, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-237156269-220879411], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fde5459a-kafka-clients-86448b75fd-pfsc8', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-237156269-220879411', maxMessages=100, kafkaUsername='arnost', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66834586}
2022-04-09 11:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-237156269-220879411 from pod my-cluster-fde5459a-kafka-clients-86448b75fd-pfsc8
2022-04-09 11:13:16 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fde5459a-kafka-clients-86448b75fd-pfsc8 -n opa-integration-st -- /opt/kafka/producer.sh --max-messages 100 USER=arnost --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-237156269-220879411
2022-04-09 11:13:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2dc03fef-kafka-clients is ready
2022-04-09 11:13:16 [ForkJoinPool-3-worker-3] [32mINFO [m [OpaIntegrationST:72] Checking KafkaUser good-user that is able to send and receive messages to/from topic 'my-topic-2118520865-1424153274'
2022-04-09 11:13:16 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7c775086, messages=[], arguments=[--max-messages, 100, USER=good_user, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-2118520865-1424153274], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-2118520865-1424153274', maxMessages=100, kafkaUsername='good-user', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3a4fdf67}
2022-04-09 11:13:16 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-2118520865-1424153274 from pod my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr
2022-04-09 11:13:16 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr -n opa-integration-st -- /opt/kafka/producer.sh --max-messages 100 USER=good_user --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-2118520865-1424153274
2022-04-09 11:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@55e707c3, messages=[], arguments=[--max-messages, 100, --group-id, consumer-group-name-2, --group-instance-id, instance2014525422, USER=arnost, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-237156269-220879411], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fde5459a-kafka-clients-86448b75fd-pfsc8', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-237156269-220879411', maxMessages=100, kafkaUsername='arnost', consumerGroupName='consumer-group-name-2', consumerInstanceId='instance2014525422', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2934d816}
2022-04-09 11:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-237156269-220879411 from pod my-cluster-fde5459a-kafka-clients-86448b75fd-pfsc8
2022-04-09 11:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fde5459a-kafka-clients-86448b75fd-pfsc8 -n opa-integration-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id consumer-group-name-2 --group-instance-id instance2014525422 USER=arnost --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-237156269-220879411
2022-04-09 11:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@16a9a5fe, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-969247170, --group-instance-id, instance237055628, USER=good_user, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-2118520865-1424153274], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-2118520865-1424153274', maxMessages=100, kafkaUsername='good-user', consumerGroupName='my-consumer-group-969247170', consumerInstanceId='instance237055628', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52d9ec25}
2022-04-09 11:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-2118520865-1424153274 from pod my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr
2022-04-09 11:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr -n opa-integration-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-969247170 --group-instance-id instance237055628 USER=good_user --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-2118520865-1424153274
2022-04-09 11:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testOpaAuthorizationSuperUser
2022-04-09 11:13:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser arnost in namespace opa-integration-st
2022-04-09 11:13:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-fde5459a-kafka-clients in namespace opa-integration-st
2022-04-09 11:13:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-237156269-220879411 in namespace opa-integration-st
2022-04-09 11:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [OpaIntegrationST:89] Checking KafkaUser bad-user that is not able to send or receive messages to/from topic 'my-topic-2118520865-1424153274'
2022-04-09 11:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1939556f, messages=[], arguments=[--max-messages, 100, USER=bad_user, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-2118520865-1424153274], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-2118520865-1424153274', maxMessages=100, kafkaUsername='bad-user', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@501c34cd}
2022-04-09 11:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-2118520865-1424153274 from pod my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr
2022-04-09 11:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr -n opa-integration-st -- /opt/kafka/producer.sh --max-messages 100 USER=bad_user --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-2118520865-1424153274
2022-04-09 11:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced -1 messages
2022-04-09 11:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5f41f991, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-969247170, --group-instance-id, instance1579483300, USER=bad_user, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-2118520865-1424153274], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-2118520865-1424153274', maxMessages=100, kafkaUsername='bad-user', consumerGroupName='my-consumer-group-969247170', consumerInstanceId='instance1579483300', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@49c3aa9}
2022-04-09 11:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-2118520865-1424153274 from pod my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr
2022-04-09 11:13:31 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2dc03fef-kafka-clients-6844f5f96-rkjdr -n opa-integration-st -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-969247170 --group-instance-id instance1579483300 USER=bad_user --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-2118520865-1424153274
2022-04-09 11:13:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:13:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 0 messages
2022-04-09 11:13:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:13:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testOpaAuthorization
2022-04-09 11:13:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bad-user in namespace opa-integration-st
2022-04-09 11:13:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser good-user in namespace opa-integration-st
2022-04-09 11:13:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2dc03fef-kafka-clients in namespace opa-integration-st
2022-04-09 11:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorizationSuperUser-FINISHED
2022-04-09 11:14:06 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:14:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:14:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorization-FINISHED
2022-04-09 11:14:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:14:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:14:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OpaIntegrationST
2022-04-09 11:14:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka opa-cluster in namespace opa-integration-st
2022-04-09 11:14:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 212.181 s - in io.strimzi.systemtest.security.OpaIntegrationST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertificates-STARTED
2022-04-09 11:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithMirrorMaker-STARTED
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewClusterCaCertsTriggeredByAnno-STARTED
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewCaCertsTriggerByExpiredCertificate-STARTED
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClientsCaKeysTriggeredByAnno-STARTED
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-56 for test case:testCertificates
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-56
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-56
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-56
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:118] Running testCertificates my-cluster-edccc3c2
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-57 for test case:testAutoReplaceClientsCaKeysTriggeredByAnno
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-57
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-edccc3c2 in namespace namespace-56
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-56
2022-04-09 11:15:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-edccc3c2 will have desired state: Ready
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-57
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-57
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-58 for test case:testAutoRenewCaCertsTriggerByExpiredCertificate
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-58
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5d48ac2f in namespace namespace-57
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-09 11:15:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5d48ac2f will have desired state: Ready
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-58
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-58
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-59 for test case:testTlsHostnameVerificationWithMirrorMaker
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:70] Creating secret my-cluster-53072883-cluster-ca-cert
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-59
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-53072883 in namespace namespace-58
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-09 11:15:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-53072883 will have desired state: Ready
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-59
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-59
2022-04-09 11:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-60 for test case:testAutoRenewClusterCaCertsTriggeredByAnno
2022-04-09 11:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-60
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-25834a7d-source in namespace namespace-59
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-59
2022-04-09 11:15:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25834a7d-source will have desired state: Ready
2022-04-09 11:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-60
2022-04-09 11:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-60
2022-04-09 11:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ec7c5d18 in namespace namespace-60
2022-04-09 11:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-09 11:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ec7c5d18 will have desired state: Ready
2022-04-09 11:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-edccc3c2 is in desired state: Ready
2022-04-09 11:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:122] Check Kafka bootstrap certificate
2022-04-09 11:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-bootstrap:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-bootstrap
2022-04-09 11:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:125] OPENSSL OUTPUT: 

CONNECTED(00000003)
---
Certificate chain
 0 s:O = io.strimzi, CN = my-cluster-edccc3c2-kafka
   i:O = io.strimzi, CN = cluster-ca v0
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
 1 s:O = io.strimzi, CN = cluster-ca v0
   i:O = io.strimzi, CN = cluster-ca v0
-----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUQpVvppRwrNdyDHrW+cjwJAuudqgwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjA0MDkxMTE1MzFaFw0yMzA0MDkxMTE1MzFaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQClI2El9EJiWWkyOeP4sltgU/cIrwn+y1MZmjFpsvao
g1EE11vIxOvaZKJG7ghFLpV+gqkh6tR8aWT1d+8O1OySOTHXZcG/TTCJ5Ct9Xq2B
gBhfKo6/M3i7r0XzBptND2tJKfL18uMS0befVaISo8bzi+TjtGfNcpR5WdRshdrO
hpmHk+HtDDZb0gfTGg+M3xp5l1z9pnr2uEtEkYK5srqZBTduIfl9uU/Ma17lzhS7
IoDiuUqGBEhi2dTF5TZkl5XkEE+icPc0+uLsCBlVvOR0NeHwREGEq3y44aXCYzzD
P0Ufds2mTACMCdNZba3qwK5BAGgbZTsoObmhG49BrDlOtycPqUqJ7y03+OgpNdOE
bqfeoZo/pyUV5FiU03Bfzi5a5ZFeTbY6voKN3L32e3WGybvUHoAf/miKHnkVQdK4
HKDR54JVwqfRiPVQ6ttnHiBCi92QKyZevkvVtvqowuT5OtQ/wnzv9QopNXeHv+fc
/cJrRlpnardVaPOa57RfdbG3Egs2JMkTSjrXuOv56OeMn2MkSwh9Ab81boyRZr4B
BDf8DcqGeK7I22WWMVmR8txve8J2jc6SS8ZwQiumzaKTsoaaPu0jBXG9WVt/6kGJ
sU49OB+AQ1hmpaew7ltMrnSImlzuaZt21AO+fqywms1y6aPHRD3I20qQYIYVX+cW
wQIDAQABo0UwQzAdBgNVHQ4EFgQUIY9WHVlxPgKYbrbxrjFXknpwgikwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AGyIB6BJcXTbVtPVcU0a19FpahYDDzdNGjyF8bjWhvd6PNSRwwtJb0OQsEWgLSAw
pLiZIjBV9woIOZhnCN2/TuOiunEEYWgn3wMaRM5ybJWsyrHTMjl6gnIxqPrv1R3/
Wf+zxrLCbMuLuwqmsMl4pAHnFP1+xA6eB9TwMrzSDOzkQgJE8akFKdLbfNlEFt77
WrI61ebxN+l7CZRPELGynHyA5obAVgTJcsaKepjC+TW6+u+6GfA6GnYf3AhlKzb9
yy1LE9L3KQhBVUe946pZzQRDVDYMrpgsyxZpbtLjIxr6/BIACUfSjhcsaYVCZRWZ
GTcY0FiIoKfYGl6dmh12Zyy/m6mzMRZyuUyXVqk0Cn9biWIyLhHlKz4W5g08h6QF
2m0gsYRb8jlSLK0Lli1AC7GnKprnwB4wT6Q28hIFsZI+TQLPxqg5SvgM+YKdNsLs
E4aG5agJmK0rUZx/YtJEvavdMRgS5YOzjS4LkcfaOEi2q7oS2J/Lxvdeiq3+UAH3
cyikdUf7/nz52MtCZHT/3t9do8z2+5/MNIkAweRpCBGMOlB7QgmHlkTFoMkkhJbt
ZyXaRtFq014bBUqm1AF8LoOfJzcCE8578s+F1NfyU9OHtqzAs0DJ/PsmVpTMgvK1
e6+E91Acdu0zptDfNa1vA5HYHQ9PUwu5Bj6kjhz3U3Zm
-----END CERTIFICATE-----
---
Server certificate
subject=O = io.strimzi, CN = my-cluster-edccc3c2-kafka

issuer=O = io.strimzi, CN = cluster-ca v0

---
No client certificate CA names sent
Peer signing digest: SHA256
Peer signature type: RSA-PSS
Server Temp Key: X25519, 253 bits
---
SSL handshake has read 3489 bytes and written 369 bytes
Verification: OK
Verified peername: my-cluster-edccc3c2-kafka-bootstrap
---
New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384
Server public key is 2048 bit
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---



2022-04-09 11:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:128] Check zookeeper client certificate
2022-04-09 11:17:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25834a7d-source is in desired state: Ready
2022-04-09 11:17:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-25834a7d-target in namespace namespace-60
2022-04-09 11:17:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-59
2022-04-09 11:17:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25834a7d-target will have desired state: Ready
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-client:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-client -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-0.key
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:139] Checking certificates for podId 0
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9091
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-0.my-cluster-edccc3c2-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-0.my-cluster-edccc3c2-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-0.key
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9093
2022-04-09 11:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-0.my-cluster-edccc3c2-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-0.my-cluster-edccc3c2-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-0.key
2022-04-09 11:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 2181
2022-04-09 11:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-zookeeper-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-0.my-cluster-edccc3c2-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-0.my-cluster-edccc3c2-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-0.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-0.key
2022-04-09 11:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 3888
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-zookeeper-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-0.my-cluster-edccc3c2-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-0.my-cluster-edccc3c2-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-0.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-0.key
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:139] Checking certificates for podId 1
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9091
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-1.my-cluster-edccc3c2-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-1.my-cluster-edccc3c2-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-1.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-1.key
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9093
2022-04-09 11:17:39 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:17:39 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCaRenewalBreakInMiddle-STARTED
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-1.my-cluster-edccc3c2-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-1.my-cluster-edccc3c2-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-1.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-1.key
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 2181
2022-04-09 11:17:40 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:17:40 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-zookeeper-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-1.my-cluster-edccc3c2-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-1.my-cluster-edccc3c2-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-1.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-1.key
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 3888
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-zookeeper-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-1.my-cluster-edccc3c2-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-1.my-cluster-edccc3c2-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-1.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-1.key
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:139] Checking certificates for podId 2
2022-04-09 11:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9091
2022-04-09 11:17:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:17:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertRegeneratedAfterInternalCAisDeleted-STARTED
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-2.my-cluster-edccc3c2-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-2.my-cluster-edccc3c2-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-2.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-2.key
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9093
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-kafka-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-kafka-2.my-cluster-edccc3c2-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-kafka-2.my-cluster-edccc3c2-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-2.crt -key /opt/kafka/broker-certs/my-cluster-edccc3c2-kafka-2.key
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 2181
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-zookeeper-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-2.my-cluster-edccc3c2-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-2.my-cluster-edccc3c2-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-2.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-2.key
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 3888
2022-04-09 11:17:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-edccc3c2-zookeeper-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-edccc3c2-zookeeper-2.my-cluster-edccc3c2-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-edccc3c2-zookeeper-2.my-cluster-edccc3c2-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-2.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-edccc3c2-zookeeper-2.key
2022-04-09 11:17:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 11:17:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:17:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCertificates
2022-04-09 11:17:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-edccc3c2 in namespace namespace-56
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-61 for test case:testCaRenewalBreakInMiddle
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-61
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-61
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-61
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-be8a7ef1 in namespace namespace-61
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-09 11:17:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-be8a7ef1 will have desired state: Ready
2022-04-09 11:17:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:17:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-56 for test case:testCertificates
2022-04-09 11:18:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertificates-FINISHED
2022-04-09 11:18:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:18:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:18:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testClientsCACertRenew-STARTED
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-62 for test case:testCertRegeneratedAfterInternalCAisDeleted
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-62
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-62
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-62
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-888d4396 in namespace namespace-62
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-09 11:18:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-888d4396 will have desired state: Ready
2022-04-09 11:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5d48ac2f is in desired state: Ready
2022-04-09 11:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-699003840-1729348461 in namespace namespace-62
2022-04-09 11:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-09 11:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-699003840-1729348461 will have desired state: Ready
2022-04-09 11:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-699003840-1729348461 is in desired state: Ready
2022-04-09 11:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1867361953-2102739105 in namespace namespace-62
2022-04-09 11:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-09 11:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1867361953-2102739105 will have desired state: Ready
2022-04-09 11:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1867361953-2102739105 is in desired state: Ready
2022-04-09 11:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5d48ac2f-kafka-clients in namespace namespace-62
2022-04-09 11:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-09 11:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5d48ac2f-kafka-clients will be ready
2022-04-09 11:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5d48ac2f-kafka-clients is ready
2022-04-09 11:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:453] Checking produced and consumed messages to pod:my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr
2022-04-09 11:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@726b9d53, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-1867361953-2102739105], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr', podNamespace='namespace-57', bootstrapServer='my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-1867361953-2102739105', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@24936c5e}
2022-04-09 11:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092:my-topic-1867361953-2102739105 from pod my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr
2022-04-09 11:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr -n namespace-57 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-1867361953-2102739105
2022-04-09 11:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5ed7f206, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1711850669, --group-instance-id, instance304438762, --bootstrap-server, my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-1867361953-2102739105], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr', podNamespace='namespace-57', bootstrapServer='my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-1867361953-2102739105', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1711850669', consumerInstanceId='instance304438762', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@163560a}
2022-04-09 11:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092#my-topic-1867361953-2102739105 from pod my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr
2022-04-09 11:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr -n namespace-57 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1711850669 --group-instance-id instance304438762 --bootstrap-server my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-1867361953-2102739105
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25834a7d-target is in desired state: Ready
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:888] Getting IP of the source bootstrap service for consumer
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:891] Getting IP of the target bootstrap service for producer
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:894] KafkaMirrorMaker without config ssl.endpoint.identification.algorithm will not connect to consumer with address 10.100.119.179:9093
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:895] KafkaMirrorMaker without config ssl.endpoint.identification.algorithm will not connect to producer with address 10.104.206.131:9093
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-25834a7d in namespace namespace-62
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-59
2022-04-09 11:18:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:245] Wait until Pod my-cluster-25834a7d-mirror-maker is present
2022-04-09 11:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:467] Triggering CA cert renewal by adding the annotation
2022-04-09 11:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:479] Patching secret my-cluster-5d48ac2f-clients-ca with strimzi.io/force-replace
2022-04-09 11:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:489] Wait for kafka to rolling restart (1)...
2022-04-09 11:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-5d48ac2f-kafka rolling update
2022-04-09 11:18:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:249] Pod my-cluster-25834a7d-mirror-maker is present
2022-04-09 11:18:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:236] Wait until Pod my-cluster-25834a7d-mirror-maker-5559b5d8dd-sxlzn is in CrashLoopBackOff state
2022-04-09 11:19:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ec7c5d18 is in desired state: Ready
2022-04-09 11:19:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-905808424-694944282 in namespace namespace-62
2022-04-09 11:19:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-09 11:19:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-905808424-694944282 will have desired state: Ready
2022-04-09 11:19:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-53072883 is in desired state: Ready
2022-04-09 11:19:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1543846601-1236311323 in namespace namespace-62
2022-04-09 11:19:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-09 11:19:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1543846601-1236311323 will have desired state: Ready
2022-04-09 11:19:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-905808424-694944282 is in desired state: Ready
2022-04-09 11:19:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1383097147-1968320726 in namespace namespace-62
2022-04-09 11:19:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-09 11:19:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1383097147-1968320726 will have desired state: Ready
2022-04-09 11:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1543846601-1236311323 is in desired state: Ready
2022-04-09 11:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-804297131-1770988647 in namespace namespace-62
2022-04-09 11:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-09 11:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-804297131-1770988647 will have desired state: Ready
2022-04-09 11:19:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1383097147-1968320726 is in desired state: Ready
2022-04-09 11:19:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-ec7c5d18-kafka-clients in namespace namespace-62
2022-04-09 11:19:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-09 11:19:12 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ec7c5d18-kafka-clients will be ready
2022-04-09 11:19:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-804297131-1770988647 is in desired state: Ready
2022-04-09 11:19:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-53072883-kafka-clients in namespace namespace-62
2022-04-09 11:19:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-09 11:19:13 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-53072883-kafka-clients will be ready
2022-04-09 11:19:14 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ec7c5d18-kafka-clients is ready
2022-04-09 11:19:14 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:19:14 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:260] Checking produced and consumed messages to pod:my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4
2022-04-09 11:19:14 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2a186d10, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092, --topic, my-topic-1383097147-1968320726], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4', podNamespace='namespace-60', bootstrapServer='my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092', topicName='my-topic-1383097147-1968320726', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ff1635a}
2022-04-09 11:19:14 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092:my-topic-1383097147-1968320726 from pod my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4
2022-04-09 11:19:14 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4 -n namespace-60 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092 --topic my-topic-1383097147-1968320726
2022-04-09 11:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-53072883-kafka-clients is ready
2022-04-09 11:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:660] Checking produced and consumed messages to pod:my-cluster-53072883-kafka-clients-56979d7658-8n65m
2022-04-09 11:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@607ff30, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-804297131-1770988647], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-53072883-kafka-clients-56979d7658-8n65m', podNamespace='namespace-58', bootstrapServer='my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-804297131-1770988647', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7b3ed9b0}
2022-04-09 11:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092:my-topic-804297131-1770988647 from pod my-cluster-53072883-kafka-clients-56979d7658-8n65m
2022-04-09 11:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-53072883-kafka-clients-56979d7658-8n65m -n namespace-58 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-804297131-1770988647
2022-04-09 11:19:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:19:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:19:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3663d643, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1257147962, --group-instance-id, instance1293608735, --bootstrap-server, my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092, --topic, my-topic-1383097147-1968320726], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4', podNamespace='namespace-60', bootstrapServer='my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092', topicName='my-topic-1383097147-1968320726', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1257147962', consumerInstanceId='instance1293608735', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6d1deeb3}
2022-04-09 11:19:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092#my-topic-1383097147-1968320726 from pod my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4
2022-04-09 11:19:16 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4 -n namespace-60 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1257147962 --group-instance-id instance1293608735 --bootstrap-server my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092 --topic my-topic-1383097147-1968320726
2022-04-09 11:19:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:19:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:19:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@41555b4, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1854707775, --group-instance-id, instance579694853, --bootstrap-server, my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-804297131-1770988647], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-53072883-kafka-clients-56979d7658-8n65m', podNamespace='namespace-58', bootstrapServer='my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-804297131-1770988647', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1854707775', consumerInstanceId='instance579694853', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@96bee70}
2022-04-09 11:19:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092#my-topic-804297131-1770988647 from pod my-cluster-53072883-kafka-clients-56979d7658-8n65m
2022-04-09 11:19:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-53072883-kafka-clients-56979d7658-8n65m -n namespace-58 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1854707775 --group-instance-id instance579694853 --bootstrap-server my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-804297131-1770988647
2022-04-09 11:19:23 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:19:23 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:19:23 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:274] Triggering CA cert renewal by adding the annotation
2022-04-09 11:19:23 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:286] Patching secret my-cluster-ec7c5d18-cluster-ca-cert with strimzi.io/force-renew
2022-04-09 11:19:23 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:291] Wait for zk to rolling restart ...
2022-04-09 11:19:23 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-ec7c5d18-zookeeper rolling update
2022-04-09 11:19:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:241] Pod my-cluster-25834a7d-mirror-maker-5559b5d8dd-sxlzn is in CrashLoopBackOff state
2022-04-09 11:19:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:930] KafkaMirrorMaker with config ssl.endpoint.identification.algorithm will connect to consumer with address 10.100.119.179:9093
2022-04-09 11:19:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:931] KafkaMirrorMaker with config ssl.endpoint.identification.algorithm will connect to producer with address 10.104.206.131:9093
2022-04-09 11:19:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:933] Adding configuration ssl.endpoint.identification.algorithm to the mirror maker...
2022-04-09 11:19:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-25834a7d will have desired state: Ready
2022-04-09 11:19:24 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:19:24 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:19:24 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:131] Waiting for Secret my-cluster-53072883-cluster-ca-cert certificate change
2022-04-09 11:19:25 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:138] Certificate in Secret my-cluster-53072883-cluster-ca-cert has changed, was -----BEGIN CERTIFICATE-----
MIIDKjCCAhKgAwIBAgIJAJBt0u1FFTG+MA0GCSqGSIb3DQEBCwUAMCoxEzARBgNV
BAoMCnN0cmltemkuaW8xEzARBgNVBAMMCmNsdXN0ZXItY2EwHhcNMTgxMDIzMTAw
MTExWhcNMTgxMDI0MTAwMTExWjAqMRMwEQYDVQQKDApzdHJpbXppLmlvMRMwEQYD
VQQDDApjbHVzdGVyLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
xpuYrNXYHqw3ajwd12aAeuTlAX4rVwVdPuIex6A4NL8J3d2DV+ngXgNTH//RhiF5
If5KRWSsLei5BUIrwuQutOUNCQwyACmri9+yrx6+tevligiokAUwhHxcDHZpwC3T
+2dzk/BkI++vbSuvjFmBKGQi9gfyoTnStTEQ85KVJUS170hzwDjzaEiJsKpOPx/G
+KTdkAopLucoxr4sxhYeO4mQ2PkT0QL+R8Ohs6LD6v/bqalFP+rS8vibolfxjMNm
lXQCOd8UfXy8OEOaNoNCvhnn/cT/hbEG/ARbV3hHmUh9COV+TSV5dhsbmS5h4MKw
LzP449nGQBmLSkZMu984DQIDAQABo1MwUTAdBgNVHQ4EFgQUOXubcHBZJ7vSzjpi
pfXdFSP3dsEwHwYDVR0jBBgwFoAUOXubcHBZJ7vSzjpipfXdFSP3dsEwDwYDVR0T
AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAFFPGykbzUREDMzh+33i3a8TF
UTQnPMN/SuVbpLfQdpkpLO+aVWjVvJP6qMrI7jRO5zhj4MecAf7YKpe+dyRTTz9B
Dy9BZcujHYjCKdcKBnBFQ7B1xQm9tL1bw+a3ABzSTlhLiBcCxhJEawlWy1Gh18ab
3x/Kqnz0mk/jt5+n9HwlKHuBQVIxRCsfHWwq+WvIfoxM+N//akV8/29hLUf5TlYH
F5CX4G2pA5sSdaDHQ4ekQWuqM6tfvsGLl2KOmEFEgVR4GfaWI4BsUCzlpBNcCGWv
V7klZvBxQPG7MVy0cB8yzTDtUpR0jUFUkZOSp5Pr3yWcwPWEgWkaXYfO+FWETA==
-----END CERTIFICATE-----
, is now -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUH7ueviUSRay2WXqWhB3fNWgnOdowDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjA0MDkxMTE1MzVaFw0yMzA0MDkxMTE1MzVaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQDUsBAnEw/Tya9EzNFX53YpgeM7cjCiitnPgKnWAByi
f9+bHtmMeeoJKYt2HyCRvp0hEsRr/kI5Cvd2KIz7cM0fUYgf+ib7TwySMGNNDw1y
ys4uHi1vRVZkzdfhXMx6rQlhfWhdsoyxmAdCk1RPi0fXWFF8qywM7Zp9PjCfKfAz
xgSnoGgSwDvwJAtK/47py+uL07vzvsuTFtYqcbwWsYz4WChC8hJK2qxFFWRwTP9E
hFMhnsUMKOApwaHZP7Bj9BVVFx09+4JaDyeSzLAKFSY44aI40IYJbpXuCynUlQs4
G+7XrnfiK64KL4sk+UK4pACtvs7TzPjh1yO3/mMiLAEQigcIowgz6RqTFJPNZrYC
3lAnQNfPzyQuQiyOPI1KaT8zDZXIA6kqZrzBtE42NCojrgJFNxB224ByrBMYGMzs
QOUM0YD6Bo71xe2EG731FERzEzS+6GIpRQabJyhwmkl00lZftLzBj9Amk5xnMRqI
M7cqmRh8HhCEcyLw3lNQHBf/xXcKObS/fpUNrKtQG/Hw1LyMh1O0QYa9exsPlYos
oiyy3j18Q8V7d2IOb3O8L/mkyrWWlIH5BtQ/1oG5rBAlLGPEYeGZrLmwym3y7R+m
mn9Bw8BTPtTf+2PB0hkWZe625+JXtVSWIXKAOjJUB4XCTzaePoQQQThZzIfn9Bon
pQIDAQABo0UwQzAdBgNVHQ4EFgQUf5yv5KFUbADbxWjx/bf0nfP7ZeEwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AKjlxXLohxiqJWT3b1qcdZxl1AghlEEkx49bDUdQFcJuSvM3XTRPtq/3Q7qvtqRw
xCI1H+lmCzAB7ACOffegTpMpVSpWKlPznn2nptlDQ1XH9OrDRoFD82fi4B+JQt5n
5TAmzOrmWNuO1kXE/SH9vyhGMEJ8bwI9cYneJqwfX1iYe0lQnEoHapw2rRVhyKt/
61w4K576s9Atskx56DhjLHKVem6vGlMPEmlakLmZG5fC0UYU7rCEhLseOQQM0lkV
mVwOUz4WWeAJjcussZbaWAvvpyvWsJbVD8XFp+LEdGRlI/NmxDeuAqjGe7uqlkRy
YR2E/ayQ3QL2I/7cE93s6qn5xfpdmAQUAoL3v3O/YbjAFnthBt7ds3qxKStc3Vx9
LG+QiY2H/OUDHGcskzlOyP5jVM01hiUfx558uEwrzPROfLSdRRV8xgG7c4tRj6hs
1cWmh8/j1je7Es3rkBGB1Tyw3pqUQL16J8Sshtz3pwKZ5biVmQkBJ2pQ4850mdiN
utlodO6a05rcQpUYEERFk7sNKZQiVzASIsPLTUvxR5EYMIBU3LrlkJI2d0Of7woy
ZywWyAozPOlrwmSHtwsuIB6hVPMgs6HRF0Wzz1ERdteRkEUk0awvxaGPqnCRiuiv
QHrCMKQrjW3X2bKrbJP5L4UYgton2OxCoWOL2ZjawWWz
-----END CERTIFICATE-----

2022-04-09 11:19:25 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-09 11:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-888d4396 is in desired state: Ready
2022-04-09 11:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1295571291-974474085 in namespace namespace-62
2022-04-09 11:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-09 11:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1295571291-974474085 will have desired state: Ready
2022-04-09 11:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1295571291-974474085 is in desired state: Ready
2022-04-09 11:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1289249929-2032295810 in namespace namespace-62
2022-04-09 11:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-09 11:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1289249929-2032295810 will have desired state: Ready
2022-04-09 11:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1289249929-2032295810 is in desired state: Ready
2022-04-09 11:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-888d4396-kafka-clients in namespace namespace-62
2022-04-09 11:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-09 11:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-888d4396-kafka-clients will be ready
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-888d4396-kafka-clients is ready
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:797] Verifying that secret Secret(apiVersion=v1, data={ca.crt=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, ca.p12=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, ca.password=QlU3SzlKM0ZuWTJF}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations={strimzi.io/ca-cert-generation=0}, clusterName=null, creationTimestamp=2022-04-09T11:18:38Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-888d4396, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-888d4396, strimzi.io/cluster=my-cluster-888d4396, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testCertRegeneratedAfterInternalCAisDeleted}, managedFields=[], name=my-cluster-888d4396-clients-ca-cert, namespace=namespace-62, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-888d4396, uid=c4c9d52c-fa63-46d2-b103-cd8fe902595e, additionalProperties={})], resourceVersion=814650, selfLink=/api/v1/namespaces/namespace-62/secrets/my-cluster-888d4396-clients-ca-cert, uid=68e443a7-07a0-42fd-844a-36a6c1cf6cea, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) with name my-cluster-888d4396-clients-ca-cert is present
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:797] Verifying that secret Secret(apiVersion=v1, data={ca.crt=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, ca.p12=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, ca.password=QzdaUXZINDltRVFO}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations={strimzi.io/ca-cert-generation=0}, clusterName=null, creationTimestamp=2022-04-09T11:18:38Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-888d4396, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-888d4396, strimzi.io/cluster=my-cluster-888d4396, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testCertRegeneratedAfterInternalCAisDeleted}, managedFields=[], name=my-cluster-888d4396-cluster-ca-cert, namespace=namespace-62, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-888d4396, uid=c4c9d52c-fa63-46d2-b103-cd8fe902595e, additionalProperties={})], resourceVersion=814649, selfLink=/api/v1/namespaces/namespace-62/secrets/my-cluster-888d4396-cluster-ca-cert, uid=233bfcc0-dfe5-4d50-becf-d841fdcab2a7, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) with name my-cluster-888d4396-cluster-ca-cert is present
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:802] Deleting secret my-cluster-888d4396-clients-ca-cert
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:802] Deleting secret my-cluster-888d4396-cluster-ca-cert
2022-04-09 11:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-888d4396-kafka are stable
2022-04-09 11:19:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 11:19:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 11:19:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 11:19:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 11:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 11:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 11:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 11:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 11:19:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 11:19:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 11:19:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 11:19:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 11:20:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 11:20:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 11:20:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 11:20:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 11:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 11:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 11:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 11:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 11:20:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 11:20:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 11:20:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 11:20:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 11:20:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 11:20:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 11:20:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 11:20:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 11:20:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 11:20:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 11:20:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 11:20:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 11:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 11:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 11:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 11:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 11:20:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 11:20:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 11:20:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 11:20:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 11:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 11:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 11:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 11:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 11:20:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 11:20:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 11:20:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 11:20:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 11:20:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 11:20:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 11:20:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 11:20:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 11:20:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 11:20:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 11:20:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 11:20:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 11:20:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 11:20:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 11:20:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 11:20:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 11:20:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 11:20:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 11:20:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 11:20:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 11:20:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 11:20:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 11:20:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 11:20:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 11:20:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-be8a7ef1 is in desired state: Ready
2022-04-09 11:20:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-398384796-953216196 in namespace namespace-62
2022-04-09 11:20:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-09 11:20:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-398384796-953216196 will have desired state: Ready
2022-04-09 11:20:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 11:20:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 11:20:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 11:20:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 11:20:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-398384796-953216196 is in desired state: Ready
2022-04-09 11:20:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1479983589-1349517763 in namespace namespace-62
2022-04-09 11:20:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-09 11:20:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1479983589-1349517763 will have desired state: Ready
2022-04-09 11:20:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 11:20:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 11:20:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 11:20:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 11:20:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1479983589-1349517763 is in desired state: Ready
2022-04-09 11:20:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-be8a7ef1-kafka-clients in namespace namespace-62
2022-04-09 11:20:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-09 11:20:15 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-be8a7ef1-kafka-clients will be ready
2022-04-09 11:20:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 11:20:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 11:20:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 11:20:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 11:20:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 11:20:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 11:20:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 11:20:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 11:20:17 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-be8a7ef1-kafka-clients is ready
2022-04-09 11:20:17 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:20:17 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@48843f35, messages=[], arguments=[--max-messages, 100, USER=my_user_398384796_953216196, --bootstrap-server, my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-1479983589-1349517763], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj', podNamespace='namespace-61', bootstrapServer='my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-1479983589-1349517763', maxMessages=100, kafkaUsername='my-user-398384796-953216196', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@e8e5a2b}
2022-04-09 11:20:17 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093:my-topic-1479983589-1349517763 from pod my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:20:17 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj -n namespace-61 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_398384796_953216196 --bootstrap-server my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-1479983589-1349517763
2022-04-09 11:20:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 11:20:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 11:20:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 11:20:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 11:20:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 11:20:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 11:20:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 11:20:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 11:20:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 11:20:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 11:20:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 11:20:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 11:20:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 11:20:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 11:20:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 11:20:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 11:20:21 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:20:21 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:20:21 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4d29c84a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-268896532, --group-instance-id, instance1871432933, USER=my_user_398384796_953216196, --bootstrap-server, my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-1479983589-1349517763], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj', podNamespace='namespace-61', bootstrapServer='my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-1479983589-1349517763', maxMessages=100, kafkaUsername='my-user-398384796-953216196', consumerGroupName='my-consumer-group-268896532', consumerInstanceId='instance1871432933', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4fbdf63f}
2022-04-09 11:20:21 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093:my-topic-1479983589-1349517763 from pod my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:20:21 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj -n namespace-61 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-268896532 --group-instance-id instance1871432933 USER=my_user_398384796_953216196 --bootstrap-server my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-1479983589-1349517763
2022-04-09 11:20:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 11:20:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 11:20:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 11:20:22 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 11:20:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 11:20:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 11:20:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 11:20:23 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 11:20:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 11:20:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 11:20:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 11:20:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 11:20:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 11:20:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 11:20:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 11:20:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 11:20:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 11:20:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 11:20:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 11:20:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 11:20:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 11:20:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 11:20:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 11:20:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 11:20:28 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-09 11:20:28 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:672] Checking produced and consumed messages to pod:my-cluster-53072883-kafka-clients-56979d7658-8n65m
2022-04-09 11:20:28 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@146b34fb, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-804297131-1770988647], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-53072883-kafka-clients-56979d7658-8n65m', podNamespace='namespace-58', bootstrapServer='my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-804297131-1770988647', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@42a54ab1}
2022-04-09 11:20:28 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092:my-topic-804297131-1770988647 from pod my-cluster-53072883-kafka-clients-56979d7658-8n65m
2022-04-09 11:20:28 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-53072883-kafka-clients-56979d7658-8n65m -n namespace-58 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-804297131-1770988647
2022-04-09 11:20:28 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-ec7c5d18-zookeeper has been successfully rolled
2022-04-09 11:20:28 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-ec7c5d18-zookeeper to be ready
2022-04-09 11:20:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 11:20:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 11:20:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 11:20:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 11:20:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 11:20:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 11:20:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 11:20:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 11:20:30 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:20:30 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:20:30 [ForkJoinPool-3-worker-11] [32mINFO [m [SecretUtils:70] Creating secret my-cluster-be8a7ef1-cluster-ca-cert
2022-04-09 11:20:30 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 11:20:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 11:20:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 11:20:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 11:20:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:20:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:20:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@793fb15a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1854707775, --group-instance-id, instance1343910568, --bootstrap-server, my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-804297131-1770988647], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-53072883-kafka-clients-56979d7658-8n65m', podNamespace='namespace-58', bootstrapServer='my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-804297131-1770988647', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1854707775', consumerInstanceId='instance1343910568', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11a7811d}
2022-04-09 11:20:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092#my-topic-804297131-1770988647 from pod my-cluster-53072883-kafka-clients-56979d7658-8n65m
2022-04-09 11:20:30 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-53072883-kafka-clients-56979d7658-8n65m -n namespace-58 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1854707775 --group-instance-id instance1343910568 --bootstrap-server my-cluster-53072883-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-804297131-1770988647
2022-04-09 11:20:31 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 11:20:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 11:20:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 11:20:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 11:20:32 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 11:20:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 11:20:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 11:20:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 11:20:33 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 11:20:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 11:20:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 11:20:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 11:20:34 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 11:20:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 11:20:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 11:20:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 11:20:35 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 11:20:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 11:20:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 11:20:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 11:20:36 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 11:20:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 11:20:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 11:20:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 11:20:37 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:20:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:20:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:20:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewCaCertsTriggerByExpiredCertificate
2022-04-09 11:20:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-804297131-1770988647 in namespace namespace-58
2022-04-09 11:20:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 11:20:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 11:20:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 11:20:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 11:20:38 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 11:20:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 11:20:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 11:20:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 11:20:39 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 11:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 11:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 11:20:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 11:20:40 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 11:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 11:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 11:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 11:20:41 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 11:20:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 11:20:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 11:20:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 11:20:42 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1218] No pods of my-cluster-be8a7ef1-zookeeper are in desired state
2022-04-09 11:20:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 11:20:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 11:20:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 11:20:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 11:20:43 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1221] Pod in 'Pending' state: my-cluster-be8a7ef1-zookeeper-1
2022-04-09 11:20:43 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@712ed20f, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-521251706, --group-instance-id, instance995567661, USER=my_user_398384796_953216196, --bootstrap-server, my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-1479983589-1349517763], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj', podNamespace='namespace-61', bootstrapServer='my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-1479983589-1349517763', maxMessages=100, kafkaUsername='my-user-398384796-953216196', consumerGroupName='my-consumer-group-521251706', consumerInstanceId='instance995567661', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5ef6a36f}
2022-04-09 11:20:43 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093:my-topic-1479983589-1349517763 from pod my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:20:43 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj -n namespace-61 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-521251706 --group-instance-id instance995567661 USER=my_user_398384796_953216196 --bootstrap-server my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-1479983589-1349517763
2022-04-09 11:20:44 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-5d48ac2f-kafka has been successfully rolled
2022-04-09 11:20:44 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:510] Wait for kafka to rolling restart (2)...
2022-04-09 11:20:44 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-5d48ac2f-kafka rolling update
2022-04-09 11:20:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 11:20:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 11:20:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 11:20:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 11:20:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 11:20:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 11:20:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 11:20:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 11:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 11:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 11:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 11:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 11:20:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 11:20:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 11:20:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 11:20:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 11:20:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-888d4396-kafka-0 ,my-cluster-888d4396-kafka-1 ,my-cluster-888d4396-kafka-2 ,my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf
2022-04-09 11:20:47 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-888d4396-kafka rolling update
2022-04-09 11:20:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-53072883-kafka-clients in namespace namespace-58
2022-04-09 11:20:51 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:20:51 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:20:51 [ForkJoinPool-3-worker-11] [32mINFO [m [SecretUtils:131] Waiting for Secret my-cluster-be8a7ef1-cluster-ca-cert certificate change
2022-04-09 11:20:51 [ForkJoinPool-3-worker-11] [32mINFO [m [SecretUtils:138] Certificate in Secret my-cluster-be8a7ef1-cluster-ca-cert has changed, was -----BEGIN CERTIFICATE-----
MIIDKjCCAhKgAwIBAgIJAJBt0u1FFTG+MA0GCSqGSIb3DQEBCwUAMCoxEzARBgNV
BAoMCnN0cmltemkuaW8xEzARBgNVBAMMCmNsdXN0ZXItY2EwHhcNMTgxMDIzMTAw
MTExWhcNMTgxMDI0MTAwMTExWjAqMRMwEQYDVQQKDApzdHJpbXppLmlvMRMwEQYD
VQQDDApjbHVzdGVyLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
xpuYrNXYHqw3ajwd12aAeuTlAX4rVwVdPuIex6A4NL8J3d2DV+ngXgNTH//RhiF5
If5KRWSsLei5BUIrwuQutOUNCQwyACmri9+yrx6+tevligiokAUwhHxcDHZpwC3T
+2dzk/BkI++vbSuvjFmBKGQi9gfyoTnStTEQ85KVJUS170hzwDjzaEiJsKpOPx/G
+KTdkAopLucoxr4sxhYeO4mQ2PkT0QL+R8Ohs6LD6v/bqalFP+rS8vibolfxjMNm
lXQCOd8UfXy8OEOaNoNCvhnn/cT/hbEG/ARbV3hHmUh9COV+TSV5dhsbmS5h4MKw
LzP449nGQBmLSkZMu984DQIDAQABo1MwUTAdBgNVHQ4EFgQUOXubcHBZJ7vSzjpi
pfXdFSP3dsEwHwYDVR0jBBgwFoAUOXubcHBZJ7vSzjpipfXdFSP3dsEwDwYDVR0T
AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAFFPGykbzUREDMzh+33i3a8TF
UTQnPMN/SuVbpLfQdpkpLO+aVWjVvJP6qMrI7jRO5zhj4MecAf7YKpe+dyRTTz9B
Dy9BZcujHYjCKdcKBnBFQ7B1xQm9tL1bw+a3ABzSTlhLiBcCxhJEawlWy1Gh18ab
3x/Kqnz0mk/jt5+n9HwlKHuBQVIxRCsfHWwq+WvIfoxM+N//akV8/29hLUf5TlYH
F5CX4G2pA5sSdaDHQ4ekQWuqM6tfvsGLl2KOmEFEgVR4GfaWI4BsUCzlpBNcCGWv
V7klZvBxQPG7MVy0cB8yzTDtUpR0jUFUkZOSp5Pr3yWcwPWEgWkaXYfO+FWETA==
-----END CERTIFICATE-----
, is now -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIULsukM7EmfDKh+4eh18Mh9Nj0t/AwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjA0MDkxMTIwMzBaFw0yMjA0MTYxMTIwMzBaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQC/sITDIZ1rBEPNPmyx5RYSQXFtppQflGHju3KJsGjr
UPRBpLtmiqrN3RdZ33Ak2oZEZuKTGWMbuyxwg2rSqr20Sx0aD7RRKo4BWYa9m/1i
bpGJSExuizGqmLUKTQnTkZpcKB5dcAZLzvZH+hes/iU+V6byGxRyjm0c5yuCaqpW
Be7ugpa5doIvsp7hGYGDsr7LXIQfyON8vn+vzk8HqODmv4sLFbrQdTr2ZQ6+TrqO
OM8B8+1yfbSpIbdjYy5qNJ9nUG0n3QWXZnztRtO4joYyqUAAiYhlRoLUTPE/s/kg
PGdsLdgIivzTPYJjzNHIMOsehKYckqeo4WUOV8PkJd/jInYb569QGEaGUfkmCTjl
yG21JbqpFovOV0BtN3aKBefea2joPxH+GTIu6L5DwVYMwq+2j75nsPvwaUR9g3KX
S+MkXRuSFuDr9dMc3W5b7SFzjYWgszk3fwbdzrJgOothiW58ME+3JMCmajYFiYWe
zkQvp2X5Sg6wkPLMZ2gkgjUfvuC6qc3zYMLpJkBpV619cL0gxWubZZ9A9nRirwHq
M4N7Dphl+t9Ds3B8MPe3fYGK45ha7v2taf8CHWw7EYg0vDryM4W6jZ08zjCiqytI
foCii0rgQ8V3jSvpWj1z/TgGTEnEmdRDpCjude9M9UOOBgGulT96JVo0jska8jsq
MQIDAQABo0UwQzAdBgNVHQ4EFgQUsHz/H0o1Ij+0bD0KxMEoSASzimkwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AB9cN15uD/ItzW+UkJ7/s5OUtg/VSVBgpLjF5GUGi9K2olp9vUCErry2b7tDy2dz
yuAVNUaywI1IXFjdlFkDctg5afnGKlNE3Q28wyVJqXdXUezYq9r8ZIEkCSSsA586
70g5FPqQJhn3DFmEOs46ZCf+95FVU5zpCGHJk5I+68N9dniSekH6m6urjG3QDr9g
Wii+eGKpO7o3QpMzAaSru8ELfUc5a4U/q0PMavSHd6D5mkPZpoUaotV9/3XP9cqY
4SDxmGI0VRSNoVUMjHFsZKV5SNsjKCsWKZIMNwaVvqo92LuO0n3vpQq46rGis2RO
9J//3hiOduTK63IYz8I5vqlVM8F80D/3NLOFihNAZrou80WMhaj8sjdd1j3w2bAw
uuqV3rV2cF3bPy5sGUKPFABOu5RjHvJtF4SN9o5Jiw/j5VzE9vhGdie+eGTqn6gG
a3maN9z5KF7TtU2ONmHvBn8E64kTwM2rfLKNP4EyY5HAA+unNIK1pV9y063ktu/n
qTRv3TU0ak33sn0SbFS6Wy5qe08himr5N36qZEsWSnEy1kbeY0Sd2iwCI3WFbO3L
kZ4PLV7VbzMYaEY/STbKBKGojw2d00Dx3+xfDClrSJjBzUH1+c2CxuoBcHKRWdAn
LaKNUsecgaXLH4ynadMxBoQFsoeKDIndGgnpuksuKxLk
-----END CERTIFICATE-----

2022-04-09 11:20:51 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-be8a7ef1-zookeeper rolling update
2022-04-09 11:20:54 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:295] Wait for kafka to rolling restart ...
2022-04-09 11:20:54 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-ec7c5d18-kafka rolling update
2022-04-09 11:21:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1543846601-1236311323 in namespace namespace-58
2022-04-09 11:21:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-53072883 in namespace namespace-58
2022-04-09 11:21:38 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-58, for cruise control Kafka cluster my-cluster-53072883
2022-04-09 11:21:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:21:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-58 for test case:testAutoRenewCaCertsTriggerByExpiredCertificate
2022-04-09 11:22:24 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-ec7c5d18-kafka has been successfully rolled
2022-04-09 11:22:24 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-ec7c5d18-kafka to be ready
2022-04-09 11:22:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewCaCertsTriggerByExpiredCertificate-FINISHED
2022-04-09 11:22:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:22:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:22:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectCipherSuites-STARTED
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-63 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-63
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-63
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-63
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2a67d948 in namespace namespace-63
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-09 11:22:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2a67d948 will have desired state: Ready
2022-04-09 11:22:56 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:299] Wait for EO to rolling restart ...
2022-04-09 11:22:56 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-ec7c5d18-entity-operator rolling update
2022-04-09 11:22:56 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ec7c5d18-entity-operator will be ready
2022-04-09 11:23:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-888d4396-kafka has been successfully rolled
2022-04-09 11:23:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-888d4396-kafka to be ready
2022-04-09 11:23:34 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-5d48ac2f-kafka has been successfully rolled
2022-04-09 11:23:34 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-5d48ac2f-kafka to be ready
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-888d4396 will have desired state: Ready
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-888d4396 is in desired state: Ready
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-888d4396 is ready
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-888d4396-clients-ca-cert
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:50] Secret my-cluster-888d4396-clients-ca-cert created
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-888d4396-cluster-ca-cert
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:50] Secret my-cluster-888d4396-cluster-ca-cert created
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:821] Checking consumed messages to pod:my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@554e0cae, messages=[], arguments=[--max-messages, 100, USER=my_user_1295571291_974474085, --bootstrap-server, my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093, --topic, my-topic-1289249929-2032295810], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf', podNamespace='namespace-62', bootstrapServer='my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093', topicName='my-topic-1289249929-2032295810', maxMessages=100, kafkaUsername='my-user-1295571291-974474085', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1d795953}
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093:my-topic-1289249929-2032295810 from pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf
2022-04-09 11:23:41 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf -n namespace-62 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1295571291_974474085 --bootstrap-server my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093 --topic my-topic-1289249929-2032295810
2022-04-09 11:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2416988a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-474754485, --group-instance-id, instance1881997701, USER=my_user_1295571291_974474085, --bootstrap-server, my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093, --topic, my-topic-1289249929-2032295810], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf', podNamespace='namespace-62', bootstrapServer='my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093', topicName='my-topic-1289249929-2032295810', maxMessages=100, kafkaUsername='my-user-1295571291-974474085', consumerGroupName='my-consumer-group-474754485', consumerInstanceId='instance1881997701', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@73f2e466}
2022-04-09 11:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093:my-topic-1289249929-2032295810 from pod my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf
2022-04-09 11:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-888d4396-kafka-clients-55b454c5cc-lrblf -n namespace-62 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-474754485 --group-instance-id instance1881997701 USER=my_user_1295571291_974474085 --bootstrap-server my-cluster-888d4396-kafka-bootstrap.namespace-62.svc:9093 --topic my-topic-1289249929-2032295810
2022-04-09 11:23:57 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:23:57 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:23:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:23:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testCertRegeneratedAfterInternalCAisDeleted
2022-04-09 11:23:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1289249929-2032295810 in namespace namespace-62
2022-04-09 11:24:00 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:525] Checking the certificates have been replaced
2022-04-09 11:24:00 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:536] Checking consumed messages to pod:my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr
2022-04-09 11:24:00 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@24a0d9ec, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-809962802, --group-instance-id, instance1622343776, --bootstrap-server, my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-1867361953-2102739105], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr', podNamespace='namespace-57', bootstrapServer='my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-1867361953-2102739105', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-809962802', consumerInstanceId='instance1622343776', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@556f4d07}
2022-04-09 11:24:00 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092#my-topic-1867361953-2102739105 from pod my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr
2022-04-09 11:24:00 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5d48ac2f-kafka-clients-66c9b6c9f-4zhcr -n namespace-57 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-809962802 --group-instance-id instance1622343776 --bootstrap-server my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-1867361953-2102739105
2022-04-09 11:24:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:24:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:24:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1870354551-304118528 in namespace namespace-63
2022-04-09 11:24:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-09 11:24:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1870354551-304118528 will have desired state: Ready
2022-04-09 11:24:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-888d4396-kafka-clients in namespace namespace-62
2022-04-09 11:24:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1870354551-304118528 is in desired state: Ready
2022-04-09 11:24:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5d48ac2f-kafka-clients-tls in namespace namespace-63
2022-04-09 11:24:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-09 11:24:08 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5d48ac2f-kafka-clients-tls will be ready
2022-04-09 11:24:10 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5d48ac2f-kafka-clients-tls is ready
2022-04-09 11:24:10 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:561] Checking consumed messages to pod:my-cluster-5d48ac2f-kafka-clients-tls-748cd686dd-2gsm2
2022-04-09 11:24:10 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@552016aa, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-2040645009, --group-instance-id, instance1271677254, --bootstrap-server, my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-1867361953-2102739105], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5d48ac2f-kafka-clients-tls-748cd686dd-2gsm2', podNamespace='namespace-57', bootstrapServer='my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-1867361953-2102739105', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2040645009', consumerInstanceId='instance1271677254', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7db8fb42}
2022-04-09 11:24:10 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092#my-topic-1867361953-2102739105 from pod my-cluster-5d48ac2f-kafka-clients-tls-748cd686dd-2gsm2
2022-04-09 11:24:10 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5d48ac2f-kafka-clients-tls-748cd686dd-2gsm2 -n namespace-57 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-2040645009 --group-instance-id instance1271677254 --bootstrap-server my-cluster-5d48ac2f-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-1867361953-2102739105
2022-04-09 11:24:16 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:24:16 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:24:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:24:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoReplaceClientsCaKeysTriggeredByAnno
2022-04-09 11:24:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5d48ac2f-kafka-clients in namespace namespace-57
2022-04-09 11:24:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1295571291-974474085 in namespace namespace-62
2022-04-09 11:24:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-888d4396 in namespace namespace-62
2022-04-09 11:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-25834a7d is in desired state: Ready
2022-04-09 11:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsHostnameVerificationWithMirrorMaker
2022-04-09 11:25:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-25834a7d-target in namespace namespace-59
2022-04-09 11:25:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:25:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-62 for test case:testCertRegeneratedAfterInternalCAisDeleted
2022-04-09 11:25:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-25834a7d in namespace namespace-59
2022-04-09 11:25:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-25834a7d-source in namespace namespace-59
2022-04-09 11:25:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2a67d948 is in desired state: Ready
2022-04-09 11:25:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-781937583-880353822 in namespace namespace-63
2022-04-09 11:25:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-09 11:25:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-781937583-880353822 will have desired state: Ready
2022-04-09 11:25:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-781937583-880353822 is in desired state: Ready
2022-04-09 11:25:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1779942863-196877450 in namespace namespace-63
2022-04-09 11:25:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-09 11:25:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1779942863-196877450 will have desired state: Ready
2022-04-09 11:25:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1779942863-196877450 is in desired state: Ready
2022-04-09 11:25:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2a67d948-kafka-clients in namespace namespace-63
2022-04-09 11:25:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-09 11:25:33 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2a67d948-kafka-clients will be ready
2022-04-09 11:25:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:25:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-59 for test case:testTlsHostnameVerificationWithMirrorMaker
2022-04-09 11:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2a67d948-kafka-clients is ready
2022-04-09 11:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:260] Checking produced and consumed messages to pod:my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh
2022-04-09 11:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6a6ec2b4, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092, --topic, my-topic-1779942863-196877450], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh', podNamespace='namespace-63', bootstrapServer='my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092', topicName='my-topic-1779942863-196877450', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1bda47f8}
2022-04-09 11:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092:my-topic-1779942863-196877450 from pod my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh
2022-04-09 11:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh -n namespace-63 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092 --topic my-topic-1779942863-196877450
2022-04-09 11:25:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5d48ac2f-kafka-clients-tls in namespace namespace-57
2022-04-09 11:25:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1870354551-304118528 in namespace namespace-57
2022-04-09 11:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1175bc58, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1717275577, --group-instance-id, instance90423380, --bootstrap-server, my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092, --topic, my-topic-1779942863-196877450], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh', podNamespace='namespace-63', bootstrapServer='my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092', topicName='my-topic-1779942863-196877450', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1717275577', consumerInstanceId='instance90423380', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68fa5e08}
2022-04-09 11:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092#my-topic-1779942863-196877450 from pod my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh
2022-04-09 11:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh -n namespace-63 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1717275577 --group-instance-id instance90423380 --bootstrap-server my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092 --topic my-topic-1779942863-196877450
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:274] Triggering CA cert renewal by adding the annotation
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:286] Patching secret my-cluster-2a67d948-cluster-ca-cert with strimzi.io/force-renew
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:286] Patching secret my-cluster-2a67d948-clients-ca-cert with strimzi.io/force-renew
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:291] Wait for zk to rolling restart ...
2022-04-09 11:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2a67d948-zookeeper rolling update
2022-04-09 11:25:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-699003840-1729348461 in namespace namespace-57
2022-04-09 11:25:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1867361953-2102739105 in namespace namespace-57
2022-04-09 11:25:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5d48ac2f in namespace namespace-57
2022-04-09 11:25:46 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-57, for cruise control Kafka cluster my-cluster-5d48ac2f
2022-04-09 11:25:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertRegeneratedAfterInternalCAisDeleted-FINISHED
2022-04-09 11:25:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:25:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:25:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertRenewalInMaintenanceWindow-STARTED
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-64 for test case:testKafkaAndKafkaConnectCipherSuites
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-64
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-64
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-64
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1362] Deploying Kafka cluster with the support TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 cipher algorithms
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-13cd8d01 in namespace namespace-64
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-09 11:25:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-13cd8d01 will have desired state: Ready
2022-04-09 11:25:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:25:56 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-57 for test case:testAutoReplaceClientsCaKeysTriggeredByAnno
2022-04-09 11:26:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithMirrorMaker-FINISHED
2022-04-09 11:26:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:26:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:26:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testOwnerReferenceOfCASecrets-STARTED
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-65 for test case:testClientsCACertRenew
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-65
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-65
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-65
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-53325cd4 in namespace namespace-65
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-65
2022-04-09 11:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-53325cd4 will have desired state: Ready
2022-04-09 11:26:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClientsCaKeysTriggeredByAnno-FINISHED
2022-04-09 11:26:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:26:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:26:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewClientsCaCertsTriggeredByAnno-STARTED
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-66 for test case:testCertRenewalInMaintenanceWindow
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-66
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-66
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-66
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:698] Maintenance window is: * 31-45 * * * ? *
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-63d1b07f in namespace namespace-66
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-09 11:26:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-63d1b07f will have desired state: Ready
2022-04-09 11:27:47 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-be8a7ef1-zookeeper has been successfully rolled
2022-04-09 11:27:47 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-be8a7ef1-zookeeper to be ready
2022-04-09 11:27:54 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2a67d948-zookeeper has been successfully rolled
2022-04-09 11:27:54 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-2a67d948-zookeeper to be ready
2022-04-09 11:27:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-63d1b07f is in desired state: Ready
2022-04-09 11:27:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1557407923-1171925920 in namespace namespace-66
2022-04-09 11:27:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-09 11:27:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1557407923-1171925920 will have desired state: Ready
2022-04-09 11:27:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-13cd8d01 is in desired state: Ready
2022-04-09 11:27:58 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1374] Verifying that Kafka Connect has the accepted configuration:
 ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2022-04-09 11:27:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-13cd8d01-kafka-clients in namespace namespace-66
2022-04-09 11:27:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-09 11:27:58 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-13cd8d01-kafka-clients will be ready
2022-04-09 11:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1557407923-1171925920 is in desired state: Ready
2022-04-09 11:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-940718875-1745585215 in namespace namespace-66
2022-04-09 11:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-09 11:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-940718875-1745585215 will have desired state: Ready
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-940718875-1745585215 is in desired state: Ready
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-940718875-1745585215 in namespace namespace-66
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-940718875-1745585215 will have desired state: Ready
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-940718875-1745585215 is in desired state: Ready
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-63d1b07f-kafka-clients in namespace namespace-66
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-09 11:28:00 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-63d1b07f-kafka-clients will be ready
2022-04-09 11:28:01 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-13cd8d01-kafka-clients is ready
2022-04-09 11:28:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-13cd8d01-scraper in namespace namespace-64
2022-04-09 11:28:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-09 11:28:01 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-13cd8d01-scraper will be ready
2022-04-09 11:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-63d1b07f-kafka-clients is ready
2022-04-09 11:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:728] Annotate secret my-cluster-63d1b07f-cluster-ca-cert with secret force-renew annotation
2022-04-09 11:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:735] Wait until maintenance windows starts
2022-04-09 11:28:03 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-13cd8d01-scraper is ready
2022-04-09 11:28:03 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-13cd8d01-scraper to be ready
2022-04-09 11:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-53325cd4 is in desired state: Ready
2022-04-09 11:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser strimzi-tls-user-1778685414 in namespace namespace-66
2022-04-09 11:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-65
2022-04-09 11:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: strimzi-tls-user-1778685414 will have desired state: Ready
2022-04-09 11:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: strimzi-tls-user-1778685414 is in desired state: Ready
2022-04-09 11:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1596] Change of kafka validity and renewal days - reconciliation should start.
2022-04-09 11:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 11:28:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 11:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 11:28:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 11:28:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 11:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-13cd8d01-scraper is ready
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-13cd8d01-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-13cd8d01-allow in namespace namespace-64
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-13cd8d01 in namespace namespace-66
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1391] Verifying that Kafka Connect status is NotReady because of different cipher suites complexity of algorithm
2022-04-09 11:28:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-13cd8d01 will have desired state: NotReady
2022-04-09 11:28:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 11:28:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 11:28:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 11:28:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 11:28:17 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 11:28:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 11:28:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 11:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 11:28:21 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 11:28:22 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:295] Wait for kafka to rolling restart ...
2022-04-09 11:28:22 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2a67d948-kafka rolling update
2022-04-09 11:28:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 11:28:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 11:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 11:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 11:28:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 11:28:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 11:28:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 11:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 11:28:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 11:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 11:28:32 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-be8a7ef1-kafka rolling update
2022-04-09 11:28:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 11:28:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 11:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 11:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 11:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 11:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 11:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 11:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 11:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 11:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 11:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 11:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 11:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 11:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 11:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 11:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 11:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 11:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 11:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 11:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 11:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 11:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 11:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 11:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 11:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 11:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-53325cd4-zookeeper-0=6705d648-2a76-471c-a732-0cb2bc2b586d, my-cluster-53325cd4-zookeeper-2=c6dd81b1-8832-40eb-aff0-a36691f99a99, my-cluster-53325cd4-zookeeper-1=35c6a606-a59c-4ba0-9301-f41366a31e9b} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 11:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-53325cd4-kafka rolling update
2022-04-09 11:29:17 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2a67d948-kafka has been successfully rolled
2022-04-09 11:29:17 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-2a67d948-kafka to be ready
2022-04-09 11:29:32 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-be8a7ef1-kafka has been successfully rolled
2022-04-09 11:29:32 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-be8a7ef1-kafka to be ready
2022-04-09 11:29:43 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:299] Wait for EO to rolling restart ...
2022-04-09 11:29:43 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-2a67d948-entity-operator rolling update
2022-04-09 11:29:43 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2a67d948-entity-operator will be ready
2022-04-09 11:30:03 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-be8a7ef1-entity-operator rolling update
2022-04-09 11:30:03 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-be8a7ef1-entity-operator will be ready
2022-04-09 11:30:24 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ec7c5d18-entity-operator is ready
2022-04-09 11:30:34 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-ec7c5d18-entity-operator rolling update finished
2022-04-09 11:30:34 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:303] Wait for CC and KE to rolling restart ...
2022-04-09 11:30:34 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-ec7c5d18-kafka-exporter rolling update
2022-04-09 11:30:52 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-be8a7ef1-entity-operator is ready
2022-04-09 11:31:01 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:741] Maintenance window starts
2022-04-09 11:31:01 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:745] Wait until rolling update is triggered during maintenance window
2022-04-09 11:31:01 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-63d1b07f-kafka rolling update
2022-04-09 11:31:02 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-be8a7ef1-entity-operator rolling update finished
2022-04-09 11:31:02 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:1252] Checking produced and consumed messages to pod:my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:31:02 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3d11d4b9, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1583453509, --group-instance-id, instance1613716166, USER=my_user_398384796_953216196, --bootstrap-server, my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-1479983589-1349517763], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj', podNamespace='namespace-61', bootstrapServer='my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-1479983589-1349517763', maxMessages=100, kafkaUsername='my-user-398384796-953216196', consumerGroupName='my-consumer-group-1583453509', consumerInstanceId='instance1613716166', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@67390f93}
2022-04-09 11:31:02 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093:my-topic-1479983589-1349517763 from pod my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:31:02 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj -n namespace-61 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1583453509 --group-instance-id instance1613716166 USER=my_user_398384796_953216196 --bootstrap-server my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-1479983589-1349517763
2022-04-09 11:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-53325cd4-kafka has been successfully rolled
2022-04-09 11:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-53325cd4-kafka to be ready
2022-04-09 11:31:10 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ec7c5d18-kafka-exporter will be ready
2022-04-09 11:31:10 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ec7c5d18-kafka-exporter is ready
2022-04-09 11:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-33353629-753051993 in namespace namespace-66
2022-04-09 11:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-09 11:31:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-33353629-753051993 will have desired state: Ready
2022-04-09 11:31:12 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2a67d948-entity-operator is ready
2022-04-09 11:31:20 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-ec7c5d18-kafka-exporter rolling update finished
2022-04-09 11:31:20 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-ec7c5d18-cruise-control rolling update
2022-04-09 11:31:20 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ec7c5d18-cruise-control will be ready
2022-04-09 11:31:20 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ec7c5d18-cruise-control is ready
2022-04-09 11:31:22 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-2a67d948-entity-operator rolling update finished
2022-04-09 11:31:22 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:303] Wait for CC and KE to rolling restart ...
2022-04-09 11:31:22 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-2a67d948-kafka-exporter rolling update
2022-04-09 11:31:30 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-ec7c5d18-cruise-control rolling update finished
2022-04-09 11:31:30 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:308] Checking the certificates have been replaced
2022-04-09 11:31:30 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:322] Checking consumed messages to pod:my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4
2022-04-09 11:31:30 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@64622883, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1770710424, --group-instance-id, instance463835880, --bootstrap-server, my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092, --topic, my-topic-1383097147-1968320726], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4', podNamespace='namespace-60', bootstrapServer='my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092', topicName='my-topic-1383097147-1968320726', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1770710424', consumerInstanceId='instance463835880', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@907cc42}
2022-04-09 11:31:30 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092#my-topic-1383097147-1968320726 from pod my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4
2022-04-09 11:31:30 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-ec7c5d18-kafka-clients-f569748cf-qwbx4 -n namespace-60 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1770710424 --group-instance-id instance463835880 --bootstrap-server my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9092 --topic my-topic-1383097147-1968320726
2022-04-09 11:31:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-53325cd4-entity-operator rolling update
2022-04-09 11:31:36 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:31:36 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:31:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-ec7c5d18 in namespace namespace-66
2022-04-09 11:31:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-09 11:31:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-ec7c5d18 will have desired state: Ready
2022-04-09 11:31:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-ec7c5d18 is in desired state: Ready
2022-04-09 11:31:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-ec7c5d18-kafka-clients-tls in namespace namespace-60
2022-04-09 11:31:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-09 11:31:37 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ec7c5d18-kafka-clients-tls will be ready
2022-04-09 11:31:39 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ec7c5d18-kafka-clients-tls is ready
2022-04-09 11:31:39 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:346] Checking consumed messages to pod:my-cluster-ec7c5d18-kafka-clients-tls-56d7469f5b-lqnmw
2022-04-09 11:31:39 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4205985a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-2097544142, --group-instance-id, instance668677137, USER=bob_my_cluster_ec7c5d18, --bootstrap-server, my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9093, --topic, my-topic-1383097147-1968320726], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-ec7c5d18-kafka-clients-tls-56d7469f5b-lqnmw', podNamespace='namespace-60', bootstrapServer='my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9093', topicName='my-topic-1383097147-1968320726', maxMessages=100, kafkaUsername='bob-my-cluster-ec7c5d18', consumerGroupName='my-consumer-group-2097544142', consumerInstanceId='instance668677137', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@635072ef}
2022-04-09 11:31:39 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9093#my-topic-1383097147-1968320726 from pod my-cluster-ec7c5d18-kafka-clients-tls-56d7469f5b-lqnmw
2022-04-09 11:31:39 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-ec7c5d18-kafka-clients-tls-56d7469f5b-lqnmw -n namespace-60 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-2097544142 --group-instance-id instance668677137 USER=bob_my_cluster_ec7c5d18 --bootstrap-server my-cluster-ec7c5d18-kafka-bootstrap.namespace-60.svc:9093 --topic my-topic-1383097147-1968320726
2022-04-09 11:31:46 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:31:46 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:31:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:31:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewClusterCaCertsTriggeredByAnno
2022-04-09 11:31:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-ec7c5d18-kafka-clients in namespace namespace-60
2022-04-09 11:31:49 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-53325cd4-entity-operator will be ready
2022-04-09 11:32:07 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2a67d948-kafka-exporter will be ready
2022-04-09 11:32:07 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2a67d948-kafka-exporter is ready
2022-04-09 11:32:17 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-2a67d948-kafka-exporter rolling update finished
2022-04-09 11:32:17 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-2a67d948-cruise-control rolling update
2022-04-09 11:32:17 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2a67d948-cruise-control will be ready
2022-04-09 11:32:17 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2a67d948-cruise-control is ready
2022-04-09 11:32:27 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-2a67d948-cruise-control rolling update finished
2022-04-09 11:32:27 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:308] Checking the certificates have been replaced
2022-04-09 11:32:27 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:322] Checking consumed messages to pod:my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh
2022-04-09 11:32:27 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@ea7ab9, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1078783908, --group-instance-id, instance335507413, --bootstrap-server, my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092, --topic, my-topic-1779942863-196877450], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh', podNamespace='namespace-63', bootstrapServer='my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092', topicName='my-topic-1779942863-196877450', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1078783908', consumerInstanceId='instance335507413', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5a87f1a9}
2022-04-09 11:32:27 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092#my-topic-1779942863-196877450 from pod my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh
2022-04-09 11:32:27 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a67d948-kafka-clients-6b6f68f46-mj5jh -n namespace-63 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1078783908 --group-instance-id instance335507413 --bootstrap-server my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9092 --topic my-topic-1779942863-196877450
2022-04-09 11:32:33 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:32:33 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:32:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-2a67d948 in namespace namespace-66
2022-04-09 11:32:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-09 11:32:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-2a67d948 will have desired state: Ready
2022-04-09 11:32:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-2a67d948 is in desired state: Ready
2022-04-09 11:32:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2a67d948-kafka-clients-tls in namespace namespace-63
2022-04-09 11:32:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-09 11:32:34 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2a67d948-kafka-clients-tls will be ready
2022-04-09 11:32:36 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2a67d948-kafka-clients-tls is ready
2022-04-09 11:32:36 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:346] Checking consumed messages to pod:my-cluster-2a67d948-kafka-clients-tls-6fcc8b4bd9-zn2jt
2022-04-09 11:32:36 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@32a2c9bb, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-517022957, --group-instance-id, instance11072371, USER=bob_my_cluster_2a67d948, --bootstrap-server, my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-1779942863-196877450], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2a67d948-kafka-clients-tls-6fcc8b4bd9-zn2jt', podNamespace='namespace-63', bootstrapServer='my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-1779942863-196877450', maxMessages=100, kafkaUsername='bob-my-cluster-2a67d948', consumerGroupName='my-consumer-group-517022957', consumerInstanceId='instance11072371', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@68c79ce9}
2022-04-09 11:32:36 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9093#my-topic-1779942863-196877450 from pod my-cluster-2a67d948-kafka-clients-tls-6fcc8b4bd9-zn2jt
2022-04-09 11:32:36 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2a67d948-kafka-clients-tls-6fcc8b4bd9-zn2jt -n namespace-63 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-517022957 --group-instance-id instance11072371 USER=bob_my_cluster_2a67d948 --bootstrap-server my-cluster-2a67d948-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-1779942863-196877450
2022-04-09 11:32:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-33353629-753051993 is in desired state: Ready
2022-04-09 11:32:41 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@121adf83, messages=[], arguments=[--max-messages, 100, USER=my_user_398384796_953216196, --bootstrap-server, my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-33353629-753051993], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj', podNamespace='namespace-61', bootstrapServer='my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-33353629-753051993', maxMessages=100, kafkaUsername='my-user-398384796-953216196', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3df01a39}
2022-04-09 11:32:41 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093:my-topic-33353629-753051993 from pod my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:32:41 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj -n namespace-61 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_398384796_953216196 --bootstrap-server my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-33353629-753051993
2022-04-09 11:32:44 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:32:44 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:32:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:32:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-04-09 11:32:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2a67d948-kafka-clients in namespace namespace-63
2022-04-09 11:32:45 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:32:45 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:32:45 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@32952c82, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1919006576, --group-instance-id, instance1224844463, USER=my_user_398384796_953216196, --bootstrap-server, my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-33353629-753051993], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj', podNamespace='namespace-61', bootstrapServer='my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-33353629-753051993', maxMessages=100, kafkaUsername='my-user-398384796-953216196', consumerGroupName='my-consumer-group-1919006576', consumerInstanceId='instance1224844463', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@f89de98}
2022-04-09 11:32:45 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093:my-topic-33353629-753051993 from pod my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj
2022-04-09 11:32:45 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-be8a7ef1-kafka-clients-7bd7bdcc96-slzbj -n namespace-61 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1919006576 --group-instance-id instance1224844463 USER=my_user_398384796_953216196 --bootstrap-server my-cluster-be8a7ef1-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-33353629-753051993
2022-04-09 11:32:52 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:32:52 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:32:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:32:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testCaRenewalBreakInMiddle
2022-04-09 11:32:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1479983589-1349517763 in namespace namespace-61
2022-04-09 11:33:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-33353629-753051993 in namespace namespace-61
2022-04-09 11:33:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-ec7c5d18-kafka-clients-tls in namespace namespace-60
2022-04-09 11:33:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-ec7c5d18 in namespace namespace-60
2022-04-09 11:33:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-905808424-694944282 in namespace namespace-60
2022-04-09 11:33:12 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-be8a7ef1-kafka-clients in namespace namespace-61
2022-04-09 11:33:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-53325cd4-entity-operator is ready
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-13cd8d01 is in desired state: NotReady
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1395] Replacing Kafka Connect config to the cipher suites same as the Kafka broker has.
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1399] Verifying that Kafka Connect has the accepted configuration:
 ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 change
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 change
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1404] Verifying that Kafka Connect is stable
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-13cd8d01-connect are stable
2022-04-09 11:33:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 11:33:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 11:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-63d1b07f-kafka has been successfully rolled
2022-04-09 11:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-63d1b07f-kafka to be ready
2022-04-09 11:33:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1383097147-1968320726 in namespace namespace-60
2022-04-09 11:33:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ec7c5d18 in namespace namespace-60
2022-04-09 11:33:16 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-60, for cruise control Kafka cluster my-cluster-ec7c5d18
2022-04-09 11:33:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 11:33:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 11:33:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 11:33:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 11:33:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 11:33:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 11:33:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 11:33:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-53325cd4-entity-operator rolling update finished
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1624] Initial ClientsCA cert dates: Sat Apr 09 11:26:06 UTC 2022 --> Fri Apr 29 11:26:06 UTC 2022
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1625] Changed ClientsCA cert dates: Sat Apr 09 11:28:07 UTC 2022 --> Wed Oct 26 11:28:07 UTC 2022
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1626] Initial userCert dates: Sat Apr 09 11:28:06 UTC 2022 --> Fri Apr 29 11:28:06 UTC 2022
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1627] Changed userCert dates: Sat Apr 09 11:31:52 UTC 2022 --> Wed Oct 26 11:31:52 UTC 2022
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClientsCACertRenew
2022-04-09 11:33:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser strimzi-tls-user-1778685414 in namespace namespace-65
2022-04-09 11:33:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 11:33:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 11:33:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:33:27 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-60 for test case:testAutoRenewClusterCaCertsTriggeredByAnno
2022-04-09 11:33:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 11:33:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 11:33:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 11:33:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 11:33:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 11:33:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 11:33:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 11:33:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 11:33:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-53325cd4 in namespace namespace-65
2022-04-09 11:33:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 11:33:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 11:33:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 11:33:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 11:33:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 11:33:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 11:33:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 11:33:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 11:33:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 11:33:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 11:33:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:33:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-65 for test case:testClientsCACertRenew
2022-04-09 11:33:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 11:33:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 11:33:47 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 11:33:48 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-63d1b07f will have desired state: Ready
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-63d1b07f is in desired state: Ready
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-63d1b07f is ready
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:750] Checking consumed messages to pod:my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@39d21b5, messages=[], arguments=[--max-messages, 100, USER=my_user_1557407923_1171925920, --bootstrap-server, my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093, --topic, my-topic-940718875-1745585215], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c', podNamespace='namespace-66', bootstrapServer='my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093', topicName='my-topic-940718875-1745585215', maxMessages=100, kafkaUsername='my-user-1557407923-1171925920', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5808338b}
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093:my-topic-940718875-1745585215 from pod my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c
2022-04-09 11:33:48 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c -n namespace-66 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1557407923_1171925920 --bootstrap-server my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093 --topic my-topic-940718875-1745585215
2022-04-09 11:33:49 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 11:33:50 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 11:33:51 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 11:33:52 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 11:33:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:33:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:33:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7a35240a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-838283206, --group-instance-id, instance1554406522, USER=my_user_1557407923_1171925920, --bootstrap-server, my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093, --topic, my-topic-940718875-1745585215], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c', podNamespace='namespace-66', bootstrapServer='my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093', topicName='my-topic-940718875-1745585215', maxMessages=100, kafkaUsername='my-user-1557407923-1171925920', consumerGroupName='my-consumer-group-838283206', consumerInstanceId='instance1554406522', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2eb2efb9}
2022-04-09 11:33:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093:my-topic-940718875-1745585215 from pod my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c
2022-04-09 11:33:52 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-63d1b07f-kafka-clients-559b8485dd-79t7c -n namespace-66 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-838283206 --group-instance-id instance1554406522 USER=my_user_1557407923_1171925920 --bootstrap-server my-cluster-63d1b07f-kafka-bootstrap.namespace-66.svc:9093 --topic my-topic-940718875-1745585215
2022-04-09 11:33:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-398384796-953216196 in namespace namespace-61
2022-04-09 11:33:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 11:33:54 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 11:33:55 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 11:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 11:33:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 11:33:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 11:33:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 11:34:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 11:34:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 11:34:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:34:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testCertRenewalInMaintenanceWindow
2022-04-09 11:34:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-940718875-1745585215 in namespace namespace-66
2022-04-09 11:34:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 11:34:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 11:34:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 11:34:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-be8a7ef1 in namespace namespace-61
2022-04-09 11:34:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 11:34:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-13cd8d01-connect-679dcdd775-gw9hg is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 11:34:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-13cd8d01-connect-679dcdd775-gw9hg
2022-04-09 11:34:04 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1408] Verifying that Kafka Connect status is Ready because of the same cipher suites complexity of algorithm
2022-04-09 11:34:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-13cd8d01 will have desired state: Ready
2022-04-09 11:34:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-63d1b07f-kafka-clients in namespace namespace-66
2022-04-09 11:34:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewClusterCaCertsTriggeredByAnno-FINISHED
2022-04-09 11:34:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:34:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:34:10 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClusterCaKeysTriggeredByAnno-STARTED
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-67 for test case:testOwnerReferenceOfCASecrets
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-67
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-67
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-67
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5a58fb49 in namespace namespace-67
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-09 11:34:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5a58fb49 will have desired state: Ready
2022-04-09 11:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testClientsCACertRenew-FINISHED
2022-04-09 11:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithKafkaConnect-STARTED
2022-04-09 11:34:12 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:34:12 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-61 for test case:testCaRenewalBreakInMiddle
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-68 for test case:testAutoRenewClientsCaCertsTriggeredByAnno
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-68
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-68
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-68
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6cd24954 in namespace namespace-68
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-09 11:34:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6cd24954 will have desired state: Ready
2022-04-09 11:34:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2a67d948-kafka-clients-tls in namespace namespace-63
2022-04-09 11:34:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-2a67d948 in namespace namespace-63
2022-04-09 11:34:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-781937583-880353822 in namespace namespace-63
2022-04-09 11:34:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCaRenewalBreakInMiddle-FINISHED
2022-04-09 11:34:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:34:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:34:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceAllCaKeysTriggeredByAnno-STARTED
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-69 for test case:testAutoReplaceClusterCaKeysTriggeredByAnno
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-69
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-69
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-69
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e85be345 in namespace namespace-69
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-09 11:34:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e85be345 will have desired state: Ready
2022-04-09 11:34:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1779942863-196877450 in namespace namespace-63
2022-04-09 11:34:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2a67d948 in namespace namespace-63
2022-04-09 11:34:54 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-63, for cruise control Kafka cluster my-cluster-2a67d948
2022-04-09 11:35:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-940718875-1745585215 in namespace namespace-66
2022-04-09 11:35:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1557407923-1171925920 in namespace namespace-66
2022-04-09 11:35:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:35:04 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-63 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-04-09 11:35:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-63d1b07f in namespace namespace-66
2022-04-09 11:35:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:35:20 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-66 for test case:testCertRenewalInMaintenanceWindow
2022-04-09 11:35:48 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertRenewalInMaintenanceWindow-FINISHED
2022-04-09 11:35:48 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:35:48 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:35:48 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testClusterCACertRenew-STARTED
2022-04-09 11:35:48 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-04-09 11:35:48 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:35:48 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:35:48 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectTlsVersion-STARTED
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-70 for test case:testAutoReplaceAllCaKeysTriggeredByAnno
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-70
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-70
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-70
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-996ad3a2 in namespace namespace-70
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-09 11:35:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-996ad3a2 will have desired state: Ready
2022-04-09 11:36:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5a58fb49 is in desired state: Ready
2022-04-09 11:36:22 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1432] Listing all cluster CAs for my-cluster-5a58fb49
2022-04-09 11:36:22 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1436] Deleting Kafka:my-cluster-5a58fb49
2022-04-09 11:36:22 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUtils:399] Waiting for deletion of Kafka:my-cluster-5a58fb49
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1440] Checking actual secrets after Kafka deletion
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-5a58fb49-clients-ca secret is still present
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-5a58fb49-clients-ca
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-5a58fb49-clients-ca-cert secret is still present
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-5a58fb49-clients-ca-cert
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-5a58fb49-cluster-ca secret is still present
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-5a58fb49-cluster-ca
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-5a58fb49-cluster-ca-cert secret is still present
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-5a58fb49-cluster-ca-cert
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1450] Deploying Kafka with generateSecretOwnerReference set to true
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-second-cluster-my-cluster-5a58fb49 in namespace namespace-70
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-09 11:36:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-second-cluster-my-cluster-5a58fb49 will have desired state: Ready
2022-04-09 11:37:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6cd24954 is in desired state: Ready
2022-04-09 11:37:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-173442933-1862875554 in namespace namespace-70
2022-04-09 11:37:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-09 11:37:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-173442933-1862875554 will have desired state: Ready
2022-04-09 11:37:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-173442933-1862875554 is in desired state: Ready
2022-04-09 11:37:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1188254852-939125808 in namespace namespace-70
2022-04-09 11:37:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-09 11:37:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1188254852-939125808 will have desired state: Ready
2022-04-09 11:37:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1188254852-939125808 is in desired state: Ready
2022-04-09 11:37:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6cd24954-kafka-clients in namespace namespace-70
2022-04-09 11:37:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-09 11:37:31 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6cd24954-kafka-clients will be ready
2022-04-09 11:37:33 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6cd24954-kafka-clients is ready
2022-04-09 11:37:33 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:37:33 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:260] Checking produced and consumed messages to pod:my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp
2022-04-09 11:37:33 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6bd1e12b, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092, --topic, my-topic-1188254852-939125808], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp', podNamespace='namespace-68', bootstrapServer='my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092', topicName='my-topic-1188254852-939125808', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@493176c6}
2022-04-09 11:37:33 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092:my-topic-1188254852-939125808 from pod my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp
2022-04-09 11:37:33 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp -n namespace-68 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092 --topic my-topic-1188254852-939125808
2022-04-09 11:37:36 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:37:36 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:37:36 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@79ac7ec1, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1839004362, --group-instance-id, instance975440167, --bootstrap-server, my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092, --topic, my-topic-1188254852-939125808], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp', podNamespace='namespace-68', bootstrapServer='my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092', topicName='my-topic-1188254852-939125808', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1839004362', consumerInstanceId='instance975440167', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3b69b82c}
2022-04-09 11:37:36 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092#my-topic-1188254852-939125808 from pod my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp
2022-04-09 11:37:36 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp -n namespace-68 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1839004362 --group-instance-id instance975440167 --bootstrap-server my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092 --topic my-topic-1188254852-939125808
2022-04-09 11:37:42 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:37:42 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:37:42 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:274] Triggering CA cert renewal by adding the annotation
2022-04-09 11:37:42 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:286] Patching secret my-cluster-6cd24954-clients-ca-cert with strimzi.io/force-renew
2022-04-09 11:37:42 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:295] Wait for kafka to rolling restart ...
2022-04-09 11:37:42 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-6cd24954-kafka rolling update
2022-04-09 11:37:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e85be345 is in desired state: Ready
2022-04-09 11:37:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-947971959-141478953 in namespace namespace-70
2022-04-09 11:37:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-09 11:37:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-947971959-141478953 will have desired state: Ready
2022-04-09 11:37:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-947971959-141478953 is in desired state: Ready
2022-04-09 11:37:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2134079331-2012819435 in namespace namespace-70
2022-04-09 11:37:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-09 11:37:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2134079331-2012819435 will have desired state: Ready
2022-04-09 11:37:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2134079331-2012819435 is in desired state: Ready
2022-04-09 11:37:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e85be345-kafka-clients in namespace namespace-70
2022-04-09 11:37:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-09 11:37:50 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-kafka-clients will be ready
2022-04-09 11:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-kafka-clients is ready
2022-04-09 11:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:453] Checking produced and consumed messages to pod:my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb
2022-04-09 11:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@afe1910, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092, --topic, my-topic-2134079331-2012819435], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb', podNamespace='namespace-69', bootstrapServer='my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092', topicName='my-topic-2134079331-2012819435', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6f996365}
2022-04-09 11:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092:my-topic-2134079331-2012819435 from pod my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb
2022-04-09 11:37:52 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb -n namespace-69 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092 --topic my-topic-2134079331-2012819435
2022-04-09 11:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@35d4d405, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1497182845, --group-instance-id, instance1707346543, --bootstrap-server, my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092, --topic, my-topic-2134079331-2012819435], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb', podNamespace='namespace-69', bootstrapServer='my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092', topicName='my-topic-2134079331-2012819435', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1497182845', consumerInstanceId='instance1707346543', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52f84c9}
2022-04-09 11:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092#my-topic-2134079331-2012819435 from pod my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb
2022-04-09 11:37:55 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb -n namespace-69 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1497182845 --group-instance-id instance1707346543 --bootstrap-server my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092 --topic my-topic-2134079331-2012819435
2022-04-09 11:37:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-996ad3a2 is in desired state: Ready
2022-04-09 11:37:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-348154837-1261087943 in namespace namespace-70
2022-04-09 11:37:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-09 11:37:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-348154837-1261087943 will have desired state: Ready
2022-04-09 11:37:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-348154837-1261087943 is in desired state: Ready
2022-04-09 11:37:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1466750127-1371920270 in namespace namespace-70
2022-04-09 11:37:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-09 11:37:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1466750127-1371920270 will have desired state: Ready
2022-04-09 11:38:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1466750127-1371920270 is in desired state: Ready
2022-04-09 11:38:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-996ad3a2-kafka-clients in namespace namespace-70
2022-04-09 11:38:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-09 11:38:00 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-kafka-clients will be ready
2022-04-09 11:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:467] Triggering CA cert renewal by adding the annotation
2022-04-09 11:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:479] Patching secret my-cluster-e85be345-cluster-ca with strimzi.io/force-replace
2022-04-09 11:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:484] Wait for zk to rolling restart (1)...
2022-04-09 11:38:01 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e85be345-zookeeper rolling update
2022-04-09 11:38:03 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-kafka-clients is ready
2022-04-09 11:38:03 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:38:03 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:453] Checking produced and consumed messages to pod:my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s
2022-04-09 11:38:03 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5ac79f26, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-1466750127-1371920270], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s', podNamespace='namespace-70', bootstrapServer='my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-1466750127-1371920270', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6ff7f252}
2022-04-09 11:38:03 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092:my-topic-1466750127-1371920270 from pod my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s
2022-04-09 11:38:03 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s -n namespace-70 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-1466750127-1371920270
2022-04-09 11:38:07 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 11:38:07 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 11:38:07 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2e5232c9, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-636765096, --group-instance-id, instance159511999, --bootstrap-server, my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-1466750127-1371920270], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s', podNamespace='namespace-70', bootstrapServer='my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-1466750127-1371920270', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-636765096', consumerInstanceId='instance159511999', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@47c0cb9c}
2022-04-09 11:38:07 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092#my-topic-1466750127-1371920270 from pod my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s
2022-04-09 11:38:07 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s -n namespace-70 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-636765096 --group-instance-id instance159511999 --bootstrap-server my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-1466750127-1371920270
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:467] Triggering CA cert renewal by adding the annotation
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:479] Patching secret my-cluster-996ad3a2-cluster-ca with strimzi.io/force-replace
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:479] Patching secret my-cluster-996ad3a2-clients-ca with strimzi.io/force-replace
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:484] Wait for zk to rolling restart (1)...
2022-04-09 11:38:14 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-996ad3a2-zookeeper rolling update
2022-04-09 11:38:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-second-cluster-my-cluster-5a58fb49 is in desired state: Ready
2022-04-09 11:38:47 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1465] Deleting Kafka:my-second-cluster-my-cluster-5a58fb49
2022-04-09 11:38:47 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUtils:399] Waiting for deletion of Kafka:my-second-cluster-my-cluster-5a58fb49
2022-04-09 11:39:17 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-6cd24954-kafka has been successfully rolled
2022-04-09 11:39:17 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-6cd24954-kafka to be ready
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1469] Checking actual secrets after Kafka deletion
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-5a58fb49-clients-ca secret is deleted
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-5a58fb49-clients-ca-cert secret is deleted
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-5a58fb49-cluster-ca secret is deleted
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-5a58fb49-cluster-ca-cert secret is deleted
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testOwnerReferenceOfCASecrets
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-second-cluster-my-cluster-5a58fb49 in namespace namespace-67
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5a58fb49 in namespace namespace-67
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-67 for test case:testOwnerReferenceOfCASecrets
2022-04-09 11:39:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-13cd8d01 is in desired state: Ready
2022-04-09 11:39:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:39:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndKafkaConnectCipherSuites
2022-04-09 11:39:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-13cd8d01-scraper in namespace namespace-64
2022-04-09 11:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testOwnerReferenceOfCASecrets-FINISHED
2022-04-09 11:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-13cd8d01-kafka-clients in namespace namespace-64
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-71 for test case:testTlsHostnameVerificationWithKafkaConnect
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-71
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-71
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-71
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d4ddcb62 in namespace namespace-71
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-71
2022-04-09 11:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d4ddcb62 will have desired state: Ready
2022-04-09 11:39:44 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:308] Checking the certificates have been replaced
2022-04-09 11:39:44 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:322] Checking consumed messages to pod:my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp
2022-04-09 11:39:44 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@8722d89, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1599363262, --group-instance-id, instance554312040, --bootstrap-server, my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092, --topic, my-topic-1188254852-939125808], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp', podNamespace='namespace-68', bootstrapServer='my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092', topicName='my-topic-1188254852-939125808', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1599363262', consumerInstanceId='instance554312040', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@37dc960a}
2022-04-09 11:39:44 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092#my-topic-1188254852-939125808 from pod my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp
2022-04-09 11:39:44 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6cd24954-kafka-clients-5f5999cd4f-7clnp -n namespace-68 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1599363262 --group-instance-id instance554312040 --bootstrap-server my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9092 --topic my-topic-1188254852-939125808
2022-04-09 11:39:56 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:39:56 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:39:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-6cd24954 in namespace namespace-71
2022-04-09 11:39:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-09 11:39:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-6cd24954 will have desired state: Ready
2022-04-09 11:39:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-6cd24954 is in desired state: Ready
2022-04-09 11:39:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6cd24954-kafka-clients-tls in namespace namespace-68
2022-04-09 11:39:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-09 11:39:57 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6cd24954-kafka-clients-tls will be ready
2022-04-09 11:39:59 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6cd24954-kafka-clients-tls is ready
2022-04-09 11:39:59 [ForkJoinPool-3-worker-15] [32mINFO [m [SecurityST:346] Checking consumed messages to pod:my-cluster-6cd24954-kafka-clients-tls-796f479c9d-vh54p
2022-04-09 11:39:59 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@61d4bed3, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1511965824, --group-instance-id, instance1708362059, USER=bob_my_cluster_6cd24954, --bootstrap-server, my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9093, --topic, my-topic-1188254852-939125808], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6cd24954-kafka-clients-tls-796f479c9d-vh54p', podNamespace='namespace-68', bootstrapServer='my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9093', topicName='my-topic-1188254852-939125808', maxMessages=100, kafkaUsername='bob-my-cluster-6cd24954', consumerGroupName='my-consumer-group-1511965824', consumerInstanceId='instance1708362059', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2081a7b3}
2022-04-09 11:39:59 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9093#my-topic-1188254852-939125808 from pod my-cluster-6cd24954-kafka-clients-tls-796f479c9d-vh54p
2022-04-09 11:39:59 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6cd24954-kafka-clients-tls-796f479c9d-vh54p -n namespace-68 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1511965824 --group-instance-id instance1708362059 USER=bob_my_cluster_6cd24954 --bootstrap-server my-cluster-6cd24954-kafka-bootstrap.namespace-68.svc:9093 --topic my-topic-1188254852-939125808
2022-04-09 11:40:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:40:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:40:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:40:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewClientsCaCertsTriggeredByAnno
2022-04-09 11:40:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6cd24954-kafka-clients in namespace namespace-68
2022-04-09 11:40:07 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e85be345-zookeeper has been successfully rolled
2022-04-09 11:40:07 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:489] Wait for kafka to rolling restart (1)...
2022-04-09 11:40:07 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e85be345-kafka rolling update
2022-04-09 11:40:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-13cd8d01 in namespace namespace-64
2022-04-09 11:40:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-13cd8d01 in namespace namespace-64
2022-04-09 11:40:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-13cd8d01-allow in namespace namespace-64
2022-04-09 11:40:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-173442933-1862875554 in namespace namespace-68
2022-04-09 11:40:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:40:30 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-64 for test case:testKafkaAndKafkaConnectCipherSuites
2022-04-09 11:40:39 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-996ad3a2-zookeeper has been successfully rolled
2022-04-09 11:40:39 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:489] Wait for kafka to rolling restart (1)...
2022-04-09 11:40:39 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-996ad3a2-kafka rolling update
2022-04-09 11:40:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1188254852-939125808 in namespace namespace-68
2022-04-09 11:40:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6cd24954 in namespace namespace-68
2022-04-09 11:40:50 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-68, for cruise control Kafka cluster my-cluster-6cd24954
2022-04-09 11:40:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectCipherSuites-FINISHED
2022-04-09 11:40:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:40:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6cd24954-kafka-clients-tls in namespace namespace-68
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-72 for test case:testClusterCACertRenew
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-72
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-72
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-72
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-19bff70c in namespace namespace-72
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-09 11:40:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-19bff70c will have desired state: Ready
2022-04-09 11:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d4ddcb62 is in desired state: Ready
2022-04-09 11:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:838] Getting IP of the bootstrap service
2022-04-09 11:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:842] KafkaConnect without config ssl.endpoint.identification.algorithm will not connect to 10.106.108.11:9093
2022-04-09 11:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d4ddcb62-kafka-clients in namespace namespace-72
2022-04-09 11:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-71
2022-04-09 11:40:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d4ddcb62-kafka-clients will be ready
2022-04-09 11:41:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-6cd24954 in namespace namespace-68
2022-04-09 11:41:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d4ddcb62-kafka-clients is ready
2022-04-09 11:41:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d4ddcb62-scraper in namespace namespace-71
2022-04-09 11:41:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-71
2022-04-09 11:41:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d4ddcb62-scraper will be ready
2022-04-09 11:41:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:41:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-68 for test case:testAutoRenewClientsCaCertsTriggeredByAnno
2022-04-09 11:41:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d4ddcb62-scraper is ready
2022-04-09 11:41:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-d4ddcb62-scraper to be ready
2022-04-09 11:41:39 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewClientsCaCertsTriggeredByAnno-FINISHED
2022-04-09 11:41:39 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-d4ddcb62-scraper is ready
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-d4ddcb62-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-d4ddcb62-allow in namespace namespace-71
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-71
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-d4ddcb62 in namespace namespace-72
2022-04-09 11:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-71
2022-04-09 11:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:245] Wait until Pod my-cluster-d4ddcb62-connect is present
2022-04-09 11:41:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:249] Pod my-cluster-d4ddcb62-connect is present
2022-04-09 11:41:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:236] Wait until Pod my-cluster-d4ddcb62-connect-58c8dc5776-r4qcx is in CrashLoopBackOff state
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-73 for test case:testKafkaAndKafkaConnectTlsVersion
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-73
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-73
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-73
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1287] Deploying Kafka cluster with the support TLSv1.2 TLS
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9e107014 in namespace namespace-73
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-09 11:41:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9e107014 will have desired state: Ready
2022-04-09 11:42:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:241] Pod my-cluster-d4ddcb62-connect-58c8dc5776-r4qcx is in CrashLoopBackOff state
2022-04-09 11:42:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:872] KafkaConnect with config ssl.endpoint.identification.algorithm will connect to 10.106.108.11:9093
2022-04-09 11:42:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-d4ddcb62 will have desired state: Ready
2022-04-09 11:42:32 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e85be345-kafka has been successfully rolled
2022-04-09 11:42:32 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:494] Wait for EO to rolling restart (1)...
2022-04-09 11:42:32 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e85be345-entity-operator rolling update
2022-04-09 11:43:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-19bff70c is in desired state: Ready
2022-04-09 11:43:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1512] Change of kafka validity and renewal days - reconciliation should start.
2022-04-09 11:43:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-19bff70c-zookeeper rolling update
2022-04-09 11:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-entity-operator will be ready
2022-04-09 11:43:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9e107014 is in desired state: Ready
2022-04-09 11:43:54 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1299] Verifying that Kafka cluster has the accepted configuration:
ssl.enabled.protocols -> TLSv1.2
ssl.protocol -> TLSv1.3
2022-04-09 11:43:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9e107014-kafka-clients in namespace namespace-73
2022-04-09 11:43:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-09 11:43:54 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9e107014-kafka-clients will be ready
2022-04-09 11:43:57 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9e107014-kafka-clients is ready
2022-04-09 11:43:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9e107014-scraper in namespace namespace-73
2022-04-09 11:43:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-09 11:43:57 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9e107014-scraper will be ready
2022-04-09 11:43:59 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9e107014-scraper is ready
2022-04-09 11:43:59 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-9e107014-scraper to be ready
2022-04-09 11:44:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-19bff70c-zookeeper has been successfully rolled
2022-04-09 11:44:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-19bff70c-zookeeper to be ready
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-9e107014-scraper is ready
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-9e107014-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-9e107014-allow in namespace namespace-73
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-9e107014 in namespace namespace-73
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1322] Verifying that Kafka Connect status is NotReady because of different TLS version
2022-04-09 11:44:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9e107014 will have desired state: NotReady
2022-04-09 11:44:29 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-996ad3a2-kafka has been successfully rolled
2022-04-09 11:44:29 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:494] Wait for EO to rolling restart (1)...
2022-04-09 11:44:29 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996ad3a2-entity-operator rolling update
2022-04-09 11:44:49 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-19bff70c-kafka rolling update
2022-04-09 11:45:00 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-entity-operator will be ready
2022-04-09 11:45:38 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-entity-operator is ready
2022-04-09 11:45:48 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996ad3a2-entity-operator rolling update finished
2022-04-09 11:45:48 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:499] Wait for KafkaExporter and CruiseControl to rolling restart (1)...
2022-04-09 11:45:48 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996ad3a2-kafka-exporter rolling update
2022-04-09 11:45:48 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-kafka-exporter will be ready
2022-04-09 11:46:13 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-kafka-exporter is ready
2022-04-09 11:46:14 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-19bff70c-kafka has been successfully rolled
2022-04-09 11:46:14 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-19bff70c-kafka to be ready
2022-04-09 11:46:23 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996ad3a2-kafka-exporter rolling update finished
2022-04-09 11:46:23 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996ad3a2-cruise-control rolling update
2022-04-09 11:46:23 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-cruise-control will be ready
2022-04-09 11:46:39 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-cruise-control is ready
2022-04-09 11:46:41 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-19bff70c-entity-operator rolling update
2022-04-09 11:46:41 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-19bff70c-entity-operator will be ready
2022-04-09 11:46:49 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996ad3a2-cruise-control rolling update finished
2022-04-09 11:46:49 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:505] Wait for zk to rolling restart (2)...
2022-04-09 11:46:49 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-996ad3a2-zookeeper rolling update
2022-04-09 11:47:00 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-entity-operator is ready
2022-04-09 11:47:10 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e85be345-entity-operator rolling update finished
2022-04-09 11:47:10 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:499] Wait for KafkaExporter and CruiseControl to rolling restart (1)...
2022-04-09 11:47:10 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e85be345-kafka-exporter rolling update
2022-04-09 11:47:15 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-kafka-exporter will be ready
2022-04-09 11:47:43 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-kafka-exporter is ready
2022-04-09 11:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-d4ddcb62 is in desired state: Ready
2022-04-09 11:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsHostnameVerificationWithKafkaConnect
2022-04-09 11:47:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d4ddcb62-scraper in namespace namespace-71
2022-04-09 11:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e85be345-kafka-exporter rolling update finished
2022-04-09 11:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e85be345-cruise-control rolling update
2022-04-09 11:47:53 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-cruise-control will be ready
2022-04-09 11:48:08 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-cruise-control is ready
2022-04-09 11:48:18 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e85be345-cruise-control rolling update finished
2022-04-09 11:48:18 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:505] Wait for zk to rolling restart (2)...
2022-04-09 11:48:18 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e85be345-zookeeper rolling update
2022-04-09 11:48:19 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-996ad3a2-zookeeper has been successfully rolled
2022-04-09 11:48:19 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-996ad3a2-zookeeper to be ready
2022-04-09 11:48:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-d4ddcb62 in namespace namespace-71
2022-04-09 11:48:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-d4ddcb62-allow in namespace namespace-71
2022-04-09 11:48:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d4ddcb62-kafka-clients in namespace namespace-71
2022-04-09 11:48:45 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:510] Wait for kafka to rolling restart (2)...
2022-04-09 11:48:45 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-996ad3a2-kafka rolling update
2022-04-09 11:48:47 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-19bff70c-entity-operator is ready
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-19bff70c-entity-operator rolling update finished
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1545] Initial ClusterCA cert dates: Sat Apr 09 11:40:58 UTC 2022 --> Fri Apr 29 11:40:58 UTC 2022
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1546] Changed ClusterCA cert dates: Sat Apr 09 11:43:01 UTC 2022 --> Wed Oct 26 11:43:01 UTC 2022
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1547] KafkaBroker cert creation dates: Sat Apr 09 11:42:06 UTC 2022 --> Fri Apr 29 11:42:06 UTC 2022
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1548] KafkaBroker cert changed dates:  Sat Apr 09 11:44:40 UTC 2022 --> Wed Oct 26 11:44:40 UTC 2022
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1549] Zookeeper cert creation dates: Sat Apr 09 11:41:03 UTC 2022 --> Fri Apr 29 11:41:03 UTC 2022
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1550] Zookeeper cert changed dates:  Sat Apr 09 11:43:02 UTC 2022 --> Wed Oct 26 11:43:02 UTC 2022
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testClusterCACertRenew
2022-04-09 11:48:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-19bff70c in namespace namespace-72
2022-04-09 11:49:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:49:08 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-72 for test case:testClusterCACertRenew
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9e107014 is in desired state: NotReady
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1326] Replacing Kafka Connect config to the newest(TLSv1.2) one same as the Kafka broker has.
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1330] Verifying that Kafka Connect has the accepted configuration:
 ssl.enabled.protocols -> TLSv1.2
 ssl.protocol -> TLSv1.3
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.enabled.protocols -> TLSv1.2 change
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.enabled.protocols -> TLSv1.2 change
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.protocol -> TLSv1.3 change
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.protocol -> TLSv1.3 change
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1339] Verifying that Kafka Connect is stable
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-9e107014-connect are stable
2022-04-09 11:49:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 11:49:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 11:49:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 11:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d4ddcb62 in namespace namespace-71
2022-04-09 11:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e85be345-zookeeper has been successfully rolled
2022-04-09 11:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-e85be345-zookeeper to be ready
2022-04-09 11:49:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 11:49:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 11:49:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 11:49:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 11:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 11:49:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 11:49:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 11:49:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 11:49:21 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 11:49:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 11:49:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:49:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-71 for test case:testTlsHostnameVerificationWithKafkaConnect
2022-04-09 11:49:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 11:49:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 11:49:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 11:49:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 11:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 11:49:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 11:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 11:49:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 11:49:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 11:49:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 11:49:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 11:49:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 11:49:35 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testClusterCACertRenew-FINISHED
2022-04-09 11:49:35 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:49:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 11:49:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 11:49:38 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 11:49:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 11:49:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 11:49:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 11:49:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 11:49:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 11:49:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 11:49:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 11:49:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 11:49:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 11:49:47 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:510] Wait for kafka to rolling restart (2)...
2022-04-09 11:49:47 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e85be345-kafka rolling update
2022-04-09 11:49:48 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 11:49:49 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 11:49:50 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 11:49:51 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 11:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithKafkaConnect-FINISHED
2022-04-09 11:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:49:52 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 11:49:53 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 11:49:54 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 11:49:55 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 11:49:56 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 11:49:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 11:49:58 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 11:49:59 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 11:50:00 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-9e107014-connect-d88794db9-pbxbx is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 11:50:00 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-9e107014-connect-d88794db9-pbxbx
2022-04-09 11:50:00 [ForkJoinPool-3-worker-13] [32mINFO [m [SecurityST:1343] Verifying that Kafka Connect status is Ready because of same TLS version
2022-04-09 11:50:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9e107014 will have desired state: Ready
2022-04-09 11:50:46 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-996ad3a2-kafka has been successfully rolled
2022-04-09 11:50:46 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-996ad3a2-kafka to be ready
2022-04-09 11:51:10 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:515] Wait for EO to rolling restart (2)...
2022-04-09 11:51:10 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996ad3a2-entity-operator rolling update
2022-04-09 11:51:17 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e85be345-kafka has been successfully rolled
2022-04-09 11:51:17 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-e85be345-kafka to be ready
2022-04-09 11:51:25 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-entity-operator will be ready
2022-04-09 11:51:48 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:515] Wait for EO to rolling restart (2)...
2022-04-09 11:51:48 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e85be345-entity-operator rolling update
2022-04-09 11:51:53 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-entity-operator will be ready
2022-04-09 11:51:57 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-entity-operator is ready
2022-04-09 11:52:07 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996ad3a2-entity-operator rolling update finished
2022-04-09 11:52:07 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:520] Wait for KafkaExporter and CruiseControl to rolling restart (2)...
2022-04-09 11:52:07 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996ad3a2-kafka-exporter rolling update
2022-04-09 11:52:27 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-entity-operator is ready
2022-04-09 11:52:37 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e85be345-entity-operator rolling update finished
2022-04-09 11:52:37 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:520] Wait for KafkaExporter and CruiseControl to rolling restart (2)...
2022-04-09 11:52:37 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e85be345-kafka-exporter rolling update
2022-04-09 11:53:02 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-kafka-exporter will be ready
2022-04-09 11:53:02 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-kafka-exporter is ready
2022-04-09 11:53:12 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996ad3a2-kafka-exporter rolling update finished
2022-04-09 11:53:12 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-996ad3a2-cruise-control rolling update
2022-04-09 11:53:12 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-cruise-control will be ready
2022-04-09 11:53:12 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-cruise-control is ready
2022-04-09 11:53:22 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-996ad3a2-cruise-control rolling update finished
2022-04-09 11:53:22 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:525] Checking the certificates have been replaced
2022-04-09 11:53:22 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:536] Checking consumed messages to pod:my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s
2022-04-09 11:53:22 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3c33e2f1, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1888832157, --group-instance-id, instance1015393726, --bootstrap-server, my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-1466750127-1371920270], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s', podNamespace='namespace-70', bootstrapServer='my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-1466750127-1371920270', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1888832157', consumerInstanceId='instance1015393726', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@9cd8b37}
2022-04-09 11:53:22 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092#my-topic-1466750127-1371920270 from pod my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s
2022-04-09 11:53:22 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996ad3a2-kafka-clients-6dd6c5cd76-4p86s -n namespace-70 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1888832157 --group-instance-id instance1015393726 --bootstrap-server my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-1466750127-1371920270
2022-04-09 11:53:28 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:53:28 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:53:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1712127414-796666548 in namespace namespace-73
2022-04-09 11:53:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-09 11:53:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1712127414-796666548 will have desired state: Ready
2022-04-09 11:53:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1712127414-796666548 is in desired state: Ready
2022-04-09 11:53:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-996ad3a2-kafka-clients-tls in namespace namespace-73
2022-04-09 11:53:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-09 11:53:29 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-996ad3a2-kafka-clients-tls will be ready
2022-04-09 11:53:31 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-996ad3a2-kafka-clients-tls is ready
2022-04-09 11:53:31 [ForkJoinPool-3-worker-11] [32mINFO [m [SecurityST:561] Checking consumed messages to pod:my-cluster-996ad3a2-kafka-clients-tls-7d64844cb7-fnzx2
2022-04-09 11:53:31 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2c5951b2, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-2070646998, --group-instance-id, instance1137709988, --bootstrap-server, my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-1466750127-1371920270], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-996ad3a2-kafka-clients-tls-7d64844cb7-fnzx2', podNamespace='namespace-70', bootstrapServer='my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-1466750127-1371920270', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2070646998', consumerInstanceId='instance1137709988', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4928ff93}
2022-04-09 11:53:31 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092#my-topic-1466750127-1371920270 from pod my-cluster-996ad3a2-kafka-clients-tls-7d64844cb7-fnzx2
2022-04-09 11:53:31 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-996ad3a2-kafka-clients-tls-7d64844cb7-fnzx2 -n namespace-70 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-2070646998 --group-instance-id instance1137709988 --bootstrap-server my-cluster-996ad3a2-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-1466750127-1371920270
2022-04-09 11:53:32 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-kafka-exporter will be ready
2022-04-09 11:53:32 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-kafka-exporter is ready
2022-04-09 11:53:37 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:53:37 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:53:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:53:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoReplaceAllCaKeysTriggeredByAnno
2022-04-09 11:53:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-996ad3a2-kafka-clients in namespace namespace-70
2022-04-09 11:53:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-348154837-1261087943 in namespace namespace-70
2022-04-09 11:53:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-996ad3a2 in namespace namespace-70
2022-04-09 11:53:37 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-70, for cruise control Kafka cluster my-cluster-996ad3a2
2022-04-09 11:53:42 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e85be345-kafka-exporter rolling update finished
2022-04-09 11:53:42 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e85be345-cruise-control rolling update
2022-04-09 11:53:42 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-cruise-control will be ready
2022-04-09 11:53:42 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-cruise-control is ready
2022-04-09 11:53:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1466750127-1371920270 in namespace namespace-70
2022-04-09 11:53:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-996ad3a2-kafka-clients-tls in namespace namespace-70
2022-04-09 11:53:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1712127414-796666548 in namespace namespace-70
2022-04-09 11:53:52 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e85be345-cruise-control rolling update finished
2022-04-09 11:53:52 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:525] Checking the certificates have been replaced
2022-04-09 11:53:52 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:536] Checking consumed messages to pod:my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb
2022-04-09 11:53:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6bb49e36, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-485050357, --group-instance-id, instance1865759034, --bootstrap-server, my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092, --topic, my-topic-2134079331-2012819435], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb', podNamespace='namespace-69', bootstrapServer='my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092', topicName='my-topic-2134079331-2012819435', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-485050357', consumerInstanceId='instance1865759034', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14300f08}
2022-04-09 11:53:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092#my-topic-2134079331-2012819435 from pod my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb
2022-04-09 11:53:52 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e85be345-kafka-clients-84bc9f5c75-5hzqb -n namespace-69 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-485050357 --group-instance-id instance1865759034 --bootstrap-server my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092 --topic my-topic-2134079331-2012819435
2022-04-09 11:53:58 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:53:58 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:53:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1018151578-615647297 in namespace namespace-73
2022-04-09 11:53:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-09 11:53:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1018151578-615647297 will have desired state: Ready
2022-04-09 11:53:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1018151578-615647297 is in desired state: Ready
2022-04-09 11:53:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e85be345-kafka-clients-tls in namespace namespace-73
2022-04-09 11:53:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-09 11:53:59 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e85be345-kafka-clients-tls will be ready
2022-04-09 11:54:01 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e85be345-kafka-clients-tls is ready
2022-04-09 11:54:01 [ForkJoinPool-3-worker-9] [32mINFO [m [SecurityST:561] Checking consumed messages to pod:my-cluster-e85be345-kafka-clients-tls-6cc446d96f-srkvw
2022-04-09 11:54:01 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@11efc9a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-951070865, --group-instance-id, instance1171596093, --bootstrap-server, my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092, --topic, my-topic-2134079331-2012819435], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e85be345-kafka-clients-tls-6cc446d96f-srkvw', podNamespace='namespace-69', bootstrapServer='my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092', topicName='my-topic-2134079331-2012819435', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-951070865', consumerInstanceId='instance1171596093', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@57d12456}
2022-04-09 11:54:01 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092#my-topic-2134079331-2012819435 from pod my-cluster-e85be345-kafka-clients-tls-6cc446d96f-srkvw
2022-04-09 11:54:01 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e85be345-kafka-clients-tls-6cc446d96f-srkvw -n namespace-69 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-951070865 --group-instance-id instance1171596093 --bootstrap-server my-cluster-e85be345-kafka-bootstrap.namespace-69.svc:9092 --topic my-topic-2134079331-2012819435
2022-04-09 11:54:07 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 11:54:07 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 11:54:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:54:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoReplaceClusterCaKeysTriggeredByAnno
2022-04-09 11:54:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e85be345-kafka-clients in namespace namespace-69
2022-04-09 11:54:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-947971959-141478953 in namespace namespace-69
2022-04-09 11:54:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2134079331-2012819435 in namespace namespace-69
2022-04-09 11:54:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e85be345 in namespace namespace-69
2022-04-09 11:54:27 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-69, for cruise control Kafka cluster my-cluster-e85be345
2022-04-09 11:54:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e85be345-kafka-clients-tls in namespace namespace-69
2022-04-09 11:54:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1018151578-615647297 in namespace namespace-69
2022-04-09 11:54:27 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:54:27 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-70 for test case:testAutoReplaceAllCaKeysTriggeredByAnno
2022-04-09 11:54:36 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceAllCaKeysTriggeredByAnno-FINISHED
2022-04-09 11:54:36 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:55:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:55:27 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-69 for test case:testAutoReplaceClusterCaKeysTriggeredByAnno
2022-04-09 11:55:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9e107014 is in desired state: Ready
2022-04-09 11:55:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:55:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndKafkaConnectTlsVersion
2022-04-09 11:55:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9e107014-scraper in namespace namespace-73
2022-04-09 11:55:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-9e107014 in namespace namespace-73
2022-04-09 11:55:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9e107014 in namespace namespace-73
2022-04-09 11:55:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9e107014-kafka-clients in namespace namespace-73
2022-04-09 11:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClusterCaKeysTriggeredByAnno-FINISHED
2022-04-09 11:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-9e107014-allow in namespace namespace-73
2022-04-09 11:56:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 11:56:31 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-73 for test case:testKafkaAndKafkaConnectTlsVersion
2022-04-09 11:56:36 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectTlsVersion-FINISHED
2022-04-09 11:56:36 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 11:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 11:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context SecurityST is everything deleted.
2022-04-09 11:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,470.739 s - in io.strimzi.systemtest.security.SecurityST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST
2022-04-09 11:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: alternative-reconcile-triggers-st
2022-04-09 11:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: alternative-reconcile-triggers-st
2022-04-09 11:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: alternative-reconcile-triggers-st
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testAddingAndRemovingJbodVolumes-STARTED
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualTriggeringRollingUpdate-STARTED
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testTriggerRollingUpdateAfterOverrideBootstrap-STARTED
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualRollingUpdateForSinglePod-STARTED
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-74 for test case:testAddingAndRemovingJbodVolumes
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-74
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-74
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-74
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-75 for test case:testManualRollingUpdateForSinglePod
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-75
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-907aa401 in namespace namespace-74
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:56:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-907aa401 will have desired state: Ready
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-75
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-75
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-76 for test case:testTriggerRollingUpdateAfterOverrideBootstrap
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-76
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f827ba5a in namespace namespace-75
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-75
2022-04-09 11:56:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f827ba5a will have desired state: Ready
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-76
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-76
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-77 for test case:testManualTriggeringRollingUpdate
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-77
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-64974169 in namespace namespace-76
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-09 11:56:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-64974169 will have desired state: Ready
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-77
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-77
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1a6f6a97 in namespace namespace-77
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:56:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1a6f6a97 will have desired state: Ready
2022-04-09 11:58:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-64974169 is in desired state: Ready
2022-04-09 11:58:53 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:228] Adding new bootstrap dns: kafka-test.XXXX.azure.XXXX.net to external listeners
2022-04-09 11:58:53 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-64974169-kafka rolling update
2022-04-09 11:59:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1a6f6a97 is in desired state: Ready
2022-04-09 11:59:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1045324280-1365452374 in namespace namespace-77
2022-04-09 11:59:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:59:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1045324280-1365452374 will have desired state: Ready
2022-04-09 11:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1045324280-1365452374 is in desired state: Ready
2022-04-09 11:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic continuous-topic in namespace namespace-77
2022-04-09 11:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: continuous-topic will have desired state: Ready
2022-04-09 11:59:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: continuous-topic is in desired state: Ready
2022-04-09 11:59:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 11:59:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace namespace-77
2022-04-09 11:59:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:59:03 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-09 11:59:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-907aa401 is in desired state: Ready
2022-04-09 11:59:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-578414466-969705319 in namespace namespace-77
2022-04-09 11:59:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:59:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-578414466-969705319 will have desired state: Ready
2022-04-09 11:59:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-consumer in namespace namespace-77
2022-04-09 11:59:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:59:04 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-consumer will be in active state
2022-04-09 11:59:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-578414466-969705319 is in desired state: Ready
2022-04-09 11:59:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic continuous-topic in namespace namespace-77
2022-04-09 11:59:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:59:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: continuous-topic will have desired state: Ready
2022-04-09 11:59:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1390470186-444643265 in namespace namespace-77
2022-04-09 11:59:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:59:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1390470186-444643265 will have desired state: Ready
2022-04-09 11:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: continuous-topic is in desired state: Ready
2022-04-09 11:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 11:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace namespace-74
2022-04-09 11:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:59:06 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-09 11:59:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1390470186-444643265 is in desired state: Ready
2022-04-09 11:59:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1a6f6a97-kafka-clients in namespace namespace-77
2022-04-09 11:59:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 11:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-consumer in namespace namespace-74
2022-04-09 11:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:59:07 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-consumer will be in active state
2022-04-09 11:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2120414675-320157272 in namespace namespace-77
2022-04-09 11:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:59:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2120414675-320157272 will have desired state: Ready
2022-04-09 11:59:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2120414675-320157272 is in desired state: Ready
2022-04-09 11:59:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-907aa401-kafka-clients in namespace namespace-77
2022-04-09 11:59:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-09 11:59:16 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:59:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1d2a8023, messages=[], arguments=[--max-messages, 100, USER=my_user_1390470186_444643265, --bootstrap-server, my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-1045324280-1365452374], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8', podNamespace='namespace-77', bootstrapServer='my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-1045324280-1365452374', maxMessages=100, kafkaUsername='my-user-1390470186-444643265', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@75b94220}
2022-04-09 11:59:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093:my-topic-1045324280-1365452374 from pod my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8
2022-04-09 11:59:16 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8 -n namespace-77 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1390470186_444643265 --bootstrap-server my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-1045324280-1365452374
2022-04-09 11:59:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f827ba5a is in desired state: Ready
2022-04-09 11:59:20 [ForkJoinPool-3-worker-13] [32mINFO [m [AlternativeReconcileTriggersST:290] Trying to roll just single Kafka and single ZK pod
2022-04-09 11:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 11:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@71afc40f, messages=[], arguments=[--max-messages, 100, USER=my_user_2120414675_320157272, --bootstrap-server, my-cluster-907aa401-kafka-bootstrap.namespace-74.svc:9093, --topic, my-topic-578414466-969705319], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-907aa401-kafka-clients-599fb474b6-bh9jg', podNamespace='namespace-74', bootstrapServer='my-cluster-907aa401-kafka-bootstrap.namespace-74.svc:9093', topicName='my-topic-578414466-969705319', maxMessages=100, kafkaUsername='my-user-2120414675-320157272', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@41584fd0}
2022-04-09 11:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-907aa401-kafka-bootstrap.namespace-74.svc:9093:my-topic-578414466-969705319 from pod my-cluster-907aa401-kafka-clients-599fb474b6-bh9jg
2022-04-09 11:59:20 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-907aa401-kafka-clients-599fb474b6-bh9jg -n namespace-74 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_2120414675_320157272 --bootstrap-server my-cluster-907aa401-kafka-bootstrap.namespace-74.svc:9093 --topic my-topic-578414466-969705319
2022-04-09 11:59:20 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f827ba5a-kafka rolling update
2022-04-09 11:59:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:59:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:59:20 [ForkJoinPool-3-worker-1] [32mINFO [m [AlternativeReconcileTriggersST:145] Annotate Kafka StatefulSet my-cluster-1a6f6a97-kafka with manual rolling update annotation
2022-04-09 11:59:20 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1a6f6a97-kafka rolling update
2022-04-09 11:59:24 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 11:59:24 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 11:59:24 [ForkJoinPool-3-worker-15] [32mINFO [m [AlternativeReconcileTriggersST:408] Add JBOD volume to the Kafka cluster my-cluster-907aa401-kafka
2022-04-09 11:59:24 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-907aa401-kafka rolling update
2022-04-09 11:59:30 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f827ba5a-kafka has been successfully rolled
2022-04-09 11:59:30 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f827ba5a-kafka to be ready
2022-04-09 12:00:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f827ba5a will have desired state: Ready
2022-04-09 12:00:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f827ba5a is in desired state: Ready
2022-04-09 12:00:01 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f827ba5a is ready
2022-04-09 12:00:01 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f827ba5a-zookeeper rolling update
2022-04-09 12:00:26 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f827ba5a-zookeeper has been successfully rolled
2022-04-09 12:00:26 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f827ba5a-zookeeper to be ready
2022-04-09 12:00:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-64974169-kafka has been successfully rolled
2022-04-09 12:00:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-64974169-kafka to be ready
2022-04-09 12:00:40 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1a6f6a97-kafka has been successfully rolled
2022-04-09 12:00:40 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-1a6f6a97-kafka to be ready
2022-04-09 12:00:54 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-907aa401-kafka has been successfully rolled
2022-04-09 12:00:54 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-907aa401-kafka to be ready
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-64974169 will have desired state: Ready
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-64974169 is in desired state: Ready
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-64974169 is ready
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-64974169 will have desired state: Ready
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-64974169 is in desired state: Ready
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:258] Encoding my-cluster-64974169-kafka-0.crt cert
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:263] Verifying that new DNS is in certificate subject alternative names
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:258] Encoding my-cluster-64974169-kafka-1.crt cert
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:263] Verifying that new DNS is in certificate subject alternative names
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:258] Encoding my-cluster-64974169-kafka-2.crt cert
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:263] Verifying that new DNS is in certificate subject alternative names
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testTriggerRollingUpdateAfterOverrideBootstrap
2022-04-09 12:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-64974169 in namespace namespace-76
2022-04-09 12:01:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1a6f6a97 will have desired state: Ready
2022-04-09 12:01:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1a6f6a97 is in desired state: Ready
2022-04-09 12:01:06 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1a6f6a97 is ready
2022-04-09 12:01:06 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2ac6ea70, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1413778512, --group-instance-id, instance497813012, USER=my_user_1390470186_444643265, --bootstrap-server, my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-1045324280-1365452374], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8', podNamespace='namespace-77', bootstrapServer='my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-1045324280-1365452374', maxMessages=100, kafkaUsername='my-user-1390470186-444643265', consumerGroupName='my-consumer-group-1413778512', consumerInstanceId='instance497813012', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11310b57}
2022-04-09 12:01:06 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093:my-topic-1045324280-1365452374 from pod my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8
2022-04-09 12:01:06 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8 -n namespace-77 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1413778512 --group-instance-id instance497813012 USER=my_user_1390470186_444643265 --bootstrap-server my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-1045324280-1365452374
2022-04-09 12:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-76 for test case:testTriggerRollingUpdateAfterOverrideBootstrap
2022-04-09 12:01:14 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:01:14 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:01:14 [ForkJoinPool-3-worker-1] [32mINFO [m [AlternativeReconcileTriggersST:166] Annotate Zookeeper StatefulSet my-cluster-1a6f6a97-zookeeper with manual rolling update annotation
2022-04-09 12:01:14 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1a6f6a97-zookeeper rolling update
2022-04-09 12:01:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-907aa401 will have desired state: Ready
2022-04-09 12:01:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-907aa401 is in desired state: Ready
2022-04-09 12:01:29 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-907aa401 is ready
2022-04-09 12:01:29 [ForkJoinPool-3-worker-15] [32mINFO [m [AlternativeReconcileTriggersST:419] Remove JBOD volume to the Kafka cluster my-cluster-907aa401-kafka
2022-04-09 12:01:29 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-907aa401-kafka rolling update
2022-04-09 12:01:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f827ba5a will have desired state: Ready
2022-04-09 12:01:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f827ba5a is in desired state: Ready
2022-04-09 12:01:39 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f827ba5a is ready
2022-04-09 12:01:39 [ForkJoinPool-3-worker-13] [32mINFO [m [AlternativeReconcileTriggersST:311] Adding anno to all ZK and Kafka pods
2022-04-09 12:01:40 [ForkJoinPool-3-worker-13] [32mINFO [m [AlternativeReconcileTriggersST:320] Checking if the rolling update will be successful for Kafka
2022-04-09 12:01:40 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f827ba5a-kafka rolling update
2022-04-09 12:01:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testTriggerRollingUpdateAfterOverrideBootstrap-FINISHED
2022-04-09 12:01:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:02:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1a6f6a97-zookeeper has been successfully rolled
2022-04-09 12:02:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-1a6f6a97-zookeeper to be ready
2022-04-09 12:02:59 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-907aa401-kafka has been successfully rolled
2022-04-09 12:02:59 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-907aa401-kafka to be ready
2022-04-09 12:03:05 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f827ba5a-kafka has been successfully rolled
2022-04-09 12:03:05 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f827ba5a-kafka to be ready
2022-04-09 12:03:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1a6f6a97 will have desired state: Ready
2022-04-09 12:03:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1a6f6a97 is in desired state: Ready
2022-04-09 12:03:18 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1a6f6a97 is ready
2022-04-09 12:03:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@395a8733, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1361215302, --group-instance-id, instance1950383912, USER=my_user_1390470186_444643265, --bootstrap-server, my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-1045324280-1365452374], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8', podNamespace='namespace-77', bootstrapServer='my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-1045324280-1365452374', maxMessages=100, kafkaUsername='my-user-1390470186-444643265', consumerGroupName='my-consumer-group-1361215302', consumerInstanceId='instance1950383912', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1cc91195}
2022-04-09 12:03:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093:my-topic-1045324280-1365452374 from pod my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8
2022-04-09 12:03:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8 -n namespace-77 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1361215302 --group-instance-id instance1950383912 USER=my_user_1390470186_444643265 --bootstrap-server my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-1045324280-1365452374
2022-04-09 12:03:25 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:03:25 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:03:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-7877463-1279858683 in namespace namespace-77
2022-04-09 12:03:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-09 12:03:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-7877463-1279858683 will have desired state: Ready
2022-04-09 12:03:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-7877463-1279858683 is in desired state: Ready
2022-04-09 12:03:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@c65da67, messages=[], arguments=[--max-messages, 100, USER=my_user_1390470186_444643265, --bootstrap-server, my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-7877463-1279858683], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8', podNamespace='namespace-77', bootstrapServer='my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-7877463-1279858683', maxMessages=100, kafkaUsername='my-user-1390470186-444643265', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@58f43d55}
2022-04-09 12:03:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093:my-topic-7877463-1279858683 from pod my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8
2022-04-09 12:03:26 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8 -n namespace-77 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1390470186_444643265 --bootstrap-server my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-7877463-1279858683
2022-04-09 12:03:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-907aa401 will have desired state: Ready
2022-04-09 12:03:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-907aa401 is in desired state: Ready
2022-04-09 12:03:30 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-907aa401 is ready
2022-04-09 12:03:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:61] Waiting till producer hello-world-producer and consumer hello-world-consumer finish
2022-04-09 12:03:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:03:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:03:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@672e58c1, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-936299499, --group-instance-id, instance210307416, USER=my_user_1390470186_444643265, --bootstrap-server, my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-7877463-1279858683], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8', podNamespace='namespace-77', bootstrapServer='my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-7877463-1279858683', maxMessages=100, kafkaUsername='my-user-1390470186-444643265', consumerGroupName='my-consumer-group-936299499', consumerInstanceId='instance210307416', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@616ba7c7}
2022-04-09 12:03:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093:my-topic-7877463-1279858683 from pod my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8
2022-04-09 12:03:30 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1a6f6a97-kafka-clients-59df96f75c-r8zr8 -n namespace-77 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-936299499 --group-instance-id instance210307416 USER=my_user_1390470186_444643265 --bootstrap-server my-cluster-1a6f6a97-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-7877463-1279858683
2022-04-09 12:03:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f827ba5a will have desired state: Ready
2022-04-09 12:03:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f827ba5a is in desired state: Ready
2022-04-09 12:03:35 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f827ba5a is ready
2022-04-09 12:03:35 [ForkJoinPool-3-worker-13] [32mINFO [m [AlternativeReconcileTriggersST:331] Checking if the rolling update will be successful for ZK
2022-04-09 12:03:35 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f827ba5a-zookeeper rolling update
2022-04-09 12:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:03:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:61] Waiting till producer hello-world-producer and consumer hello-world-consumer finish
2022-04-09 12:04:55 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f827ba5a-zookeeper has been successfully rolled
2022-04-09 12:04:55 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f827ba5a-zookeeper to be ready
2022-04-09 12:05:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f827ba5a will have desired state: Ready
2022-04-09 12:05:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f827ba5a is in desired state: Ready
2022-04-09 12:05:21 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f827ba5a is ready
2022-04-09 12:05:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:05:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testManualRollingUpdateForSinglePod
2022-04-09 12:05:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f827ba5a in namespace namespace-75
2022-04-09 12:05:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:05:31 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-75 for test case:testManualRollingUpdateForSinglePod
2022-04-09 12:06:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualRollingUpdateForSinglePod-FINISHED
2022-04-09 12:06:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAddingAndRemovingJbodVolumes
2022-04-09 12:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-consumer in namespace namespace-74
2022-04-09 12:07:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-578414466-969705319 in namespace namespace-74
2022-04-09 12:07:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-907aa401 in namespace namespace-74
2022-04-09 12:07:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-907aa401-kafka-clients in namespace namespace-74
2022-04-09 12:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace namespace-74
2022-04-09 12:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic continuous-topic in namespace namespace-74
2022-04-09 12:08:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2120414675-320157272 in namespace namespace-74
2022-04-09 12:08:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:08:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testManualTriggeringRollingUpdate
2022-04-09 12:08:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1390470186-444643265 in namespace namespace-77
2022-04-09 12:08:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic continuous-topic in namespace namespace-77
2022-04-09 12:08:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-consumer in namespace namespace-77
2022-04-09 12:08:11 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-7877463-1279858683 in namespace namespace-77
2022-04-09 12:08:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1045324280-1365452374 in namespace namespace-77
2022-04-09 12:08:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace namespace-77
2022-04-09 12:08:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1a6f6a97-kafka-clients in namespace namespace-77
2022-04-09 12:08:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1a6f6a97 in namespace namespace-77
2022-04-09 12:08:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:08:46 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-74 for test case:testAddingAndRemovingJbodVolumes
2022-04-09 12:08:56 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testAddingAndRemovingJbodVolumes-FINISHED
2022-04-09 12:08:56 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:09:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:09:01 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-77 for test case:testManualTriggeringRollingUpdate
2022-04-09 12:09:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualTriggeringRollingUpdate-FINISHED
2022-04-09 12:09:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context AlternativeReconcileTriggersST is everything deleted.
2022-04-09 12:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 751.002 s - in io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-04-09 12:09:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: rolling-update-st
2022-04-09 12:09:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: rolling-update-st
2022-04-09 12:09:13 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: rolling-update-st
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringZookeeperRollingUpdate-STARTED
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testZookeeperScaleUpScaleDown-STARTED
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testExternalLoggingChangeTriggerRollingUpdate-STARTED
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testBrokerConfigurationChangeTriggerRollingUpdate-STARTED
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testManualKafkaConfigMapChangeDontTriggerRollingUpdate-STARTED
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-78 for test case:testRecoveryDuringZookeeperRollingUpdate
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-79 for test case:testManualKafkaConfigMapChangeDontTriggerRollingUpdate
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-79
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6bae7b37 in namespace namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-275185964-1628247106 in namespace namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-09 12:09:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6bae7b37 will have desired state: Ready
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-79
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-79
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-80 for test case:testExternalLoggingChangeTriggerRollingUpdate
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-80
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f6fb2739 in namespace namespace-79
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-79
2022-04-09 12:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f6fb2739 will have desired state: Ready
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-80
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-80
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-81 for test case:testZookeeperScaleUpScaleDown
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-81
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-98c93e9f in namespace namespace-80
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-80
2022-04-09 12:09:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-98c93e9f will have desired state: Ready
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-81
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-81
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-82 for test case:testBrokerConfigurationChangeTriggerRollingUpdate
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-82
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fa9ceff4 in namespace namespace-81
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-09 12:09:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa9ceff4 will have desired state: Ready
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-82
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-82
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7bb25826 in namespace namespace-82
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-82
2022-04-09 12:09:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7bb25826 will have desired state: Ready
2022-04-09 12:11:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6bae7b37 is in desired state: Ready
2022-04-09 12:11:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-275185964-1628247106 will have desired state: Ready
2022-04-09 12:11:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-275185964-1628247106 is in desired state: Ready
2022-04-09 12:11:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1862237733-661079429 in namespace namespace-78
2022-04-09 12:11:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-09 12:11:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1862237733-661079429 will have desired state: Ready
2022-04-09 12:11:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1862237733-661079429 is in desired state: Ready
2022-04-09 12:11:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6bae7b37-kafka-clients in namespace namespace-82
2022-04-09 12:11:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-09 12:11:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7bb25826 is in desired state: Ready
2022-04-09 12:11:38 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-7bb25826-kafka rolling update
2022-04-09 12:11:45 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 12:11:45 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5178aa51, messages=[], arguments=[--max-messages, 100, USER=my_user_1862237733_661079429, --bootstrap-server, my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-275185964-1628247106], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th', podNamespace='namespace-78', bootstrapServer='my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-275185964-1628247106', maxMessages=100, kafkaUsername='my-user-1862237733-661079429', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@77424b41}
2022-04-09 12:11:45 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093:my-topic-275185964-1628247106 from pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th
2022-04-09 12:11:45 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th -n namespace-78 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1862237733_661079429 --bootstrap-server my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-275185964-1628247106
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f6fb2739 is in desired state: Ready
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-f6fb2739 are stable
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:11:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:11:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:11:49 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:117] Update resources for pods
2022-04-09 12:11:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4a86e04f, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1076505306, --group-instance-id, instance1746555014, USER=my_user_1862237733_661079429, --bootstrap-server, my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-275185964-1628247106], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th', podNamespace='namespace-78', bootstrapServer='my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-275185964-1628247106', maxMessages=100, kafkaUsername='my-user-1862237733-661079429', consumerGroupName='my-consumer-group-1076505306', consumerInstanceId='instance1746555014', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e38e3fb}
2022-04-09 12:11:49 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093:my-topic-275185964-1628247106 from pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th
2022-04-09 12:11:49 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th -n namespace-78 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1076505306 --group-instance-id instance1746555014 USER=my_user_1862237733_661079429 --bootstrap-server my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-275185964-1628247106
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:11:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-98c93e9f is in desired state: Ready
2022-04-09 12:11:51 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-98c93e9f-zookeeper rolling update
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:11:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa9ceff4 is in desired state: Ready
2022-04-09 12:11:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1332761117-1399139486 in namespace namespace-82
2022-04-09 12:11:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-09 12:11:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1332761117-1399139486 will have desired state: Ready
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:11:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1332761117-1399139486 is in desired state: Ready
2022-04-09 12:11:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-658160128-937693644 in namespace namespace-81
2022-04-09 12:11:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-09 12:11:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-658160128-937693644 will have desired state: Ready
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:11:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-658160128-937693644 is in desired state: Ready
2022-04-09 12:11:55 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:398] Running zookeeperScaleUpScaleDown with cluster my-cluster-fa9ceff4
2022-04-09 12:11:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-fa9ceff4-kafka-clients in namespace namespace-82
2022-04-09 12:11:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:11:57 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:11:57 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:11:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-09 12:11:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: my-cluster-6bae7b37-zookeeper will be in pending phase
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:11:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:03 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:130] Verifying stability of zookeeper pods except the one, which is in pending phase
2022-04-09 12:12:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-6bae7b37-zookeeper are stable
2022-04-09 12:12:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:12:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:12:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:12:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:12:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 12:12:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@270f91e1, messages=[], arguments=[--max-messages, 100, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-1332761117-1399139486], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-1332761117-1399139486', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7837f2ad}
2022-04-09 12:12:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-1332761117-1399139486 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:12:05 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-1332761117-1399139486
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:12:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:12:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:426] Scale up Zookeeper to 7
2022-04-09 12:12:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:12:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4535013a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1548452884, --group-instance-id, instance775000312, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-1332761117-1399139486], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-1332761117-1399139486', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='my-consumer-group-1548452884', consumerInstanceId='instance775000312', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2511ed58}
2022-04-09 12:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-1332761117-1399139486 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1548452884 --group-instance-id instance775000312 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-1332761117-1399139486
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:12:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:12:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:12:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:12:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:12:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:12:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:12:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:17 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:12:17 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:12:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-fa9ceff4-zookeeper to be ready
2022-04-09 12:12:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:26 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:26 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-f6fb2739-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-f6fb2739-entity-operator-5c96f8f978-t4rwn ,my-cluster-f6fb2739-kafka-0 ,my-cluster-f6fb2739-kafka-1 ,my-cluster-f6fb2739-kafka-2 ,my-cluster-f6fb2739-zookeeper-0 ,my-cluster-f6fb2739-zookeeper-1 ,my-cluster-f6fb2739-zookeeper-2
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testManualKafkaConfigMapChangeDontTriggerRollingUpdate
2022-04-09 12:12:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f6fb2739 in namespace namespace-79
2022-04-09 12:12:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:12:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:12:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:12:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:12:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:12:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:12:44 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:44 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:12:45 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:45 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:12:46 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:46 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:12:47 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:47 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:12:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:12:47 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-79 for test case:testManualKafkaConfigMapChangeDontTriggerRollingUpdate
2022-04-09 12:12:48 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:48 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:12:48 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-7bb25826-kafka has been successfully rolled
2022-04-09 12:12:48 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-7bb25826-kafka to be ready
2022-04-09 12:12:49 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:49 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:12:50 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:50 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:12:51 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:51 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:12:52 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:52 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-6bae7b37-zookeeper-0 ,my-cluster-6bae7b37-zookeeper-2
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:133] Verifying stability of kafka pods
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-6bae7b37-kafka are stable
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:12:53 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:12:54 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:12:54 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:12:54 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:12:54 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:12:55 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:12:55 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:12:55 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:12:55 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:12:56 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:12:56 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:12:56 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:12:56 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:12:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-98c93e9f-zookeeper has been successfully rolled
2022-04-09 12:12:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-98c93e9f-zookeeper to be ready
2022-04-09 12:12:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:12:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:12:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:12:57 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:12:58 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:12:58 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:12:58 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:12:58 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:12:59 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:12:59 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:12:59 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:12:59 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:13:00 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:13:00 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:13:00 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:13:00 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:13:01 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:13:01 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:13:01 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:13:01 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:13:02 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:13:02 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:13:02 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:13:02 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:13:03 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:13:04 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringKafkaRollingUpdate-STARTED
2022-04-09 12:13:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:13:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:13:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:13:04 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:13:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:13:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:13:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:13:05 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:13:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:13:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:13:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:13:06 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:13:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:13:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:13:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:13:07 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:13:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:13:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:13:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:13:08 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-83 for test case:testRecoveryDuringKafkaRollingUpdate
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-83
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-83
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-83
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-145dcd4c in namespace namespace-83
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-09 12:13:09 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-145dcd4c will have desired state: Ready
2022-04-09 12:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:13:09 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:13:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:13:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:13:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:13:10 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:13:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:13:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:13:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:13:11 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:13:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:13:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:13:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:13:12 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:13:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:13:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:13:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:13:13 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:13:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:13:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:13:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:13:14 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:13:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:13:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:13:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:13:15 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:13:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:13:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:13:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:13:16 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:13:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:13:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:13:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:13:17 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:13:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:13:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:13:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:13:18 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:13:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testManualKafkaConfigMapChangeDontTriggerRollingUpdate-FINISHED
2022-04-09 12:13:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:13:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-STARTED
2022-04-09 12:13:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:13:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:13:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:13:19 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:13:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7bb25826 will have desired state: Ready
2022-04-09 12:13:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7bb25826 is in desired state: Ready
2022-04-09 12:13:19 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-7bb25826 is ready
2022-04-09 12:13:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:13:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testBrokerConfigurationChangeTriggerRollingUpdate
2022-04-09 12:13:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7bb25826 in namespace namespace-82
2022-04-09 12:13:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:13:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:13:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:13:20 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:13:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:13:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:13:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:13:21 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:13:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:13:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:13:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:13:22 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:13:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:13:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:13:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:13:23 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-84 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-84
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-84
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-84
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-bc2f1d68 in namespace namespace-84
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-09 12:13:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bc2f1d68 will have desired state: Ready
2022-04-09 12:13:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:13:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:13:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:13:24 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:13:26 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-98c93e9f-kafka rolling update
2022-04-09 12:13:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:13:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:13:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:13:27 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:13:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:13:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:13:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:13:28 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:13:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:13:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:13:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:13:29 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:13:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:13:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-82 for test case:testBrokerConfigurationChangeTriggerRollingUpdate
2022-04-09 12:13:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:13:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:13:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:13:30 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:13:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:13:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:13:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:13:31 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:13:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:13:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:13:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:13:32 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:13:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:13:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:13:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:13:33 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:13:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:13:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:13:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:13:34 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:13:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:13:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:13:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:13:36 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:13:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:13:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:13:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:13:37 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:13:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:13:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:13:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:13:38 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:13:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:13:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:13:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:13:39 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:13:40 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:13:40 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:13:40 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:13:40 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:13:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:13:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:13:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:13:41 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:13:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:13:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:13:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:13:42 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:13:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:13:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:13:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:13:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:322] Pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:13:43 [ForkJoinPool-3-worker-11] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-6bae7b37-kafka-0 ,my-cluster-6bae7b37-kafka-1 ,my-cluster-6bae7b37-kafka-2 ,my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th
2022-04-09 12:13:43 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-6bae7b37-zookeeper to be ready
2022-04-09 12:14:13 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testBrokerConfigurationChangeTriggerRollingUpdate-FINISHED
2022-04-09 12:14:13 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:14:46 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-98c93e9f-kafka has been successfully rolled
2022-04-09 12:14:46 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-98c93e9f-kafka to be ready
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa9ceff4 will have desired state: Ready
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa9ceff4 is in desired state: Ready
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-fa9ceff4 is ready
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-0 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-1 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:53 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-2 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-3 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-4 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-5 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-6 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3e81580c, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1994467939, --group-instance-id, instance1974217782, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-1332761117-1399139486], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-1332761117-1399139486', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='my-consumer-group-1994467939', consumerInstanceId='instance1974217782', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6c46853a}
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-1332761117-1399139486 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:14:55 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1994467939 --group-instance-id instance1974217782 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-1332761117-1399139486
2022-04-09 12:15:02 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:15:02 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:15:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-550611410-1896050294 in namespace namespace-84
2022-04-09 12:15:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-09 12:15:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-550611410-1896050294 will have desired state: Ready
2022-04-09 12:15:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-145dcd4c is in desired state: Ready
2022-04-09 12:15:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1641671317-455063862 in namespace namespace-84
2022-04-09 12:15:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-09 12:15:03 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1641671317-455063862 will have desired state: Ready
2022-04-09 12:15:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-550611410-1896050294 is in desired state: Ready
2022-04-09 12:15:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1e68a9fa, messages=[], arguments=[--max-messages, 100, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-550611410-1896050294], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-550611410-1896050294', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@24829d79}
2022-04-09 12:15:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-550611410-1896050294 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:15:04 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-550611410-1896050294
2022-04-09 12:15:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1641671317-455063862 is in desired state: Ready
2022-04-09 12:15:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1598371417-150285471 in namespace namespace-84
2022-04-09 12:15:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-09 12:15:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1598371417-150285471 will have desired state: Ready
2022-04-09 12:15:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1598371417-150285471 is in desired state: Ready
2022-04-09 12:15:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-145dcd4c-kafka-clients in namespace namespace-84
2022-04-09 12:15:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-09 12:15:10 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:15:10 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:15:10 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@722af1b7, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1423884084, --group-instance-id, instance1467098484, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-550611410-1896050294], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-550611410-1896050294', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='my-consumer-group-1423884084', consumerInstanceId='instance1467098484', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@46df9058}
2022-04-09 12:15:10 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-550611410-1896050294 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:15:10 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1423884084 --group-instance-id instance1467098484 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-550611410-1896050294
2022-04-09 12:15:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-98c93e9f will have desired state: Ready
2022-04-09 12:15:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-98c93e9f is in desired state: Ready
2022-04-09 12:15:16 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-98c93e9f is ready
2022-04-09 12:15:16 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-98c93e9f-zookeeper rolling update
2022-04-09 12:15:16 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 12:15:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@387a60bc, messages=[], arguments=[--max-messages, 100, USER=my_user_1598371417_150285471, --bootstrap-server, my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1641671317-455063862], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs', podNamespace='namespace-83', bootstrapServer='my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1641671317-455063862', maxMessages=100, kafkaUsername='my-user-1598371417-150285471', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55e046e9}
2022-04-09 12:15:16 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093:my-topic-1641671317-455063862 from pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:15:16 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs -n namespace-83 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1598371417_150285471 --bootstrap-server my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1641671317-455063862
2022-04-09 12:15:17 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:15:17 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:15:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateST:453] Scale down Zookeeper to 3
2022-04-09 12:15:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-fa9ceff4-zookeeper to be ready
2022-04-09 12:15:20 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:15:20 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:15:20 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateST:203] Update resources for pods
2022-04-09 12:15:20 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@76baea0a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1880290816, --group-instance-id, instance1991448814, USER=my_user_1598371417_150285471, --bootstrap-server, my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1641671317-455063862], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs', podNamespace='namespace-83', bootstrapServer='my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1641671317-455063862', maxMessages=100, kafkaUsername='my-user-1598371417-150285471', consumerGroupName='my-consumer-group-1880290816', consumerInstanceId='instance1991448814', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1d2e1693}
2022-04-09 12:15:20 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093:my-topic-1641671317-455063862 from pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:15:20 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs -n namespace-83 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1880290816 --group-instance-id instance1991448814 USER=my_user_1598371417_150285471 --bootstrap-server my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1641671317-455063862
2022-04-09 12:15:29 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:15:29 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:15:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-09 12:15:29 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: my-cluster-145dcd4c-kafka will be in pending phase
2022-04-09 12:15:30 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateST:220] Verifying stability of kafka pods except the one, which is in pending phase
2022-04-09 12:15:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-145dcd4c-kafka are stable
2022-04-09 12:15:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:15:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:15:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:15:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:15:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:15:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:15:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:15:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:15:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:15:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:15:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:15:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:15:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:15:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:15:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:15:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:15:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:15:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:15:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:15:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:15:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:15:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:15:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:15:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:15:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:15:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:15:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:15:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:15:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:15:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:15:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:15:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:15:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:15:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:15:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:15:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:15:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:15:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:15:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bc2f1d68 is in desired state: Ready
2022-04-09 12:15:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-207449775-272185985 in namespace namespace-84
2022-04-09 12:15:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-09 12:15:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-207449775-272185985 will have desired state: Ready
2022-04-09 12:15:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:15:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:15:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-207449775-272185985 is in desired state: Ready
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:489] Verifying docker image names
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:525] Docker images verified
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:292] Running kafkaScaleUpScaleDown my-cluster-bc2f1d68
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1217380909-1781660153 in namespace namespace-84
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-09 12:15:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1217380909-1781660153 will have desired state: Ready
2022-04-09 12:15:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:15:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:15:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:15:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1217380909-1781660153 is in desired state: Ready
2022-04-09 12:15:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bc2f1d68-kafka-clients in namespace namespace-84
2022-04-09 12:15:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-09 12:15:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:15:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:15:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:15:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:15:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:15:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:15:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:15:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:15:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:15:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:15:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:15:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:15:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:15:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:15:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:15:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:15:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:15:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:15:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:15:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:15:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:15:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:15:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:15:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:15:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:15:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:15:56 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 12:15:56 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@67c32f0c, messages=[], arguments=[--max-messages, 100, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-1217380909-1781660153], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-1217380909-1781660153', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@47c52b3f}
2022-04-09 12:15:56 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-1217380909-1781660153 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:15:56 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-1217380909-1781660153
2022-04-09 12:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:15:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:15:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:15:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:15:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:15:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:15:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:15:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:16:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:16:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:16:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:16:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1c9f8e24, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1946409282, --group-instance-id, instance2144441237, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-1217380909-1781660153], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-1217380909-1781660153', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='my-consumer-group-1946409282', consumerInstanceId='instance2144441237', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@500314b9}
2022-04-09 12:16:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-1217380909-1781660153 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:16:00 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1946409282 --group-instance-id instance2144441237 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-1217380909-1781660153
2022-04-09 12:16:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:16:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:16:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:16:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:16:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:16:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:16:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:16:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:16:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa9ceff4 will have desired state: Ready
2022-04-09 12:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa9ceff4 is in desired state: Ready
2022-04-09 12:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-fa9ceff4 is ready
2022-04-09 12:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-0 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:16:03 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-1 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:16:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:16:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:16:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-81 exec my-cluster-fa9ceff4-zookeeper-2 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@23689b3a, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-268446136, --group-instance-id, instance388947424, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-550611410-1896050294], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-550611410-1896050294', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='my-consumer-group-268446136', consumerInstanceId='instance388947424', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@190f9513}
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-550611410-1896050294 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:16:04 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-268446136 --group-instance-id instance388947424 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-550611410-1896050294
2022-04-09 12:16:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:16:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:16:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:16:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:16:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:16:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:16:07 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:16:07 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:16:07 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:16:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:317] Scale up Kafka to 7
2022-04-09 12:16:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-bc2f1d68-kafka rolling update
2022-04-09 12:16:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:16:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:16:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:16:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:16:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:16:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:16:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:16:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:16:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:16:11 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:16:11 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:16:11 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:16:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:16:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:16:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-804959270-27524616 in namespace namespace-84
2022-04-09 12:16:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-09 12:16:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-804959270-27524616 will have desired state: Ready
2022-04-09 12:16:12 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:16:12 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:16:12 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:16:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-804959270-27524616 is in desired state: Ready
2022-04-09 12:16:12 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5a5b2f99, messages=[], arguments=[--max-messages, 100, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-804959270-27524616], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-804959270-27524616', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6ea71a9}
2022-04-09 12:16:12 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-804959270-27524616 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:16:12 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-804959270-27524616
2022-04-09 12:16:13 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:16:13 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:16:13 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:16:14 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:16:14 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:16:14 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:16:15 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:16:15 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:16:15 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:16:16 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:16:16 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:16:16 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:16:16 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:16:16 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:16:16 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@584f734e, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1107132902, --group-instance-id, instance502745187, USER=my_user_658160128_937693644, --bootstrap-server, my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093, --topic, my-topic-804959270-27524616], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6', podNamespace='namespace-81', bootstrapServer='my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093', topicName='my-topic-804959270-27524616', maxMessages=100, kafkaUsername='my-user-658160128-937693644', consumerGroupName='my-consumer-group-1107132902', consumerInstanceId='instance502745187', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3f999ecc}
2022-04-09 12:16:16 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093:my-topic-804959270-27524616 from pod my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6
2022-04-09 12:16:16 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9ceff4-kafka-clients-77c5b7fdc8-7qck6 -n namespace-81 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1107132902 --group-instance-id instance502745187 USER=my_user_658160128_937693644 --bootstrap-server my-cluster-fa9ceff4-kafka-bootstrap.namespace-81.svc:9093 --topic my-topic-804959270-27524616
2022-04-09 12:16:17 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:16:17 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:16:17 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:16:18 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:16:18 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:16:18 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:16:19 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:16:19 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:16:19 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-145dcd4c-kafka-1 ,my-cluster-145dcd4c-kafka-2 ,my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateST:223] Verifying stability of zookeeper pods
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-145dcd4c-zookeeper are stable
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:16:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:16:21 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:16:21 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:16:21 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:16:22 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:16:22 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:16:22 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:16:23 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:16:23 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:16:23 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:16:23 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:16:23 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:16:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:16:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testZookeeperScaleUpScaleDown
2022-04-09 12:16:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-fa9ceff4-kafka-clients in namespace namespace-81
2022-04-09 12:16:24 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:16:24 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:16:24 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:16:25 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:16:25 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:16:25 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:16:26 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:16:26 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:16:26 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:16:27 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:16:27 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:16:27 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:16:28 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:16:28 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:16:28 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:16:29 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:16:29 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:16:29 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:16:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:16:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:16:30 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-98c93e9f-zookeeper has been successfully rolled
2022-04-09 12:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-98c93e9f-zookeeper to be ready
2022-04-09 12:16:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:16:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:16:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:16:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:16:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:16:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:16:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:16:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:16:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:16:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:16:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:16:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:16:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:16:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:16:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:16:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:16:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:16:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:16:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:16:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:16:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:16:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:16:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:16:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:16:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:16:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:16:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:16:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:16:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:16:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:16:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:16:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:16:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:16:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:16:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:16:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:16:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:16:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:16:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:16:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:16:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:16:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:16:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:16:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:16:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:16:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:16:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:16:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:16:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:16:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:16:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:16:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:16:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:16:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:16:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:16:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:16:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:16:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:16:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:16:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:16:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:16:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:16:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:16:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:16:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:16:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:16:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:16:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:16:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:16:54 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:16:54 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:16:54 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:16:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:16:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:16:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:16:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:16:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:16:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:16:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:16:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:16:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:16:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:16:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:16:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-98c93e9f-kafka rolling update
2022-04-09 12:16:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:16:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:16:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:17:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:17:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:17:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:17:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:17:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:17:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:17:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:17:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:17:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:17:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-804959270-27524616 in namespace namespace-81
2022-04-09 12:17:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:17:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:17:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:17:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:17:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:17:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:17:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:17:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:17:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:17:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:17:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:17:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:17:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:17:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:17:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:17:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:17:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:17:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-145dcd4c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-145dcd4c-zookeeper-0 ,my-cluster-145dcd4c-zookeeper-1 ,my-cluster-145dcd4c-zookeeper-2
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@55126175, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-745706883, --group-instance-id, instance2067391311, USER=my_user_1598371417_150285471, --bootstrap-server, my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1641671317-455063862], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs', podNamespace='namespace-83', bootstrapServer='my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1641671317-455063862', maxMessages=100, kafkaUsername='my-user-1598371417-150285471', consumerGroupName='my-consumer-group-745706883', consumerInstanceId='instance2067391311', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6931bd3c}
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093:my-topic-1641671317-455063862 from pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:17:10 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs -n namespace-83 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-745706883 --group-instance-id instance2067391311 USER=my_user_1598371417_150285471 --bootstrap-server my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1641671317-455063862
2022-04-09 12:17:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-550611410-1896050294 in namespace namespace-81
2022-04-09 12:17:17 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:17:17 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:17:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-09 12:17:17 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-145dcd4c-kafka to be ready
2022-04-09 12:17:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1332761117-1399139486 in namespace namespace-81
2022-04-09 12:17:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-658160128-937693644 in namespace namespace-81
2022-04-09 12:17:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-bc2f1d68-kafka has been successfully rolled
2022-04-09 12:17:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-bc2f1d68-kafka to be ready
2022-04-09 12:17:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fa9ceff4 in namespace namespace-81
2022-04-09 12:17:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:17:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-81 for test case:testZookeeperScaleUpScaleDown
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bc2f1d68 will have desired state: Ready
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bc2f1d68 is in desired state: Ready
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-bc2f1d68 is ready
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:327] Kafka scale up to 7 finished
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1631575c, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-49046159, --group-instance-id, instance1092923421, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-1217380909-1781660153], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-1217380909-1781660153', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='my-consumer-group-49046159', consumerInstanceId='instance1092923421', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3b35f01a}
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-1217380909-1781660153 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:18:37 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-49046159 --group-instance-id instance1092923421 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-1217380909-1781660153
2022-04-09 12:18:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testZookeeperScaleUpScaleDown-FINISHED
2022-04-09 12:18:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:18:44 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:18:44 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:18:44 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:339] Scale up Zookeeper to 5
2022-04-09 12:18:44 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 5 Pod(s) of my-cluster-bc2f1d68-zookeeper to be ready
2022-04-09 12:19:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6bae7b37 will have desired state: Ready
2022-04-09 12:19:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6bae7b37 is in desired state: Ready
2022-04-09 12:19:02 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-6bae7b37 is ready
2022-04-09 12:19:02 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1366c905, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1420377740, --group-instance-id, instance1883766908, USER=my_user_1862237733_661079429, --bootstrap-server, my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-275185964-1628247106], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th', podNamespace='namespace-78', bootstrapServer='my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-275185964-1628247106', maxMessages=100, kafkaUsername='my-user-1862237733-661079429', consumerGroupName='my-consumer-group-1420377740', consumerInstanceId='instance1883766908', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b34a073}
2022-04-09 12:19:02 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093:my-topic-275185964-1628247106 from pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th
2022-04-09 12:19:02 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th -n namespace-78 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1420377740 --group-instance-id instance1883766908 USER=my_user_1862237733_661079429 --bootstrap-server my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-275185964-1628247106
2022-04-09 12:19:09 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:19:09 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:19:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-758471674-1902427938 in namespace namespace-84
2022-04-09 12:19:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-09 12:19:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-758471674-1902427938 will have desired state: Ready
2022-04-09 12:19:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-758471674-1902427938 is in desired state: Ready
2022-04-09 12:19:11 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7a586189, messages=[], arguments=[--max-messages, 100, USER=my_user_1862237733_661079429, --bootstrap-server, my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-758471674-1902427938], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th', podNamespace='namespace-78', bootstrapServer='my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-758471674-1902427938', maxMessages=100, kafkaUsername='my-user-1862237733-661079429', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@19d9911}
2022-04-09 12:19:11 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093:my-topic-758471674-1902427938 from pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th
2022-04-09 12:19:11 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th -n namespace-78 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1862237733_661079429 --bootstrap-server my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-758471674-1902427938
2022-04-09 12:19:14 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:19:14 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:19:15 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@112ab8f9, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1802867765, --group-instance-id, instance118014098, USER=my_user_1862237733_661079429, --bootstrap-server, my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-758471674-1902427938], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th', podNamespace='namespace-78', bootstrapServer='my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-758471674-1902427938', maxMessages=100, kafkaUsername='my-user-1862237733-661079429', consumerGroupName='my-consumer-group-1802867765', consumerInstanceId='instance118014098', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@34a17189}
2022-04-09 12:19:15 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093:my-topic-758471674-1902427938 from pod my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th
2022-04-09 12:19:15 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bae7b37-kafka-clients-7c96764c74-dd4th -n namespace-78 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1802867765 --group-instance-id instance118014098 USER=my_user_1862237733_661079429 --bootstrap-server my-cluster-6bae7b37-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-758471674-1902427938
2022-04-09 12:19:22 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:19:22 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:19:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:19:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryDuringZookeeperRollingUpdate
2022-04-09 12:19:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1862237733-661079429 in namespace namespace-78
2022-04-09 12:19:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-275185964-1628247106 in namespace namespace-78
2022-04-09 12:19:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-758471674-1902427938 in namespace namespace-78
2022-04-09 12:19:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6bae7b37 in namespace namespace-78
2022-04-09 12:19:39 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-98c93e9f-kafka has been successfully rolled
2022-04-09 12:19:39 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-98c93e9f-kafka to be ready
2022-04-09 12:19:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6bae7b37-kafka-clients in namespace namespace-78
2022-04-09 12:20:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-98c93e9f will have desired state: Ready
2022-04-09 12:20:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-98c93e9f is in desired state: Ready
2022-04-09 12:20:03 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-98c93e9f is ready
2022-04-09 12:20:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:20:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testExternalLoggingChangeTriggerRollingUpdate
2022-04-09 12:20:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-98c93e9f in namespace namespace-80
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bc2f1d68 will have desired state: Ready
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bc2f1d68 is in desired state: Ready
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-bc2f1d68 is ready
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:342] Kafka scale up to 5 finished
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@40e00c88, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1268288233, --group-instance-id, instance1569866117, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-1217380909-1781660153], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-1217380909-1781660153', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='my-consumer-group-1268288233', consumerInstanceId='instance1569866117', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5c773bed}
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-1217380909-1781660153 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:20:12 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1268288233 --group-instance-id instance1569866117 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-1217380909-1781660153
2022-04-09 12:20:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:20:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-80 for test case:testExternalLoggingChangeTriggerRollingUpdate
2022-04-09 12:20:19 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:20:19 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:20:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:351] Scale down Kafka to 3
2022-04-09 12:20:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-bc2f1d68-kafka rolling update
2022-04-09 12:20:22 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:20:22 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-78 for test case:testRecoveryDuringZookeeperRollingUpdate
2022-04-09 12:20:27 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringZookeeperRollingUpdate-FINISHED
2022-04-09 12:20:27 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:20:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testExternalLoggingChangeTriggerRollingUpdate-FINISHED
2022-04-09 12:20:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:22:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-145dcd4c will have desired state: Ready
2022-04-09 12:22:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-145dcd4c is in desired state: Ready
2022-04-09 12:22:40 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-145dcd4c is ready
2022-04-09 12:22:40 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7d92ae22, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1911528611, --group-instance-id, instance2133927898, USER=my_user_1598371417_150285471, --bootstrap-server, my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1641671317-455063862], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs', podNamespace='namespace-83', bootstrapServer='my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1641671317-455063862', maxMessages=100, kafkaUsername='my-user-1598371417-150285471', consumerGroupName='my-consumer-group-1911528611', consumerInstanceId='instance2133927898', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@192e2866}
2022-04-09 12:22:40 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093:my-topic-1641671317-455063862 from pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:22:40 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs -n namespace-83 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1911528611 --group-instance-id instance2133927898 USER=my_user_1598371417_150285471 --bootstrap-server my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1641671317-455063862
2022-04-09 12:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-09 12:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1753584026-520936782 in namespace namespace-84
2022-04-09 12:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-09 12:22:47 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1753584026-520936782 will have desired state: Ready
2022-04-09 12:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1753584026-520936782 is in desired state: Ready
2022-04-09 12:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@404032d6, messages=[], arguments=[--max-messages, 100, USER=my_user_1598371417_150285471, --bootstrap-server, my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1753584026-520936782], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs', podNamespace='namespace-83', bootstrapServer='my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1753584026-520936782', maxMessages=100, kafkaUsername='my-user-1598371417-150285471', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@284552a0}
2022-04-09 12:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093:my-topic-1753584026-520936782 from pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs -n namespace-83 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1598371417_150285471 --bootstrap-server my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1753584026-520936782
2022-04-09 12:22:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:22:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:22:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@785fa776, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-631153727, --group-instance-id, instance1943258089, USER=my_user_1598371417_150285471, --bootstrap-server, my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1753584026-520936782], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs', podNamespace='namespace-83', bootstrapServer='my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1753584026-520936782', maxMessages=100, kafkaUsername='my-user-1598371417-150285471', consumerGroupName='my-consumer-group-631153727', consumerInstanceId='instance1943258089', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@284b7742}
2022-04-09 12:22:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093:my-topic-1753584026-520936782 from pod my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs
2022-04-09 12:22:52 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-145dcd4c-kafka-clients-74d798b84d-d8tzs -n namespace-83 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-631153727 --group-instance-id instance1943258089 USER=my_user_1598371417_150285471 --bootstrap-server my-cluster-145dcd4c-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1753584026-520936782
2022-04-09 12:22:59 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:22:59 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:22:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:22:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryDuringKafkaRollingUpdate
2022-04-09 12:22:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1598371417-150285471 in namespace namespace-83
2022-04-09 12:22:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-145dcd4c in namespace namespace-83
2022-04-09 12:22:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1641671317-455063862 in namespace namespace-83
2022-04-09 12:22:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1753584026-520936782 in namespace namespace-83
2022-04-09 12:23:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-145dcd4c-kafka-clients in namespace namespace-83
2022-04-09 12:23:14 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-bc2f1d68-kafka has been successfully rolled
2022-04-09 12:23:14 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-bc2f1d68-kafka to be ready
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bc2f1d68 will have desired state: Ready
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bc2f1d68 is in desired state: Ready
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-bc2f1d68 is ready
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateST:356] Kafka scale down to 3 finished
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@69d3482d, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-438128488, --group-instance-id, instance1694493222, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-1217380909-1781660153], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-1217380909-1781660153', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='my-consumer-group-438128488', consumerInstanceId='instance1694493222', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@78b3b877}
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-1217380909-1781660153 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:23:38 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-438128488 --group-instance-id instance1694493222 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-1217380909-1781660153
2022-04-09 12:23:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:23:49 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-83 for test case:testRecoveryDuringKafkaRollingUpdate
2022-04-09 12:23:54 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:23:54 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:23:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-365093870-522575263 in namespace namespace-84
2022-04-09 12:23:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-09 12:23:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-365093870-522575263 will have desired state: Ready
2022-04-09 12:23:54 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringKafkaRollingUpdate-FINISHED
2022-04-09 12:23:54 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:25:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-365093870-522575263 is in desired state: Ready
2022-04-09 12:25:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@471f01eb, messages=[], arguments=[--max-messages, 100, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-365093870-522575263], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-365093870-522575263', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@220a65dd}
2022-04-09 12:25:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-365093870-522575263 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:25:38 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-365093870-522575263
2022-04-09 12:25:42 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 12:25:42 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 12:25:42 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4fd65cc4, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-727572249, --group-instance-id, instance668682626, USER=my_user_207449775_272185985, --bootstrap-server, my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-365093870-522575263], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275', podNamespace='namespace-84', bootstrapServer='my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-365093870-522575263', maxMessages=100, kafkaUsername='my-user-207449775-272185985', consumerGroupName='my-consumer-group-727572249', consumerInstanceId='instance668682626', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3e03f254}
2022-04-09 12:25:42 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093:my-topic-365093870-522575263 from pod my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275
2022-04-09 12:25:42 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bc2f1d68-kafka-clients-5b86bb7c59-w6275 -n namespace-84 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-727572249 --group-instance-id instance668682626 USER=my_user_207449775_272185985 --bootstrap-server my-cluster-bc2f1d68-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-365093870-522575263
2022-04-09 12:25:49 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 12:25:49 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 12:25:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:25:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndZookeeperScaleUpScaleDown
2022-04-09 12:25:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1217380909-1781660153 in namespace namespace-84
2022-04-09 12:25:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-207449775-272185985 in namespace namespace-84
2022-04-09 12:25:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bc2f1d68-kafka-clients in namespace namespace-84
2022-04-09 12:25:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-bc2f1d68 in namespace namespace-84
2022-04-09 12:25:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-365093870-522575263 in namespace namespace-84
2022-04-09 12:26:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:26:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-84 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-04-09 12:26:45 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-FINISHED
2022-04-09 12:26:45 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testMetricsChange-STARTED
2022-04-09 12:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a0d22038 in namespace rolling-update-st
2022-04-09 12:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a0d22038 will have desired state: Ready
2022-04-09 12:28:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a0d22038 is in desired state: Ready
2022-04-09 12:28:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a0d22038-kafka-clients in namespace rolling-update-st
2022-04-09 12:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:755] Check if metrics are present in pod of Kafka and Zookeeper
2022-04-09 12:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:28:32 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:28:33 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:28:33 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.8 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.7 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:765] Changing metrics to something else
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-a0d22038-zookeeper are stable
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:28:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:28:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:28:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:28:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:28:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:28:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:28:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:28:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:28:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:28:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:28:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:28:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:29:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:29:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:29:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:29:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:29:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:29:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:29:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:29:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:29:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:29:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:29:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:29:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:29:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:29:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:29:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:29:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:29:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:29:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-a0d22038-zookeeper-0 ,my-cluster-a0d22038-zookeeper-1 ,my-cluster-a0d22038-zookeeper-2
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-a0d22038-kafka are stable
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 12:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:29:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 12:29:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:29:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:29:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:29:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:29:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 12:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:29:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 12:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 12:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:29:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 12:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 12:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:29:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 12:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 12:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 12:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 12:29:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:29:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:29:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:29:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:29:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 12:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:29:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 12:29:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:29:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:29:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:29:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:29:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 12:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 12:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 12:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 12:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 12:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 12:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 12:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 12:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 12:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 12:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 12:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 12:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 12:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 12:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 12:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 12:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 12:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 12:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 12:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 12:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 12:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 12:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 12:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 12:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 12:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 12:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 12:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 12:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 12:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 12:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 12:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 12:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 12:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:30:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 12:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 12:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-a0d22038-kafka-0 ,my-cluster-a0d22038-kafka-1 ,my-cluster-a0d22038-kafka-2 ,my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr ,my-cluster-a0d22038-kafka-exporter-88d6c7444-m8mmv
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:800] Check if Kafka and Zookeeper pods didn't roll
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:804] Check if Kafka and Zookeeper metrics are changed
2022-04-09 12:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:818] Check if metrics are present in pod of Kafka and Zookeeper
2022-04-09 12:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.8 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.7 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 0
2022-04-09 12:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:829] Removing metrics from Kafka and Zookeeper and setting them to null
2022-04-09 12:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:836] Wait if Kafka and Zookeeper pods will roll
2022-04-09 12:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a0d22038-zookeeper rolling update
2022-04-09 12:31:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a0d22038-zookeeper has been successfully rolled
2022-04-09 12:31:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-a0d22038-zookeeper to be ready
2022-04-09 12:31:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a0d22038-kafka rolling update
2022-04-09 12:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a0d22038-kafka has been successfully rolled
2022-04-09 12:32:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-a0d22038-kafka to be ready
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a0d22038 will have desired state: Ready
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a0d22038 is in desired state: Ready
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-a0d22038 is ready
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:840] Check if metrics are not existing in pods
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 7
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 7
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 7
2022-04-09 12:33:10 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 7
2022-04-09 12:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.8 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 7
2022-04-09 12:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.7 from Pod my-cluster-a0d22038-kafka-clients-564c57d44c-9z5vr finished with return code: 7
2022-04-09 12:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMetricsChange
2022-04-09 12:33:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a0d22038-kafka-clients in namespace rolling-update-st
2022-04-09 12:33:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a0d22038 in namespace rolling-update-st
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testMetricsChange-FINISHED
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testClusterOperatorFinishAllRollingUpdates-STARTED
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6eacf695 in namespace rolling-update-st
2022-04-09 12:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6eacf695 will have desired state: Ready
2022-04-09 12:35:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6eacf695 is in desired state: Ready
2022-04-09 12:35:49 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:630] Deleting Cluster Operator pod with labels LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})
2022-04-09 12:35:49 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:632] Cluster Operator pod deleted
2022-04-09 12:35:49 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-6eacf695-zookeeper rolling update
2022-04-09 12:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-6eacf695-zookeeper has been successfully rolled
2022-04-09 12:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-6eacf695-zookeeper to be ready
2022-04-09 12:37:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6eacf695 will have desired state: Ready
2022-04-09 12:37:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6eacf695 is in desired state: Ready
2022-04-09 12:37:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-6eacf695 is ready
2022-04-09 12:37:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:639] Deleting Cluster Operator pod with labels LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})
2022-04-09 12:37:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:641] Cluster Operator pod deleted
2022-04-09 12:37:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-6eacf695-kafka rolling update
2022-04-09 12:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-6eacf695-kafka has been successfully rolled
2022-04-09 12:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-6eacf695-kafka to be ready
2022-04-09 12:39:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6eacf695 will have desired state: Ready
2022-04-09 12:39:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6eacf695 is in desired state: Ready
2022-04-09 12:39:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-6eacf695 is ready
2022-04-09 12:39:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:39:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClusterOperatorFinishAllRollingUpdates
2022-04-09 12:39:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6eacf695 in namespace rolling-update-st
2022-04-09 12:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testClusterOperatorFinishAllRollingUpdates-FINISHED
2022-04-09 12:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context RollingUpdateST is everything deleted.
2022-04-09 12:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,872.549 s - in io.strimzi.systemtest.rollingupdate.RollingUpdateST
[[1;34mINFO[m] Running io.strimzi.systemtest.log.LoggingChangeST
2022-04-09 12:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: logging-change-st
2022-04-09 12:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: logging-change-st
2022-04-09 12:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: logging-change-st
2022-04-09 12:40:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:40:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testMM2LoggingLevelsHierarchy-STARTED
2022-04-09 12:40:25 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:40:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:40:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:40:25 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testNotExistingCMSetsDefaultLogging-STARTED
2022-04-09 12:40:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetConnectLoggingLevels-STARTED
2022-04-09 12:40:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLogger-STARTED
2022-04-09 12:40:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:40:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaExternalLogging-STARTED
2022-04-09 12:40:25 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:40:25 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-85 for test case:testMM2LoggingLevelsHierarchy
2022-04-09 12:40:25 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-85
2022-04-09 12:40:26 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-85
2022-04-09 12:40:26 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-85
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-86 for test case:testDynamicallySetConnectLoggingLevels
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-86
2022-04-09 12:40:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-02719a3b-source in namespace namespace-85
2022-04-09 12:40:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-85
2022-04-09 12:40:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-02719a3b-source will have desired state: Ready
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-86
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-86
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-87 for test case:testDynamicallySetKafkaExternalLogging
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-87
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-948d87fc in namespace namespace-86
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-86
2022-04-09 12:40:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-948d87fc will have desired state: Ready
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-87
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-87
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-88 for test case:testDynamicallySetUnknownKafkaLogger
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-88
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0a627881 in namespace namespace-87
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-87
2022-04-09 12:40:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0a627881 will have desired state: Ready
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-88
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-88
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-89 for test case:testNotExistingCMSetsDefaultLogging
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-89
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4f3ceb6d in namespace namespace-88
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-88
2022-04-09 12:40:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4f3ceb6d will have desired state: Ready
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-89
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-89
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestUtils:197] /home/ec2-user/strimzi-kafka-operator/systemtest/../cluster-operator/src/main/resources/kafkaDefaultLoggingProperties
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:1320] Deploying Kafka with custom logging
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3ba00c96 in namespace namespace-89
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-89
2022-04-09 12:40:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3ba00c96 will have desired state: Ready
2022-04-09 12:42:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4f3ceb6d is in desired state: Ready
2022-04-09 12:42:31 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4f3ceb6d-kafka rolling update
2022-04-09 12:42:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0a627881 is in desired state: Ready
2022-04-09 12:42:47 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 12:42:48 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 12:42:49 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 12:42:50 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 12:42:51 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 12:42:52 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 12:42:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-02719a3b-source is in desired state: Ready
2022-04-09 12:42:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-02719a3b-target in namespace namespace-89
2022-04-09 12:42:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-85
2022-04-09 12:42:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-02719a3b-target will have desired state: Ready
2022-04-09 12:42:53 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 12:42:54 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 12:42:55 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 12:42:56 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 12:42:57 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 12:42:58 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 12:42:59 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 12:43:00 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 12:43:01 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 12:43:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-948d87fc is in desired state: Ready
2022-04-09 12:43:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-948d87fc-scraper in namespace namespace-86
2022-04-09 12:43:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-86
2022-04-09 12:43:02 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-948d87fc-scraper will be ready
2022-04-09 12:43:02 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 12:43:03 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 12:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-948d87fc-scraper is ready
2022-04-09 12:43:04 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-948d87fc-scraper to be ready
2022-04-09 12:43:05 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 12:43:06 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 12:43:07 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 12:43:08 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 12:43:09 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 12:43:10 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 12:43:11 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 12:43:12 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 12:43:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3ba00c96 is in desired state: Ready
2022-04-09 12:43:12 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:1344] Changing external logging's CM to not existing one
2022-04-09 12:43:12 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 12:43:13 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 12:43:13 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 12:43:14 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 12:43:14 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-948d87fc-scraper is ready
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-948d87fc-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-948d87fc-allow in namespace namespace-86
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-86
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-948d87fc in namespace namespace-89
2022-04-09 12:43:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-86
2022-04-09 12:43:15 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-948d87fc will have desired state: Ready
2022-04-09 12:43:15 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 12:43:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 12:43:16 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 12:43:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 12:43:17 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 12:43:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 12:43:18 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 12:43:18 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 12:43:19 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 12:43:19 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 12:43:20 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 12:43:20 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 12:43:21 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 12:43:21 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 12:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 12:43:22 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 12:43:23 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 12:43:23 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 12:43:24 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 12:43:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 12:43:25 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 12:43:25 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 12:43:26 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 12:43:26 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 12:43:27 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 12:43:27 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 12:43:28 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 12:43:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 12:43:29 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 12:43:29 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 12:43:30 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 12:43:30 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 12:43:31 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 12:43:31 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 12:43:32 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 12:43:32 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 12:43:33 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 12:43:33 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 12:43:34 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 12:43:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 12:43:35 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 12:43:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 12:43:36 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 12:43:36 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 12:43:37 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 12:43:37 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 12:43:38 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0a627881-kafka-0=e361ac99-3e96-436c-bc1b-a900940be600, my-cluster-0a627881-kafka-2=1c03bf3d-20d1-43f8-a0c6-f9d8fad83423, my-cluster-0a627881-kafka-1=ef83ad82-e954-432b-a017-6eaa713ad2dd} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 12:43:38 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 12:43:39 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 12:43:40 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 12:43:41 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-0a627881-kafka rolling update
2022-04-09 12:43:41 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 12:43:42 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 12:43:43 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 12:43:44 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 12:43:45 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 12:43:46 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 12:43:47 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 12:43:48 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 12:43:49 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 12:43:50 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 12:43:51 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 12:43:52 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 12:43:53 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 12:43:54 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 12:43:55 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 12:43:56 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 12:43:56 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4f3ceb6d-kafka has been successfully rolled
2022-04-09 12:43:57 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 12:43:58 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 12:43:59 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 12:44:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 12:44:01 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 12:44:02 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3ba00c96-kafka-1=cf258b3f-6ecb-4111-90cf-4e863d9d7ea0, my-cluster-3ba00c96-kafka-0=cc62aa08-66d6-43a4-a569-fc43abf97216, my-cluster-3ba00c96-kafka-2=72bb4be4-2a04-4aa0-a156-df186376bcdc} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 12:44:02 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:1358] Checking that log4j.properties in custom-config isn't empty and configuration is default
2022-04-09 12:44:03 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:1366] Checking if Kafka:my-cluster-3ba00c96 contains error about non-existing CM
2022-04-09 12:44:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:44:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testNotExistingCMSetsDefaultLogging
2022-04-09 12:44:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3ba00c96 in namespace namespace-89
2022-04-09 12:44:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:44:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetUnknownKafkaLogger
2022-04-09 12:44:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4f3ceb6d in namespace namespace-88
2022-04-09 12:44:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:44:13 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-89 for test case:testNotExistingCMSetsDefaultLogging
2022-04-09 12:44:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-02719a3b-target is in desired state: Ready
2022-04-09 12:44:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-02719a3b-kafka-clients in namespace namespace-89
2022-04-09 12:44:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-85
2022-04-09 12:44:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-02719a3b in namespace namespace-89
2022-04-09 12:44:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-85
2022-04-09 12:44:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-02719a3b will have desired state: Ready
2022-04-09 12:44:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:44:20 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-88 for test case:testDynamicallySetUnknownKafkaLogger
2022-04-09 12:44:23 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:44:23 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testLoggingHierarchy-STARTED
2022-04-09 12:44:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-948d87fc is in desired state: Ready
2022-04-09 12:44:23 [ForkJoinPool-3-worker-9] [32mINFO [m [LoggingChangeST:703] Asserting if log is without records
2022-04-09 12:44:23 [ForkJoinPool-3-worker-9] [32mINFO [m [LoggingChangeST:706] Changing rootLogger level to DEBUG with inline logging
2022-04-09 12:44:23 [ForkJoinPool-3-worker-9] [32mINFO [m [LoggingChangeST:715] Waiting for log4j.properties will contain desired settings
2022-04-09 12:44:23 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-86 exec my-cluster-948d87fc-scraper-7fc5bcd8d4-svgmh -- curl http://my-cluster-948d87fc-connect-api:8083/admin/loggers/root
2022-04-09 12:44:23 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-86 exec my-cluster-948d87fc-scraper-7fc5bcd8d4-svgmh -- curl http://my-cluster-948d87fc-connect-api:8083/admin/loggers/root
2022-04-09 12:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:44:25 [ForkJoinPool-3-worker-9] [32mINFO [m [LoggingChangeST:760] Setting log level of Connect to OFF
2022-04-09 12:44:26 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-86 exec my-cluster-948d87fc-scraper-7fc5bcd8d4-svgmh -- curl http://my-cluster-948d87fc-connect-api:8083/admin/loggers/root
2022-04-09 12:44:26 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:44:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:44:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testJSONFormatLogging-STARTED
2022-04-09 12:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-86 exec my-cluster-948d87fc-scraper-7fc5bcd8d4-svgmh -- curl http://my-cluster-948d87fc-connect-api:8083/admin/loggers/root
2022-04-09 12:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-90 for test case:testLoggingHierarchy
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-90
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-90
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-90
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1170b0fd in namespace namespace-90
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-90
2022-04-09 12:44:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1170b0fd will have desired state: Ready
2022-04-09 12:44:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testNotExistingCMSetsDefaultLogging-FINISHED
2022-04-09 12:44:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:44:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:44:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaLoggingLevels-STARTED
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-91 for test case:testJSONFormatLogging
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-91
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-91
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-91
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cee158d7 in namespace namespace-91
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-91
2022-04-09 12:44:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cee158d7 will have desired state: Ready
2022-04-09 12:44:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:44:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetConnectLoggingLevels
2022-04-09 12:44:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-948d87fc-allow in namespace namespace-86
2022-04-09 12:44:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-948d87fc in namespace namespace-86
2022-04-09 12:45:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLogger-FINISHED
2022-04-09 12:45:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:45:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:45:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetEOloggingLevels-STARTED
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-92 for test case:testDynamicallySetKafkaLoggingLevels
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-92
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-92
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-92
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f41bb1a2 in namespace namespace-92
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-92
2022-04-09 12:45:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f41bb1a2 will have desired state: Ready
2022-04-09 12:45:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-948d87fc-scraper in namespace namespace-86
2022-04-09 12:45:16 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-0a627881-kafka has been successfully rolled
2022-04-09 12:45:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-02719a3b is in desired state: Ready
2022-04-09 12:45:24 [ForkJoinPool-3-worker-11] [32mINFO [m [LoggingChangeST:1253] Waiting for log4j.properties will contain desired settings
2022-04-09 12:45:24 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:24 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:24 [ForkJoinPool-3-worker-11] [32mINFO [m [LoggingChangeST:1258] Changing log levels
2022-04-09 12:45:25 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:25 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:26 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:26 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:27 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:27 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:29 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:29 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:30 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:30 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:31 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:31 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:33 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:33 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:34 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:34 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:35 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:35 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:37 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:37 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:38 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:39 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:39 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:40 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:40 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:42 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:42 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:43 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:43 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:44 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:44 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:45 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:45 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-948d87fc in namespace namespace-86
2022-04-09 12:45:47 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:47 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:48 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:48 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1170b0fd is in desired state: Ready
2022-04-09 12:45:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1170b0fd-scraper in namespace namespace-90
2022-04-09 12:45:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-90
2022-04-09 12:45:48 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1170b0fd-scraper will be ready
2022-04-09 12:45:49 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:49 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:50 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1170b0fd-scraper is ready
2022-04-09 12:45:50 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-1170b0fd-scraper to be ready
2022-04-09 12:45:51 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:51 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:52 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:52 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:53 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:45:53 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:53 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/org.eclipse.jetty.util.thread.strategy.EatWhatYouKill
2022-04-09 12:45:53 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:54 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-85 exec my-cluster-02719a3b-mirrormaker2-6c6669758c-4zl5h -- curl http://localhost:8083/admin/loggers/org.apache.kafka.connect.runtime.WorkerTask
2022-04-09 12:45:54 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:45:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:45:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testMM2LoggingLevelsHierarchy
2022-04-09 12:45:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-02719a3b-kafka-clients in namespace namespace-85
2022-04-09 12:45:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:45:56 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-86 for test case:testDynamicallySetConnectLoggingLevels
2022-04-09 12:45:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:45:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetKafkaExternalLogging
2022-04-09 12:45:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0a627881 in namespace namespace-87
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-1170b0fd-scraper is ready
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1170b0fd-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-1170b0fd-allow in namespace namespace-90
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-90
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-1170b0fd in namespace namespace-92
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-90
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-1170b0fd in namespace namespace-92
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-90
2022-04-09 12:46:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-1170b0fd will have desired state: Ready
2022-04-09 12:46:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:46:08 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-87 for test case:testDynamicallySetKafkaExternalLogging
2022-04-09 12:46:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetConnectLoggingLevels-FINISHED
2022-04-09 12:46:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:46:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:46:24 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLoggerValue-STARTED
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-93 for test case:testDynamicallySetEOloggingLevels
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-93
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-93
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-93
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1874d8a2 in namespace namespace-93
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-93
2022-04-09 12:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1874d8a2 will have desired state: Ready
2022-04-09 12:46:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-02719a3b in namespace namespace-85
2022-04-09 12:46:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-02719a3b-target in namespace namespace-85
2022-04-09 12:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaExternalLogging-FINISHED
2022-04-09 12:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:46:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetBridgeLoggingLevels-STARTED
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cee158d7 is in desired state: Ready
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: strimzi-cluster-operator-78689684d4-l9685
2022-04-09 12:46:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-02719a3b-source in namespace namespace-85
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-kafka-2
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-kafka-1
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-kafka-0
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-zookeeper-2
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-94 for test case:testDynamicallySetUnknownKafkaLoggerValue
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-94
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-zookeeper-1
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-94
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-94
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dda55f54 in namespace namespace-94
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-94
2022-04-09 12:46:54 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dda55f54 will have desired state: Ready
2022-04-09 12:46:54 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-zookeeper-0
2022-04-09 12:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-entity-operator-5974f54d65-z6wsz
2022-04-09 12:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-cee158d7-entity-operator-5974f54d65-z6wsz
2022-04-09 12:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testJSONFormatLogging
2022-04-09 12:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cee158d7 in namespace namespace-91
2022-04-09 12:47:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:47:04 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-85 for test case:testMM2LoggingLevelsHierarchy
2022-04-09 12:47:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:47:05 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-91 for test case:testJSONFormatLogging
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-1170b0fd is in desired state: Ready
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-1170b0fd will have desired state: Ready
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-1170b0fd is in desired state: Ready
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [LoggingChangeST:1398] Changing rootLogger level in KafkaConnector to ERROR with inline logging
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [LoggingChangeST:1404] Waiting for Connect API loggers will contain desired settings
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [LoggingChangeST:1410] Restarting Kafka connector my-cluster-1170b0fd with class name org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl -X POST http://my-cluster-1170b0fd-connect-api:8083/connectors/my-cluster-1170b0fd/restart
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:10 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:168] Wait until KafkaConnector my-cluster-1170b0fd's worker will be in RUNNING state
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl GET http://my-cluster-1170b0fd-connect-api:8083/connectors/my-cluster-1170b0fd/status
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [LoggingChangeST:1417] Checking that logger is same for connector with class name org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [LoggingChangeST:1423] Changing KafkaConnect's root logger to WARN, KafkaConnector: my-cluster-1170b0fd shouldn't inherit it
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/root
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [LoggingChangeST:1437] Checking if KafkaConnector org.apache.kafka.connect.file.FileStreamSourceConnector doesn't inherit logger from KafkaConnect
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:11 [ForkJoinPool-3-worker-13] [33mWARN [m [KafkaConnectorUtils:191] Logger level has changed: {"level":"WARN"}
. Reseting counter from 0 to 0
2022-04-09 12:47:13 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:13 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:13 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 49
2022-04-09 12:47:14 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:14 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:14 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 48
2022-04-09 12:47:15 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:15 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:15 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 47
2022-04-09 12:47:16 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:16 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:16 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 46
2022-04-09 12:47:18 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:18 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:18 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 45
2022-04-09 12:47:19 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:19 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:19 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 44
2022-04-09 12:47:20 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:20 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:20 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 43
2022-04-09 12:47:21 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:21 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:21 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 42
2022-04-09 12:47:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f41bb1a2 is in desired state: Ready
2022-04-09 12:47:22 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:828] Changing rootLogger level to DEBUG with inline logging
2022-04-09 12:47:22 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:835] Waiting for dynamic change in the kafka pod
2022-04-09 12:47:22 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:22 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:22 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 41
2022-04-09 12:47:24 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:24 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:24 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 40
2022-04-09 12:47:25 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:25 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:25 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 39
2022-04-09 12:47:26 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:26 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:26 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 38
2022-04-09 12:47:27 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:27 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:27 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 37
2022-04-09 12:47:28 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:28 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:28 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 36
2022-04-09 12:47:29 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:853] Setting external logging INFO
2022-04-09 12:47:29 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:889] Setting log level of kafka INFO
2022-04-09 12:47:29 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:895] Waiting for dynamic change in the kafka pod
2022-04-09 12:47:30 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:30 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:30 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 35
2022-04-09 12:47:31 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:31 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:31 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 34
2022-04-09 12:47:32 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:32 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:32 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 33
2022-04-09 12:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testJSONFormatLogging-FINISHED
2022-04-09 12:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetMM2LoggingLevels-STARTED
2022-04-09 12:47:32 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testMM2LoggingLevelsHierarchy-FINISHED
2022-04-09 12:47:32 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:47:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:47:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetKafkaLoggingLevels
2022-04-09 12:47:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f41bb1a2 in namespace namespace-92
2022-04-09 12:47:33 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:33 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:33 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 32
2022-04-09 12:47:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:35 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:35 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 31
2022-04-09 12:47:36 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:36 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:36 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 30
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-95 for test case:testDynamicallySetBridgeLoggingLevels
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1568a47f in namespace namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1568a47f-kafka-clients in namespace namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-1568a47f in namespace namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-09 12:47:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1568a47f will have desired state: Ready
2022-04-09 12:47:37 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:37 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:37 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 29
2022-04-09 12:47:38 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:38 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:38 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 28
2022-04-09 12:47:40 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:40 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:40 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 27
2022-04-09 12:47:41 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:41 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:41 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 26
2022-04-09 12:47:42 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:42 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:42 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 25
2022-04-09 12:47:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:47:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-92 for test case:testDynamicallySetKafkaLoggingLevels
2022-04-09 12:47:43 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:43 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:43 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 24
2022-04-09 12:47:44 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:44 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:44 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 23
2022-04-09 12:47:46 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:46 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:46 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 22
2022-04-09 12:47:47 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:47 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:47 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 21
2022-04-09 12:47:48 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:48 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:48 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 20
2022-04-09 12:47:49 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:49 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:49 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 19
2022-04-09 12:47:50 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:50 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:50 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 18
2022-04-09 12:47:52 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:52 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:52 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 17
2022-04-09 12:47:53 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:53 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:53 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 16
2022-04-09 12:47:54 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:54 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:54 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 15
2022-04-09 12:47:56 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:56 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:56 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 14
2022-04-09 12:47:57 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:57 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:57 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 13
2022-04-09 12:47:58 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:58 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:58 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 12
2022-04-09 12:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:47:59 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 11
2022-04-09 12:48:01 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:01 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:01 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 10
2022-04-09 12:48:02 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:02 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:02 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 9
2022-04-09 12:48:03 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:03 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:03 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 8
2022-04-09 12:48:05 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:05 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:05 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 7
2022-04-09 12:48:06 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:06 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:06 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 6
2022-04-09 12:48:07 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:07 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:07 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 5
2022-04-09 12:48:08 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:08 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:08 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 4
2022-04-09 12:48:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:10 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:10 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 3
2022-04-09 12:48:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:11 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:11 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 2
2022-04-09 12:48:12 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:12 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:12 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 1
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-90 exec my-cluster-1170b0fd-scraper-655749d6d5-gqczf -- curl http://my-cluster-1170b0fd-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 0
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:196] Logger for connector org.apache.kafka.connect.file.FileStreamSourceConnector is stable
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testLoggingHierarchy
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-1170b0fd-allow in namespace namespace-90
2022-04-09 12:48:13 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-1170b0fd in namespace namespace-90
2022-04-09 12:48:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1874d8a2 is in desired state: Ready
2022-04-09 12:48:15 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:285] Checking if EO pod contains any log (except configuration)
2022-04-09 12:48:16 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:288] Changing rootLogger level to DEBUG with inline logging
2022-04-09 12:48:16 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:296] Waiting for log4j2.properties will contain desired settings
2022-04-09 12:48:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1170b0fd-scraper in namespace namespace-90
2022-04-09 12:48:23 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-1170b0fd in namespace namespace-90
2022-04-09 12:48:24 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaLoggingLevels-FINISHED
2022-04-09 12:48:24 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:48:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1170b0fd in namespace namespace-90
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-96 for test case:testDynamicallySetMM2LoggingLevels
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-96
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-96
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-96
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1c9f4ba5-source in namespace namespace-96
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-09 12:48:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1c9f4ba5-source will have desired state: Ready
2022-04-09 12:48:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dda55f54 is in desired state: Ready
2022-04-09 12:48:46 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 50
2022-04-09 12:48:47 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 49
2022-04-09 12:48:48 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 48
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1568a47f is in desired state: Ready
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1568a47f-kafka-clients will be ready
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1568a47f-kafka-clients is ready
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-1568a47f will have desired state: Ready
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-1568a47f is in desired state: Ready
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:485] Asserting if log is without records
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:488] Changing rootLogger level to DEBUG with inline logging
2022-04-09 12:48:49 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 47
2022-04-09 12:48:49 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:500] Waiting for log4j2.properties will contain desired settings
2022-04-09 12:48:50 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 46
2022-04-09 12:48:51 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 45
2022-04-09 12:48:52 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 44
2022-04-09 12:48:53 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 43
2022-04-09 12:48:54 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 42
2022-04-09 12:48:55 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 41
2022-04-09 12:48:56 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 40
2022-04-09 12:48:57 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 39
2022-04-09 12:48:58 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 38
2022-04-09 12:48:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:48:58 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-90 for test case:testLoggingHierarchy
2022-04-09 12:48:59 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 37
2022-04-09 12:49:00 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 36
2022-04-09 12:49:01 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 35
2022-04-09 12:49:02 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 34
2022-04-09 12:49:03 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 33
2022-04-09 12:49:04 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testLoggingHierarchy-FINISHED
2022-04-09 12:49:04 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:49:04 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 32
2022-04-09 12:49:05 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 31
2022-04-09 12:49:06 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 30
2022-04-09 12:49:07 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 29
2022-04-09 12:49:08 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 28
2022-04-09 12:49:09 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 27
2022-04-09 12:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 26
2022-04-09 12:49:11 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 25
2022-04-09 12:49:12 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 24
2022-04-09 12:49:13 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 23
2022-04-09 12:49:14 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 22
2022-04-09 12:49:15 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 21
2022-04-09 12:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 20
2022-04-09 12:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 19
2022-04-09 12:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 18
2022-04-09 12:49:19 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 17
2022-04-09 12:49:20 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 16
2022-04-09 12:49:21 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 15
2022-04-09 12:49:22 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 14
2022-04-09 12:49:23 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 13
2022-04-09 12:49:24 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 12
2022-04-09 12:49:25 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 11
2022-04-09 12:49:26 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:313] Setting external logging OFF
2022-04-09 12:49:26 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:371] Setting log level of TO and UO to OFF - records should not appear in log
2022-04-09 12:49:26 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 10
2022-04-09 12:49:26 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:378] Waiting for log4j2.properties will contain desired settings
2022-04-09 12:49:27 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 9
2022-04-09 12:49:28 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 8
2022-04-09 12:49:29 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 7
2022-04-09 12:49:30 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 6
2022-04-09 12:49:31 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 5
2022-04-09 12:49:32 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 4
2022-04-09 12:49:33 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 3
2022-04-09 12:49:34 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 2
2022-04-09 12:49:35 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 1
2022-04-09 12:49:36 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-dda55f54-kafka-2=47be4b48-f21e-4f28-bb25-359d3072290d, my-cluster-dda55f54-kafka-0=aee1d409-728a-4548-b177-015ebed52755, my-cluster-dda55f54-kafka-1=2ea8dca2-dd01-4e7f-b5cc-a37fa6d6a8ee} pods didn't roll. Remaining seconds for stability: 0
2022-04-09 12:49:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:49:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetUnknownKafkaLoggerValue
2022-04-09 12:49:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-dda55f54 in namespace namespace-94
2022-04-09 12:49:46 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:49:46 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-94 for test case:testDynamicallySetUnknownKafkaLoggerValue
2022-04-09 12:49:48 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:557] Setting log level of Bridge to OFF - records should not appear in the log
2022-04-09 12:49:48 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:563] Waiting for log4j2.properties will contain desired settings
2022-04-09 12:50:30 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLoggerValue-FINISHED
2022-04-09 12:50:30 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:50:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1c9f4ba5-source is in desired state: Ready
2022-04-09 12:50:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1c9f4ba5-target in namespace namespace-96
2022-04-09 12:50:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-09 12:50:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1c9f4ba5-target will have desired state: Ready
2022-04-09 12:50:50 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:396] Setting external logging OFF
2022-04-09 12:50:50 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:432] Waiting for log4j2.properties will contain desired settings
2022-04-09 12:51:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:51:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetBridgeLoggingLevels
2022-04-09 12:51:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1568a47f-kafka-clients in namespace namespace-95
2022-04-09 12:51:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-1568a47f in namespace namespace-95
2022-04-09 12:51:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1568a47f in namespace namespace-95
2022-04-09 12:51:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:51:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetEOloggingLevels
2022-04-09 12:51:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1874d8a2 in namespace namespace-93
2022-04-09 12:52:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:52:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-93 for test case:testDynamicallySetEOloggingLevels
2022-04-09 12:52:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1c9f4ba5-target is in desired state: Ready
2022-04-09 12:52:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1c9f4ba5-kafka-clients in namespace namespace-96
2022-04-09 12:52:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-09 12:52:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-1c9f4ba5 in namespace namespace-96
2022-04-09 12:52:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-09 12:52:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-1c9f4ba5 will have desired state: Ready
2022-04-09 12:52:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:52:08 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-95 for test case:testDynamicallySetBridgeLoggingLevels
2022-04-09 12:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetBridgeLoggingLevels-FINISHED
2022-04-09 12:52:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:52:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetEOloggingLevels-FINISHED
2022-04-09 12:52:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-1c9f4ba5 is in desired state: Ready
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:1123] Changing rootLogger level to DEBUG with inline logging
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:1132] Waiting for log4j.properties will contain desired settings
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-96 exec my-cluster-1c9f4ba5-mirrormaker2-7bf7d79f8c-rn46k -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:1176] Setting log level of MM2 to OFF
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-96 exec my-cluster-1c9f4ba5-mirrormaker2-7bf7d79f8c-rn46k -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:53:16 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:53:18 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-96 exec my-cluster-1c9f4ba5-mirrormaker2-7bf7d79f8c-rn46k -- curl http://localhost:8083/admin/loggers/root
2022-04-09 12:53:18 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:53:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:53:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetMM2LoggingLevels
2022-04-09 12:53:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1c9f4ba5-kafka-clients in namespace namespace-96
2022-04-09 12:53:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-1c9f4ba5 in namespace namespace-96
2022-04-09 12:53:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1c9f4ba5-source in namespace namespace-96
2022-04-09 12:53:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1c9f4ba5-target in namespace namespace-96
2022-04-09 12:54:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:54:08 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-96 for test case:testDynamicallySetMM2LoggingLevels
2022-04-09 12:54:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetMM2LoggingLevels-FINISHED
2022-04-09 12:54:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:54:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:54:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels-STARTED
2022-04-09 12:54:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:54:14 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:618] Checking that original logging config is different from the new one
2022-04-09 12:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:621] Changing logging for cluster-operator
2022-04-09 12:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:624] Waiting for log4j2.properties will contain desired settings
2022-04-09 12:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:50 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:50 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:55 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:55 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:54:59 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:54:59 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:629] Checking log4j2.properties in CO pod
2022-04-09 12:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-l9685 -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-09 12:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 12:55:32 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:633] Checking if CO rolled its pod
2022-04-09 12:55:35 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:37 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:40 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:43 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:45 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:48 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:51 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 74085000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508920,
    "nanoOfSecond" : 449755000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:53 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508922,
    "nanoOfSecond" : 919591000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:56 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:55:59 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:01 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:04 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:07 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:09 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:12 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:15 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:17 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:20 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:23 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508952,
    "nanoOfSecond" : 919263000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:25 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}

2022-04-09 12:56:28 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:31 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:33 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:36 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:39 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:41 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:44 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:47 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:49 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

2022-04-09 12:56:52 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] {
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 74017000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508980,
    "nanoOfSecond" : 449662000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-96/my-cluster-1c9f4ba5-target)"
  },
  "message" : "Reconciliation #277(watch) Kafka(namespace-96/my-cluster-1c9f4ba5-target): Reconciliation is in progress",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508982,
    "nanoOfSecond" : 915775000
  },
  "thread" : "vert.x-eventloop-thread-1",
  "level" : "INFO",
  "loggerName" : "io.strimzi.operator.cluster.ClusterOperator",
  "message" : "Triggering periodic reconciliation for namespace *",
  "endOfBatch" : false,
  "loggerFqcn" : "org.apache.logging.log4j.spi.AbstractLogger",
  "threadId" : 26,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954556000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.Util",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 954837000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): createOrUpdate failed",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 957113000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "ERROR",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Current Kafka resource not found",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959103000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Removed metric strimzi.resource.statenamespace-95:Kafka/my-cluster-1568a47f",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959347000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "WARN",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Failed to reconcile",
  "thrown" : {
    "commonElementCount" : 0,
    "localizedMessage" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "message" : "Exceeded timeout of 300000ms while waiting for Pods resource my-cluster-1568a47f-zookeeper-0 in namespace namespace-95 to be ready",
    "name" : "io.strimzi.operator.common.operator.resource.TimeoutException",
    "extendedStackTrace" : [ {
      "class" : "io.strimzi.operator.common.Util$1",
      "method" : "lambda$handle$1",
      "file" : "Util.java",
      "line" : 154,
      "exact" : false,
      "location" : "io.strimzi.operator-common-0.29.0-SNAPSHOT.jar",
      "version" : "0.29.0-SNAPSHOT"
    }, {
      "class" : "io.vertx.core.impl.future.FutureImpl$3",
      "method" : "onFailure",
      "file" : "FutureImpl.java",
      "line" : 153,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.vertx.core.impl.future.FutureBase",
      "method" : "lambda$emitFailure$1",
      "file" : "FutureBase.java",
      "line" : 69,
      "exact" : false,
      "location" : "io.vertx.vertx-core-4.2.4.jar",
      "version" : "4.2.4"
    }, {
      "class" : "io.netty.util.concurrent.AbstractEventExecutor",
      "method" : "safeExecute",
      "file" : "AbstractEventExecutor.java",
      "line" : 164,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor",
      "method" : "runAllTasks",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 469,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.channel.nio.NioEventLoop",
      "method" : "run",
      "file" : "NioEventLoop.java",
      "line" : 503,
      "exact" : false,
      "location" : "io.netty.netty-transport-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.SingleThreadEventExecutor$4",
      "method" : "run",
      "file" : "SingleThreadEventExecutor.java",
      "line" : 986,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.internal.ThreadExecutorMap$2",
      "method" : "run",
      "file" : "ThreadExecutorMap.java",
      "line" : 74,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "io.netty.util.concurrent.FastThreadLocalRunnable",
      "method" : "run",
      "file" : "FastThreadLocalRunnable.java",
      "line" : 30,
      "exact" : false,
      "location" : "io.netty.netty-common-4.1.74.Final.jar",
      "version" : "4.1.74.Final"
    }, {
      "class" : "java.lang.Thread",
      "method" : "run",
      "file" : "Thread.java",
      "line" : 829,
      "exact" : false,
      "location" : "?",
      "version" : "?"
    } ]
  },
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}
{
  "instant" : {
    "epochSecond" : 1649508985,
    "nanoOfSecond" : 959567000
  },
  "thread" : "vert.x-eventloop-thread-0",
  "level" : "DEBUG",
  "loggerName" : "io.strimzi.operator.common.AbstractOperator",
  "marker" : {
    "name" : "Kafka(namespace-95/my-cluster-1568a47f)"
  },
  "message" : "Reconciliation #247(watch) Kafka(namespace-95/my-cluster-1568a47f): Lock lock::namespace-95::Kafka::my-cluster-1568a47f released",
  "endOfBatch" : false,
  "loggerFqcn" : "io.strimzi.operator.common.ReconciliationLogger",
  "threadId" : 23,
  "threadPriority" : 5
}

io.strimzi.test.WaitException: Timeout after 80000 ms waiting for log to be empty
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels(LoggingChangeST.java:636)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 12:56:52 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestExecutionWatcher:28] LoggingChangeST - Exception Timeout after 80000 ms waiting for log to be empty has been thrown in @Test. Going to collect logs from components.
2022-04-09 12:56:52 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-09 12:56:52 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-09 12:56:52 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace logging-change-st
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace logging-change-st
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace logging-change-st
2022-04-09 12:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace logging-change-st
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace logging-change-st
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace logging-change-st
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace logging-change-st
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testDynamicallySetClusterOperatorLoggingLevels is everything deleted.
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels-FINISHED
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context LoggingChangeST is everything deleted.
2022-04-09 12:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 13, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1,007.214 s <<< FAILURE! - in io.strimzi.systemtest.log.LoggingChangeST
[[1;31mERROR[m] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels(ExtensionContext)  Time elapsed: 172.657 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 80000 ms waiting for log to be empty
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels(LoggingChangeST.java:636)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.log.LogSettingST
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: log-setting-st
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: log-setting-st
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: log-setting-st
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka log-setting-cluster-name in namespace log-setting-st
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka gc-set-logging in namespace log-setting-st
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment shared-kafka-clients in namespace log-setting-st
2022-04-09 12:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: log-setting-cluster-name will have desired state: Ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: log-setting-cluster-name is in desired state: Ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: gc-set-logging will have desired state: Ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: gc-set-logging is in desired state: Ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: shared-kafka-clients will be ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: shared-kafka-clients is ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:59:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:59:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testConnectLogSetting-STARTED
2022-04-09 12:59:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:59:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testBridgeLogSetting-STARTED
2022-04-09 12:59:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testMirrorMaker2LogSetting-STARTED
2022-04-09 12:59:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 12:59:18 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:59:18 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testMirrorMakerLogSetting-STARTED
2022-04-09 12:59:18 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:59:18 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:59:18 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 12:59:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-20ec4e61-bridge in namespace log-setting-st
2022-04-09 12:59:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-28d9ae81-mirror-maker in namespace log-setting-st
2022-04-09 12:59:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5e339abb-connect-scraper in namespace log-setting-st
2022-04-09 12:59:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-770f03b5-mirror-maker-2 in namespace log-setting-st
2022-04-09 12:59:18 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5e339abb-connect-scraper will be ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-20ec4e61-bridge will have desired state: Ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-28d9ae81-mirror-maker will have desired state: Ready
2022-04-09 12:59:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-770f03b5-mirror-maker-2 will have desired state: Ready
2022-04-09 12:59:21 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5e339abb-connect-scraper is ready
2022-04-09 12:59:21 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-5e339abb-connect-scraper to be ready
2022-04-09 12:59:31 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-5e339abb-connect-scraper is ready
2022-04-09 12:59:31 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-5e339abb-connect-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 12:59:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-5e339abb-connect-allow in namespace log-setting-st
2022-04-09 12:59:31 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 12:59:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-5e339abb-connect in namespace log-setting-st
2022-04-09 12:59:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-5e339abb-connect will have desired state: Ready
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-20ec4e61-bridge is in desired state: Ready
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:392] Checking if Bridge has log level set properly
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.subscribe.name Expected: http.openapi.operation.subscribe
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.createConsumer.name Expected: http.openapi.operation.createConsumer
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.createConsumer.level Expected: INFO
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.deleteConsumer.name Expected: http.openapi.operation.deleteConsumer
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.poll.name Expected: http.openapi.operation.poll
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.sendToPartition.level Expected: TRACE
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.unsubscribe.name Expected: http.openapi.operation.unsubscribe
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.unsubscribe.level Expected: DEBUG
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.ready.level Expected: WARN
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.assign.name Expected: http.openapi.operation.assign
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToEnd.name Expected: http.openapi.operation.seekToEnd
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.poll.level Expected: INFO
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.deleteConsumer.level Expected: DEBUG
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.subscribe.level Expected: TRACE
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.openapi.level Expected: TRACE
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.commit.name Expected: http.openapi.operation.commit
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.send.level Expected: ERROR
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seek.name Expected: http.openapi.operation.seek
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.healthy.level Expected: ERROR
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.ready.name Expected: http.openapi.operation.ready
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.logger.bridge.level Expected: ERROR
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seek.level Expected: INFO
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.assign.level Expected: TRACE
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.sendToPartition.name Expected: http.openapi.operation.sendToPartition
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.send.name Expected: http.openapi.operation.send
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.openapi.name Expected: http.openapi.operation.openapi
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.healthy.name Expected: http.openapi.operation.healthy
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.commit.level Expected: DEBUG
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToEnd.level Expected: WARN
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToBeginning.name Expected: http.openapi.operation.seekToBeginning
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToBeginning.level Expected: DEBUG
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-20ec4e61-bridge-bridge
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-20ec4e61-bridge-bridge
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=true
2022-04-09 12:59:40 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-20ec4e61-bridge-bridge rolling update
2022-04-09 13:00:10 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-20ec4e61-bridge-bridge will be ready
2022-04-09 13:00:10 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-20ec4e61-bridge-bridge is ready
2022-04-09 13:00:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-20ec4e61-bridge-bridge rolling update finished
2022-04-09 13:00:20 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-20ec4e61-bridge-bridge
2022-04-09 13:00:20 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-20ec4e61-bridge-bridge
2022-04-09 13:00:20 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-09 13:00:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-20ec4e61-bridge-bridge-7864c8458f-w842l container my-cluster-20ec4e61-bridge-bridge will be ready
2022-04-09 13:00:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:347] Pod my-cluster-20ec4e61-bridge-bridge-7864c8458f-w842l container my-cluster-20ec4e61-bridge-bridge is ready
2022-04-09 13:00:21 [ForkJoinPool-3-worker-5] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-20ec4e61-bridge-bridge-7864c8458f-w842l with container my-cluster-20ec4e61-bridge-bridge
2022-04-09 13:00:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:00:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testBridgeLogSetting
2022-04-09 13:00:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-20ec4e61-bridge in namespace log-setting-st
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-770f03b5-mirror-maker-2 is in desired state: Ready
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:357] Checking if MirrorMaker2 has log level set properly
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:476] Check log level setting for logger: mirrormaker.root.logger Expected: TRACE
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.mirrormaker.logger.level Expected: TRACE
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-770f03b5-mirror-maker-2-mirrormaker2
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-770f03b5-mirror-maker-2-mirrormaker2
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:00:21 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-770f03b5-mirror-maker-2-mirrormaker2 rolling update
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-28d9ae81-mirror-maker is in desired state: Ready
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:322] Checking if MirrorMaker has log level set properly
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:476] Check log level setting for logger: mirrormaker.root.logger Expected: TRACE
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.mirrormaker.logger.level Expected: TRACE
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-28d9ae81-mirror-maker-mirror-maker
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-28d9ae81-mirror-maker-mirror-maker
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:00:24 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-28d9ae81-mirror-maker-mirror-maker rolling update
2022-04-09 13:00:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:00:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testBridgeLogSetting-FINISHED
2022-04-09 13:00:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-5e339abb-connect is in desired state: Ready
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:287] Checking if Connect has log level set properly
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.I0Itec.zkclient Expected: ERROR
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:476] Check log level setting for logger: connect.root.logger.level Expected: INFO
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.connect.logger.level Expected: DEBUG
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.reflections Expected: WARN
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-5e339abb-connect-connect
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-5e339abb-connect-connect
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:00:40 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-5e339abb-connect-connect rolling update
2022-04-09 13:01:37 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-770f03b5-mirror-maker-2-mirrormaker2 will be ready
2022-04-09 13:01:37 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-770f03b5-mirror-maker-2-mirrormaker2 is ready
2022-04-09 13:01:40 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-28d9ae81-mirror-maker-mirror-maker will be ready
2022-04-09 13:01:40 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-28d9ae81-mirror-maker-mirror-maker is ready
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-770f03b5-mirror-maker-2-mirrormaker2 rolling update finished
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-770f03b5-mirror-maker-2-mirrormaker2
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-770f03b5-mirror-maker-2-mirrormaker2
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-770f03b5-mirror-maker-2-mirrormaker2-585c9b64bcjhbrm container my-cluster-770f03b5-mirror-maker-2-mirrormaker2 will be ready
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:347] Pod my-cluster-770f03b5-mirror-maker-2-mirrormaker2-585c9b64bcjhbrm container my-cluster-770f03b5-mirror-maker-2-mirrormaker2 is ready
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-770f03b5-mirror-maker-2-mirrormaker2-585c9b64bcjhbrm with container my-cluster-770f03b5-mirror-maker-2-mirrormaker2
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2LogSetting
2022-04-09 13:01:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-770f03b5-mirror-maker-2 in namespace log-setting-st
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-28d9ae81-mirror-maker-mirror-maker rolling update finished
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-28d9ae81-mirror-maker-mirror-maker
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-28d9ae81-mirror-maker-mirror-maker
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:01:50 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5e339abb-connect-connect will be ready
2022-04-09 13:01:50 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5e339abb-connect-connect is ready
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-28d9ae81-mirror-maker-mirror-maker-55cb6f8d45-2zhz7 container my-cluster-28d9ae81-mirror-maker-mirror-maker will be ready
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:347] Pod my-cluster-28d9ae81-mirror-maker-mirror-maker-55cb6f8d45-2zhz7 container my-cluster-28d9ae81-mirror-maker-mirror-maker is ready
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-28d9ae81-mirror-maker-mirror-maker-55cb6f8d45-2zhz7 with container my-cluster-28d9ae81-mirror-maker-mirror-maker
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerLogSetting
2022-04-09 13:01:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-28d9ae81-mirror-maker in namespace log-setting-st
2022-04-09 13:01:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:01:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testMirrorMaker2LogSetting-FINISHED
2022-04-09 13:01:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-5e339abb-connect-connect rolling update finished
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-5e339abb-connect-connect
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-5e339abb-connect-connect
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-5e339abb-connect-connect-6d65d67f6d-phr6s container my-cluster-5e339abb-connect-connect will be ready
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:347] Pod my-cluster-5e339abb-connect-connect-6d65d67f6d-phr6s container my-cluster-5e339abb-connect-connect is ready
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-5e339abb-connect-connect-6d65d67f6d-phr6s with container my-cluster-5e339abb-connect-connect
2022-04-09 13:02:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:02:00 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testMirrorMakerLogSetting-FINISHED
2022-04-09 13:02:00 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testConnectLogSetting
2022-04-09 13:02:00 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-5e339abb-connect-allow in namespace log-setting-st
2022-04-09 13:02:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-5e339abb-connect in namespace log-setting-st
2022-04-09 13:02:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5e339abb-connect-scraper in namespace log-setting-st
2022-04-09 13:02:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:02:50 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testConnectLogSetting-FINISHED
2022-04-09 13:02:50 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testCruiseControlLogChange-STARTED
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:409] Check that default/actual root logging level is info
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:02:50 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:414] Check logs in CruiseControl - make sure no DEBUG is found there.
2022-04-09 13:02:51 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:422] Wait for change of root logger in log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s.
2022-04-09 13:02:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:02:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:02:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:02:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:03:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:03:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:03:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:03:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:03:12 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:03:12 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:03:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-09 13:03:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 13:03:17 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:428] Check cruise control logs in pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s and it's container cruise-control .
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testCruiseControlLogChange is everything deleted.
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testCruiseControlLogChange-FINISHED
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testKafkaLogSetting-STARTED
2022-04-09 13:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1941532959-921479390 in namespace log-setting-st
2022-04-09 13:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1941532959-921479390 will have desired state: Ready
2022-04-09 13:03:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1941532959-921479390 is in desired state: Ready
2022-04-09 13:03:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1009089065-1993801659 in namespace log-setting-st
2022-04-09 13:03:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1009089065-1993801659 will have desired state: Ready
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1009089065-1993801659 is in desired state: Ready
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:217] Checking if Kafka, Zookeeper, TO and UO of cluster:log-setting-cluster-name has log level set properly
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.apache.zookeeper Expected: WARN
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.network.Processor Expected: OFF
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.request.logger Expected: FATAL
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.controller Expected: WARN
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: kafka.root.logger.level Expected: INFO
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.state.change.logger Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.network.RequestChannel$ Expected: ERROR
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.server.KafkaApis Expected: INFO
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka Expected: TRACE
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.apache.kafka Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.authorizer.logger Expected: FATAL
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.log.LogCleaner Expected: TRACE
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.I0Itec.zkclient.ZkClient Expected: ERROR
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.kafka.logger.level Expected: INFO
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: zookeeper.root.logger Expected: OFF
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.zookeeper.logger.level Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: rootLogger.level Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.operator.logger.level Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: rootLogger.level Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.operator.logger.level Expected: DEBUG
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:226] Checking if Kafka, Zookeeper, TO and UO of cluster:log-setting-cluster-name has GC logging enabled in stateful sets/deployments
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-kafka, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-0, container: kafka
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-1, container: kafka
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-2, container: kafka
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-zookeeper, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-zookeeper-0, container: zookeeper
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: topic-operator
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=true
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: user-operator
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=true
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:232] Changing JVM options - setting GC logging to false
2022-04-09 13:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: log-setting-cluster-name-zookeeper rolling update
2022-04-09 13:03:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: log-setting-cluster-name-zookeeper has been successfully rolled
2022-04-09 13:03:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 1 Pod(s) of log-setting-cluster-name-zookeeper to be ready
2022-04-09 13:04:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: log-setting-cluster-name-kafka rolling update
2022-04-09 13:05:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: log-setting-cluster-name-kafka has been successfully rolled
2022-04-09 13:05:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of log-setting-cluster-name-kafka to be ready
2022-04-09 13:06:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment log-setting-cluster-name-entity-operator rolling update
2022-04-09 13:06:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: log-setting-cluster-name-entity-operator will be ready
2022-04-09 13:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: log-setting-cluster-name-entity-operator is ready
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment log-setting-cluster-name-entity-operator rolling update finished
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:244] Checking if Kafka, Zookeeper, TO and UO of cluster: log-setting-cluster-name has GC logging disabled in stateful sets/deployments
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-kafka, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-0, container: kafka
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-1, container: kafka
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-2, container: kafka
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-zookeeper, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-zookeeper-0, container: zookeeper
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: topic-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: user-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:250] Checking if Kafka, Zookeeper, TO and UO of cluster: gc-set-logging has GC logging disabled in stateful sets/deployments
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-kafka, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-0, container: kafka
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-1, container: kafka
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-2, container: kafka
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-zookeeper, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-zookeeper-0, container: zookeeper
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: topic-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: user-operator
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s container cruise-control will be ready
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s container cruise-control is ready
2022-04-09 13:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s with container cruise-control
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s container tls-sidecar will be ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s container tls-sidecar is ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-cruise-control-64c7f69cfb-74d4s with container tls-sidecar
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz container topic-operator will be ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz container topic-operator is ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz with container topic-operator
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz container user-operator will be ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz container user-operator is ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz with container user-operator
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz container tls-sidecar will be ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz container tls-sidecar is ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-entity-operator-d78c874cf-5nrqz with container tls-sidecar
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-0 container kafka will be ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-0 container kafka is ready
2022-04-09 13:09:23 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-0 with container kafka
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-1 container kafka will be ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-1 container kafka is ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-1 with container kafka
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-2 container kafka will be ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-2 container kafka is ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-2 with container kafka
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-exporter-7b59f7cd48-pc7v9 container log-setting-cluster-name-kafka-exporter will be ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-exporter-7b59f7cd48-pc7v9 container log-setting-cluster-name-kafka-exporter is ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-exporter-7b59f7cd48-pc7v9 with container log-setting-cluster-name-kafka-exporter
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-zookeeper-0 container zookeeper will be ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-zookeeper-0 container zookeeper is ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-zookeeper-0 with container zookeeper
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x container topic-operator will be ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x container topic-operator is ready
2022-04-09 13:09:24 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x with container topic-operator
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x container user-operator will be ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x container user-operator is ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x with container user-operator
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x container tls-sidecar will be ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x container tls-sidecar is ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-entity-operator-6dccd5cb65-6kk6x with container tls-sidecar
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-kafka-0 container kafka will be ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-kafka-0 container kafka is ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-kafka-0 with container kafka
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-zookeeper-0 container zookeeper will be ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-zookeeper-0 container zookeeper is ready
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-zookeeper-0 with container zookeeper
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaLogSetting
2022-04-09 13:09:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1009089065-1993801659 in namespace log-setting-st
2022-04-09 13:09:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1941532959-921479390 in namespace log-setting-st
2022-04-09 13:09:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:09:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testKafkaLogSetting-FINISHED
2022-04-09 13:09:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:09:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:09:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for LogSettingST
2022-04-09 13:09:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka gc-set-logging in namespace log-setting-st
2022-04-09 13:09:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka log-setting-cluster-name in namespace log-setting-st
2022-04-09 13:09:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment shared-kafka-clients in namespace log-setting-st
2022-04-09 13:09:35 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace log-setting-st, for cruise control Kafka cluster log-setting-cluster-name
2022-04-09 13:10:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 804.856 s - in io.strimzi.systemtest.log.LogSettingST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.FeatureGatesIsolatedST
2022-04-09 13:10:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testSwitchingStrimziPodSetFeatureGateOnAndOff-STARTED
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:270] Deploying CO with STS - SPS is disabled
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:11:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:11:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:11:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:11:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:11:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:11:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:13 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:23 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_FEATURE_GATES, value=-UseStrimziPodSets, valueFrom=null, additionalProperties={})]
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:11:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:12:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:12:12 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:281] Deploying Kafka
2022-04-09 13:12:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a4e9ab95 in namespace infra-namespace
2022-04-09 13:12:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a4e9ab95 will have desired state: Ready
2022-04-09 13:13:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a4e9ab95 is in desired state: Ready
2022-04-09 13:13:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-274197574-440021945 in namespace infra-namespace
2022-04-09 13:13:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-274197574-440021945 will have desired state: Ready
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-274197574-440021945 is in desired state: Ready
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-test-1008008956 in namespace infra-namespace
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-test-1255418248 in namespace infra-namespace
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-test-1008008956 will be in active state
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-1255418248 will be in active state
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:304] Changing FG env variable to enable SPS
2022-04-09 13:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-09 13:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:14:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-09 13:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a4e9ab95-zookeeper rolling update
2022-04-09 13:14:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a4e9ab95-zookeeper has been successfully rolled
2022-04-09 13:14:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-a4e9ab95-zookeeper to be ready
2022-04-09 13:15:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a4e9ab95 will have desired state: Ready
2022-04-09 13:15:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a4e9ab95 is in desired state: Ready
2022-04-09 13:15:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-a4e9ab95 is ready
2022-04-09 13:15:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a4e9ab95-kafka rolling update
2022-04-09 13:16:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a4e9ab95-kafka has been successfully rolled
2022-04-09 13:16:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-a4e9ab95-kafka to be ready
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a4e9ab95 will have desired state: Ready
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a4e9ab95 is in desired state: Ready
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-a4e9ab95 is ready
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a4e9ab95 will have desired state: Ready
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a4e9ab95 is in desired state: Ready
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:319] Changing FG env variable to disable again SPS
2022-04-09 13:16:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-09 13:16:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:17:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:17:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-09 13:17:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a4e9ab95-zookeeper rolling update
2022-04-09 13:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a4e9ab95-zookeeper has been successfully rolled
2022-04-09 13:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-a4e9ab95-zookeeper to be ready
2022-04-09 13:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a4e9ab95 will have desired state: Ready
2022-04-09 13:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a4e9ab95 is in desired state: Ready
2022-04-09 13:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-a4e9ab95 is ready
2022-04-09 13:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a4e9ab95-kafka rolling update
2022-04-09 13:19:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a4e9ab95-kafka has been successfully rolled
2022-04-09 13:19:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-a4e9ab95-kafka to be ready
2022-04-09 13:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a4e9ab95 will have desired state: Ready
2022-04-09 13:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a4e9ab95 is in desired state: Ready
2022-04-09 13:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-a4e9ab95 is ready
2022-04-09 13:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer producer-test-1008008956 and consumer consumer-test-1255418248 finish
2022-04-09 13:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSwitchingStrimziPodSetFeatureGateOnAndOff
2022-04-09 13:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-test-1008008956 in namespace infra-namespace
2022-04-09 13:22:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-274197574-440021945 in namespace infra-namespace
2022-04-09 13:22:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job consumer-test-1255418248 in namespace infra-namespace
2022-04-09 13:22:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a4e9ab95 in namespace infra-namespace
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testSwitchingStrimziPodSetFeatureGateOnAndOff-FINISHED
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testControlPlaneListenerFeatureGate-STARTED
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:22:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:22:58 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_FEATURE_GATES, value=-ControlPlaneListener, valueFrom=null, additionalProperties={})]
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:23:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-381fa90c in namespace infra-namespace
2022-04-09 13:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-381fa90c will have desired state: Ready
2022-04-09 13:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-381fa90c is in desired state: Ready
2022-04-09 13:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:96] Check for presence of ContainerPort 9090/tcp (tcp-ctrlplane) in first Kafka pod.
2022-04-09 13:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:104] Try to send some messages to Kafka over next few minutes.
2022-04-09 13:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-265579729-1081245341 in namespace infra-namespace
2022-04-09 13:26:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-265579729-1081245341 will have desired state: Ready
2022-04-09 13:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-265579729-1081245341 is in desired state: Ready
2022-04-09 13:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 13:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-test-34989303 in namespace infra-namespace
2022-04-09 13:26:27 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-test-34989303 will be in active state
2022-04-09 13:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-test-814867002 in namespace infra-namespace
2022-04-09 13:26:28 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-814867002 will be in active state
2022-04-09 13:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-814867002 will be in active state
2022-04-09 13:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:127] Delete first found Kafka broker pod.
2022-04-09 13:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-381fa90c-zookeeper to be ready
2022-04-09 13:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-381fa90c will have desired state: Ready
2022-04-09 13:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-381fa90c is in desired state: Ready
2022-04-09 13:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-381fa90c is ready
2022-04-09 13:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:131] Force Rolling Update of Kafka via annotation.
2022-04-09 13:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:139] Wait for next reconciliation to happen.
2022-04-09 13:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-381fa90c-zookeeper rolling update
2022-04-09 13:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-381fa90c-zookeeper has been successfully rolled
2022-04-09 13:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-381fa90c-zookeeper to be ready
2022-04-09 13:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-381fa90c will have desired state: Ready
2022-04-09 13:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-381fa90c is in desired state: Ready
2022-04-09 13:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-381fa90c is ready
2022-04-09 13:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:142] Waiting for clients to finish sending/receiving messages.
2022-04-09 13:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-test-34989303 to finished
2022-04-09 13:29:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-test-814867002 to finished
2022-04-09 13:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testControlPlaneListenerFeatureGate
2022-04-09 13:29:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-test-34989303 in namespace infra-namespace
2022-04-09 13:29:14 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job consumer-test-814867002 in namespace infra-namespace
2022-04-09 13:29:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-381fa90c in namespace infra-namespace
2022-04-09 13:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-265579729-1081245341 in namespace infra-namespace
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testControlPlaneListenerFeatureGate-FINISHED
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testStrimziPodSetsFeatureGate-STARTED
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:29:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:29:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_FEATURE_GATES, value=+UseStrimziPodSets, valueFrom=null, additionalProperties={})]
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:30:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:30:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:30:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:30:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cef4e46c in namespace infra-namespace
2022-04-09 13:30:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cef4e46c will have desired state: Ready
2022-04-09 13:33:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cef4e46c is in desired state: Ready
2022-04-09 13:33:42 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:182] Try to send some messages to Kafka over next few minutes.
2022-04-09 13:33:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1206796389-1023689058 in namespace infra-namespace
2022-04-09 13:33:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1206796389-1023689058 will have desired state: Ready
2022-04-09 13:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1206796389-1023689058 is in desired state: Ready
2022-04-09 13:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 13:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-test-313890081 in namespace infra-namespace
2022-04-09 13:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-test-313890081 will be in active state
2022-04-09 13:33:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-test-408557822 in namespace infra-namespace
2022-04-09 13:33:44 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-408557822 will be in active state
2022-04-09 13:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-408557822 will be in active state
2022-04-09 13:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:207] Delete first found ZooKeeper pod my-cluster-cef4e46c-zookeeper-0
2022-04-09 13:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-cef4e46c-zookeeper to be ready
2022-04-09 13:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cef4e46c will have desired state: Ready
2022-04-09 13:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cef4e46c is in desired state: Ready
2022-04-09 13:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-cef4e46c is ready
2022-04-09 13:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:213] Delete first found Kafka broker pod my-cluster-cef4e46c-kafka-0
2022-04-09 13:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-cef4e46c-kafka to be ready
2022-04-09 13:34:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cef4e46c will have desired state: Ready
2022-04-09 13:34:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cef4e46c is in desired state: Ready
2022-04-09 13:34:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-cef4e46c is ready
2022-04-09 13:34:57 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:218] Force Rolling Update of ZooKeeper via annotation.
2022-04-09 13:34:57 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:228] Wait for next reconciliation to happen.
2022-04-09 13:34:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-cef4e46c-zookeeper rolling update
2022-04-09 13:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-cef4e46c-zookeeper has been successfully rolled
2022-04-09 13:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-cef4e46c-zookeeper to be ready
2022-04-09 13:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cef4e46c will have desired state: Ready
2022-04-09 13:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cef4e46c is in desired state: Ready
2022-04-09 13:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-cef4e46c is ready
2022-04-09 13:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:232] Force Rolling Update of Kafka via annotation.
2022-04-09 13:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:242] Wait for next reconciliation to happen.
2022-04-09 13:36:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-cef4e46c-kafka rolling update
2022-04-09 13:38:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-cef4e46c-kafka has been successfully rolled
2022-04-09 13:38:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-cef4e46c-kafka to be ready
2022-04-09 13:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cef4e46c will have desired state: Ready
2022-04-09 13:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cef4e46c is in desired state: Ready
2022-04-09 13:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-cef4e46c is ready
2022-04-09 13:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:245] Waiting for clients to finish sending/receiving messages.
2022-04-09 13:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-test-313890081 to finished
2022-04-09 13:39:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-test-408557822 to finished
2022-04-09 13:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testStrimziPodSetsFeatureGate
2022-04-09 13:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-test-313890081 in namespace infra-namespace
2022-04-09 13:39:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job consumer-test-408557822 in namespace infra-namespace
2022-04-09 13:39:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cef4e46c in namespace infra-namespace
2022-04-09 13:39:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1206796389-1023689058 in namespace infra-namespace
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testStrimziPodSetsFeatureGate-FINISHED
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context FeatureGatesIsolatedST is everything deleted.
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,718.07 s - in io.strimzi.systemtest.operators.FeatureGatesIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST
2022-04-09 13:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST.testNamespacedRbacScopeDeploysRoles-STARTED
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:39:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:39:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:39:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_RBAC_SCOPE, value=NAMESPACE, valueFrom=null, additionalProperties={})]
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-09 13:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator10278126827322142280.yaml in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation9896840823687216007.yaml in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:40:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:40:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:40:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-37af8930 in namespace infra-namespace
2022-04-09 13:40:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-37af8930 will have desired state: Ready
2022-04-09 13:41:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-37af8930 is in desired state: Ready
2022-04-09 13:41:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-37af8930 will have desired state: Ready
2022-04-09 13:41:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-37af8930 is in desired state: Ready
2022-04-09 13:41:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:41:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testNamespacedRbacScopeDeploysRoles
2022-04-09 13:41:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-37af8930 in namespace infra-namespace
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST.testNamespacedRbacScopeDeploysRoles-FINISHED
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context NamespaceRbacScopeOperatorIsolatedST is everything deleted.
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 171.942 s - in io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.RecoveryIsolatedST
2022-04-09 13:42:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperMetricsConfigDeletion-STARTED
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:42:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:42:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:32 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:42:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:42:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:42:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 13:42:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 13:42:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 13:42:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 13:42:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 13:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:42:43 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:42:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:43:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:43:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:43:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:43:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1087180944 in namespace infra-namespace
2022-04-09 13:43:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1087180944 will have desired state: Ready
2022-04-09 13:45:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1087180944 is in desired state: Ready
2022-04-09 13:45:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1087180944 in namespace infra-namespace
2022-04-09 13:45:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1087180944 will be ready
2022-04-09 13:45:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1087180944 is ready
2022-04-09 13:45:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1087180944 in namespace infra-namespace
2022-04-09 13:45:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1087180944 will have desired state: Ready
2022-04-09 13:45:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1087180944 is in desired state: Ready
2022-04-09 13:45:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:191] Running deleteZookeeperMetricsConfig with cluster recovery-cluster-1087180944
2022-04-09 13:45:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:199] Waiting for creation recovery-cluster-1087180944-zookeeper-config
2022-04-09 13:45:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:30] Waiting for config map recovery-cluster-1087180944-zookeeper-config-2f048a07-25f3-4364-8786-4f07946c51b4 recovery in namespace infra-namespace
2022-04-09 13:46:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:33] Config map recovery-cluster-1087180944-zookeeper-config was recovered
2022-04-09 13:46:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:46:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperMetricsConfigDeletion
2022-04-09 13:46:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1087180944 in namespace infra-namespace
2022-04-09 13:46:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1087180944 in namespace infra-namespace
2022-04-09 13:46:05 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1087180944 in namespace infra-namespace
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperMetricsConfigDeletion-FINISHED
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeServiceDeletion-STARTED
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:46:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:46:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:46:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:46:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:46:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:47:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:47:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:48:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:48:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1378454703 in namespace infra-namespace
2022-04-09 13:48:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1378454703 will have desired state: Ready
2022-04-09 13:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1378454703 is in desired state: Ready
2022-04-09 13:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1378454703 in namespace infra-namespace
2022-04-09 13:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1378454703 will be ready
2022-04-09 13:50:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1378454703 is ready
2022-04-09 13:50:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1378454703 in namespace infra-namespace
2022-04-09 13:50:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1378454703 will have desired state: Ready
2022-04-09 13:50:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1378454703 is in desired state: Ready
2022-04-09 13:50:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:222] Running deleteKafkaBridgeService with cluster recovery-cluster-1378454703
2022-04-09 13:50:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:227] Waiting for service recovery-cluster-1378454703-bridge-service recovery
2022-04-09 13:50:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-1378454703-bridge-service-b011ecf7-0c9a-4552-8bef-9ab38882496b in namespace infra-namespace will be recovered
2022-04-09 13:50:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-1378454703-bridge-service in namespace infra-namespace is recovered
2022-04-09 13:50:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:50:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaBridgeServiceDeletion
2022-04-09 13:50:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1378454703 in namespace infra-namespace
2022-04-09 13:50:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1378454703 in namespace infra-namespace
2022-04-09 13:50:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1378454703 in namespace infra-namespace
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeServiceDeletion-FINISHED
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaHeadlessServiceDeletion-STARTED
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:51:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:51:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:51:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 13:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:52:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:52:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:52:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:52:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-746603163 in namespace infra-namespace
2022-04-09 13:52:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-746603163 will have desired state: Ready
2022-04-09 13:54:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-746603163 is in desired state: Ready
2022-04-09 13:54:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-746603163 in namespace infra-namespace
2022-04-09 13:54:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-746603163 will be ready
2022-04-09 13:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-746603163 is ready
2022-04-09 13:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-746603163 in namespace infra-namespace
2022-04-09 13:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-746603163 will have desired state: Ready
2022-04-09 13:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-746603163 is in desired state: Ready
2022-04-09 13:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:143] Running deleteKafkaHeadlessService with cluster recovery-cluster-746603163
2022-04-09 13:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:150] Waiting for creation recovery-cluster-746603163-kafka-brokers
2022-04-09 13:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-746603163-kafka-brokers-03c01e6b-b0ad-49f5-a0b3-c7dc63d898a6 in namespace infra-namespace will be recovered
2022-04-09 13:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-746603163-kafka-brokers in namespace infra-namespace is recovered
2022-04-09 13:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaHeadlessServiceDeletion
2022-04-09 13:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-746603163 in namespace infra-namespace
2022-04-09 13:54:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-746603163 in namespace infra-namespace
2022-04-09 13:54:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-746603163 in namespace infra-namespace
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaHeadlessServiceDeletion-FINISHED
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaStatefulSetDeletion-STARTED
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:55:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:55:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:55:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:55:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:55:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:55:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:55:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:55:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:55:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:56:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 13:56:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 13:56:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 13:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 13:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-269439149 in namespace infra-namespace
2022-04-09 13:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-269439149 will have desired state: Ready
2022-04-09 13:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-269439149 is in desired state: Ready
2022-04-09 13:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-269439149 in namespace infra-namespace
2022-04-09 13:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-269439149 will be ready
2022-04-09 13:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-269439149 is ready
2022-04-09 13:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-269439149 in namespace infra-namespace
2022-04-09 13:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-269439149 will have desired state: Ready
2022-04-09 13:58:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-269439149 is in desired state: Ready
2022-04-09 13:58:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-269439149-kafka will be deleted
2022-04-09 13:58:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-269439149-kafka-0 will be deleted
2022-04-09 13:58:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-269439149-kafka-0 deleted
2022-04-09 13:58:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-269439149-kafka-1 will be deleted
2022-04-09 13:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-269439149-kafka-1 deleted
2022-04-09 13:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-269439149-kafka-2 will be deleted
2022-04-09 13:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-269439149-kafka-2 deleted
2022-04-09 13:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:90] Waiting for recovery recovery-cluster-269439149-kafka
2022-04-09 13:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:73] Waiting for StatefulSet recovery-cluster-269439149-kafka-e9cf25e6-d6f7-4cd8-a5dc-e5a49b8282bf recovery in namespace infra-namespace
2022-04-09 13:58:35 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:76] StatefulSet recovery-cluster-269439149-kafka was recovered
2022-04-09 13:58:35 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:39] Waiting for StatefulSet recovery-cluster-269439149-kafka to be ready
2022-04-09 13:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:44] Waiting for 3 Pod(s) of StatefulSet recovery-cluster-269439149-kafka to be ready
2022-04-09 13:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:47] StatefulSet recovery-cluster-269439149-kafka is ready
2022-04-09 13:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaStatefulSetDeletion
2022-04-09 13:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-269439149 in namespace infra-namespace
2022-04-09 13:59:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-269439149 in namespace infra-namespace
2022-04-09 13:59:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-269439149 in namespace infra-namespace
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaStatefulSetDeletion-FINISHED
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [StrimziPodSetTestCondition:23] According to STRIMZI_FEATURE_GATES env variable with value: , the StatefulSets are used, skipping this StrimziPodSet related test
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromEntityOperatorDeletion-STARTED
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 13:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 13:59:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 13:59:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 13:59:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:59:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 13:59:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 13:59:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 13:59:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 13:59:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 13:59:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:00:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:00:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:00:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:00:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-2133064474 in namespace infra-namespace
2022-04-09 14:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-2133064474 will have desired state: Ready
2022-04-09 14:02:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-2133064474 is in desired state: Ready
2022-04-09 14:02:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-2133064474 in namespace infra-namespace
2022-04-09 14:02:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-2133064474 will be ready
2022-04-09 14:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-2133064474 is ready
2022-04-09 14:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-2133064474 in namespace infra-namespace
2022-04-09 14:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-2133064474 will have desired state: Ready
2022-04-09 14:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-2133064474 is in desired state: Ready
2022-04-09 14:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:64] Running testRecoveryFromEntityOperatorDeletion with cluster recovery-cluster-2133064474
2022-04-09 14:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-2133064474-entity-operator will be deleted
2022-04-09 14:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2133064474-entity-operator-79b5bd8cf-dlmvh will be deleted
2022-04-09 14:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2133064474-entity-operator-79b5bd8cf-dlmvh deleted
2022-04-09 14:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:72] Waiting for recovery recovery-cluster-2133064474-entity-operator
2022-04-09 14:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:154] Waiting for Deployment recovery-cluster-2133064474-entity-operator-f98698bd-804b-4679-b5ad-2d85abc70237 recovery in namespace infra-namespace
2022-04-09 14:03:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:157] Deployment recovery-cluster-2133064474-entity-operator was recovered
2022-04-09 14:03:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: recovery-cluster-2133064474-entity-operator will be ready
2022-04-09 14:03:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: recovery-cluster-2133064474-entity-operator is ready
2022-04-09 14:03:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment recovery-cluster-2133064474-entity-operator to be ready
2022-04-09 14:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment recovery-cluster-2133064474-entity-operator is ready
2022-04-09 14:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromEntityOperatorDeletion
2022-04-09 14:03:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-2133064474 in namespace infra-namespace
2022-04-09 14:03:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-2133064474 in namespace infra-namespace
2022-04-09 14:03:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-2133064474 in namespace infra-namespace
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromEntityOperatorDeletion-FINISHED
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeDeploymentDeletion-STARTED
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:04:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:04:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:04:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:04:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:04:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:04:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:05:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:05:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1327489968 in namespace infra-namespace
2022-04-09 14:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1327489968 will have desired state: Ready
2022-04-09 14:07:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1327489968 is in desired state: Ready
2022-04-09 14:07:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1327489968 in namespace infra-namespace
2022-04-09 14:07:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1327489968 will be ready
2022-04-09 14:07:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1327489968 is ready
2022-04-09 14:07:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1327489968 in namespace infra-namespace
2022-04-09 14:07:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1327489968 will have desired state: Ready
2022-04-09 14:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1327489968 is in desired state: Ready
2022-04-09 14:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:206] Running deleteKafkaBridgeDeployment with cluster recovery-cluster-1327489968
2022-04-09 14:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-1327489968-bridge will be deleted
2022-04-09 14:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-1327489968-bridge-d8ffcd5f9-bzspm will be deleted
2022-04-09 14:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-1327489968-bridge-d8ffcd5f9-bzspm deleted
2022-04-09 14:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:215] Waiting for deployment recovery-cluster-1327489968-bridge recovery
2022-04-09 14:07:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:154] Waiting for Deployment recovery-cluster-1327489968-bridge-8100612b-47fb-4841-8339-4f3cd2b7ba03 recovery in namespace infra-namespace
2022-04-09 14:08:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:157] Deployment recovery-cluster-1327489968-bridge was recovered
2022-04-09 14:08:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:08:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaBridgeDeploymentDeletion
2022-04-09 14:08:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1327489968 in namespace infra-namespace
2022-04-09 14:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1327489968 in namespace infra-namespace
2022-04-09 14:08:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1327489968 in namespace infra-namespace
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeDeploymentDeletion-FINISHED
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaServiceDeletion-STARTED
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:08:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:08:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:08:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:08:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:09:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:09:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:09:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:09:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:09:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:09:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:09:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1422743154 in namespace infra-namespace
2022-04-09 14:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1422743154 will have desired state: Ready
2022-04-09 14:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1422743154 is in desired state: Ready
2022-04-09 14:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1422743154 in namespace infra-namespace
2022-04-09 14:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1422743154 will be ready
2022-04-09 14:11:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1422743154 is ready
2022-04-09 14:11:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1422743154 in namespace infra-namespace
2022-04-09 14:11:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1422743154 will have desired state: Ready
2022-04-09 14:12:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1422743154 is in desired state: Ready
2022-04-09 14:12:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:115] Running deleteKafkaService with cluster recovery-cluster-1422743154
2022-04-09 14:12:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:122] Waiting for creation recovery-cluster-1422743154-kafka-bootstrap
2022-04-09 14:12:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-1422743154-kafka-bootstrap-6256e3a4-4d70-4d0e-a499-cad58addcc42 in namespace infra-namespace will be recovered
2022-04-09 14:12:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-1422743154-kafka-bootstrap in namespace infra-namespace is recovered
2022-04-09 14:12:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:12:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaServiceDeletion
2022-04-09 14:12:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1422743154 in namespace infra-namespace
2022-04-09 14:12:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1422743154 in namespace infra-namespace
2022-04-09 14:12:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1422743154 in namespace infra-namespace
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaServiceDeletion-FINISHED
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperStatefulSetDeletion-STARTED
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:13:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:13:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:13:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:13:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:14:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:14:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-2068732602 in namespace infra-namespace
2022-04-09 14:14:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-2068732602 will have desired state: Ready
2022-04-09 14:16:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-2068732602 is in desired state: Ready
2022-04-09 14:16:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-2068732602 in namespace infra-namespace
2022-04-09 14:16:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-2068732602 will be ready
2022-04-09 14:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-2068732602 is ready
2022-04-09 14:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-2068732602 in namespace infra-namespace
2022-04-09 14:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-2068732602 will have desired state: Ready
2022-04-09 14:16:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-2068732602 is in desired state: Ready
2022-04-09 14:16:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-2068732602-zookeeper will be deleted
2022-04-09 14:16:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2068732602-zookeeper-0 will be deleted
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2068732602-zookeeper-0 deleted
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2068732602-zookeeper-1 will be deleted
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2068732602-zookeeper-1 deleted
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2068732602-zookeeper-2 will be deleted
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2068732602-zookeeper-2 deleted
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:107] Waiting for recovery recovery-cluster-2068732602-zookeeper
2022-04-09 14:16:46 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:73] Waiting for StatefulSet recovery-cluster-2068732602-zookeeper-509e2f41-52b5-4ff8-b693-026333208b50 recovery in namespace infra-namespace
2022-04-09 14:16:55 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:76] StatefulSet recovery-cluster-2068732602-zookeeper was recovered
2022-04-09 14:16:55 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:39] Waiting for StatefulSet recovery-cluster-2068732602-zookeeper to be ready
2022-04-09 14:17:48 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:44] Waiting for 3 Pod(s) of StatefulSet recovery-cluster-2068732602-zookeeper to be ready
2022-04-09 14:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:47] StatefulSet recovery-cluster-2068732602-zookeeper is ready
2022-04-09 14:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperStatefulSetDeletion
2022-04-09 14:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-2068732602 in namespace infra-namespace
2022-04-09 14:17:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-2068732602 in namespace infra-namespace
2022-04-09 14:17:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-2068732602 in namespace infra-namespace
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperStatefulSetDeletion-FINISHED
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeMetricsConfigDeletion-STARTED
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:18:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:18:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:18:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:18:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:18:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:18:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:18:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:18:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:18:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:19:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:19:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:19:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:19:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1690303390 in namespace infra-namespace
2022-04-09 14:19:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1690303390 will have desired state: Ready
2022-04-09 14:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1690303390 is in desired state: Ready
2022-04-09 14:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1690303390 in namespace infra-namespace
2022-04-09 14:21:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1690303390 will be ready
2022-04-09 14:21:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1690303390 is ready
2022-04-09 14:21:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1690303390 in namespace infra-namespace
2022-04-09 14:21:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1690303390 will have desired state: Ready
2022-04-09 14:21:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1690303390 is in desired state: Ready
2022-04-09 14:21:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:234] Running deleteKafkaBridgeMetricsConfig with cluster recovery-cluster-1690303390
2022-04-09 14:21:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:239] Waiting for metric config recovery-cluster-1690303390-bridge-config re-creation
2022-04-09 14:21:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:30] Waiting for config map recovery-cluster-1690303390-bridge-config-65934015-61f7-4f26-8bb3-010f644f7588 recovery in namespace infra-namespace
2022-04-09 14:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:33] Config map recovery-cluster-1690303390-bridge-config was recovered
2022-04-09 14:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaBridgeMetricsConfigDeletion
2022-04-09 14:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1690303390 in namespace infra-namespace
2022-04-09 14:22:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1690303390 in namespace infra-namespace
2022-04-09 14:22:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1690303390 in namespace infra-namespace
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeMetricsConfigDeletion-FINISHED
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperHeadlessServiceDeletion-STARTED
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:23:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:01 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:23:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:23:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:23:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:24:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:24:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:24:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:24:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1389207572 in namespace infra-namespace
2022-04-09 14:24:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1389207572 will have desired state: Ready
2022-04-09 14:25:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1389207572 is in desired state: Ready
2022-04-09 14:25:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1389207572 in namespace infra-namespace
2022-04-09 14:25:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1389207572 will be ready
2022-04-09 14:25:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1389207572 is ready
2022-04-09 14:25:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1389207572 in namespace infra-namespace
2022-04-09 14:25:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1389207572 will have desired state: Ready
2022-04-09 14:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1389207572 is in desired state: Ready
2022-04-09 14:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:157] Running deleteKafkaHeadlessService with cluster recovery-cluster-1389207572
2022-04-09 14:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:164] Waiting for creation recovery-cluster-1389207572-zookeeper-nodes
2022-04-09 14:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-1389207572-zookeeper-nodes-0993a38b-1645-4d6c-b6bc-f91bd58c8a7d in namespace infra-namespace will be recovered
2022-04-09 14:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-1389207572-zookeeper-nodes in namespace infra-namespace is recovered
2022-04-09 14:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperHeadlessServiceDeletion
2022-04-09 14:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1389207572 in namespace infra-namespace
2022-04-09 14:26:05 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1389207572 in namespace infra-namespace
2022-04-09 14:26:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1389207572 in namespace infra-namespace
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperHeadlessServiceDeletion-FINISHED
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaMetricsConfigDeletion-STARTED
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:26:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:26:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:27:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:27:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:27:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-542246443 in namespace infra-namespace
2022-04-09 14:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-542246443 will have desired state: Ready
2022-04-09 14:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-542246443 is in desired state: Ready
2022-04-09 14:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-542246443 in namespace infra-namespace
2022-04-09 14:29:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-542246443 will be ready
2022-04-09 14:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-542246443 is ready
2022-04-09 14:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-542246443 in namespace infra-namespace
2022-04-09 14:29:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-542246443 will have desired state: Ready
2022-04-09 14:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-542246443 is in desired state: Ready
2022-04-09 14:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:171] Running deleteKafkaMetricsConfig with cluster recovery-cluster-542246443
2022-04-09 14:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:185] Waiting for creation recovery-cluster-542246443-kafka-config
2022-04-09 14:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:30] Waiting for config map recovery-cluster-542246443-kafka-config-35f0a074-ea93-40e5-8763-9f7ee295081c recovery in namespace infra-namespace
2022-04-09 14:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:33] Config map recovery-cluster-542246443-kafka-config was recovered
2022-04-09 14:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaMetricsConfigDeletion
2022-04-09 14:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-542246443 in namespace infra-namespace
2022-04-09 14:30:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-542246443 in namespace infra-namespace
2022-04-09 14:30:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-542246443 in namespace infra-namespace
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaMetricsConfigDeletion-FINISHED
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperServiceDeletion-STARTED
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:31:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:31:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:31:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:31:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:31:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:32:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-417499659 in namespace infra-namespace
2022-04-09 14:32:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-417499659 will have desired state: Ready
2022-04-09 14:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-417499659 is in desired state: Ready
2022-04-09 14:33:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-417499659 in namespace infra-namespace
2022-04-09 14:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-417499659 will be ready
2022-04-09 14:33:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-417499659 is ready
2022-04-09 14:33:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-417499659 in namespace infra-namespace
2022-04-09 14:33:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-417499659 will have desired state: Ready
2022-04-09 14:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-417499659 is in desired state: Ready
2022-04-09 14:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:129] Running deleteKafkaService with cluster recovery-cluster-417499659
2022-04-09 14:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:136] Waiting for creation recovery-cluster-417499659-zookeeper-client
2022-04-09 14:34:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-417499659-zookeeper-client-05e56311-3a2e-4ebf-abd7-e2e923d14b8a in namespace infra-namespace will be recovered
2022-04-09 14:34:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-417499659-zookeeper-client in namespace infra-namespace is recovered
2022-04-09 14:34:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:34:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperServiceDeletion
2022-04-09 14:34:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-417499659 in namespace infra-namespace
2022-04-09 14:34:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-417499659 in namespace infra-namespace
2022-04-09 14:34:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-417499659 in namespace infra-namespace
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperServiceDeletion-FINISHED
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromImpossibleMemoryRequest-STARTED
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:14 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:14 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:35:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:35:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:35:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:35:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-37810503 in namespace infra-namespace
2022-04-09 14:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-37810503 will have desired state: Ready
2022-04-09 14:38:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-37810503 is in desired state: Ready
2022-04-09 14:38:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-37810503 in namespace infra-namespace
2022-04-09 14:38:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-37810503 will be ready
2022-04-09 14:38:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-37810503 is ready
2022-04-09 14:38:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-37810503 in namespace infra-namespace
2022-04-09 14:38:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-37810503 will have desired state: Ready
2022-04-09 14:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-37810503 is in desired state: Ready
2022-04-09 14:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: recovery-cluster-37810503-kafka will be in pending phase
2022-04-09 14:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:306] Verify that all pods with prefix: recovery-cluster-37810503-kafka are stable in pending phase
2022-04-09 14:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 50
2022-04-09 14:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 49
2022-04-09 14:38:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 48
2022-04-09 14:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 47
2022-04-09 14:38:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 46
2022-04-09 14:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 45
2022-04-09 14:38:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 44
2022-04-09 14:38:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 43
2022-04-09 14:38:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 42
2022-04-09 14:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 41
2022-04-09 14:38:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 40
2022-04-09 14:38:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 39
2022-04-09 14:38:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 38
2022-04-09 14:38:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 37
2022-04-09 14:38:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 36
2022-04-09 14:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 35
2022-04-09 14:38:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 34
2022-04-09 14:38:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 33
2022-04-09 14:38:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 32
2022-04-09 14:38:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 31
2022-04-09 14:38:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 30
2022-04-09 14:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 29
2022-04-09 14:38:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 28
2022-04-09 14:38:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 27
2022-04-09 14:38:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 26
2022-04-09 14:39:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 25
2022-04-09 14:39:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 24
2022-04-09 14:39:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 23
2022-04-09 14:39:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 22
2022-04-09 14:39:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 21
2022-04-09 14:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 20
2022-04-09 14:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 19
2022-04-09 14:39:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 18
2022-04-09 14:39:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 17
2022-04-09 14:39:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 16
2022-04-09 14:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 15
2022-04-09 14:39:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 14
2022-04-09 14:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 13
2022-04-09 14:39:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 12
2022-04-09 14:39:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 11
2022-04-09 14:39:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 10
2022-04-09 14:39:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 9
2022-04-09 14:39:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 8
2022-04-09 14:39:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 7
2022-04-09 14:39:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 6
2022-04-09 14:39:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 5
2022-04-09 14:39:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 4
2022-04-09 14:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 3
2022-04-09 14:39:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 2
2022-04-09 14:39:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-37810503-kafka-0 is in the Pending state. Remaining seconds pod to be stable 1
2022-04-09 14:39:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable recovery-cluster-37810503-kafka-0
2022-04-09 14:39:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of recovery-cluster-37810503-kafka to be ready
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-37810503 will have desired state: Ready
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-37810503 is in desired state: Ready
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: recovery-cluster-37810503 is ready
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-37810503 will have desired state: Ready
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-37810503 is in desired state: Ready
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromImpossibleMemoryRequest
2022-04-09 14:45:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-37810503 in namespace infra-namespace
2022-04-09 14:45:42 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-37810503 in namespace infra-namespace
2022-04-09 14:45:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-37810503 in namespace infra-namespace
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromImpossibleMemoryRequest-FINISHED
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context RecoveryIsolatedST is everything deleted.
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 14, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 3,864.3 s - in io.strimzi.systemtest.operators.RecoveryIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST
2022-04-09 14:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:46:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:46:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:46:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:46:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:46:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:47:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 14:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:47:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 14:47:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 14:47:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 14:47:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 14:47:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:47:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-83383454-4042747 in namespace infra-namespace
2022-04-09 14:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-83383454-4042747 will have desired state: Ready
2022-04-09 14:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-83383454-4042747 is in desired state: Ready
2022-04-09 14:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-09 14:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-shared-kafka-clients will be ready
2022-04-09 14:49:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-shared-kafka-clients is ready
2022-04-09 14:49:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatus-STARTED
2022-04-09 14:49:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatus-STARTED
2022-04-09 14:49:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatusNotReady-STARTED
2022-04-09 14:49:16 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicChangingInSyncReplicasStatus-STARTED
2022-04-09 14:49:16 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatusNotReady-STARTED
2022-04-09 14:49:16 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:16 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-257911503-1680599368 in namespace infra-namespace
2022-04-09 14:49:16 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace infra-namespace
2022-04-09 14:49:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-22800381-149868516 in namespace infra-namespace
2022-04-09 14:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-904352634-1538469950 in namespace infra-namespace
2022-04-09 14:49:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-b16b959b-mirror-maker in namespace infra-namespace
2022-04-09 14:49:16 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-904352634-1538469950 will have desired state: NotReady
2022-04-09 14:49:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-22800381-149868516 will have desired state: Ready
2022-04-09 14:49:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker will have desired state: Ready
2022-04-09 14:49:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: NotReady
2022-04-09 14:49:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-257911503-1680599368 will have desired state: Ready
2022-04-09 14:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-22800381-149868516 is in desired state: Ready
2022-04-09 14:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [CustomResourceStatusIsolatedST:481] Changing min.insync.replicas to random char
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-904352634-1538469950 is in desired state: NotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-22800381-149868516 will have desired state: NotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: NotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:179] Checking status of deployed KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:181] KafkaUser Status: True
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:182] KafkaUser Type: NotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:183] KafkaUser Message: Spec cannot be null
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:184] KafkaUser Reason: InvalidResourceException
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:186] KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: NotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-257911503-1680599368 is in desired state: Ready
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [CustomResourceStatusIsolatedST:162] Checking status of deployed KafkaUser
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [CustomResourceStatusIsolatedST:164] KafkaUser Status: True
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [CustomResourceStatusIsolatedST:165] KafkaUser Type: Ready
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [CustomResourceStatusIsolatedST:167] KafkaUser is in desired state: Ready
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaUserStatus
2022-04-09 14:49:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-257911503-1680599368 in namespace infra-namespace
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef deleted
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaUserStatusNotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace infra-namespace
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatusNotReady-FINISHED
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaStatusCertificate-STARTED
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [CustomResourceStatusIsolatedST:381] Check if KafkaStatus certificates are the same as secret certificates
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:346] In context testKafkaStatusCertificate is everything deleted.
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaStatusCertificate-FINISHED
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaBridgeStatus-STARTED
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:49:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-904352634-1538469950 deletion
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicStatusNotReady
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-904352634-1538469950 in namespace infra-namespace
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatusNotReady-FINISHED
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatus-STARTED
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1747027080-865603086 in namespace infra-namespace
2022-04-09 14:49:17 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1747027080-865603086 will have desired state: Ready
2022-04-09 14:49:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-22800381-149868516 is in desired state: NotReady
2022-04-09 14:49:18 [ForkJoinPool-3-worker-15] [32mINFO [m [CustomResourceStatusIsolatedST:488] Wait 245000 ms for next reconciliation
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1747027080-865603086 is in desired state: Ready
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1747027080-865603086 will have desired state: Ready
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1747027080-865603086 is in desired state: Ready
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicStatus
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1747027080-865603086 in namespace infra-namespace
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatus-FINISHED
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2Status-STARTED
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-07d21a1c in namespace infra-namespace
2022-04-09 14:49:18 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-07d21a1c will have desired state: Ready
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatus-FINISHED
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectorWithoutClusterConfig-STARTED
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-65a3ef23 in namespace infra-namespace
2022-04-09 14:49:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-65a3ef23 will have desired state: NotReady
2022-04-09 14:49:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-65a3ef23 is in desired state: NotReady
2022-04-09 14:49:28 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectorUtils:98] KafkaConnector: my-cluster-65a3ef23 is not deleted yet, triggering force delete
2022-04-09 14:49:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatusWrongBootstrap-STARTED
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectorWithoutClusterConfig
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-65a3ef23 in namespace infra-namespace
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectorWithoutClusterConfig-FINISHED
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:49:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectAndConnectorStatus-STARTED
2022-04-09 14:49:33 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:49:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-13b8c402 in namespace infra-namespace
2022-04-09 14:49:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-13b8c402 will have desired state: Ready
2022-04-09 14:49:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:49:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:49:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:49:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-09 14:50:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-09 14:50:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:50:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-07d21a1c is in desired state: Ready
2022-04-09 14:50:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-07d21a1c-mirror-maker-2 in namespace infra-namespace
2022-04-09 14:50:29 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 will have desired state: Ready
2022-04-09 14:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker is in desired state: Ready
2022-04-09 14:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker will have desired state: Ready
2022-04-09 14:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker is in desired state: Ready
2022-04-09 14:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker will have desired state: NotReady
2022-04-09 14:50:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-13b8c402 is in desired state: Ready
2022-04-09 14:50:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-13b8c402 will have desired state: Ready
2022-04-09 14:50:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-13b8c402 is in desired state: Ready
2022-04-09 14:50:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-13b8c402 will have desired state: NotReady
2022-04-09 14:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker is in desired state: NotReady
2022-04-09 14:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker will have desired state: Ready
2022-04-09 14:51:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-13b8c402 is in desired state: NotReady
2022-04-09 14:51:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-13b8c402 will have desired state: Ready
2022-04-09 14:51:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:51:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:51:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeStatus
2022-04-09 14:51:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:51:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:51:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaBridgeStatus-FINISHED
2022-04-09 14:51:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:51:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:51:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2WrongBootstrap-STARTED
2022-04-09 14:51:29 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:51:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment custom-resource-status-cluster-name-scraper in namespace infra-namespace
2022-04-09 14:51:29 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: custom-resource-status-cluster-name-scraper will be ready
2022-04-09 14:51:32 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: custom-resource-status-cluster-name-scraper is ready
2022-04-09 14:51:32 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment custom-resource-status-cluster-name-scraper to be ready
2022-04-09 14:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 is in desired state: Ready
2022-04-09 14:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 will have desired state: Ready
2022-04-09 14:51:38 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 is in desired state: Ready
2022-04-09 14:51:42 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment custom-resource-status-cluster-name-scraper is ready
2022-04-09 14:51:42 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to custom-resource-status-cluster-name-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 14:51:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy custom-resource-status-cluster-name-allow in namespace infra-namespace
2022-04-09 14:51:42 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 14:51:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:51:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:52:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-13b8c402 is in desired state: Ready
2022-04-09 14:52:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:52:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMakerStatusWrongBootstrap
2022-04-09 14:52:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-13b8c402 in namespace infra-namespace
2022-04-09 14:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatusWrongBootstrap-FINISHED
2022-04-09 14:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicDecreaseStatus-STARTED
2022-04-09 14:52:12 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:52:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-c6a075e8-mirror-maker-2 in namespace infra-namespace
2022-04-09 14:52:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-c6a075e8-mirror-maker-2 will have desired state: NotReady
2022-04-09 14:52:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-b16b959b-mirror-maker is in desired state: Ready
2022-04-09 14:52:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:52:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMakerStatus
2022-04-09 14:52:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-b16b959b-mirror-maker in namespace infra-namespace
2022-04-09 14:52:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-c6a075e8-mirror-maker-2 is in desired state: NotReady
2022-04-09 14:52:46 [ForkJoinPool-3-worker-5] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-c6a075e8-mirror-maker-2-mirrormaker2 is not deleted yet! Triggering force delete by cmd client!
2022-04-09 14:52:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 will have desired state: NotReady
2022-04-09 14:52:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:52:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatus-FINISHED
2022-04-09 14:52:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:52:51 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:52:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1460456622-1543719620 in namespace infra-namespace
2022-04-09 14:52:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1460456622-1543719620 will have desired state: Ready
2022-04-09 14:52:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:52:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMaker2WrongBootstrap
2022-04-09 14:52:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-c6a075e8-mirror-maker-2 in namespace infra-namespace
2022-04-09 14:52:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:52:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2WrongBootstrap-FINISHED
2022-04-09 14:52:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:52:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1460456622-1543719620 is in desired state: Ready
2022-04-09 14:52:52 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:457] Decreasing number of partitions to 1
2022-04-09 14:52:52 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-1460456622-1543719620
2022-04-09 14:52:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1460456622-1543719620 will have desired state: NotReady
2022-04-09 14:52:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1460456622-1543719620 is in desired state: NotReady
2022-04-09 14:52:53 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:465] Wait 245000 ms for next reconciliation
2022-04-09 14:53:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:53:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:53:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:53:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:53:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-09 14:53:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 is in desired state: NotReady
2022-04-09 14:53:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 will have desired state: Ready
2022-04-09 14:53:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:53:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicChangingInSyncReplicasStatus
2022-04-09 14:53:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-22800381-149868516 in namespace infra-namespace
2022-04-09 14:53:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:53:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicChangingInSyncReplicasStatus-FINISHED
2022-04-09 14:53:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:53:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-09 14:53:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:55:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-07d21a1c-mirror-maker-2 is in desired state: Ready
2022-04-09 14:55:08 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:55:08 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-09 14:55:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-09 14:55:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:55:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:55:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-09 14:55:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-09 14:55:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: Ready
2022-04-09 14:55:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: Ready
2022-04-09 14:55:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:55:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectAndConnectorStatus
2022-04-09 14:55:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:55:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy custom-resource-status-cluster-name-allow in namespace infra-namespace
2022-04-09 14:55:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:55:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment custom-resource-status-cluster-name-scraper in namespace infra-namespace
2022-04-09 14:55:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-07d21a1c-mirror-maker-2-mirrormaker2 are stable
2022-04-09 14:55:31 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 14:55:32 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 14:55:33 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 14:55:34 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 14:55:35 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 14:55:36 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 14:55:37 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 14:55:38 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 14:55:39 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 14:55:40 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 14:55:41 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 14:55:42 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 14:55:43 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 14:55:44 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 14:55:45 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 14:55:46 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 14:55:47 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 14:55:48 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 14:55:49 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 14:55:50 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 14:55:51 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 14:55:52 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 14:55:53 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 14:55:54 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 14:55:55 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 14:55:56 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 14:55:57 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 14:55:58 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 14:55:59 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 14:56:00 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 14:56:01 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 14:56:02 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 14:56:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:56:02 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectAndConnectorStatus-FINISHED
2022-04-09 14:56:02 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:56:03 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 14:56:04 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 14:56:05 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 14:56:06 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 14:56:07 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 14:56:08 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 14:56:09 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 14:56:10 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 14:56:11 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 14:56:12 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 14:56:13 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 14:56:14 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 14:56:15 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 14:56:16 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 14:56:17 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 14:56:18 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 14:56:19 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 14:56:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:322] Pod my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 14:56:20 [ForkJoinPool-3-worker-9] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-07d21a1c-mirror-maker-2-mirrormaker2-bc9f8c684-znmxn
2022-04-09 14:56:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:56:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMaker2Status
2022-04-09 14:56:20 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-07d21a1c-mirror-maker-2 in namespace infra-namespace
2022-04-09 14:56:20 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-07d21a1c in namespace infra-namespace
2022-04-09 14:56:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:56:30 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2Status-FINISHED
2022-04-09 14:56:30 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:56:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:56:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicDecreaseStatus
2022-04-09 14:56:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1460456622-1543719620 in namespace infra-namespace
2022-04-09 14:57:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:57:08 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicDecreaseStatus-FINISHED
2022-04-09 14:57:08 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 14:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for CustomResourceStatusIsolatedST
2022-04-09 14:57:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka custom-resource-status-cluster-name in namespace infra-namespace
2022-04-09 14:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-83383454-4042747 in namespace infra-namespace
2022-04-09 14:57:08 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-09 14:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 685.968 s - in io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST
2022-04-09 14:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 14:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 14:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 14:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 14:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 14:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 14:58:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:58:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:58:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:58:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:58:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 14:58:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 14:58:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:33 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:58:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs-STARTED
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating first-strimzi-cluster-operator in multiple-co-cluster-test namespace
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@1458c13d, clusterOperatorName='first-strimzi-cluster-operator', namespaceInstallTo='multiple-co-cluster-test', namespaceToWatch='multiple-co-cluster-test', bindingsNamespaces=[multiple-co-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=first-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=first-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: multiple-co-cluster-test
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-co-cluster-test
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: multiple-co-cluster-test
2022-04-09 14:58:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment first-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:58:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: first-strimzi-cluster-operator will be ready
2022-04-09 14:59:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: first-strimzi-cluster-operator is ready
2022-04-09 14:59:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment first-strimzi-cluster-operator to be ready
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment first-strimzi-cluster-operator is ready
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating second-strimzi-cluster-operator in multiple-co-cluster-test namespace
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@1458c13d, clusterOperatorName='second-strimzi-cluster-operator', namespaceInstallTo='multiple-co-cluster-test', namespaceToWatch='multiple-co-cluster-test', bindingsNamespaces=[multiple-co-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=second-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=second-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:254] Environment for ClusterOperator was already prepared! Going to install it now.
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 14:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-strimzi-cluster-operator will be ready
2022-04-09 14:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-strimzi-cluster-operator is ready
2022-04-09 14:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment second-strimzi-cluster-operator to be ready
2022-04-09 15:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment second-strimzi-cluster-operator is ready
2022-04-09 15:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:171] Deploying Kafka with {app.kubernetes.io/operator=first-strimzi-cluster-operator} selector of first-strimzi-cluster-operator
2022-04-09 15:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-81b26840 in namespace multiple-co-cluster-test
2022-04-09 15:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-81b26840 will have desired state: Ready
2022-04-09 15:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-81b26840 is in desired state: Ready
2022-04-09 15:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:180] Removing CR selector from Kafka and increasing number of replicas to 4, new pod should not appear
2022-04-09 15:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:188] Creating KafkaRebalance when CC doesn't have label for CO, the KR should be ignored
2022-04-09 15:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-81b26840 in namespace multiple-co-cluster-test
2022-04-09 15:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-09 15:03:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-09 15:03:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:204] Checking if KafkaRebalance is still ignored, after the cluster stability wait
2022-04-09 15:03:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:209] Adding {app.kubernetes.io/operator=second-strimzi-cluster-operator} selector of second-strimzi-cluster-operator to Kafka
2022-04-09 15:03:02 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:212] Waiting for Kafka to scales pods to 4
2022-04-09 15:03:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-81b26840-kafka to be ready
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-81b26840 will have desired state: Ready
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-81b26840 is in desired state: Ready
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-81b26840 is ready
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-81b26840): ============================================================================
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-81b26840): NotReady
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-81b26840): ============================================================================
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-81b26840): Verifying that KafkaRebalance resource is in PendingProposal state
2022-04-09 15:06:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-81b26840 will have desired state: PendingProposal
2022-04-09 15:12:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:414] KafkaRebalance status:

Conditions:

Pods with conditions and messages:

my-cluster-81b26840-cruise-control-694b4b4897-76dfk:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-entity-operator-5897bfc4b6-l4qpg:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-kafka-0:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-kafka-1:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-kafka-2:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-kafka-3:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-zookeeper-0:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-zookeeper-1:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>


my-cluster-81b26840-zookeeper-2:
	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>

	Type: <EMPTY>
	Message: <EMPTY>
io.strimzi.test.WaitException: Timeout after 360000 ms waiting for KafkaRebalance: my-cluster-81b26840 will have desired state: PendingProposal
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:435)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:428)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaRebalanceUtils.waitForKafkaRebalanceCustomResourceState(KafkaRebalanceUtils.java:55)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaRebalanceUtils.doRebalancingProcess(KafkaRebalanceUtils.java:83)
	at io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs(MultipleClusterOperatorsIsolatedST.java:217)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 15:12:26 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestExecutionWatcher:28] MultipleClusterOperatorsIsolatedST - Exception Timeout after 360000 ms waiting for KafkaRebalance: my-cluster-81b26840 will have desired state: PendingProposal has been thrown in @Test. Going to collect logs from components.
2022-04-09 15:12:26 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace multiple-co-cluster-test
2022-04-09 15:12:26 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace multiple-co-cluster-test
2022-04-09 15:12:26 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaCCAndRebalanceWithMultipleCOs
2022-04-09 15:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-81b26840 in namespace multiple-co-cluster-test
2022-04-09 15:12:30 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace multiple-co-cluster-test, for cruise control Kafka cluster my-cluster-81b26840
2022-04-09 15:12:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:12:40 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:12:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-81b26840 in namespace multiple-co-cluster-test
2022-04-09 15:12:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment second-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment first-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:12:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs-FINISHED
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testMultipleCOsInDifferentNamespaces-STARTED
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding first-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding first-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating first-strimzi-cluster-operator in first-co-namespace namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@20002886, clusterOperatorName='first-strimzi-cluster-operator', namespaceInstallTo='first-co-namespace', namespaceToWatch='*', bindingsNamespaces=[first-co-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=first-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=first-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment first-strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:13:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: first-strimzi-cluster-operator will be ready
2022-04-09 15:13:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: first-strimzi-cluster-operator is ready
2022-04-09 15:13:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment first-strimzi-cluster-operator to be ready
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment first-strimzi-cluster-operator is ready
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding second-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding second-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating second-strimzi-cluster-operator in second-co-namespace namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@20002886, clusterOperatorName='second-strimzi-cluster-operator', namespaceInstallTo='second-co-namespace', namespaceToWatch='*', bindingsNamespaces=[second-co-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=second-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=second-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:13:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-strimzi-cluster-operator will be ready
2022-04-09 15:14:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-strimzi-cluster-operator is ready
2022-04-09 15:14:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment second-strimzi-cluster-operator to be ready
2022-04-09 15:14:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment second-strimzi-cluster-operator is ready
2022-04-09 15:14:11 [ForkJoinPool-3-worker-3] [33mWARN [m [KubeClusterResource:151] Namespace multiple-co-cluster-test is already created, going to delete it
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: multiple-co-cluster-test
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-co-cluster-test
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-co-cluster-test
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:100] Deploying Kafka without CR selector
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-139d6168 in namespace multiple-co-cluster-test
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-139d6168 will have stable 0 replicas
2022-04-09 15:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-09 15:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-09 15:14:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-09 15:14:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-09 15:14:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-09 15:14:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-09 15:14:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-09 15:14:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-09 15:14:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-09 15:14:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-09 15:14:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-09 15:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-09 15:14:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-09 15:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-09 15:14:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-09 15:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-09 15:14:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-09 15:14:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-09 15:14:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-09 15:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-09 15:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:228] Pod my-cluster-139d6168 has 0 replicas
2022-04-09 15:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:110] Adding {app.kubernetes.io/operator=first-strimzi-cluster-operator} selector of first-strimzi-cluster-operator into Kafka CR
2022-04-09 15:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-139d6168 will have desired state: Ready
2022-04-09 15:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-139d6168 is in desired state: Ready
2022-04-09 15:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1619913874-714539197 in namespace multiple-co-cluster-test
2022-04-09 15:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-139d6168 in namespace multiple-co-cluster-test
2022-04-09 15:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1619913874-714539197 will have desired state: Ready
2022-04-09 15:15:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1619913874-714539197 is in desired state: Ready
2022-04-09 15:15:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-139d6168 will have desired state: Ready
2022-04-09 15:17:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-139d6168 is in desired state: Ready
2022-04-09 15:17:05 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:130] Deploying KafkaConnector with file sink and CR selector - {app.kubernetes.io/operator=second-strimzi-cluster-operator} - different than selector in Kafka
2022-04-09 15:17:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-139d6168 in namespace multiple-co-cluster-test
2022-04-09 15:17:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-139d6168 will have desired state: Ready
2022-04-09 15:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-139d6168 is in desired state: Ready
2022-04-09 15:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 15:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace multiple-co-cluster-test
2022-04-09 15:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-09 15:17:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:hello-world-producer to finished
2022-04-09 15:17:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-139d6168-connect-5f9f869df8-qklkr
2022-04-09 15:17:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-139d6168-connect-5f9f869df8-qklkr
2022-04-09 15:17:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:17:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMultipleCOsInDifferentNamespaces
2022-04-09 15:17:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment second-strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-139d6168 in namespace multiple-co-cluster-test
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-co-namespace
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-139d6168 in namespace multiple-co-cluster-test
2022-04-09 15:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:17:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace multiple-co-cluster-test
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1619913874-714539197 in namespace multiple-co-cluster-test
2022-04-09 15:17:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-139d6168 in namespace multiple-co-cluster-test
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace second-co-namespace
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment first-strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace first-co-namespace
2022-04-09 15:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:17:35 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding second-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding first-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding first-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding second-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace first-co-namespace
2022-04-09 15:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testMultipleCOsInDifferentNamespaces-FINISHED
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MultipleClusterOperatorsIsolatedST is everything deleted.
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1,177.988 s <<< FAILURE! - in io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs(ExtensionContext)  Time elapsed: 851.761 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 360000 ms waiting for KafkaRebalance: my-cluster-81b26840 will have desired state: PendingProposal
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:435)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:428)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaRebalanceUtils.waitForKafkaRebalanceCustomResourceState(KafkaRebalanceUtils.java:55)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaRebalanceUtils.doRebalancingProcess(KafkaRebalanceUtils.java:83)
	at io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs(MultipleClusterOperatorsIsolatedST.java:217)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST
2022-04-09 15:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorsWhenRackAwarenessIsEnabled-STARTED
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context JUnit Jupiter is everything deleted.
2022-04-09 15:18:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=NAMESPACE, testClassName='null', testMethodName='null'}
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/020-Role-strimzi-cluster-operator-role7147923621262509207.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/021-Role-strimzi-cluster-operator-role14029639890013745392.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/030-Role-strimzi-kafka-broker338137457330896656.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/031-Role-strimzi-entity-operator6770934449768846387.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/033-Role-strimzi-kafka-client3474998294927807083.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator15364598339935562242.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation7235860717945136464.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator4820321277143678943.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation17045694318164485175.yaml in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:18:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:19:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:19:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:99] Deploying Kafka: my-cluster-8a2caa66, which should not be deployed and error should be present in CR status message
2022-04-09 15:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8a2caa66 in namespace infra-namespace
2022-04-09 15:20:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-8a2caa66-kafka-clients in namespace infra-namespace
2022-04-09 15:20:21 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-8a2caa66-kafka-clients will be ready
2022-04-09 15:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-8a2caa66-kafka-clients is ready
2022-04-09 15:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-8a2caa66-scraper in namespace infra-namespace
2022-04-09 15:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-8a2caa66-scraper will be ready
2022-04-09 15:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-8a2caa66-scraper is ready
2022-04-09 15:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-8a2caa66-scraper to be ready
2022-04-09 15:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-8a2caa66-scraper is ready
2022-04-09 15:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-8a2caa66-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 15:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-8a2caa66-allow in namespace infra-namespace
2022-04-09 15:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 15:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-8a2caa66 in namespace infra-namespace
2022-04-09 15:20:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:20:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCRBDeletionErrorsWhenRackAwarenessIsEnabled
2022-04-09 15:20:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-8a2caa66-scraper in namespace infra-namespace
2022-04-09 15:20:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-8a2caa66 in namespace infra-namespace
2022-04-09 15:20:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-8a2caa66-kafka-clients in namespace infra-namespace
2022-04-09 15:20:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8a2caa66 in namespace infra-namespace
2022-04-09 15:20:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-8a2caa66-allow in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorsWhenRackAwarenessIsEnabled-FINISHED
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorIsIgnoredWhenRackAwarenessIsNotEnabled-STARTED
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:21:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=NAMESPACE, testClassName='null', testMethodName='null'}
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/020-Role-strimzi-cluster-operator-role9690171807313185055.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/021-Role-strimzi-cluster-operator-role4889569956401563998.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/030-Role-strimzi-kafka-broker9090048177694998306.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/031-Role-strimzi-entity-operator1966722488409189296.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/033-Role-strimzi-kafka-client14578515211765632996.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator5555218483522206110.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation3027379634645533826.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator10109302372946357996.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation17996706960625072033.yaml in namespace infra-namespace
2022-04-09 15:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:21:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:21:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:63] Deploying Kafka: my-cluster-eb38935b, which should be deployed even the CRBs are not present
2022-04-09 15:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-eb38935b in namespace infra-namespace
2022-04-09 15:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-eb38935b will have desired state: Ready
2022-04-09 15:23:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-eb38935b is in desired state: Ready
2022-04-09 15:23:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:67] CO log should contain some information about ignoring forbidden access to CRB for Kafka
2022-04-09 15:23:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:71] Deploying KafkaConnect: my-cluster-eb38935b without rack awareness, the CR should be deployed without error
2022-04-09 15:23:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-eb38935b in namespace infra-namespace
2022-04-09 15:23:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-eb38935b will have desired state: Ready
2022-04-09 15:24:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-eb38935b is in desired state: Ready
2022-04-09 15:24:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:74] CO log should contain some information about ignoring forbidden access to CRB for KafkaConnect
2022-04-09 15:24:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:24:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCRBDeletionErrorIsIgnoredWhenRackAwarenessIsNotEnabled
2022-04-09 15:24:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-eb38935b in namespace infra-namespace
2022-04-09 15:24:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-eb38935b in namespace infra-namespace
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorIsIgnoredWhenRackAwarenessIsNotEnabled-FINISHED
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ClusterOperatorRbacIsolatedST is everything deleted.
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 433.859 s - in io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeIsolatedST
2022-04-09 15:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:25:25 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-09 15:25:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-09 15:25:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_KAFKA_BRIDGE_SERVICE_LABELS, value=app=bar, valueFrom=null, additionalProperties={}), EnvVar(name=STRIMZI_CUSTOM_KAFKA_BRIDGE_SERVICE_ANNOTATIONS, value=bar=app, valueFrom=null, additionalProperties={})]
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeIsolatedST:434] Deploy Kafka and KafkaBridge before tests
2022-04-09 15:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-cluster-name in namespace infra-namespace
2022-04-09 15:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-cluster-name will have desired state: Ready
2022-04-09 15:27:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-cluster-name is in desired state: Ready
2022-04-09 15:27:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-09 15:27:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-shared-kafka-clients will be ready
2022-04-09 15:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-shared-kafka-clients is ready
2022-04-09 15:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-cluster-name in namespace infra-namespace
2022-04-09 15:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-cluster-name will have desired state: Ready
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-cluster-name is in desired state: Ready
2022-04-09 15:27:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testSendSimpleMessage-STARTED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testDiscoveryAnnotation-STARTED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testReceiveSimpleMessage-STARTED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeToZero-STARTED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testConfigureDeploymentStrategy-STARTED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:27:57 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 15:27:57 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:27:57 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-396441818-1558180707 in namespace infra-namespace
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:27:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1597020789-395016318 in namespace infra-namespace
2022-04-09 15:27:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge scaling-bridge-down in namespace infra-namespace
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:346] In context testDiscoveryAnnotation is everything deleted.
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testDiscoveryAnnotation-FINISHED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomBridgeLabelsAreProperlySet-STARTED
2022-04-09 15:27:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge example-bridge in namespace infra-namespace
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:27:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1597020789-395016318 will have desired state: Ready
2022-04-09 15:27:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-396441818-1558180707 will have desired state: Ready
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge bridge-my-cluster-da44aea8 in namespace infra-namespace
2022-04-09 15:27:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: scaling-bridge-down will have desired state: Ready
2022-04-09 15:27:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: example-bridge will have desired state: Ready
2022-04-09 15:27:57 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: bridge-my-cluster-da44aea8 will have desired state: Ready
2022-04-09 15:27:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1597020789-395016318 is in desired state: Ready
2022-04-09 15:27:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 15:27:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1753182088 in namespace infra-namespace
2022-04-09 15:27:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-396441818-1558180707 is in desired state: Ready
2022-04-09 15:27:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-570323928 in namespace infra-namespace
2022-04-09 15:27:58 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1753182088 will be in active state
2022-04-09 15:27:58 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-570323928 will be in active state
2022-04-09 15:27:59 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 15:27:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job producer-836467796 in namespace infra-namespace
2022-04-09 15:27:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-570323928 to finished
2022-04-09 15:27:59 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: producer-836467796 will be in active state
2022-04-09 15:27:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:61] Waiting till producer producer-836467796 and consumer consumer-1753182088 finish
2022-04-09 15:28:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:28:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessage
2022-04-09 15:28:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1753182088 in namespace infra-namespace
2022-04-09 15:28:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job producer-836467796 in namespace infra-namespace
2022-04-09 15:28:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1597020789-395016318 in namespace infra-namespace
2022-04-09 15:28:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: scaling-bridge-down is in desired state: Ready
2022-04-09 15:28:19 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:285] Scaling KafkaBridge to zero replicas
2022-04-09 15:28:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-cluster-name will have desired state: Ready
2022-04-09 15:28:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-cluster-name is in desired state: Ready
2022-04-09 15:28:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaBridge: bridge-my-cluster-da44aea8 is in desired state: Ready
2022-04-09 15:28:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:28:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomBridgeLabelsAreProperlySet
2022-04-09 15:28:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge bridge-my-cluster-da44aea8 in namespace infra-namespace
2022-04-09 15:28:23 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaBridge: example-bridge is in desired state: Ready
2022-04-09 15:28:23 [ForkJoinPool-3-worker-13] [32mINFO [m [HttpBridgeIsolatedST:351] Adding label to KafkaBridge resource, the CR should be recreated
2022-04-09 15:28:23 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: example-bridge-bridge will be ready
2022-04-09 15:28:23 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: example-bridge-bridge is ready
2022-04-09 15:28:23 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment example-bridge-bridge to be ready
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testReceiveSimpleMessage-FINISHED
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeSubresource-STARTED
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge scaling-bridge-up in namespace infra-namespace
2022-04-09 15:28:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: scaling-bridge-up will have desired state: Ready
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomBridgeLabelsAreProperlySet-FINISHED
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomAndUpdatedValues-STARTED
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge custom-bridge in namespace infra-namespace
2022-04-09 15:28:31 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-bridge will have desired state: Ready
2022-04-09 15:28:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:28:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleBridgeToZero
2022-04-09 15:28:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge scaling-bridge-down in namespace infra-namespace
2022-04-09 15:28:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:28:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeToZero-FINISHED
2022-04-09 15:28:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:28:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaBridge: scaling-bridge-up is in desired state: Ready
2022-04-09 15:28:52 [ForkJoinPool-3-worker-5] [32mINFO [m [HttpBridgeIsolatedST:312] -------> Scaling KafkaBridge subresource <-------
2022-04-09 15:28:52 [ForkJoinPool-3-worker-5] [32mINFO [m [HttpBridgeIsolatedST:313] Scaling subresource replicas to 4
2022-04-09 15:28:52 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: scaling-bridge-up-bridge will be ready
2022-04-09 15:28:52 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: scaling-bridge-up-bridge is ready
2022-04-09 15:28:52 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment scaling-bridge-up-bridge to be ready
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment example-bridge-bridge is ready
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [HttpBridgeIsolatedST:358] Checking that observed gen. is still on 1 (recreation) and new label is present
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [HttpBridgeIsolatedST:363] Changing deployment strategy to ROLLING_UPDATE
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: example-bridge will have desired state: Ready
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaBridge: example-bridge is in desired state: Ready
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [HttpBridgeIsolatedST:368] Adding another label to KafkaBridge resource, pods should be rolled
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: example-bridge-bridge will be ready
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: example-bridge-bridge is ready
2022-04-09 15:29:02 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment example-bridge-bridge to be ready
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-bridge is in desired state: Ready
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [HttpBridgeIsolatedST:225] Verify values before update
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:255] Getting pods by prefix custom-bridge-bridge in pod name
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container custom-bridge-bridge
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:221] Getting pods by prefix in name custom-bridge-bridge
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container custom-bridge-bridge
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [HttpBridgeIsolatedST:230] Check if actual env variable KAFKA_BRIDGE_PRODUCER_CONFIG has different value than test.value
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [HttpBridgeIsolatedST:236] Updating values in Bridge container
2022-04-09 15:29:04 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment custom-bridge-bridge rolling update
2022-04-09 15:29:22 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment scaling-bridge-up-bridge is ready
2022-04-09 15:29:22 [ForkJoinPool-3-worker-5] [32mINFO [m [HttpBridgeIsolatedST:317] Check if replicas is set to 4, naming prefix should be same and observed generation higher
2022-04-09 15:29:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:29:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleBridgeSubresource
2022-04-09 15:29:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge scaling-bridge-up in namespace infra-namespace
2022-04-09 15:29:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:29:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeSubresource-FINISHED
2022-04-09 15:29:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:29:39 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: custom-bridge-bridge will be ready
2022-04-09 15:29:39 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:168] Deployment: custom-bridge-bridge is ready
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment example-bridge-bridge is ready
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [HttpBridgeIsolatedST:372] Checking that observed gen. higher (rolling update) and label is changed
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigureDeploymentStrategy
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge example-bridge in namespace infra-namespace
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testConfigureDeploymentStrategy-FINISHED
2022-04-09 15:29:42 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [DeploymentUtils:141] Deployment custom-bridge-bridge rolling update finished
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [HttpBridgeIsolatedST:253] Verify values after update
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:255] Getting pods by prefix custom-bridge-bridge in pod name
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:221] Getting pods by prefix in name custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:190] Getting pods by prefix in name custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:194] Testing configuration for container custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:190] Getting pods by prefix in name custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:194] Testing configuration for container custom-bridge-bridge
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-09 15:29:49 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge custom-bridge in namespace infra-namespace
2022-04-09 15:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 15:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-132752153 in namespace infra-namespace
2022-04-09 15:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-132752153 will be in active state
2022-04-09 15:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-132752153 to finished
2022-04-09 15:29:59 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:29:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomAndUpdatedValues-FINISHED
2022-04-09 15:29:59 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:311] Verifying labels on pod type my-bridge
2022-04-09 15:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-09 15:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessage
2022-04-09 15:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-570323928 in namespace infra-namespace
2022-04-09 15:30:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-396441818-1558180707 in namespace infra-namespace
2022-04-09 15:30:02 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job consumer-132752153 in namespace infra-namespace
2022-04-09 15:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testSendSimpleMessage-FINISHED
2022-04-09 15:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeIsolatedST
2022-04-09 15:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-09 15:30:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-cluster-name in namespace infra-namespace
2022-04-09 15:30:12 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-cluster-name in namespace infra-namespace
2022-04-09 15:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 372.471 s - in io.strimzi.systemtest.bridge.HttpBridgeIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.HelmChartIsolatedST
2022-04-09 15:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:31:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:31:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:31:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:31:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:31:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:31:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:31:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:31:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:31:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:31:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmChartIsolatedST:67] Creating resources before the test class
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: kube-system
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace kube-system apply -f -
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:31:53 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmClient:44] Installing helm-chart strimzi-systemtests
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: helm install strimzi-systemtests --set defaultImageRegistry=quay.io,defaultImageRepository=strimzi,fullReconciliationIntervalMs=30000,kafkaBridge.image.tag=latest,resources.limits.memory=512Mi,kafkaBridge.image.repository=strimzi,featureGates=,image.imagePullPolicy=Always,watchAnyNamespace=false,resources.requests.memory=512Mi,operationTimeoutMs=300000,resources.limits.cpu=1000m,logLevelOverride=DEBUG,defaultImageTag=latest,resources.requests.cpu=200m,kafkaBridge.image.registry=quay.io --timeout 120s --debug /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/helm-charts/helm3/strimzi-kafka-operator --namespace infra-namespace --wait
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.HelmChartIsolatedST.testStrimziComponentsViaHelmChart-STARTED
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d81a12e7-kafka-clients in namespace infra-namespace
2022-04-09 15:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d81a12e7-kafka-clients will be ready
2022-04-09 15:32:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d81a12e7-kafka-clients is ready
2022-04-09 15:32:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:32:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d81a12e7 will have desired state: Ready
2022-04-09 15:33:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d81a12e7 is in desired state: Ready
2022-04-09 15:33:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d81a12e7-scraper in namespace infra-namespace
2022-04-09 15:33:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d81a12e7-scraper will be ready
2022-04-09 15:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d81a12e7-scraper is ready
2022-04-09 15:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-d81a12e7-scraper to be ready
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-d81a12e7-scraper is ready
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-d81a12e7-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-d81a12e7-allow in namespace infra-namespace
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1675760671-750605331 in namespace infra-namespace
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:34:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1675760671-750605331 will have desired state: Ready
2022-04-09 15:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1675760671-750605331 is in desired state: Ready
2022-04-09 15:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-d81a12e7 will have desired state: Ready
2022-04-09 15:35:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-d81a12e7 is in desired state: Ready
2022-04-09 15:35:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-d81a12e7 will have desired state: Ready
2022-04-09 15:35:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-d81a12e7 is in desired state: Ready
2022-04-09 15:35:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:35:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-d81a12e7 will have desired state: Ready
2022-04-09 15:35:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-d81a12e7 is in desired state: Ready
2022-04-09 15:35:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:35:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testStrimziComponentsViaHelmChart
2022-04-09 15:35:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:35:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d81a12e7-scraper in namespace infra-namespace
2022-04-09 15:35:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:35:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d81a12e7-kafka-clients in namespace infra-namespace
2022-04-09 15:35:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-d81a12e7-allow in namespace infra-namespace
2022-04-09 15:35:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:35:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-d81a12e7 in namespace infra-namespace
2022-04-09 15:35:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1675760671-750605331 in namespace infra-namespace
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.HelmChartIsolatedST.testStrimziComponentsViaHelmChart-FINISHED
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmClient:71] Deleting helm-chart:strimzi-systemtests in namespace:infra-namespace
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HelmChartIsolatedST
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmClient:71] Deleting helm-chart:strimzi-systemtests in namespace:infra-namespace
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:35:57 [ForkJoinPool-3-worker-3] [33mWARN [m [KubeClusterResource:151] Namespace infra-namespace is already created, going to delete it
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:36:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:36:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:36:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 328.467 s - in io.strimzi.systemtest.specific.HelmChartIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.SpecificIsolatedST
2022-04-09 15:36:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:37:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:37:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:37:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:38:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:38:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:508] 0.21.4
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAware-STARTED
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9858504c in namespace infra-namespace
2022-04-09 15:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9858504c will have desired state: Ready
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9858504c is in desired state: Ready
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-9858504c-kafka-0 -- /bin/bash -c cat /opt/kafka/init/rack.id
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-9858504c-kafka-0 -- /bin/bash -c cat /tmp/strimzi.properties | grep broker.rack
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace infra-namespace
2022-04-09 15:39:28 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-09 15:39:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-consumer in namespace infra-namespace
2022-04-09 15:39:30 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-consumer will be in active state
2022-04-09 15:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAware
2022-04-09 15:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace infra-namespace
2022-04-09 15:39:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9858504c in namespace infra-namespace
2022-04-09 15:39:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-consumer in namespace infra-namespace
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAware-FINISHED
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectWrongDeployment-STARTED
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:39:41 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:39:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:39:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:40:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:40:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:40:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:40:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:40:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-09 15:40:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:40:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:40:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-09 15:40:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e7c227d2 in namespace infra-namespace
2022-04-09 15:40:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e7c227d2 will have desired state: Ready
2022-04-09 15:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e7c227d2 is in desired state: Ready
2022-04-09 15:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e7c227d2-kafka-clients in namespace infra-namespace
2022-04-09 15:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e7c227d2-kafka-clients will be ready
2022-04-09 15:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e7c227d2-kafka-clients is ready
2022-04-09 15:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:196] Deploy KafkaConnect with wrong rack-aware topology key: wrong-key
2022-04-09 15:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e7c227d2-scraper in namespace infra-namespace
2022-04-09 15:42:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e7c227d2-scraper will be ready
2022-04-09 15:42:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e7c227d2-scraper is ready
2022-04-09 15:42:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-e7c227d2-scraper to be ready
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-e7c227d2-scraper is ready
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-e7c227d2-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-e7c227d2-allow in namespace infra-namespace
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-e7c227d2 in namespace infra-namespace
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-e7c227d2-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-e7c227d2-allow in namespace infra-namespace
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 15:42:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: my-cluster-e7c227d2-connect will be in pending phase
2022-04-09 15:42:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:227] Waiting for ClusterOperator to get timeout operation of incorrectly set up KafkaConnect
2022-04-09 15:43:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-e7c227d2 will have desired state: Ready
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-e7c227d2 is in desired state: Ready
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:238] KafkaConnect is ready with changed rack key: 'rack-key'.
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:239] Verify KafkaConnect rack key update
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:156] Send and receive messages through KafkaConnect
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-e7c227d2-connect-6c5fb77b7d-d2grk -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-e7c227d2-scraper-769fb8f46d-t6wn8 -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-83383454-4042747", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-e7c227d2-connect-api.infra-namespace.svc:8083/connectors
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@9e5dc04, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-83383454-4042747], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e7c227d2-kafka-clients-77c76b6d9-z777p', podNamespace='infra-namespace', bootstrapServer='my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@46e66a6e}
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092:my-topic-83383454-4042747 from pod my-cluster-e7c227d2-kafka-clients-77c76b6d9-z777p
2022-04-09 15:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e7c227d2-kafka-clients-77c76b6d9-z777p -n infra-namespace -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-83383454-4042747
2022-04-09 15:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 15:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 15:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1da72ab5, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-891366422, --group-instance-id, instance137950163, --bootstrap-server, my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-83383454-4042747], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e7c227d2-kafka-clients-77c76b6d9-z777p', podNamespace='infra-namespace', bootstrapServer='my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-891366422', consumerInstanceId='instance137950163', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5284fde6}
2022-04-09 15:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092#my-topic-83383454-4042747 from pod my-cluster-e7c227d2-kafka-clients-77c76b6d9-z777p
2022-04-09 15:44:54 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e7c227d2-kafka-clients-77c76b6d9-z777p -n infra-namespace -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-891366422 --group-instance-id instance137950163 --bootstrap-server my-cluster-e7c227d2-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-83383454-4042747
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-e7c227d2-connect-6c5fb77b7d-d2grk
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-e7c227d2-connect-6c5fb77b7d-d2grk
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:45:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:45:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:45:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:45:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:45:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:45:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:45:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:45:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:45:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@4740d2b0, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:46:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:46:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-09 15:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:46:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAwareConnectWrongDeployment
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-e7c227d2-allow in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e7c227d2-kafka-clients in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e7c227d2 in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-e7c227d2 in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-e7c227d2-allow in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e7c227d2-scraper in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectWrongDeployment-FINISHED
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testDeployUnsupportedKafka-STARTED
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-92249234 in namespace infra-namespace
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:414] Kafka with version 6.6.6 deployed.
2022-04-09 15:46:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-92249234 will have desired state: NotReady
2022-04-09 15:46:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-92249234 is in desired state: NotReady
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployUnsupportedKafka
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-92249234 in namespace infra-namespace
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testDeployUnsupportedKafka-FINISHED
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context SpecificIsolatedST is everything deleted.
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 626.462 s - in io.strimzi.systemtest.specific.SpecificIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.DrainCleanerIsolatedST
2022-04-09 15:46:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:47:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:47:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:47:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:47:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:47:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=strimzi-drain-cleaner
namespaceToWatch=strimzi-drain-cleaner
bindingsNamespaces=[strimzi-drain-cleaner]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: strimzi-drain-cleaner
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: strimzi-drain-cleaner
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:47:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:47:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:48:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:48:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:48:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:48:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RequiredMinKubeApiVersionCondition:30] testDrainCleanerWithComponents is @RequiredMinKubeApiVersion with version 1.17, but the running on cluster with 1.16: Ignoring testDrainCleanerWithComponents
2022-04-09 15:48:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:48:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context DrainCleanerIsolatedST is everything deleted.
2022-04-09 15:48:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 78.175 s - in io.strimzi.systemtest.specific.DrainCleanerIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST
2022-04-09 15:48:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:48:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:48:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:48:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:48:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:48:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:48:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:48:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace strimzi-drain-cleaner
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:48:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:48:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:48:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:48:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-09 15:48:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:48:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:48:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:48:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-namespace-test
bindingsNamespaces=[infra-namespace, second-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:49:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:49:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:49:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:49:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:49:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:49:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace second-namespace-test
2022-04-09 15:49:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleNamespaceIsolatedST:59] Deploying Kafka in different namespace than CO when CO watches multiple namespaces
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster in namespace second-namespace-test
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-STARTED
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleNamespaceIsolatedST:45] Deploying TO to watch a different namespace that it is deployed in
2022-04-09 15:51:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:51:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace second-namespace-test exec my-cluster-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 15:51:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:51:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-645724863-391618503 in namespace infra-namespace
2022-04-09 15:51:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-645724863-391618503 will have desired state: Ready
2022-04-09 15:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-645724863-391618503 is in desired state: Ready
2022-04-09 15:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicOperatorWatchingOtherNamespace
2022-04-09 15:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-645724863-391618503 in namespace infra-namespace
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-FINISHED
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-STARTED
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleNamespaceIsolatedST:69] Deploying KafkaMirrorMaker in different namespace than CO when CO watches multiple namespaces
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-target in namespace second-namespace-test
2022-04-09 15:51:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-target will have desired state: Ready
2022-04-09 15:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-target is in desired state: Ready
2022-04-09 15:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-09 15:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:71] Waiting for creation my-cluster-mirror-maker in namespace second-namespace-test
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployMirrorMakerAcrossMultipleNamespace
2022-04-09 15:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-09 15:53:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-target in namespace second-namespace-test
2022-04-09 15:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-FINISHED
2022-04-09 15:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MultipleNamespaceIsolatedST
2022-04-09 15:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace second-namespace-test
2022-04-09 15:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 340.867 s - in io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-04-09 15:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:190] Creating resources before the test class
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-09 15:54:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:54:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:54:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: third-namespace-test
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: third-namespace-test
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 15:54:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 15:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 15:55:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 15:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 15:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-04-09 15:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace third-namespace-test
2022-04-09 15:55:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-04-09 15:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-04-09 15:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-second in namespace second-namespace-test
2022-04-09 15:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-second will have desired state: Ready
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-second is in desired state: Ready
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:82] Deploying Kafka cluster in different namespace than CO when CO watches all namespaces
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster-second in namespace second-namespace-test
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-STARTED
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:66] Deploying TO to watch a different namespace that it is deployed in
2022-04-09 15:58:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-04-09 15:58:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace third-namespace-test exec my-cluster-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-09 15:58:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 15:58:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-510509065-538987425 in namespace second-namespace-test
2022-04-09 15:58:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-510509065-538987425 will have desired state: Ready
2022-04-09 15:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-510509065-538987425 is in desired state: Ready
2022-04-09 15:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicOperatorWatchingOtherNamespace
2022-04-09 15:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-510509065-538987425 in namespace second-namespace-test
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-FINISHED
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUOWatchingOtherNamespace-STARTED
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:121] Creating user in other namespace than CO and Kafka cluster with UO
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1577833048-1701092451 in namespace second-namespace-test
2022-04-09 15:58:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1577833048-1701092451 will have desired state: Ready
2022-04-09 15:58:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1577833048-1701092451 is in desired state: Ready
2022-04-09 15:58:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:58:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:58:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUOWatchingOtherNamespace
2022-04-09 15:58:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1577833048-1701092451 in namespace second-namespace-test
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUOWatchingOtherNamespace-FINISHED
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUserInDifferentNamespace-STARTED
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1577833048-1701092451 in namespace second-namespace-test
2022-04-09 15:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1577833048-1701092451 will have desired state: Ready
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1577833048-1701092451 is in desired state: Ready
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:137] KafkaUser condition status: True
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:138] KafkaUser condition type: Ready
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:148] Copying secret Secret(apiVersion=v1, data={ca.crt=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, user.crt=LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVJakNDQWdxZ0F3SUJBZ0lVUkZyNHZKbUN5SDJvNzU5NmxtSG5FcldTc3U4d0RRWUpLb1pJaHZjTkFRRU4KQlFBd0xURVRNQkVHQTFVRUNnd0thVzh1YzNSeWFXMTZhVEVXTUJRR0ExVUVBd3dOWTJ4cFpXNTBjeTFqWVNCMgpNREFlRncweU1qQTBNRGt4TlRVNE16bGFGdzB5TXpBME1Ea3hOVFU0TXpsYU1DZ3hKakFrQmdOVkJBTU1IVzE1CkxYVnpaWEl0TVRVM056Z3pNekEwT0MweE56QXhNRGt5TkRVeE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0MKQVE4QU1JSUJDZ0tDQVFFQTRBZFYyMnZSZ0lPWDJHTXczTG8wRmdxOWVkQ0NMakFhN1N4WUNmK2l1K0w4ZHQwTgpJUTBvL21NRHcyR0xUS0Z3eGl0N3ZHZnFNV0dKVEZyVG4vTmRZc09DM2Y5QUl4S1RzYmlQVUJaVkYrRURoK1lLClU5bjhWRGFjR3BYTk1zK0YxZjBuQVlCY0o1bVdTSGxidHNlaUoxQ0NxOGhnTlNGZ1F4b1RJTHNZTitJcGVvWHoKUEhHcERnc0F5Ui9vUW5WblZqR1VPbUN3WHpOL2FBNGZqbHBBZUVXcFNiYjY4Q2FiU2NkNTcxU2J6UVMyRVdHSwpQLzNZbFBsaUwzY0JndUU2dTUxYkJUTDI2MC8wQnZsU1p0eDZpWmlaMUhCL2xRZlNsZ0VPenNTRnFueWxMYU5WCk1FamRMUG1YVG9mYUZSa1hpVHl5Q2tqa1dsMUh5QnM2Rm54UGl3SURBUUFCb3o4d1BUQWRCZ05WSFE0RUZnUVUKWHMvcTdrZjg5dS9tUkljQTNxTHVXTmpsN21vd0RBWURWUjBUQVFIL0JBSXdBREFPQmdOVkhROEJBZjhFQkFNQwpCYUF3RFFZSktvWklodmNOQVFFTkJRQURnZ0lCQUlBbit5ZURnK0J3THlnSDFKWFRVYnhzNWlqWm5Wd09tQkRSCkFxMDVIMGVaYU94dmNHdlJiWkdTVzNIeGFSQ2JPSklHNHhiWkp2dWt5QVRYSk1kMlUxOGc1bjAzVjZ0RExkTDMKMVRmNkhOOFE0M0toTFQrT0JjOTNQcmVDbUU1djUrYmMvcnZyOGFrVkhzbE5JanJqcWJBYmt5ZW9LaEp0amcrTQpaSjF2cWw5QWNTb1dpVHVRVk9JaG1xZTNLUGtvaGQ2WFltS2hZUVB0Ym01V3hjeTQzMHNETEJwdHYyc0pyNlh5CkdlRSt1R2UzdVk0MXg2RnNDRmJHSzYwM0xjNEZZbGVlSkd0aDhXWWNCQkhtTDhRN3RkS2h3V1BKY0U5VXh6WGYKdXFBTXY1TmtxTTZRdTBDYnUrL0VhWE5LUXcwbzFSUGRvd2ltaWxkdVlxVGtUSXpYR29Ed01qeEZqZExmNW8xSgprei8wRkpoWGRlVWQ0UGpnWnMvcUFHdGIrQ0pYK0lRUTczODM1bWxqRk51QlVyUVJlc2E1b0QwT0NVQUlzR0RBCjFibnZ4VTRsUCtWVW1aaWViNmRWL0FSSE40NGVMbFZxSWl6WEpsTFVTMHRvK1hMUnMrbk03MS9mOU5JcDBDeFoKVFE2cnVSMENlNnBsSWJrS0g3QUU2azdrakltYVR4ZFo0YitBMHh2NTl6TnhVbXdJMEttVCt2SnVxZ2pPclZoLwpMRExQY0x2WEltdWVIektvQzVlaTFjbjVGLzZSWTl4ZHFSVG03N1kvT0FOdTNCTzkzS1FqTmdMYm5kOWExQlJJCkJZNGlnTHdNbWFmZG5SNmtrTlppSzlGbUtNWUNydTBYME9lV2hZSzg2b0oya1RscndGdHZDdlhFbHNJUWp0RFQKNjFkS3B5QzEKLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=, user.key=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, user.p12=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, user.password=VFh6aVFNOEZseUdq}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=2022-04-09T15:58:40Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-user-1577833048-1701092451, app.kubernetes.io/managed-by=strimzi-user-operator, app.kubernetes.io/name=strimzi-user-operator, app.kubernetes.io/part-of=strimzi-my-user-1577833048-1701092451, strimzi.io/cluster=my-cluster, strimzi.io/kind=KafkaUser, test.case=testUserInDifferentNamespace}, managedFields=[], name=my-user-1577833048-1701092451, namespace=second-namespace-test, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=KafkaUser, name=my-user-1577833048-1701092451, uid=5ffbdc33-42a0-4dab-a78e-3de75f95b265, additionalProperties={})], resourceVersion=885018, selfLink=/api/v1/namespaces/second-namespace-test/secrets/my-user-1577833048-1701092451, uid=e4e4ea81-71ea-4c7e-8d53-4349a1203ce8, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) from namespace second-namespace-test to namespace third-namespace-test
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-kafka-clients in namespace third-namespace-test
2022-04-09 15:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-kafka-clients will be ready
2022-04-09 15:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-kafka-clients is ready
2022-04-09 15:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 15:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:168] Checking produced and consumed messages to pod:my-cluster-kafka-clients-555d6cb89b-p2rvj
2022-04-09 15:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1e48727c, messages=[], arguments=[--max-messages, 100, USER=my_user_1577833048_1701092451, --bootstrap-server, my-cluster-kafka-bootstrap.third-namespace-test.svc:9093, --topic, my-topic-83383454-4042747], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-kafka-clients-555d6cb89b-p2rvj', podNamespace='third-namespace-test', bootstrapServer='my-cluster-kafka-bootstrap.third-namespace-test.svc:9093', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='my-user-1577833048-1701092451', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@97d2e34}
2022-04-09 15:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-kafka-bootstrap.third-namespace-test.svc:9093:my-topic-83383454-4042747 from pod my-cluster-kafka-clients-555d6cb89b-p2rvj
2022-04-09 15:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-kafka-clients-555d6cb89b-p2rvj -n third-namespace-test -- /opt/kafka/producer.sh --max-messages 100 USER=my_user_1577833048_1701092451 --bootstrap-server my-cluster-kafka-bootstrap.third-namespace-test.svc:9093 --topic my-topic-83383454-4042747
2022-04-09 15:58:46 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 15:58:46 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-09 15:58:46 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@28b022ce, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1979891232, --group-instance-id, instance1420580847, USER=my_user_1577833048_1701092451, --bootstrap-server, my-cluster-kafka-bootstrap.third-namespace-test.svc:9093, --topic, my-topic-83383454-4042747], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-kafka-clients-555d6cb89b-p2rvj', podNamespace='third-namespace-test', bootstrapServer='my-cluster-kafka-bootstrap.third-namespace-test.svc:9093', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='my-user-1577833048-1701092451', consumerGroupName='my-consumer-group-1979891232', consumerInstanceId='instance1420580847', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@151b9bf9}
2022-04-09 15:58:46 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-kafka-bootstrap.third-namespace-test.svc:9093:my-topic-83383454-4042747 from pod my-cluster-kafka-clients-555d6cb89b-p2rvj
2022-04-09 15:58:46 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-kafka-clients-555d6cb89b-p2rvj -n third-namespace-test -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1979891232 --group-instance-id instance1420580847 USER=my_user_1577833048_1701092451 --bootstrap-server my-cluster-kafka-bootstrap.third-namespace-test.svc:9093 --topic my-topic-83383454-4042747
2022-04-09 15:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 15:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-09 15:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 15:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 15:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUserInDifferentNamespace
2022-04-09 15:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-kafka-clients in namespace third-namespace-test
2022-04-09 15:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1577833048-1701092451 in namespace second-namespace-test
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUserInDifferentNamespace-FINISHED
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-STARTED
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:92] Deploying KafkaMirrorMaker in different namespace than CO when CO watches all namespaces
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-target in namespace second-namespace-test
2022-04-09 15:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-target will have desired state: Ready
2022-04-09 16:00:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-target is in desired state: Ready
2022-04-09 16:00:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-09 16:00:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:71] Waiting for creation my-cluster-mirror-maker in namespace second-namespace-test
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployMirrorMakerAcrossMultipleNamespace
2022-04-09 16:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-09 16:01:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-target in namespace second-namespace-test
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-FINISHED
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployKafkaConnectAndKafkaConnectorInOtherNamespaceThanCO-STARTED
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-555f02d2-kafka-clients in namespace second-namespace-test
2022-04-09 16:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-555f02d2-kafka-clients will be ready
2022-04-09 16:01:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-555f02d2-kafka-clients is ready
2022-04-09 16:01:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-555f02d2kafka-connect-scraper in namespace second-namespace-test
2022-04-09 16:01:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-555f02d2kafka-connect-scraper will be ready
2022-04-09 16:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-555f02d2kafka-connect-scraper is ready
2022-04-09 16:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-555f02d2kafka-connect-scraper to be ready
2022-04-09 16:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-555f02d2kafka-connect-scraper is ready
2022-04-09 16:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-555f02d2kafka-connect-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 16:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-555f02d2kafka-connect-allow in namespace second-namespace-test
2022-04-09 16:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 16:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-555f02d2kafka-connect in namespace second-namespace-test
2022-04-09 16:02:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-555f02d2kafka-connect will have desired state: Ready
2022-04-09 16:03:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-555f02d2kafka-connect is in desired state: Ready
2022-04-09 16:03:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-555f02d2kafka-connect in namespace second-namespace-test
2022-04-09 16:03:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-555f02d2kafka-connect will have desired state: Ready
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-555f02d2kafka-connect is in desired state: Ready
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-555f02d2kafka-connect will have desired state: Ready
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-555f02d2kafka-connect is in desired state: Ready
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace second-namespace-test exec my-cluster-555f02d2kafka-connect-connect-64cbb6bb64-zk9k4 -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-555f02d2kafka-connect-kafka-clients in namespace second-namespace-test
2022-04-09 16:03:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-555f02d2kafka-connect-kafka-clients will be ready
2022-04-09 16:03:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-555f02d2kafka-connect-kafka-clients is ready
2022-04-09 16:03:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 16:03:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@32a9692d, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092, --topic, my-topic-83383454-4042747], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-555f02d2-kafka-clients-5bbbcb7874-kkv5j', podNamespace='second-namespace-test', bootstrapServer='my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092', topicName='my-topic-83383454-4042747', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@336e6944}
2022-04-09 16:03:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092:my-topic-83383454-4042747 from pod my-cluster-555f02d2-kafka-clients-5bbbcb7874-kkv5j
2022-04-09 16:03:20 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-555f02d2-kafka-clients-5bbbcb7874-kkv5j -n second-namespace-test -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092 --topic my-topic-83383454-4042747
2022-04-09 16:03:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 16:03:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 16:03:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-555f02d2kafka-connect-connect-64cbb6bb64-zk9k4
2022-04-09 16:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-555f02d2kafka-connect-connect-64cbb6bb64-zk9k4
2022-04-09 16:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployKafkaConnectAndKafkaConnectorInOtherNamespaceThanCO
2022-04-09 16:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-555f02d2kafka-connect in namespace second-namespace-test
2022-04-09 16:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-555f02d2kafka-connect-scraper in namespace second-namespace-test
2022-04-09 16:03:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-555f02d2kafka-connect in namespace second-namespace-test
2022-04-09 16:03:24 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-555f02d2-kafka-clients in namespace second-namespace-test
2022-04-09 16:03:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-555f02d2kafka-connect-kafka-clients in namespace second-namespace-test
2022-04-09 16:03:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-555f02d2kafka-connect-allow in namespace second-namespace-test
2022-04-09 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployKafkaConnectAndKafkaConnectorInOtherNamespaceThanCO-FINISHED
2022-04-09 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for AllNamespaceIsolatedST
2022-04-09 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-second in namespace second-namespace-test
2022-04-09 16:04:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace third-namespace-test
2022-04-09 16:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 617.849 s - in io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST
2022-04-09 16:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-04-09 16:04:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:04:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-04-09 16:04:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-09 16:04:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:04:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:04:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:05:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 16:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 16:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 16:05:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 16:05:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:05:35 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-09 16:05:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-09 16:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to internal realm
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to internal realm
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to internal realm
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:60] Using HTTP endpoints
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:07:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-plain-name will have desired state: Ready
2022-04-09 16:08:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-plain-name is in desired state: Ready
2022-04-09 16:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainAuthenticationKafkaConnectIsAbleToConnectToKafkaOAuth-STARTED
2022-04-09 16:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-f2b13ee7-kafka-clients in namespace infra-namespace
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks-STARTED
2022-04-09 16:08:39 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:08:39 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumer-STARTED
2022-04-09 16:08:39 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:08:39 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerBridge-STARTED
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainProducerConsumer-STARTED
2022-04-09 16:08:39 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:08:39 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthPlainIsolatedST:161] Setting producer and consumer properties
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:08:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2063636603-309867334 in namespace infra-namespace
2022-04-09 16:08:39 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthPlainIsolatedST:174] Use clients without access token containing audience token
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:08:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1214478643-456554198 in namespace infra-namespace
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-producer-my-cluster-0a9f0625 in namespace infra-namespace
2022-04-09 16:08:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2063636603-309867334 will have desired state: Ready
2022-04-09 16:08:39 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-f2b13ee7-kafka-clients will be ready
2022-04-09 16:08:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1214478643-456554198 will have desired state: Ready
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-a4d5498f will be in active state
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-producer-my-cluster-0a9f0625 will be in active state
2022-04-09 16:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:oauth-producer-my-cluster-a4d5498f to finish with failure.
2022-04-09 16:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-producer-my-cluster-0a9f0625 to finished
2022-04-09 16:08:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2063636603-309867334 is in desired state: Ready
2022-04-09 16:08:40 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:08:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-be81b85c in namespace infra-namespace
2022-04-09 16:08:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1214478643-456554198 is in desired state: Ready
2022-04-09 16:08:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-a02820a0 in namespace infra-namespace
2022-04-09 16:08:40 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-be81b85c will be in active state
2022-04-09 16:08:40 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-a02820a0 will be in active state
2022-04-09 16:08:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-be81b85c to finished
2022-04-09 16:08:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-a02820a0 to finished
2022-04-09 16:08:42 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-f2b13ee7-kafka-clients is ready
2022-04-09 16:08:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-f2b13ee7-scraper in namespace infra-namespace
2022-04-09 16:08:42 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-f2b13ee7-scraper will be ready
2022-04-09 16:08:44 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-f2b13ee7-scraper is ready
2022-04-09 16:08:44 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-f2b13ee7-scraper to be ready
2022-04-09 16:08:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-be81b85c in namespace infra-namespace
2022-04-09 16:08:50 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-be81b85c will be in active state
2022-04-09 16:08:50 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-a02820a0 in namespace infra-namespace
2022-04-09 16:08:50 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-a02820a0 will be in active state
2022-04-09 16:08:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-be81b85c to finished
2022-04-09 16:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-consumer-my-cluster-0a9f0625 in namespace infra-namespace
2022-04-09 16:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-consumer-my-cluster-0a9f0625 will be in active state
2022-04-09 16:08:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-a02820a0 to finished
2022-04-09 16:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-consumer-my-cluster-0a9f0625 to finished
2022-04-09 16:08:54 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-f2b13ee7-scraper is ready
2022-04-09 16:08:54 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-f2b13ee7-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 16:08:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-f2b13ee7-allow in namespace infra-namespace
2022-04-09 16:08:54 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 16:08:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-f2b13ee7 in namespace infra-namespace
2022-04-09 16:08:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-f2b13ee7 will have desired state: Ready
2022-04-09 16:09:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:09:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumer
2022-04-09 16:09:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-be81b85c in namespace infra-namespace
2022-04-09 16:09:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-be81b85c in namespace infra-namespace
2022-04-09 16:09:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2063636603-309867334 in namespace infra-namespace
2022-04-09 16:09:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a02820a0-kafka-clients in namespace infra-namespace
2022-04-09 16:09:02 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a02820a0-kafka-clients will be ready
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testSaslPlainProducerConsumer
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-consumer-my-cluster-0a9f0625 in namespace infra-namespace
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-producer-my-cluster-0a9f0625 in namespace infra-namespace
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainProducerConsumer-FINISHED
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerConnect-STARTED
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-21003559-1179103345 in namespace infra-namespace
2022-04-09 16:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-21003559-1179103345 will have desired state: Ready
2022-04-09 16:09:03 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a02820a0-kafka-clients is ready
2022-04-09 16:09:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:09:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: oauth-cluster-plain-name will have desired state: Ready
2022-04-09 16:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-21003559-1179103345 is in desired state: Ready
2022-04-09 16:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-1ff0451e in namespace infra-namespace
2022-04-09 16:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-1ff0451e will be in active state
2022-04-09 16:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-1ff0451e to finished
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumer-FINISHED
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck-STARTED
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthPlainIsolatedST:213] Use clients with clientId not containing 'hello-world' in access token.
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-producer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:09:12 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-producer-my-cluster-9c129b34 will be in active state
2022-04-09 16:09:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:kafka-audience-producer-my-cluster-9c129b34 to finish with failure.
2022-04-09 16:09:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-1ff0451e in namespace infra-namespace
2022-04-09 16:09:14 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-1ff0451e will be in active state
2022-04-09 16:09:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-1ff0451e to finished
2022-04-09 16:09:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1ff0451e-scraper in namespace infra-namespace
2022-04-09 16:09:26 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1ff0451e-scraper will be ready
2022-04-09 16:09:28 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1ff0451e-scraper is ready
2022-04-09 16:09:28 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-1ff0451e-scraper to be ready
2022-04-09 16:09:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaBridge: oauth-cluster-plain-name is in desired state: Ready
2022-04-09 16:09:28 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:09:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer-my-cluster-a02820a0 in namespace infra-namespace
2022-04-09 16:09:28 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer-my-cluster-a02820a0 will be in active state
2022-04-09 16:09:29 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:bridge-producer-my-cluster-a02820a0 to finished
2022-04-09 16:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-1ff0451e-scraper is ready
2022-04-09 16:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1ff0451e-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 16:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-1ff0451e-allow in namespace infra-namespace
2022-04-09 16:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 16:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-1ff0451e in namespace infra-namespace
2022-04-09 16:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-1ff0451e will have desired state: Ready
2022-04-09 16:10:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-f2b13ee7 is in desired state: Ready
2022-04-09 16:10:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:10:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testSaslPlainAuthenticationKafkaConnectIsAbleToConnectToKafkaOAuth
2022-04-09 16:10:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-f2b13ee7-allow in namespace infra-namespace
2022-04-09 16:10:04 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-f2b13ee7 in namespace infra-namespace
2022-04-09 16:10:14 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-f2b13ee7-scraper in namespace infra-namespace
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-1ff0451e is in desired state: Ready
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-1ff0451e-connect-6b677dc8b4-4ntcx -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-1ff0451e-connect-6b677dc8b4-4ntcx -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-21003559-1179103345", "file": "/tmp/test-file-sink.txt" } }' http://localhost:8083/connectors
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-1ff0451e-connect-6b677dc8b4-4ntcx
2022-04-09 16:10:45 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-1ff0451e-connect-6b677dc8b4-4ntcx
2022-04-09 16:10:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-f2b13ee7-kafka-clients in namespace infra-namespace
2022-04-09 16:10:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:10:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerConnect
2022-04-09 16:10:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1ff0451e-scraper in namespace infra-namespace
2022-04-09 16:11:19 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:11:19 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerBridge
2022-04-09 16:11:19 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a02820a0-kafka-clients in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-1ff0451e in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-1ff0451e-allow in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-a02820a0 in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:11:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainAuthenticationKafkaConnectIsAbleToConnectToKafkaOAuth-FINISHED
2022-04-09 16:11:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-1ff0451e in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer-my-cluster-a02820a0 in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-a02820a0 in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-1ff0451e in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1214478643-456554198 in namespace infra-namespace
2022-04-09 16:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-21003559-1179103345 in namespace infra-namespace
2022-04-09 16:11:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:11:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerConnect-FINISHED
2022-04-09 16:11:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:11:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:11:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerBridge-FINISHED
2022-04-09 16:11:59 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:12:19 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testProducerConsumerAudienceTokenChecks$0(OauthPlainIsolatedST.java:176)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks(OauthPlainIsolatedST.java:176)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 16:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:100] Client job 'oauth-producer-my-cluster-a4d5498f' finished with expected timeout.
2022-04-09 16:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-a4d5498f will be in active state
2022-04-09 16:12:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:oauth-consumer-my-cluster-a4d5498f to finish with failure.
2022-04-09 16:12:53 [ForkJoinPool-3-worker-15] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testAccessTokenClaimCheck$2(OauthPlainIsolatedST.java:229)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck(OauthPlainIsolatedST.java:229)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 16:12:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:100] Client job 'kafka-audience-producer-my-cluster-9c129b34' finished with expected timeout.
2022-04-09 16:12:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-consumer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:12:53 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-consumer-my-cluster-9c129b34 will be in active state
2022-04-09 16:12:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:kafka-audience-consumer-my-cluster-9c129b34 to finish with failure.
2022-04-09 16:16:00 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testProducerConsumerAudienceTokenChecks$1(OauthPlainIsolatedST.java:178)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks(OauthPlainIsolatedST.java:178)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 16:16:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:100] Client job 'oauth-consumer-my-cluster-a4d5498f' finished with expected timeout.
2022-04-09 16:16:10 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthPlainIsolatedST:183] Use clients with Access token containing audience token
2022-04-09 16:16:10 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:16:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-producer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:16:10 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-producer-my-cluster-a4d5498f will be in active state
2022-04-09 16:16:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-producer-my-cluster-a4d5498f to finished
2022-04-09 16:16:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-consumer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:16:19 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-consumer-my-cluster-a4d5498f will be in active state
2022-04-09 16:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-consumer-my-cluster-a4d5498f to finished
2022-04-09 16:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerAudienceTokenChecks
2022-04-09 16:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-producer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:16:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-consumer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:16:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:16:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-a4d5498f in namespace infra-namespace
2022-04-09 16:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks-FINISHED
2022-04-09 16:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:16:34 [ForkJoinPool-3-worker-15] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testAccessTokenClaimCheck$3(OauthPlainIsolatedST.java:231)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck(OauthPlainIsolatedST.java:231)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 16:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:100] Client job 'kafka-audience-consumer-my-cluster-9c129b34' finished with expected timeout.
2022-04-09 16:16:44 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthPlainIsolatedST:236] Use clients with clientId containing 'hello-world' in access token.
2022-04-09 16:16:44 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:16:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:16:44 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-9c129b34 will be in active state
2022-04-09 16:16:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-9c129b34 to finished
2022-04-09 16:16:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:16:54 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-9c129b34 will be in active state
2022-04-09 16:16:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-9c129b34 to finished
2022-04-09 16:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testAccessTokenClaimCheck
2022-04-09 16:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:17:06 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-consumer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:17:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-producer-my-cluster-9c129b34 in namespace infra-namespace
2022-04-09 16:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck-FINISHED
2022-04-09 16:17:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker-STARTED
2022-04-09 16:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1390777264-1191102896 in namespace infra-namespace
2022-04-09 16:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1390777264-1191102896 will have desired state: Ready
2022-04-09 16:17:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1390777264-1191102896 is in desired state: Ready
2022-04-09 16:17:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-e9e73c3a in namespace infra-namespace
2022-04-09 16:17:08 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-e9e73c3a will be in active state
2022-04-09 16:17:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-e9e73c3a to finished
2022-04-09 16:17:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-e9e73c3a in namespace infra-namespace
2022-04-09 16:17:12 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-e9e73c3a will be in active state
2022-04-09 16:17:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-e9e73c3a to finished
2022-04-09 16:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e9e73c3a-target in namespace infra-namespace
2022-04-09 16:17:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e9e73c3a-target will have desired state: Ready
2022-04-09 16:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e9e73c3a-target is in desired state: Ready
2022-04-09 16:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:18:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: oauth-cluster-plain-name will have desired state: Ready
2022-04-09 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: oauth-cluster-plain-name is in desired state: Ready
2022-04-09 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:438] Deleting the Job
2022-04-09 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:441] Creating new client with new consumer-group and also to point on my-cluster-e9e73c3a-target cluster
2022-04-09 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer in namespace infra-namespace
2022-04-09 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer will be in active state
2022-04-09 16:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer to finished
2022-04-09 16:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMaker
2022-04-09 16:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e9e73c3a-target in namespace infra-namespace
2022-04-09 16:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-e9e73c3a in namespace infra-namespace
2022-04-09 16:19:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1390777264-1191102896 in namespace infra-namespace
2022-04-09 16:19:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer in namespace infra-namespace
2022-04-09 16:19:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-e9e73c3a in namespace infra-namespace
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker-FINISHED
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker2-STARTED
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1735400743-1673552465 in namespace infra-namespace
2022-04-09 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1735400743-1673552465 will have desired state: Ready
2022-04-09 16:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1735400743-1673552465 is in desired state: Ready
2022-04-09 16:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-49818744 in namespace infra-namespace
2022-04-09 16:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-49818744 will be in active state
2022-04-09 16:20:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-49818744 to finished
2022-04-09 16:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-49818744 in namespace infra-namespace
2022-04-09 16:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-49818744 will be in active state
2022-04-09 16:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-49818744 to finished
2022-04-09 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-49818744-target in namespace infra-namespace
2022-04-09 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-49818744-target will have desired state: Ready
2022-04-09 16:21:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-49818744-target is in desired state: Ready
2022-04-09 16:21:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:21:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: oauth-cluster-plain-name will have desired state: Ready
2022-04-09 16:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: oauth-cluster-plain-name is in desired state: Ready
2022-04-09 16:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:588] Deleting the Job oauth-consumer-my-cluster-49818744
2022-04-09 16:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:591] Creating new client with new consumer-group and also to point on my-cluster-49818744-target cluster
2022-04-09 16:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-49818744 in namespace infra-namespace
2022-04-09 16:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-49818744 will be in active state
2022-04-09 16:22:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-49818744 to finished
2022-04-09 16:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMaker2
2022-04-09 16:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-49818744-target in namespace infra-namespace
2022-04-09 16:22:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:22:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1735400743-1673552465 in namespace infra-namespace
2022-04-09 16:22:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-49818744 in namespace infra-namespace
2022-04-09 16:22:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-49818744 in namespace infra-namespace
2022-04-09 16:22:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-49818744 in namespace infra-namespace
2022-04-09 16:23:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:23:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker2-FINISHED
2022-04-09 16:23:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:23:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthPlainIsolatedST
2022-04-09 16:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-plain-name in namespace infra-namespace
2022-04-09 16:23:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:23:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:23:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:23:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthPlainIsolatedST is everything deleted.
2022-04-09 16:23:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,148.443 s - in io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
2022-04-09 16:23:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 16:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 16:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 16:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 16:23:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:23:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:23:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:23:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:23:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:23:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:23:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:23:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:23:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:23:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:23:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:23:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:23:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:24:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 16:24:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 16:24:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 16:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 16:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-09 16:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-09 16:26:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-09 16:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to kafka-authz realm
2022-04-09 16:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to kafka-authz realm
2022-04-09 16:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to kafka-authz realm
2022-04-09 16:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:55] Using HTTPS endpoints
2022-04-09 16:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-authz-name in namespace infra-namespace
2022-04-09 16:26:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-09 16:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-09 16:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:680] Setting producer and consumer properties
2022-04-09 16:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-a-client in namespace infra-namespace
2022-04-09 16:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-a-client will have desired state: Ready
2022-04-09 16:27:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: team-a-client is in desired state: Ready
2022-04-09 16:27:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-b-client in namespace infra-namespace
2022-04-09 16:27:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-b-client will have desired state: Ready
2022-04-09 16:27:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: team-b-client is in desired state: Ready
2022-04-09 16:27:55 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:27:55 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:27:55 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:27:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:27:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:27:55 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic-STARTED
2022-04-09 16:27:55 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic-STARTED
2022-04-09 16:27:55 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.smokeTestForClients-STARTED
2022-04-09 16:27:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopicStartingWithXAndTeamBReadFromTopicStartingWithX-STARTED
2022-04-09 16:27:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic-STARTED
2022-04-09 16:27:55 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:27:55 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:27:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1328338904-598226331 in namespace infra-namespace
2022-04-09 16:27:55 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:27:55 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:27:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-637863254-2076655810 in namespace infra-namespace
2022-04-09 16:27:55 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:27:55 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-2068962692-1562584796 in namespace infra-namespace
2022-04-09 16:27:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-1257079818-146215755 in namespace infra-namespace
2022-04-09 16:27:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topicmy-topic-68439345-105310239 in namespace infra-namespace
2022-04-09 16:27:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1328338904-598226331 will have desired state: Ready
2022-04-09 16:27:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-1257079818-146215755 will have desired state: Ready
2022-04-09 16:27:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-637863254-2076655810 will have desired state: Ready
2022-04-09 16:27:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topicmy-topic-68439345-105310239 will have desired state: Ready
2022-04-09 16:27:55 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-2068962692-1562584796 will have desired state: Ready
2022-04-09 16:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-1257079818-146215755 is in desired state: Ready
2022-04-09 16:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [OauthAuthorizationIsolatedST:208] Sending 100 messages to broker with topic name a-topic-my-topic-1257079818-146215755
2022-04-09 16:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-98eac76c in namespace infra-namespace
2022-04-09 16:27:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-637863254-2076655810 is in desired state: Ready
2022-04-09 16:27:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1328338904-598226331 is in desired state: Ready
2022-04-09 16:27:56 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:259] Sending 100 messages to broker with topic name my-topic-83383454-4042747
2022-04-09 16:27:56 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthAuthorizationIsolatedST:146] Sending 100 messages to broker with topic name my-topic-1328338904-598226331
2022-04-09 16:27:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-2068962692-1562584796 is in desired state: Ready
2022-04-09 16:27:56 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthAuthorizationIsolatedST:147] Producer will not produce messages because authorization topic will failed. Team A can write only to topic starting with 'x-'
2022-04-09 16:27:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-89184217 in namespace infra-namespace
2022-04-09 16:27:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:27:56 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-965e5760 in namespace infra-namespace
2022-04-09 16:27:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topicmy-topic-68439345-105310239 is in desired state: Ready
2022-04-09 16:27:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-b242cca8 in namespace infra-namespace
2022-04-09 16:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-98eac76c will be in active state
2022-04-09 16:27:56 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-89184217 will be in active state
2022-04-09 16:27:56 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-b242cca8 will be in active state
2022-04-09 16:27:56 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-608be0f5 will be in active state
2022-04-09 16:27:56 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-965e5760 will be in active state
2022-04-09 16:27:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-98eac76c to finished
2022-04-09 16:27:57 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-producer-my-cluster-608be0f5 will be in error state
2022-04-09 16:27:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-b242cca8 to finished
2022-04-09 16:27:57 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:70] Waiting for job: team-b-client-producer-my-cluster-89184217 will be in error state
2022-04-09 16:27:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-965e5760 to finished
2022-04-09 16:28:08 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-consumer-my-cluster-b242cca8 in namespace infra-namespace
2022-04-09 16:28:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-98eac76c in namespace infra-namespace
2022-04-09 16:28:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-965e5760 in namespace infra-namespace
2022-04-09 16:28:08 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-consumer-my-cluster-b242cca8 will be in active state
2022-04-09 16:28:08 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-98eac76c will be in active state
2022-04-09 16:28:08 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-965e5760 will be in active state
2022-04-09 16:28:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-b-client-consumer-my-cluster-b242cca8 to finished
2022-04-09 16:28:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-consumer-my-cluster-965e5760 to finished
2022-04-09 16:28:09 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-consumer-my-cluster-98eac76c will be in error state
2022-04-09 16:28:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-98eac76c in namespace infra-namespace
2022-04-09 16:28:14 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-98eac76c will be in active state
2022-04-09 16:28:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-98eac76c to finished
2022-04-09 16:28:18 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthAuthorizationIsolatedST:154] Sending 100 messages to broker with topic name x-topic-my-cluster-608be0f5
2022-04-09 16:28:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:18 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:265] Sending 100 messages to broker with topic name b-topic
2022-04-09 16:28:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-89184217 in namespace infra-namespace
2022-04-09 16:28:18 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-608be0f5 will be in active state
2022-04-09 16:28:18 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-89184217 will be in active state
2022-04-09 16:28:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-consumer-my-cluster-89184217 in namespace infra-namespace
2022-04-09 16:28:18 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-consumer-my-cluster-89184217 will be in active state
2022-04-09 16:28:19 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-producer-my-cluster-608be0f5 will be in error state
2022-04-09 16:28:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:61] Waiting till producer team-b-client-producer-my-cluster-89184217 and consumer team-b-client-consumer-my-cluster-89184217 finish
2022-04-09 16:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for smokeTestForClients
2022-04-09 16:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-965e5760 in namespace infra-namespace
2022-04-09 16:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-965e5760 in namespace infra-namespace
2022-04-09 16:28:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-2068962692-1562584796 in namespace infra-namespace
2022-04-09 16:28:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:28:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAWriteToTopicStartingWithXAndTeamBReadFromTopicStartingWithX
2022-04-09 16:28:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-b242cca8 in namespace infra-namespace
2022-04-09 16:28:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-consumer-my-cluster-b242cca8 in namespace infra-namespace
2022-04-09 16:28:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topicmy-topic-68439345-105310239 in namespace infra-namespace
2022-04-09 16:28:24 [ForkJoinPool-3-worker-7] [32mINFO [m [OauthAbstractST:153] Deleting team-a-client-consumer-my-cluster-98eac76c job
2022-04-09 16:28:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:28:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAReadFromTopic
2022-04-09 16:28:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-98eac76c in namespace infra-namespace
2022-04-09 16:28:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-98eac76c in namespace infra-namespace
2022-04-09 16:28:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-98eac76c in namespace infra-namespace
2022-04-09 16:28:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-1257079818-146215755 in namespace infra-namespace
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.smokeTestForClients-FINISHED
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSuperUserWithOauthAuthorization-STARTED
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:28:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:28:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamBWriteToTopic
2022-04-09 16:28:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-89184217 in namespace infra-namespace
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topicmy-topic-1690331932-781907737 in namespace infra-namespace
2022-04-09 16:28:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-consumer-my-cluster-89184217 in namespace infra-namespace
2022-04-09 16:28:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topicmy-topic-1690331932-781907737 will have desired state: Ready
2022-04-09 16:28:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-89184217 in namespace infra-namespace
2022-04-09 16:28:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-637863254-2076655810 in namespace infra-namespace
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopicStartingWithXAndTeamBReadFromTopicStartingWithX-FINISHED
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testKeycloakAuthorizerToDelegateToSimpleAuthorizer-STARTED
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-97 for test case:testKeycloakAuthorizerToDelegateToSimpleAuthorizer
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Secret sso-x509-https-secret in namespace infra-namespace
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Secret team-a-client-secret in namespace infra-namespace
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Secret team-b-client-secret in namespace infra-namespace
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3a6877b9 in namespace namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:28:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3a6877b9 will have desired state: Ready
2022-04-09 16:28:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topic-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topic-my-cluster-608be0f5 will have desired state: Ready
2022-04-09 16:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topicmy-topic-1690331932-781907737 is in desired state: Ready
2022-04-09 16:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [OauthAuthorizationIsolatedST:347] Verifying that team B is not able write to topic starting with 'x-' because in kafka clusterdoes not have super-users to break authorization rules
2022-04-09 16:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1739910460-620549375 in namespace infra-namespace
2022-04-09 16:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1739910460-620549375 will have desired state: Ready
2022-04-09 16:28:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topic-my-cluster-608be0f5 is in desired state: Ready
2022-04-09 16:28:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:34 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-608be0f5 will be in active state
2022-04-09 16:28:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1739910460-620549375 is in desired state: Ready
2022-04-09 16:28:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:28:34 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-b6d6d3e9 will be in active state
2022-04-09 16:28:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-608be0f5 to finished
2022-04-09 16:28:35 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:70] Waiting for job: team-b-client-producer-my-cluster-b6d6d3e9 will be in error state
2022-04-09 16:28:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:28:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic-FINISHED
2022-04-09 16:28:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:28:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:28:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic-FINISHED
2022-04-09 16:28:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthAuthorizationIsolatedST:172] Sending 100 messages to broker with topic name a-topic-my-cluster-608be0f5
2022-04-09 16:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-608be0f5 will be in active state
2022-04-09 16:28:44 [ForkJoinPool-3-worker-11] [32mINFO [m [OauthAuthorizationIsolatedST:370] Verifying that team A is not able read to topic starting with 'x-' because in kafka clusterdoes not have super-users to break authorization rules
2022-04-09 16:28:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:28:44 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-b6d6d3e9 will be in active state
2022-04-09 16:28:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-608be0f5 to finished
2022-04-09 16:28:45 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-consumer-my-cluster-b6d6d3e9 will be in error state
2022-04-09 16:28:53 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: oauth-cluster-authz-name-kafka rolling update
2022-04-09 16:28:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:28:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAWriteToTopic
2022-04-09 16:28:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topic-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-608be0f5 in namespace infra-namespace
2022-04-09 16:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1328338904-598226331 in namespace infra-namespace
2022-04-09 16:29:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:29:05 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic-FINISHED
2022-04-09 16:29:05 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:29:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3a6877b9 is in desired state: Ready
2022-04-09 16:29:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-a-client in namespace namespace-97
2022-04-09 16:29:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:29:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-a-client will have desired state: Ready
2022-04-09 16:29:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: team-a-client is in desired state: Ready
2022-04-09 16:29:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-b-client in namespace namespace-97
2022-04-09 16:29:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:29:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-b-client will have desired state: Ready
2022-04-09 16:29:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: team-b-client is in desired state: Ready
2022-04-09 16:29:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-1254014242-1211862598 in namespace namespace-97
2022-04-09 16:29:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:29:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-1254014242-1211862598 will have desired state: Ready
2022-04-09 16:29:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-1254014242-1211862598 is in desired state: Ready
2022-04-09 16:29:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-3a6877b9 in namespace namespace-97
2022-04-09 16:29:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:29:52 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-3a6877b9 will be in active state
2022-04-09 16:29:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-3a6877b9 to finished
2022-04-09 16:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-3a6877b9 in namespace namespace-97
2022-04-09 16:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-09 16:30:02 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-3a6877b9 will be in active state
2022-04-09 16:30:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-consumer-my-cluster-3a6877b9 to finished
2022-04-09 16:30:13 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: oauth-cluster-authz-name-kafka has been successfully rolled
2022-04-09 16:30:13 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of oauth-cluster-authz-name-kafka to be ready
2022-04-09 16:30:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:30:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKeycloakAuthorizerToDelegateToSimpleAuthorizer
2022-04-09 16:30:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-b-client in namespace namespace-97
2022-04-09 16:30:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-3a6877b9 in namespace namespace-97
2022-04-09 16:30:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-1254014242-1211862598 in namespace namespace-97
2022-04-09 16:30:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Secret team-b-client-secret in namespace namespace-97
2022-04-09 16:30:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-3a6877b9 in namespace namespace-97
2022-04-09 16:30:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-a-client in namespace namespace-97
2022-04-09 16:30:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3a6877b9 in namespace namespace-97
2022-04-09 16:30:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Secret team-a-client-secret in namespace namespace-97
2022-04-09 16:30:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Secret sso-x509-https-secret in namespace namespace-97
2022-04-09 16:30:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:30:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-97 for test case:testKeycloakAuthorizerToDelegateToSimpleAuthorizer
2022-04-09 16:30:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-09 16:30:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-09 16:30:40 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: oauth-cluster-authz-name is ready
2022-04-09 16:30:40 [ForkJoinPool-3-worker-11] [32mINFO [m [OauthAuthorizationIsolatedST:404] Verifying that team B is able to write to topic starting with 'x-' and break authorization rule
2022-04-09 16:30:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:30:40 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-b6d6d3e9 will be in active state
2022-04-09 16:30:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-b-client-producer-my-cluster-b6d6d3e9 to finished
2022-04-09 16:30:45 [ForkJoinPool-3-worker-11] [32mINFO [m [OauthAuthorizationIsolatedST:409] Verifying that team A is able to write to topic starting with 'x-' and break authorization rule
2022-04-09 16:30:45 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:30:45 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-b6d6d3e9 will be in active state
2022-04-09 16:30:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-consumer-my-cluster-b6d6d3e9 to finished
2022-04-09 16:30:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:30:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testSuperUserWithOauthAuthorization
2022-04-09 16:30:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:30:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:30:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1739910460-620549375 in namespace infra-namespace
2022-04-09 16:30:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topicmy-topic-1690331932-781907737 in namespace infra-namespace
2022-04-09 16:30:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:30:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-b6d6d3e9 in namespace infra-namespace
2022-04-09 16:31:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testKeycloakAuthorizerToDelegateToSimpleAuthorizer-FINISHED
2022-04-09 16:31:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:31:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:31:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSuperUserWithOauthAuthorization-FINISHED
2022-04-09 16:31:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSessionReAuthentication-STARTED
2022-04-09 16:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:443] Verifying that team A producer is able to send messages to the x-topic-example-topic topic -> the topic starting with 'x'
2022-04-09 16:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topic-example-topic in namespace infra-namespace
2022-04-09 16:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topic-example-topic will have desired state: Ready
2022-04-09 16:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topic-example-topic is in desired state: Ready
2022-04-09 16:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-example-topic in namespace infra-namespace
2022-04-09 16:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-example-topic will have desired state: Ready
2022-04-09 16:31:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-example-topic is in desired state: Ready
2022-04-09 16:31:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-c84dd5a5 in namespace infra-namespace
2022-04-09 16:31:10 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-c84dd5a5 will be in active state
2022-04-09 16:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-c84dd5a5 to finished
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:465] Adding the maxSecondsWithoutReauthentication to Kafka listener with OAuth authentication
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:493] Setting the master realm token's lifespan to 3600s
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X POST -d client_id=admin-cli&client_secret=aGVsbG8td29ybGQtcHJvZHVjZXItc2VjcmV0&grant_type=password&username=admin&password=4_BTg0ess_nm1g== https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/master/protocol/openid-connect/token
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X GET https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/master -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJDSWxXMTVvNUMtelhpQlRkWndUWjdqZmt2UUJlN1ZhN013LWxPb0QtRHBzIn0.eyJleHAiOjE2NDk1MjE5MzksImlhdCI6MTY0OTUyMTg3OSwianRpIjoiY2Y0NjE5OTgtMDEyNy00NGZlLTg3ZWItMmJiYWRhMjM5MzQ1IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiJiYWRiYjc4OC02YzU0LTRkYmMtYWVhMy0xNjM0NTMzMmY3N2QiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiNTNhYTA3N2EtMGQ4OS00MDQyLTg0OTQtYTY0ODMwYTI3ZjU1IiwiYWNyIjoiMSIsInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.Narow1e9oe-PHC2nyaIDzEfulldOYxkDB8n6By7XHZQVqcBhnRpLyl6RuYK-OC0MgvnqpzvlP-RcKS76SDMsW0DS1s4-bEqEPARJKCWCxM-hJl_zd5oXWe3RBy5RJwMRplf78pvSkrPviFnZZxYtDhfc29WBNRYZXaxexs5B9fSuesBpuXvxL2_zlR3P2PGcffLaNHj0yGU5ZBvA6fS2PS5UlPXilGZzQYtjquu74HJjge_CSB0Lo7iBkmZz6rRHKVefqCxB8cQFuh_n7enfv2FAZoRWbTFXkfNKX80nUmX68e9yFGO-OOH3x1BbVqxm35LwLtCjlQYbKzZ6cNU8hQ
2022-04-09 16:31:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X PUT https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/master -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJDSWxXMTVvNUMtelhpQlRkWndUWjdqZmt2UUJlN1ZhN013LWxPb0QtRHBzIn0.eyJleHAiOjE2NDk1MjE5MzksImlhdCI6MTY0OTUyMTg3OSwianRpIjoiY2Y0NjE5OTgtMDEyNy00NGZlLTg3ZWItMmJiYWRhMjM5MzQ1IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiJiYWRiYjc4OC02YzU0LTRkYmMtYWVhMy0xNjM0NTMzMmY3N2QiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiNTNhYTA3N2EtMGQ4OS00MDQyLTg0OTQtYTY0ODMwYTI3ZjU1IiwiYWNyIjoiMSIsInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.Narow1e9oe-PHC2nyaIDzEfulldOYxkDB8n6By7XHZQVqcBhnRpLyl6RuYK-OC0MgvnqpzvlP-RcKS76SDMsW0DS1s4-bEqEPARJKCWCxM-hJl_zd5oXWe3RBy5RJwMRplf78pvSkrPviFnZZxYtDhfc29WBNRYZXaxexs5B9fSuesBpuXvxL2_zlR3P2PGcffLaNHj0yGU5ZBvA6fS2PS5UlPXilGZzQYtjquu74HJjge_CSB0Lo7iBkmZz6rRHKVefqCxB8cQFuh_n7enfv2FAZoRWbTFXkfNKX80nUmX68e9yFGO-OOH3x1BbVqxm35LwLtCjlQYbKzZ6cNU8hQ -d {"id":"master","realm":"master","displayName":"Keycloak","displayNameHtml":"<div class=\"kc-logo-text\"><span>Keycloak</span></div>","notBefore":0,"revokeRefreshToken":false,"refreshTokenMaxReuse":0,"accessTokenLifespan":"3600","accessTokenLifespanForImplicitFlow":900,"ssoSessionIdleTimeout":1800,"ssoSessionMaxLifespan":36000,"ssoSessionIdleTimeoutRememberMe":0,"ssoSessionMaxLifespanRememberMe":0,"offlineSessionIdleTimeout":2592000,"offlineSessionMaxLifespanEnabled":false,"offlineSessionMaxLifespan":5184000,"clientSessionIdleTimeout":0,"clientSessionMaxLifespan":0,"clientOfflineSessionIdleTimeout":0,"clientOfflineSessionMaxLifespan":0,"accessCodeLifespan":60,"accessCodeLifespanUserAction":300,"accessCodeLifespanLogin":1800,"actionTokenGeneratedByAdminLifespan":43200,"actionTokenGeneratedByUserLifespan":300,"enabled":true,"sslRequired":"external","registrationAllowed":false,"registrationEmailAsUsername":false,"rememberMe":false,"verifyEmail":false,"loginWithEmailAllowed":true,"duplicateEmailsAllowed":false,"resetPasswordAllowed":false,"editUsernameAllowed":false,"bruteForceProtected":false,"permanentLockout":false,"maxFailureWaitSeconds":900,"minimumQuickLoginWaitSeconds":60,"waitIncrementSeconds":60,"quickLoginCheckMilliSeconds":1000,"maxDeltaTimeSeconds":43200,"failureFactor":30,"defaultRoles":["offline_access","uma_authorization"],"requiredCredentials":["password"],"otpPolicyType":"totp","otpPolicyAlgorithm":"HmacSHA1","otpPolicyInitialCounter":0,"otpPolicyDigits":6,"otpPolicyLookAheadWindow":1,"otpPolicyPeriod":30,"otpSupportedApplications":["FreeOTP","Google Authenticator"],"webAuthnPolicyRpEntityName":"keycloak","webAuthnPolicySignatureAlgorithms":["ES256"],"webAuthnPolicyRpId":"","webAuthnPolicyAttestationConveyancePreference":"not specified","webAuthnPolicyAuthenticatorAttachment":"not specified","webAuthnPolicyRequireResidentKey":"not specified","webAuthnPolicyUserVerificationRequirement":"not specified","webAuthnPolicyCreateTimeout":0,"webAuthnPolicyAvoidSameAuthenticatorRegister":false,"webAuthnPolicyAcceptableAaguids":[],"webAuthnPolicyPasswordlessRpEntityName":"keycloak","webAuthnPolicyPasswordlessSignatureAlgorithms":["ES256"],"webAuthnPolicyPasswordlessRpId":"","webAuthnPolicyPasswordlessAttestationConveyancePreference":"not specified","webAuthnPolicyPasswordlessAuthenticatorAttachment":"not specified","webAuthnPolicyPasswordlessRequireResidentKey":"not specified","webAuthnPolicyPasswordlessUserVerificationRequirement":"not specified","webAuthnPolicyPasswordlessCreateTimeout":0,"webAuthnPolicyPasswordlessAvoidSameAuthenticatorRegister":false,"webAuthnPolicyPasswordlessAcceptableAaguids":[],"browserSecurityHeaders":{"contentSecurityPolicyReportOnly":"","xContentTypeOptions":"nosniff","xRobotsTag":"none","xFrameOptions":"SAMEORIGIN","xXSSProtection":"1; mode=block","contentSecurityPolicy":"frame-src 'self'; frame-ancestors 'self'; object-src 'none';","strictTransportSecurity":"max-age=31536000; includeSubDomains"},"smtpServer":{},"eventsEnabled":false,"eventsListeners":["jboss-logging"],"enabledEventTypes":[],"adminEventsEnabled":false,"adminEventsDetailsEnabled":false,"internationalizationEnabled":false,"supportedLocales":[],"browserFlow":"browser","registrationFlow":"registration","directGrantFlow":"direct grant","resetCredentialsFlow":"reset credentials","clientAuthenticationFlow":"clients","dockerAuthenticationFlow":"docker auth","attributes":{},"userManagedAccessAllowed":false} -H Content-Type: application/json
2022-04-09 16:31:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X POST -d client_id=admin-cli&client_secret=aGVsbG8td29ybGQtcHJvZHVjZXItc2VjcmV0&grant_type=password&username=admin&password=4_BTg0ess_nm1g== https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/master/protocol/openid-connect/token
2022-04-09 16:31:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:20 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:508] Getting the kafka-authz kafka client for obtaining the Dev A Team policy for the x topics
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X GET https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJDSWxXMTVvNUMtelhpQlRkWndUWjdqZmt2UUJlN1ZhN013LWxPb0QtRHBzIn0.eyJleHAiOjE2NDk1MjU0ODAsImlhdCI6MTY0OTUyMTg4MCwianRpIjoiM2MzNDY1N2ItNjcxOS00MGZiLWFmMWEtYjUyOTQ3OGM0NjQ5IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiJiYWRiYjc4OC02YzU0LTRkYmMtYWVhMy0xNjM0NTMzMmY3N2QiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZjlhMGY3NTItOTliNC00ODAxLWE3MGUtMzQ0ZTdkY2E1MGU4IiwiYWNyIjoiMSIsInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.O6pP6CPlDzEEHEn_QAE-6BQDHdtSWu7aiyEx1rVRTji8He4dblTyF2fVjcZGQfka5IY5vacov_uIJwrmkHJaVf3yikVCdj2NaoAUIuBHFgicsyjQFPbvyxZxUKPiS2AxPhBAZn7BPbufNu-JEW692UM_rSqWRPJ_Bf5PpFHjJuPAykgKMWVwBtfJFMzTcMFSDPfznsUfrMpbojDpgEdP_oe_7E2B0IhUTgBM05A6dNufGP2VrKsTrj9y1KnBq57xvZWcWJ1nNj-UK-gujkf1jxbbbE-oNOafR-HHUrnZJrY_UXD52cKDHVMhg4dpi8F9wbwRH_peQKEUd05PFk-muw
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X GET https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients/30ee9e96-cb74-4603-8a16-45e5b296b1ab/authz/resource-server/policy -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJDSWxXMTVvNUMtelhpQlRkWndUWjdqZmt2UUJlN1ZhN013LWxPb0QtRHBzIn0.eyJleHAiOjE2NDk1MjU0ODAsImlhdCI6MTY0OTUyMTg4MCwianRpIjoiM2MzNDY1N2ItNjcxOS00MGZiLWFmMWEtYjUyOTQ3OGM0NjQ5IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiJiYWRiYjc4OC02YzU0LTRkYmMtYWVhMy0xNjM0NTMzMmY3N2QiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZjlhMGY3NTItOTliNC00ODAxLWE3MGUtMzQ0ZTdkY2E1MGU4IiwiYWNyIjoiMSIsInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.O6pP6CPlDzEEHEn_QAE-6BQDHdtSWu7aiyEx1rVRTji8He4dblTyF2fVjcZGQfka5IY5vacov_uIJwrmkHJaVf3yikVCdj2NaoAUIuBHFgicsyjQFPbvyxZxUKPiS2AxPhBAZn7BPbufNu-JEW692UM_rSqWRPJ_Bf5PpFHjJuPAykgKMWVwBtfJFMzTcMFSDPfznsUfrMpbojDpgEdP_oe_7E2B0IhUTgBM05A6dNufGP2VrKsTrj9y1KnBq57xvZWcWJ1nNj-UK-gujkf1jxbbbE-oNOafR-HHUrnZJrY_UXD52cKDHVMhg4dpi8F9wbwRH_peQKEUd05PFk-muw
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:539] Changing the Dev Team A policy for topics starting with x- and checking that job will not be successful
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X PUT https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients/30ee9e96-cb74-4603-8a16-45e5b296b1ab/authz/resource-server/policy/a565b9cf-7f15-4da0-8320-705011c90f7b -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJDSWxXMTVvNUMtelhpQlRkWndUWjdqZmt2UUJlN1ZhN013LWxPb0QtRHBzIn0.eyJleHAiOjE2NDk1MjU0ODAsImlhdCI6MTY0OTUyMTg4MCwianRpIjoiM2MzNDY1N2ItNjcxOS00MGZiLWFmMWEtYjUyOTQ3OGM0NjQ5IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiJiYWRiYjc4OC02YzU0LTRkYmMtYWVhMy0xNjM0NTMzMmY3N2QiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZjlhMGY3NTItOTliNC00ODAxLWE3MGUtMzQ0ZTdkY2E1MGU4IiwiYWNyIjoiMSIsInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.O6pP6CPlDzEEHEn_QAE-6BQDHdtSWu7aiyEx1rVRTji8He4dblTyF2fVjcZGQfka5IY5vacov_uIJwrmkHJaVf3yikVCdj2NaoAUIuBHFgicsyjQFPbvyxZxUKPiS2AxPhBAZn7BPbufNu-JEW692UM_rSqWRPJ_Bf5PpFHjJuPAykgKMWVwBtfJFMzTcMFSDPfznsUfrMpbojDpgEdP_oe_7E2B0IhUTgBM05A6dNufGP2VrKsTrj9y1KnBq57xvZWcWJ1nNj-UK-gujkf1jxbbbE-oNOafR-HHUrnZJrY_UXD52cKDHVMhg4dpi8F9wbwRH_peQKEUd05PFk-muw -d {"id":"a565b9cf-7f15-4da0-8320-705011c90f7b","name":"Dev Team A can write to topics that start with x- on any cluster","type":"scope","logic":"POSITIVE","decisionStrategy":"UNANIMOUS","config":{"resources":"[\"Topic:x-*\"]","scopes":"[\"Describe\"]","applyPolicies":"[\"Dev Team A\"]"}} -H Content-Type: application/json
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:31:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-c84dd5a5 to finished
2022-04-09 16:35:01 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestUtils:162] Exception waiting for job finished, null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientSuccess(ClientUtils.java:77)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientSuccess(ClientUtils.java:72)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.lambda$testSessionReAuthentication$2(OauthAuthorizationIsolatedST.java:541)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3082)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSessionReAuthentication(OauthAuthorizationIsolatedST.java:541)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 16:35:01 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:545] Sending messages to topic starting with a- -> the messages should be successfully sent
2022-04-09 16:35:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-c84dd5a5 in namespace infra-namespace
2022-04-09 16:35:01 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-c84dd5a5 will be in active state
2022-04-09 16:35:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-c84dd5a5 to finished
2022-04-09 16:35:11 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:554] Changing back to the original settings and checking, if the producer will be successful
2022-04-09 16:35:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-rbgl2 -- curl -v --insecure -X PUT https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients/30ee9e96-cb74-4603-8a16-45e5b296b1ab/authz/resource-server/policy/a565b9cf-7f15-4da0-8320-705011c90f7b -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJDSWxXMTVvNUMtelhpQlRkWndUWjdqZmt2UUJlN1ZhN013LWxPb0QtRHBzIn0.eyJleHAiOjE2NDk1MjU0ODAsImlhdCI6MTY0OTUyMTg4MCwianRpIjoiM2MzNDY1N2ItNjcxOS00MGZiLWFmMWEtYjUyOTQ3OGM0NjQ5IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiJiYWRiYjc4OC02YzU0LTRkYmMtYWVhMy0xNjM0NTMzMmY3N2QiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZjlhMGY3NTItOTliNC00ODAxLWE3MGUtMzQ0ZTdkY2E1MGU4IiwiYWNyIjoiMSIsInNjb3BlIjoicHJvZmlsZSBlbWFpbCIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.O6pP6CPlDzEEHEn_QAE-6BQDHdtSWu7aiyEx1rVRTji8He4dblTyF2fVjcZGQfka5IY5vacov_uIJwrmkHJaVf3yikVCdj2NaoAUIuBHFgicsyjQFPbvyxZxUKPiS2AxPhBAZn7BPbufNu-JEW692UM_rSqWRPJ_Bf5PpFHjJuPAykgKMWVwBtfJFMzTcMFSDPfznsUfrMpbojDpgEdP_oe_7E2B0IhUTgBM05A6dNufGP2VrKsTrj9y1KnBq57xvZWcWJ1nNj-UK-gujkf1jxbbbE-oNOafR-HHUrnZJrY_UXD52cKDHVMhg4dpi8F9wbwRH_peQKEUd05PFk-muw -d {"id":"a565b9cf-7f15-4da0-8320-705011c90f7b","name":"Dev Team A can write to topics that start with x- on any cluster","type":"scope","logic":"POSITIVE","decisionStrategy":"UNANIMOUS","config":{"resources":"[\"Topic:x-*\"]","scopes":"[\"Describe\",\"Write\"]","applyPolicies":"[\"Dev Team A\"]"}} -H Content-Type: application/json
2022-04-09 16:35:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:35:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-c84dd5a5 in namespace infra-namespace
2022-04-09 16:35:11 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-c84dd5a5 will be in active state
2022-04-09 16:35:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-c84dd5a5 to finished
2022-04-09 16:37:02 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:568] Changing configuration of Kafka back to it's original form
2022-04-09 16:37:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-09 16:37:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-09 16:37:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:37:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSessionReAuthentication
2022-04-09 16:37:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-c84dd5a5 in namespace infra-namespace
2022-04-09 16:37:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-example-topic in namespace infra-namespace
2022-04-09 16:37:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-c84dd5a5 in namespace infra-namespace
2022-04-09 16:37:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topic-example-topic in namespace infra-namespace
2022-04-09 16:37:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-c84dd5a5 in namespace infra-namespace
2022-04-09 16:37:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:37:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSessionReAuthentication-FINISHED
2022-04-09 16:37:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:37:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthAuthorizationIsolatedST
2022-04-09 16:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-a-client in namespace infra-namespace
2022-04-09 16:37:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-authz-name in namespace infra-namespace
2022-04-09 16:37:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-b-client in namespace infra-namespace
2022-04-09 16:37:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:37:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:37:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:37:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthAuthorizationIsolatedST is everything deleted.
2022-04-09 16:37:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 10, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 844.033 s - in io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST
2022-04-09 16:37:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 16:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 16:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 16:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 16:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:37:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:37:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:37:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:37:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:37:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:37:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:37:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:38:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:38:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:38:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 16:38:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 16:38:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 16:38:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 16:38:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:38:55 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-09 16:38:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to scope-test realm
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to scope-test realm
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to scope-test realm
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:60] Using HTTP endpoints
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-scope-name in namespace infra-namespace
2022-04-09 16:40:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-scope-name will have desired state: Ready
2022-04-09 16:42:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-scope-name is in desired state: Ready
2022-04-09 16:42:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:42:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetIncorrectly-STARTED
2022-04-09 16:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetCorrectly-STARTED
2022-04-09 16:42:04 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:42:04 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetCorrectly-STARTED
2022-04-09 16:42:04 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:42:04 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:42:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1ad82de3-kafka-clients in namespace infra-namespace
2022-04-09 16:42:04 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a7e6a689-kafka-clients in namespace infra-namespace
2022-04-09 16:42:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1032020606-899703005 in namespace infra-namespace
2022-04-09 16:42:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1032020606-899703005 will have desired state: Ready
2022-04-09 16:42:04 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1ad82de3-kafka-clients will be ready
2022-04-09 16:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a7e6a689-kafka-clients will be ready
2022-04-09 16:42:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1032020606-899703005 is in desired state: Ready
2022-04-09 16:42:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-ed8be1e9 in namespace infra-namespace
2022-04-09 16:42:05 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-ed8be1e9 will be in active state
2022-04-09 16:42:06 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1ad82de3-kafka-clients is ready
2022-04-09 16:42:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a7e6a689-kafka-clients is ready
2022-04-09 16:42:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-ed8be1e9 to finished
2022-04-09 16:42:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1ad82de3-scraper in namespace infra-namespace
2022-04-09 16:42:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a7e6a689-scraper in namespace infra-namespace
2022-04-09 16:42:06 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1ad82de3-scraper will be ready
2022-04-09 16:42:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a7e6a689-scraper will be ready
2022-04-09 16:42:09 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1ad82de3-scraper is ready
2022-04-09 16:42:09 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-1ad82de3-scraper to be ready
2022-04-09 16:42:09 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a7e6a689-scraper is ready
2022-04-09 16:42:09 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-a7e6a689-scraper to be ready
2022-04-09 16:42:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:42:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testClientScopeKafkaSetCorrectly
2022-04-09 16:42:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-ed8be1e9 in namespace infra-namespace
2022-04-09 16:42:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1032020606-899703005 in namespace infra-namespace
2022-04-09 16:42:19 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-1ad82de3-scraper is ready
2022-04-09 16:42:19 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-1ad82de3-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 16:42:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-1ad82de3-allow in namespace infra-namespace
2022-04-09 16:42:19 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-a7e6a689-scraper is ready
2022-04-09 16:42:19 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-a7e6a689-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 16:42:19 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 16:42:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-a7e6a689-allow in namespace infra-namespace
2022-04-09 16:42:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-1ad82de3 in namespace infra-namespace
2022-04-09 16:42:19 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 16:42:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-a7e6a689 in namespace infra-namespace
2022-04-09 16:42:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-a7e6a689 will have desired state: Ready
2022-04-09 16:42:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:42:25 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetCorrectly-FINISHED
2022-04-09 16:42:25 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:42:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:42:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testScopeKafkaConnectSetIncorrectly
2022-04-09 16:42:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-1ad82de3-allow in namespace infra-namespace
2022-04-09 16:42:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1ad82de3-scraper in namespace infra-namespace
2022-04-09 16:42:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-1ad82de3 in namespace infra-namespace
2022-04-09 16:42:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1ad82de3-kafka-clients in namespace infra-namespace
2022-04-09 16:43:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:43:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetIncorrectly-FINISHED
2022-04-09 16:43:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:43:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-a7e6a689 is in desired state: Ready
2022-04-09 16:43:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:43:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testScopeKafkaConnectSetCorrectly
2022-04-09 16:43:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-a7e6a689-allow in namespace infra-namespace
2022-04-09 16:43:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a7e6a689-scraper in namespace infra-namespace
2022-04-09 16:43:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a7e6a689-kafka-clients in namespace infra-namespace
2022-04-09 16:43:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-a7e6a689 in namespace infra-namespace
2022-04-09 16:44:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:44:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetCorrectly-FINISHED
2022-04-09 16:44:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:44:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:44:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetIncorrectly-STARTED
2022-04-09 16:44:04 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:44:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:44:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of oauth-cluster-scope-name-kafka to be ready
2022-04-09 16:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-scope-name will have desired state: Ready
2022-04-09 16:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-scope-name is in desired state: Ready
2022-04-09 16:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: oauth-cluster-scope-name is ready
2022-04-09 16:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1023639925-1323454015 in namespace infra-namespace
2022-04-09 16:44:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1023639925-1323454015 will have desired state: Ready
2022-04-09 16:44:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1023639925-1323454015 is in desired state: Ready
2022-04-09 16:44:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-7f118545 in namespace infra-namespace
2022-04-09 16:44:46 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-7f118545 will be in active state
2022-04-09 16:44:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:oauth-producer-my-cluster-7f118545 to finish with failure.
2022-04-09 16:48:27 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetIncorrectly(OauthScopeIsolatedST.java:224)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 16:48:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:100] Client job 'oauth-producer-my-cluster-7f118545' finished with expected timeout.
2022-04-09 16:48:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of oauth-cluster-scope-name-kafka to be ready
2022-04-09 16:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-scope-name will have desired state: Ready
2022-04-09 16:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-scope-name is in desired state: Ready
2022-04-09 16:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: oauth-cluster-scope-name is ready
2022-04-09 16:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClientScopeKafkaSetIncorrectly
2022-04-09 16:49:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-7f118545 in namespace infra-namespace
2022-04-09 16:49:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1023639925-1323454015 in namespace infra-namespace
2022-04-09 16:49:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:49:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetIncorrectly-FINISHED
2022-04-09 16:49:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:49:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthScopeIsolatedST
2022-04-09 16:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-scope-name in namespace infra-namespace
2022-04-09 16:49:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:49:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:49:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:49:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthScopeIsolatedST is everything deleted.
2022-04-09 16:49:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 731.364 s - in io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST
2022-04-09 16:49:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:50:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 16:50:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 16:50:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 16:50:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:50:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 16:50:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 16:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 16:51:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 16:51:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 16:51:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 16:51:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 16:51:20 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-09 16:51:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to internal realm
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to internal realm
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to internal realm
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:55] Using HTTPS endpoints
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthTlsIsolatedST:480] Keycloak settings KeycloakInstance{jwksExpireSeconds=500, jwksRefreshSeconds=400, username='admin', password='Bl0FyEBx07j8iQ==', httpsUri='keycloak.infra-namespace.svc.cluster.local:8443', httpUri='keycloak-discovery.infra-namespace.svc.cluster.local:8080', validIssuerUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal', jwksEndpointUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs', oauthTokenEndpointUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token', introspectionEndpointUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token/introspect', userNameClaim='preferred_username', keystorePattern=<tls>\s*<key-stores>\s*<key-store name="kcKeyStore">\s*<credential-reference clear-text=".*"\/>, keystorePasswordPattern=\".*\"}
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-tls-name in namespace infra-namespace
2022-04-09 16:52:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-tls-name will have desired state: Ready
2022-04-09 16:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-tls-name is in desired state: Ready
2022-04-09 16:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser hello-world-producer in namespace infra-namespace
2022-04-09 16:54:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: hello-world-producer will have desired state: Ready
2022-04-09 16:54:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: hello-world-producer is in desired state: Ready
2022-04-09 16:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerBridge-STARTED
2022-04-09 16:54:24 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:54:24 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:54:24 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testIntrospectionEndpoint-STARTED
2022-04-09 16:54:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:54:24 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerConnect-STARTED
2022-04-09 16:54:24 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumer-STARTED
2022-04-09 16:54:24 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:54:24 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:54:24 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:54:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1684283107-801651286 in namespace infra-namespace
2022-04-09 16:54:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-83309144-711539344 in namespace infra-namespace
2022-04-09 16:54:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2097389724-1321331990 in namespace infra-namespace
2022-04-09 16:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1080484173-91082905 in namespace infra-namespace
2022-04-09 16:54:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2097389724-1321331990 will have desired state: Ready
2022-04-09 16:54:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1684283107-801651286 will have desired state: Ready
2022-04-09 16:54:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-83309144-711539344 will have desired state: Ready
2022-04-09 16:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1080484173-91082905 will have desired state: Ready
2022-04-09 16:54:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2097389724-1321331990 is in desired state: Ready
2022-04-09 16:54:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:54:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-96a88199 in namespace infra-namespace
2022-04-09 16:54:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-83309144-711539344 is in desired state: Ready
2022-04-09 16:54:25 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:54:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1080484173-91082905 is in desired state: Ready
2022-04-09 16:54:25 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:54:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-26eeaaa3 in namespace infra-namespace
2022-04-09 16:54:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-aad573b3 in namespace infra-namespace
2022-04-09 16:54:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1684283107-801651286 is in desired state: Ready
2022-04-09 16:54:25 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-96a88199 will be in active state
2022-04-09 16:54:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-tls-name-intro in namespace infra-namespace
2022-04-09 16:54:25 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-aad573b3 will be in active state
2022-04-09 16:54:25 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-26eeaaa3 will be in active state
2022-04-09 16:54:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-tls-name-intro will have desired state: Ready
2022-04-09 16:54:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-96a88199 to finished
2022-04-09 16:54:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-26eeaaa3 to finished
2022-04-09 16:54:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-aad573b3 to finished
2022-04-09 16:54:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-26eeaaa3 in namespace infra-namespace
2022-04-09 16:54:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-aad573b3 in namespace infra-namespace
2022-04-09 16:54:37 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-26eeaaa3 will be in active state
2022-04-09 16:54:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-96a88199 in namespace infra-namespace
2022-04-09 16:54:37 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-aad573b3 will be in active state
2022-04-09 16:54:37 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-96a88199 will be in active state
2022-04-09 16:54:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-26eeaaa3 to finished
2022-04-09 16:54:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-aad573b3 to finished
2022-04-09 16:54:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-96a88199 to finished
2022-04-09 16:54:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment oauth-cluster-tls-name-kafka-clients in namespace infra-namespace
2022-04-09 16:54:45 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: oauth-cluster-tls-name-kafka-clients will be ready
2022-04-09 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: oauth-cluster-tls-name-kafka-clients is ready
2022-04-09 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-26eeaaa3-scraper in namespace infra-namespace
2022-04-09 16:54:46 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-26eeaaa3-scraper will be ready
2022-04-09 16:54:48 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-26eeaaa3-scraper is ready
2022-04-09 16:54:48 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-26eeaaa3-scraper to be ready
2022-04-09 16:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumer
2022-04-09 16:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-96a88199 in namespace infra-namespace
2022-04-09 16:54:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2097389724-1321331990 in namespace infra-namespace
2022-04-09 16:54:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-96a88199 in namespace infra-namespace
2022-04-09 16:54:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-aad573b3-kafka-clients in namespace infra-namespace
2022-04-09 16:54:51 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-aad573b3-kafka-clients will be ready
2022-04-09 16:54:53 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-aad573b3-kafka-clients is ready
2022-04-09 16:54:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge oauth-cluster-tls-name in namespace infra-namespace
2022-04-09 16:54:53 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: oauth-cluster-tls-name will have desired state: Ready
2022-04-09 16:54:58 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-26eeaaa3-scraper is ready
2022-04-09 16:54:58 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-26eeaaa3-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 16:54:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-26eeaaa3-allow in namespace infra-namespace
2022-04-09 16:54:58 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 16:54:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-26eeaaa3 in namespace infra-namespace
2022-04-09 16:54:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-26eeaaa3 will have desired state: Ready
2022-04-09 16:54:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:54:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumer-FINISHED
2022-04-09 16:54:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:55:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaBridge: oauth-cluster-tls-name is in desired state: Ready
2022-04-09 16:55:14 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:55:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer-my-cluster-aad573b3 in namespace infra-namespace
2022-04-09 16:55:14 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer-my-cluster-aad573b3 will be in active state
2022-04-09 16:55:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:bridge-producer-my-cluster-aad573b3 to finished
2022-04-09 16:55:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:55:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerBridge
2022-04-09 16:55:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-aad573b3-kafka-clients in namespace infra-namespace
2022-04-09 16:55:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-aad573b3 in namespace infra-namespace
2022-04-09 16:55:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1080484173-91082905 in namespace infra-namespace
2022-04-09 16:55:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-aad573b3 in namespace infra-namespace
2022-04-09 16:55:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer-my-cluster-aad573b3 in namespace infra-namespace
2022-04-09 16:55:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge oauth-cluster-tls-name in namespace infra-namespace
2022-04-09 16:55:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-tls-name-intro is in desired state: Ready
2022-04-09 16:55:35 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:55:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-6cddd99e in namespace infra-namespace
2022-04-09 16:55:35 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-6cddd99e will be in active state
2022-04-09 16:55:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-6cddd99e to finished
2022-04-09 16:55:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-6cddd99e in namespace infra-namespace
2022-04-09 16:55:45 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-6cddd99e will be in active state
2022-04-09 16:55:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-6cddd99e to finished
2022-04-09 16:55:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:55:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testIntrospectionEndpoint
2022-04-09 16:55:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-6cddd99e in namespace infra-namespace
2022-04-09 16:55:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-6cddd99e in namespace infra-namespace
2022-04-09 16:55:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-tls-name-intro in namespace infra-namespace
2022-04-09 16:55:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1684283107-801651286 in namespace infra-namespace
2022-04-09 16:56:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-26eeaaa3 is in desired state: Ready
2022-04-09 16:56:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 16:56:03 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-26eeaaa3-connect-5bc7fd886b-gscsm -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 16:56:03 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:56:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 16:56:04 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec oauth-cluster-tls-name-kafka-clients-8cb45f87d-wd4xp -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-83309144-711539344", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-26eeaaa3-connect-api.infra-namespace.svc:8083/connectors
2022-04-09 16:56:04 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:56:04 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-26eeaaa3-connect-5bc7fd886b-gscsm
2022-04-09 16:56:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:56:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testIntrospectionEndpoint-FINISHED
2022-04-09 16:56:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-26eeaaa3-connect-5bc7fd886b-gscsm
2022-04-09 16:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerConnect
2022-04-09 16:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-26eeaaa3-scraper in namespace infra-namespace
2022-04-09 16:56:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-26eeaaa3 in namespace infra-namespace
2022-04-09 16:56:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-26eeaaa3 in namespace infra-namespace
2022-04-09 16:56:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-26eeaaa3-allow in namespace infra-namespace
2022-04-09 16:56:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-26eeaaa3 in namespace infra-namespace
2022-04-09 16:56:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-83309144-711539344 in namespace infra-namespace
2022-04-09 16:56:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment oauth-cluster-tls-name-kafka-clients in namespace infra-namespace
2022-04-09 16:56:13 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:56:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerBridge-FINISHED
2022-04-09 16:56:13 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:56:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:56:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerConnect-FINISHED
2022-04-09 16:56:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 16:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testMirrorMaker-STARTED
2022-04-09 16:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 16:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-309118493-999064206 in namespace infra-namespace
2022-04-09 16:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-309118493-999064206 will have desired state: Ready
2022-04-09 16:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-309118493-999064206 is in desired state: Ready
2022-04-09 16:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-77fa1d3b in namespace infra-namespace
2022-04-09 16:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-77fa1d3b will be in active state
2022-04-09 16:56:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-77fa1d3b to finished
2022-04-09 16:57:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-77fa1d3b in namespace infra-namespace
2022-04-09 16:57:00 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-77fa1d3b will be in active state
2022-04-09 16:57:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-77fa1d3b to finished
2022-04-09 16:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-tls-name-target in namespace infra-namespace
2022-04-09 16:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-tls-name-target will have desired state: Ready
2022-04-09 16:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-tls-name-target is in desired state: Ready
2022-04-09 16:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker oauth-cluster-tls-name in namespace infra-namespace
2022-04-09 16:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: oauth-cluster-tls-name will have desired state: Ready
2022-04-09 16:59:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: oauth-cluster-tls-name is in desired state: Ready
2022-04-09 16:59:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1577833048-1701092451 in namespace infra-namespace
2022-04-09 16:59:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1577833048-1701092451 will have desired state: Ready
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1577833048-1701092451 is in desired state: Ready
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-1577833048-1701092451
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret my-user-1577833048-1701092451 created
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1577833048-1701092451 will have desired state: Ready
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1577833048-1701092451 is in desired state: Ready
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthTlsIsolatedST:390] Creating new client with new consumer-group and also to point on oauth-cluster-tls-name-target cluster
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-77fa1d3b in namespace infra-namespace
2022-04-09 16:59:31 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-77fa1d3b will be in active state
2022-04-09 16:59:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-77fa1d3b to finished
2022-04-09 16:59:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:59:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker
2022-04-09 16:59:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker oauth-cluster-tls-name in namespace infra-namespace
2022-04-09 16:59:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-77fa1d3b in namespace infra-namespace
2022-04-09 16:59:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-77fa1d3b in namespace infra-namespace
2022-04-09 16:59:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-309118493-999064206 in namespace infra-namespace
2022-04-09 16:59:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-77fa1d3b in namespace infra-namespace
2022-04-09 16:59:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1577833048-1701092451 in namespace infra-namespace
2022-04-09 16:59:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-tls-name-target in namespace infra-namespace
2022-04-09 16:59:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 16:59:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testMirrorMaker-FINISHED
2022-04-09 16:59:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 16:59:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-09 16:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-09 16:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 16:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 16:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthTlsIsolatedST
2022-04-09 16:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-tls-name in namespace infra-namespace
2022-04-09 16:59:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser hello-world-producer in namespace infra-namespace
2022-04-09 16:59:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-09 17:00:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:00:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:00:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthTlsIsolatedST is everything deleted.
2022-04-09 17:00:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 630.163 s - in io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST
2022-04-09 17:00:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:00:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 17:00:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 17:00:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 17:00:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:00:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 17:00:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:00:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:00:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:00:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:43 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 17:01:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 17:01:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:01:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodCrashLooping-STARTED
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:01:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodImagePullBackOff-STARTED
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaRollsWhenTopicIsUnderReplicated-STARTED
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPendingDueToRack-STARTED
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPending-STARTED
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-98 for test case:testKafkaPodCrashLooping
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-98
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-98
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-98
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-99 for test case:testKafkaPodPending
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-99
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-30c8b261 in namespace namespace-98
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-98
2022-04-09 17:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-30c8b261 will have desired state: Ready
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-99
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-99
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-100 for test case:testKafkaRollsWhenTopicIsUnderReplicated
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-100
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-100243af in namespace namespace-99
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-99
2022-04-09 17:01:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-100243af will have desired state: Ready
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-100
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-100
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-101 for test case:testKafkaPodPendingDueToRack
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-101
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d612fe85 in namespace namespace-100
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-100
2022-04-09 17:01:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d612fe85 will have desired state: Ready
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-101
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-101
2022-04-09 17:01:36 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:01:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-102 for test case:testKafkaPodImagePullBackOff
2022-04-09 17:01:36 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-102
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6c8debbc in namespace namespace-101
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-101
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-6c8debbc-kafka will have stable 3 replicas
2022-04-09 17:01:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:37 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-102
2022-04-09 17:01:37 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-102
2022-04-09 17:01:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4cc61c4b in namespace namespace-102
2022-04-09 17:01:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-102
2022-04-09 17:01:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4cc61c4b will have desired state: Ready
2022-04-09 17:01:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:47 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:49 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:50 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:51 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:52 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:54 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:55 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:01:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:09 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:49 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:50 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:51 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:52 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-09 17:02:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-09 17:02:54 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-09 17:02:55 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-09 17:02:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-09 17:02:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-09 17:02:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-09 17:02:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-09 17:03:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-09 17:03:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-09 17:03:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-09 17:03:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-09 17:03:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-09 17:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-09 17:03:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-09 17:03:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-09 17:03:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-09 17:03:09 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-09 17:03:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-09 17:03:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-09 17:03:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-09 17:03:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:228] Pod my-cluster-6c8debbc-kafka has 3 replicas
2022-04-09 17:03:12 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaRollerIsolatedST:309] Removing requirement for the affinity
2022-04-09 17:03:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6c8debbc will have desired state: Ready
2022-04-09 17:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-30c8b261 is in desired state: Ready
2022-04-09 17:03:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-30c8b261 will have desired state: NotReady
2022-04-09 17:03:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-100243af is in desired state: Ready
2022-04-09 17:03:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-100243af will have desired state: NotReady
2022-04-09 17:03:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d612fe85 is in desired state: Ready
2022-04-09 17:03:54 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRollerIsolatedST:105] Running kafkaScaleUpScaleDown my-cluster-d612fe85
2022-04-09 17:03:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d612fe85-kafka rolling update
2022-04-09 17:04:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4cc61c4b is in desired state: Ready
2022-04-09 17:04:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4cc61c4b will have desired state: NotReady
2022-04-09 17:05:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d612fe85-kafka has been successfully rolled
2022-04-09 17:05:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-d612fe85-kafka to be ready
2022-04-09 17:05:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-30c8b261 is in desired state: NotReady
2022-04-09 17:05:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-30c8b261 will have desired state: Ready
2022-04-09 17:05:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-100243af is in desired state: NotReady
2022-04-09 17:05:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-100243af will have desired state: Ready
2022-04-09 17:06:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d612fe85 will have desired state: Ready
2022-04-09 17:06:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d612fe85 is in desired state: Ready
2022-04-09 17:06:10 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d612fe85 is ready
2022-04-09 17:06:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2021587028-332157807 in namespace namespace-102
2022-04-09 17:06:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-100
2022-04-09 17:06:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2021587028-332157807 will have desired state: Ready
2022-04-09 17:06:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2021587028-332157807 is in desired state: Ready
2022-04-09 17:06:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaRollerIsolatedST:124] Scaling down to 3
2022-04-09 17:06:11 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d612fe85-kafka rolling update
2022-04-09 17:06:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4cc61c4b is in desired state: NotReady
2022-04-09 17:06:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4cc61c4b will have desired state: Ready
2022-04-09 17:06:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6c8debbc is in desired state: Ready
2022-04-09 17:06:54 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUtils:399] Waiting for deletion of Kafka:my-cluster-6c8debbc
2022-04-09 17:07:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:07:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodPendingDueToRack
2022-04-09 17:07:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6c8debbc in namespace namespace-101
2022-04-09 17:07:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:07:08 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-101 for test case:testKafkaPodPendingDueToRack
2022-04-09 17:07:35 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:07:35 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaTopicRFLowerThanMinInSyncReplicas-STARTED
2022-04-09 17:07:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPendingDueToRack-FINISHED
2022-04-09 17:07:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-103 for test case:testKafkaTopicRFLowerThanMinInSyncReplicas
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-103
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-103
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-103
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f5fef1a5 in namespace namespace-103
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-103
2022-04-09 17:07:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f5fef1a5 will have desired state: Ready
2022-04-09 17:07:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d612fe85-kafka has been successfully rolled
2022-04-09 17:07:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-d612fe85-kafka to be ready
2022-04-09 17:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-100243af is in desired state: Ready
2022-04-09 17:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodPending
2022-04-09 17:07:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-100243af in namespace namespace-99
2022-04-09 17:08:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:08:05 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-99 for test case:testKafkaPodPending
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d612fe85 will have desired state: Ready
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d612fe85 is in desired state: Ready
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d612fe85 is ready
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-d612fe85 are stable
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:43 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:44 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:45 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:46 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:48 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-09 17:08:49 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPending-FINISHED
2022-04-09 17:08:49 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:50 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:51 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:52 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:53 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:56 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:08:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:00 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:01 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:322] Pod my-cluster-d612fe85-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-d612fe85-entity-operator-69b9675c9b-jrtc6 ,my-cluster-d612fe85-kafka-0 ,my-cluster-d612fe85-kafka-1 ,my-cluster-d612fe85-kafka-2 ,my-cluster-d612fe85-zookeeper-0 ,my-cluster-d612fe85-zookeeper-1 ,my-cluster-d612fe85-zookeeper-2
2022-04-09 17:09:05 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d612fe85-kafka rolling update
2022-04-09 17:09:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f5fef1a5 is in desired state: Ready
2022-04-09 17:09:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1079602512-472497468 in namespace namespace-103
2022-04-09 17:09:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-103
2022-04-09 17:09:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1079602512-472497468 will have desired state: Ready
2022-04-09 17:09:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1079602512-472497468 is in desired state: Ready
2022-04-09 17:09:59 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRollerIsolatedST:155] Setting KafkaTopic's min.insync.replicas to be higher than replication factor
2022-04-09 17:09:59 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaRollerIsolatedST:159] Annotate Kafka StatefulSet my-cluster-f5fef1a5-kafka with manual rolling update annotation
2022-04-09 17:09:59 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-f5fef1a5-kafka rolling update
2022-04-09 17:10:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d612fe85-kafka has been successfully rolled
2022-04-09 17:10:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-d612fe85-kafka to be ready
2022-04-09 17:11:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4cc61c4b is in desired state: Ready
2022-04-09 17:11:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:11:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodImagePullBackOff
2022-04-09 17:11:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4cc61c4b in namespace namespace-102
2022-04-09 17:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-30c8b261 is in desired state: Ready
2022-04-09 17:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodCrashLooping
2022-04-09 17:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-30c8b261 in namespace namespace-98
2022-04-09 17:11:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:11:25 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-102 for test case:testKafkaPodImagePullBackOff
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d612fe85 will have desired state: Ready
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d612fe85 is in desired state: Ready
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d612fe85 is ready
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 591 seconds
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaRollsWhenTopicIsUnderReplicated
2022-04-09 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2021587028-332157807 in namespace namespace-100
2022-04-09 17:11:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d612fe85 in namespace namespace-100
2022-04-09 17:11:29 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-f5fef1a5-kafka has been successfully rolled
2022-04-09 17:11:29 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-f5fef1a5-kafka to be ready
2022-04-09 17:11:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:11:32 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-98 for test case:testKafkaPodCrashLooping
2022-04-09 17:11:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:11:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-100 for test case:testKafkaRollsWhenTopicIsUnderReplicated
2022-04-09 17:12:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaRollsWhenTopicIsUnderReplicated-FINISHED
2022-04-09 17:12:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f5fef1a5 will have desired state: Ready
2022-04-09 17:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f5fef1a5 is in desired state: Ready
2022-04-09 17:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-f5fef1a5 is ready
2022-04-09 17:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicRFLowerThanMinInSyncReplicas
2022-04-09 17:12:07 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1079602512-472497468 in namespace namespace-103
2022-04-09 17:12:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f5fef1a5 in namespace namespace-103
2022-04-09 17:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodImagePullBackOff-FINISHED
2022-04-09 17:12:09 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:12:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodCrashLooping-FINISHED
2022-04-09 17:12:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:12:17 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:12:17 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-103 for test case:testKafkaTopicRFLowerThanMinInSyncReplicas
2022-04-09 17:13:00 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaTopicRFLowerThanMinInSyncReplicas-FINISHED
2022-04-09 17:13:00 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:13:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:13:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context KafkaRollerIsolatedST is everything deleted.
2022-04-09 17:13:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 771.92 s - in io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-09 17:13:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 17:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 17:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 17:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 17:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:13:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:13:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 17:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 17:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectScaleUpScaleDown-STARTED
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-STARTED
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectAndConnectorSubresource-STARTED
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testCustomAndUpdatedValues-STARTED
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndPausedConnectorWithFileSinkPlugin-STARTED
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-104 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-104
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-104
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-104
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-105 for test case:testCustomAndUpdatedValues
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-105
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-50f43016 in namespace namespace-104
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:14:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-50f43016 will have desired state: Ready
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-105
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-105
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-106 for test case:testKafkaConnectAndPausedConnectorWithFileSinkPlugin
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-106
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f51747af in namespace namespace-105
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-105
2022-04-09 17:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f51747af will have desired state: Ready
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-106
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-106
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-107 for test case:testKafkaConnectScaleUpScaleDown
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-107
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5d54ac78 in namespace namespace-106
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:14:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5d54ac78 will have desired state: Ready
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-107
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-107
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-108 for test case:testScaleConnectAndConnectorSubresource
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-108
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ad638bf7 in namespace namespace-107
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-107
2022-04-09 17:14:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ad638bf7 will have desired state: Ready
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-108
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-108
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7d4feff8 in namespace namespace-108
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-108
2022-04-09 17:14:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7d4feff8 will have desired state: Ready
2022-04-09 17:16:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5d54ac78 is in desired state: Ready
2022-04-09 17:16:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-619798312-1412741644 in namespace namespace-108
2022-04-09 17:16:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:16:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-619798312-1412741644 will have desired state: Ready
2022-04-09 17:16:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-619798312-1412741644 is in desired state: Ready
2022-04-09 17:16:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5d54ac78-scraper in namespace namespace-106
2022-04-09 17:16:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:16:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-5d54ac78-scraper will be ready
2022-04-09 17:16:16 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-5d54ac78-scraper is ready
2022-04-09 17:16:16 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-5d54ac78-scraper to be ready
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-5d54ac78-scraper is ready
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-5d54ac78-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-5d54ac78-allow in namespace namespace-106
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-5d54ac78 in namespace namespace-108
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:16:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-5d54ac78 will have desired state: Ready
2022-04-09 17:16:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-50f43016 is in desired state: Ready
2022-04-09 17:16:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-50f43016-scraper in namespace namespace-104
2022-04-09 17:16:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:16:47 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-50f43016-scraper will be ready
2022-04-09 17:16:50 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-50f43016-scraper is ready
2022-04-09 17:16:50 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-50f43016-scraper to be ready
2022-04-09 17:16:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f51747af is in desired state: Ready
2022-04-09 17:16:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-f51747af in namespace namespace-108
2022-04-09 17:16:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-105
2022-04-09 17:16:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-f51747af will have desired state: Ready
2022-04-09 17:16:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ad638bf7 is in desired state: Ready
2022-04-09 17:16:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ConnectIsolatedST:395] Running kafkaConnectScaleUP namespace-107 in namespace
2022-04-09 17:16:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-ad638bf7 in namespace namespace-108
2022-04-09 17:16:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-107
2022-04-09 17:16:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-ad638bf7 will have desired state: Ready
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-50f43016-scraper is ready
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-50f43016-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-50f43016-allow in namespace namespace-104
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-50f43016 in namespace namespace-108
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:17:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-50f43016 will have desired state: Ready
2022-04-09 17:17:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7d4feff8 is in desired state: Ready
2022-04-09 17:17:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-7d4feff8 in namespace namespace-108
2022-04-09 17:17:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-108
2022-04-09 17:17:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-7d4feff8 will have desired state: Ready
2022-04-09 17:17:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-5d54ac78 is in desired state: Ready
2022-04-09 17:17:30 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-106 exec my-cluster-5d54ac78-connect-565fc6d957-6b6dt -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:181] Creating KafkaConnector with 'pause: true'
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-5d54ac78 in namespace namespace-108
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:17:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-5d54ac78 will have desired state: Ready
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-f51747af is in desired state: Ready
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:629] Verify values before update
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-f51747af-connect in pod name
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container my-cluster-f51747af-connect
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-f51747af-connect
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container my-cluster-f51747af-connect
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:634] Check if actual env variable KAFKA_CONNECT_CONFIGURATION has different value than test.value
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:640] Updating values in MirrorMaker container
2022-04-09 17:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-f51747af-connect rolling update
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-5d54ac78 is in desired state: Ready
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-5d54ac78-hello-world-producer in namespace namespace-106
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-5d54ac78-hello-world-consumer in namespace namespace-106
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-5d54ac78-hello-world-producer will be in active state
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-5d54ac78-hello-world-consumer will be in active state
2022-04-09 17:17:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-5d54ac78-hello-world-producer and consumer my-cluster-5d54ac78-hello-world-consumer finish
2022-04-09 17:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-5d54ac78-connect-565fc6d957-6b6dt
2022-04-09 17:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-5d54ac78-connect-565fc6d957-6b6dt
2022-04-09 17:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:207] Pausing KafkaConnector: my-cluster-5d54ac78
2022-04-09 17:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-5d54ac78 will have desired state: Ready
2022-04-09 17:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-5d54ac78 is in desired state: Ready
2022-04-09 17:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:213] Clearing FileSink file to check if KafkaConnector will be really paused
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-106 exec my-cluster-5d54ac78-connect-565fc6d957-6b6dt -- /bin/bash -c truncate -s 0 /tmp/test-file-sink.txt
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-5d54ac78-hello-world-producer in namespace namespace-106
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-5d54ac78-hello-world-consumer in namespace namespace-106
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-5d54ac78-hello-world-producer will be in active state
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-5d54ac78-hello-world-consumer will be in active state
2022-04-09 17:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-5d54ac78-hello-world-producer and consumer my-cluster-5d54ac78-hello-world-consumer finish
2022-04-09 17:18:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-ad638bf7 is in desired state: Ready
2022-04-09 17:18:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ConnectIsolatedST:407] Scaling up to 4
2022-04-09 17:18:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ad638bf7-connect will be ready
2022-04-09 17:18:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ad638bf7-connect is ready
2022-04-09 17:18:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-ad638bf7-connect to be ready
2022-04-09 17:18:12 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-50f43016 is in desired state: Ready
2022-04-09 17:18:12 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-50f43016 in namespace namespace-108
2022-04-09 17:18:12 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:18:12 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-50f43016 will have desired state: Ready
2022-04-09 17:18:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-7d4feff8 is in desired state: Ready
2022-04-09 17:18:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-7d4feff8 in namespace namespace-108
2022-04-09 17:18:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-108
2022-04-09 17:18:14 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-7d4feff8 will have desired state: Ready
2022-04-09 17:18:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-50f43016 is in desired state: Ready
2022-04-09 17:18:14 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-104 exec my-cluster-50f43016-connect-55fff6995b-brzc9 -- curl -X GET http://localhost:8083/connectors/my-cluster-50f43016/status
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-50f43016-hello-world-producer in namespace namespace-104
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-50f43016-hello-world-consumer in namespace namespace-104
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-50f43016-hello-world-producer will be in active state
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-50f43016-hello-world-consumer will be in active state
2022-04-09 17:18:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-50f43016-hello-world-producer and consumer my-cluster-50f43016-hello-world-consumer finish
2022-04-09 17:18:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-7d4feff8 is in desired state: Ready
2022-04-09 17:18:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:979] -------> Scaling KafkaConnect subresource <-------
2022-04-09 17:18:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:980] Scaling subresource replicas to 4
2022-04-09 17:18:16 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7d4feff8-connect will be ready
2022-04-09 17:18:16 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7d4feff8-connect is ready
2022-04-09 17:18:16 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-7d4feff8-connect to be ready
2022-04-09 17:18:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-f51747af-connect will be ready
2022-04-09 17:18:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-f51747af-connect is ready
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-f51747af-connect rolling update finished
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:657] Verify values after update
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-f51747af-connect in pod name
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container my-cluster-f51747af-connect
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-f51747af-connect
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container my-cluster-f51747af-connect
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-f51747af-connect
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container my-cluster-f51747af-connect
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-09 17:18:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-f51747af in namespace namespace-105
2022-04-09 17:18:33 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-50f43016-connect-55fff6995b-brzc9
2022-04-09 17:18:34 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-50f43016-connect-55fff6995b-brzc9
2022-04-09 17:18:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:18:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testMultiNodeKafkaConnectWithConnectorCreation
2022-04-09 17:18:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-50f43016 in namespace namespace-104
2022-04-09 17:18:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f51747af in namespace namespace-105
2022-04-09 17:18:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:219] Because KafkaConnector is paused, no messages should appear to FileSink file
2022-04-09 17:18:37 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-5d54ac78-connect-565fc6d957-6b6dt
2022-04-09 17:18:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-50f43016-scraper in namespace namespace-104
2022-04-09 17:18:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-50f43016 in namespace namespace-104
2022-04-09 17:18:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-50f43016 in namespace namespace-104
2022-04-09 17:18:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:18:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-105 for test case:testCustomAndUpdatedValues
2022-04-09 17:18:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-50f43016-hello-world-consumer in namespace namespace-104
2022-04-09 17:18:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-50f43016-hello-world-producer in namespace namespace-104
2022-04-09 17:18:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-50f43016-allow in namespace namespace-104
2022-04-09 17:18:54 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:18:54 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged-STARTED
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-109 for test case:testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-109
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-109
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-109
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-87f2ccf4 in namespace namespace-109
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-09 17:18:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-87f2ccf4 will have desired state: Ready
2022-04-09 17:19:17 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:19:17 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-104 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-04-09 17:19:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:19:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication-STARTED
2022-04-09 17:19:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-ad638bf7-connect is ready
2022-04-09 17:19:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ConnectIsolatedST:414] Scaling down to 1
2022-04-09 17:19:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ad638bf7-connect will be ready
2022-04-09 17:19:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ad638bf7-connect is ready
2022-04-09 17:19:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-ad638bf7-connect to be ready
2022-04-09 17:19:23 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-FINISHED
2022-04-09 17:19:23 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:19:23 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:19:23 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testConfigureDeploymentStrategy-STARTED
2022-04-09 17:19:27 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:19:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-110 for test case:testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication
2022-04-09 17:19:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-110
2022-04-09 17:19:28 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-110
2022-04-09 17:19:28 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-110
2022-04-09 17:19:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-43a686be in namespace namespace-110
2022-04-09 17:19:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:19:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-43a686be will have desired state: Ready
2022-04-09 17:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testCustomAndUpdatedValues-FINISHED
2022-04-09 17:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithPlainAndScramShaAuthentication-STARTED
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-111 for test case:testConfigureDeploymentStrategy
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-111
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-111
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-111
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3cb21d1e in namespace namespace-111
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-111
2022-04-09 17:19:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3cb21d1e will have desired state: Ready
io.strimzi.test.WaitException: Timeout after 60000 ms waiting for messages in file sink
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaConnectUtils.waitForMessagesInKafkaConnectFileSink(KafkaConnectUtils.java:75)
	at io.strimzi.systemtest.connect.ConnectIsolatedST.lambda$testKafkaConnectAndPausedConnectorWithFileSinkPlugin$1(ConnectIsolatedST.java:220)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3082)
	at io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndPausedConnectorWithFileSinkPlugin(ConnectIsolatedST.java:220)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 17:19:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:222] Unpausing KafkaConnector, messages should again appear to FileSink file
2022-04-09 17:19:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-5d54ac78 will have desired state: Ready
2022-04-09 17:19:37 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-5d54ac78 is in desired state: Ready
2022-04-09 17:19:37 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-5d54ac78-connect-565fc6d957-6b6dt
2022-04-09 17:19:38 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-5d54ac78-connect-565fc6d957-6b6dt
2022-04-09 17:19:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:19:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectAndPausedConnectorWithFileSinkPlugin
2022-04-09 17:19:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-5d54ac78-hello-world-producer in namespace namespace-106
2022-04-09 17:19:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-5d54ac78 in namespace namespace-106
2022-04-09 17:19:39 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-7d4feff8-connect is ready
2022-04-09 17:19:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:984] Check if replicas is set to 4, observed generation is higher - for spec and status - naming prefix should be same
2022-04-09 17:19:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:998] -------> Scaling KafkaConnector subresource <-------
2022-04-09 17:19:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:999] Scaling subresource task max to 4
2022-04-09 17:19:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:1003] Check if taskMax is set to 4
2022-04-09 17:19:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:1007] Check taskMax on Connect pods API
2022-04-09 17:19:41 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-108 exec my-cluster-7d4feff8-connect-9f96f75dc-fp98h -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-7d4feff8
2022-04-09 17:19:41 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:19:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-108 exec my-cluster-7d4feff8-connect-9f96f75dc-jjqt9 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-7d4feff8
2022-04-09 17:19:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:19:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-108 exec my-cluster-7d4feff8-connect-9f96f75dc-lqbg8 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-7d4feff8
2022-04-09 17:19:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-108 exec my-cluster-7d4feff8-connect-9f96f75dc-vdsqb -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-7d4feff8
2022-04-09 17:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectAndConnectorSubresource
2022-04-09 17:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-7d4feff8 in namespace namespace-108
2022-04-09 17:19:43 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-ad638bf7-connect is ready
2022-04-09 17:19:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:19:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectScaleUpScaleDown
2022-04-09 17:19:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-ad638bf7 in namespace namespace-107
2022-04-09 17:19:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-5d54ac78-hello-world-producer in namespace namespace-106
2022-04-09 17:19:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-5d54ac78-hello-world-consumer in namespace namespace-106
2022-04-09 17:19:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-5d54ac78-hello-world-consumer in namespace namespace-106
2022-04-09 17:19:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5d54ac78-scraper in namespace namespace-106
2022-04-09 17:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-7d4feff8 in namespace namespace-108
2022-04-09 17:19:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ad638bf7 in namespace namespace-107
2022-04-09 17:20:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7d4feff8 in namespace namespace-108
2022-04-09 17:20:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:20:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-107 for test case:testKafkaConnectScaleUpScaleDown
2022-04-09 17:20:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:20:13 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-108 for test case:testScaleConnectAndConnectorSubresource
2022-04-09 17:20:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-5d54ac78 in namespace namespace-106
2022-04-09 17:20:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-5d54ac78-allow in namespace namespace-106
2022-04-09 17:20:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-619798312-1412741644 in namespace namespace-106
2022-04-09 17:20:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5d54ac78 in namespace namespace-106
2022-04-09 17:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectAndConnectorSubresource-FINISHED
2022-04-09 17:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:20:41 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndConnectorFileSinkPlugin-STARTED
2022-04-09 17:20:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-87f2ccf4 is in desired state: Ready
2022-04-09 17:20:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1541545016-30830947 in namespace namespace-111
2022-04-09 17:20:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-09 17:20:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1541545016-30830947 will have desired state: Ready
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1541545016-30830947 is in desired state: Ready
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1541545016-30830947 in namespace namespace-111
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1541545016-30830947 will have desired state: Ready
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1541545016-30830947 is in desired state: Ready
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1737600286-1255507072 in namespace namespace-111
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-09 17:20:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1737600286-1255507072 will have desired state: Ready
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-112 for test case:testKafkaConnectWithPlainAndScramShaAuthentication
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-112
2022-04-09 17:20:46 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1737600286-1255507072 is in desired state: Ready
2022-04-09 17:20:46 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-87f2ccf4 in namespace namespace-111
2022-04-09 17:20:46 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-09 17:20:46 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-87f2ccf4 will have desired state: Ready
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-112
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-112
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2062e614 in namespace namespace-112
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:20:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2062e614 will have desired state: Ready
2022-04-09 17:20:47 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectScaleUpScaleDown-FINISHED
2022-04-09 17:20:47 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:20:47 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:20:47 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testDeployUndeploy-STARTED
2022-04-09 17:20:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:20:48 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-106 for test case:testKafkaConnectAndPausedConnectorWithFileSinkPlugin
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-113 for test case:testKafkaConnectAndConnectorFileSinkPlugin
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-113
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-113
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-113
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-cba72165 in namespace namespace-113
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-09 17:20:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-cba72165 will have desired state: Ready
2022-04-09 17:20:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-43a686be is in desired state: Ready
2022-04-09 17:20:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-43a686be-user in namespace namespace-113
2022-04-09 17:20:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:20:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-43a686be-user will have desired state: Ready
2022-04-09 17:20:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-43a686be-user is in desired state: Ready
2022-04-09 17:20:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1105416594-1267124606 in namespace namespace-113
2022-04-09 17:20:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:20:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1105416594-1267124606 will have desired state: Ready
2022-04-09 17:20:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1105416594-1267124606 is in desired state: Ready
2022-04-09 17:20:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-43a686be-scraper in namespace namespace-110
2022-04-09 17:20:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:20:55 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-43a686be-scraper will be ready
2022-04-09 17:20:59 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-43a686be-scraper is ready
2022-04-09 17:20:59 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-43a686be-scraper to be ready
2022-04-09 17:21:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3cb21d1e is in desired state: Ready
2022-04-09 17:21:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-3cb21d1e in namespace namespace-113
2022-04-09 17:21:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-111
2022-04-09 17:21:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-3cb21d1e will have desired state: Ready
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-43a686be-scraper is ready
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-43a686be-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-43a686be-allow in namespace namespace-110
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-43a686be in namespace namespace-113
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:21:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-43a686be will have desired state: Ready
2022-04-09 17:21:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndPausedConnectorWithFileSinkPlugin-FINISHED
2022-04-09 17:21:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:21:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:21:17 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMountingSecretAndConfigMapAsVolumesAndEnvVars-STARTED
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-114 for test case:testDeployUndeploy
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-114
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-114
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-114
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-746478a2 in namespace namespace-114
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-114
2022-04-09 17:21:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-746478a2 will have desired state: Ready
2022-04-09 17:21:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-87f2ccf4 is in desired state: Ready
2022-04-09 17:21:56 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-109 exec my-cluster-87f2ccf4-connect-749d9d948b-t4ntl -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1541545016-30830947 in namespace namespace-114
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1541545016-30830947 will have desired state: Ready
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1541545016-30830947 is in desired state: Ready
2022-04-09 17:21:57 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-87f2ccf4-connect rolling update
2022-04-09 17:22:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-3cb21d1e is in desired state: Ready
2022-04-09 17:22:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectIsolatedST:1191] Adding label to Connect resource, the CR should be recreated
2022-04-09 17:22:08 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-3cb21d1e-connect will be ready
2022-04-09 17:22:08 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-3cb21d1e-connect is ready
2022-04-09 17:22:08 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-3cb21d1e-connect to be ready
2022-04-09 17:22:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-43a686be is in desired state: Ready
2022-04-09 17:22:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ConnectIsolatedST:547] Verifying that KafkaConnect pod logs don't contain ERRORs
2022-04-09 17:22:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ConnectIsolatedST:550] Creating FileStreamSink connector via pod my-cluster-43a686be-scraper-858cbcf498-cg2hh with topic my-topic-1105416594-1267124606
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-110 exec my-cluster-43a686be-scraper-858cbcf498-cg2hh -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-1105416594-1267124606", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-43a686be-connect-api.namespace-110.svc:8083/connectors
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-43a686be-hello-world-producer in namespace namespace-110
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-43a686be-hello-world-consumer in namespace namespace-110
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-09 17:22:15 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-43a686be-hello-world-producer will be in active state
2022-04-09 17:22:16 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-43a686be-hello-world-consumer will be in active state
2022-04-09 17:22:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-43a686be-hello-world-producer and consumer my-cluster-43a686be-hello-world-consumer finish
2022-04-09 17:22:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-cba72165 is in desired state: Ready
2022-04-09 17:22:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-cba72165-scraper in namespace namespace-113
2022-04-09 17:22:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-09 17:22:18 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-cba72165-scraper will be ready
2022-04-09 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2062e614 is in desired state: Ready
2022-04-09 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-2062e614-user in namespace namespace-114
2022-04-09 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-2062e614-user will have desired state: Ready
2022-04-09 17:22:21 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-cba72165-scraper is ready
2022-04-09 17:22:21 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-cba72165-scraper to be ready
2022-04-09 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-2062e614-user is in desired state: Ready
2022-04-09 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1346826496-349371495 in namespace namespace-114
2022-04-09 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1346826496-349371495 will have desired state: Ready
2022-04-09 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1346826496-349371495 is in desired state: Ready
2022-04-09 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2062e614-scraper in namespace namespace-112
2022-04-09 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2062e614-scraper will be ready
2022-04-09 17:22:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2062e614-scraper is ready
2022-04-09 17:22:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-2062e614-scraper to be ready
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-cba72165-scraper is ready
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-cba72165-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-cba72165-allow in namespace namespace-113
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-cba72165 in namespace namespace-114
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-09 17:22:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-cba72165 will have desired state: Ready
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-2062e614-scraper is ready
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-2062e614-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-2062e614-allow in namespace namespace-112
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-2062e614 in namespace namespace-114
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-2062e614 will have desired state: Ready
2022-04-09 17:22:35 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-43a686be-connect-c6f667b54-nsrhk
2022-04-09 17:22:35 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-43a686be-connect-c6f667b54-nsrhk
2022-04-09 17:22:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:22:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication
2022-04-09 17:22:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-43a686be in namespace namespace-110
2022-04-09 17:22:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-43a686be-allow in namespace namespace-110
2022-04-09 17:22:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-43a686be-hello-world-consumer in namespace namespace-110
2022-04-09 17:22:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-43a686be-hello-world-producer in namespace namespace-110
2022-04-09 17:22:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1105416594-1267124606 in namespace namespace-110
2022-04-09 17:22:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-746478a2 is in desired state: Ready
2022-04-09 17:22:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-746478a2-scraper in namespace namespace-114
2022-04-09 17:22:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-114
2022-04-09 17:22:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-746478a2-scraper will be ready
2022-04-09 17:22:51 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-746478a2-scraper is ready
2022-04-09 17:22:51 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-746478a2-scraper to be ready
2022-04-09 17:22:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-43a686be-scraper in namespace namespace-110
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-746478a2-scraper is ready
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-746478a2-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-746478a2-allow in namespace namespace-114
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-114
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-746478a2 in namespace namespace-114
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-114
2022-04-09 17:23:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-746478a2 will have desired state: Ready
2022-04-09 17:23:07 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-87f2ccf4-connect will be ready
2022-04-09 17:23:07 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-87f2ccf4-connect is ready
2022-04-09 17:23:17 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-87f2ccf4-connect rolling update finished
2022-04-09 17:23:17 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 17:23:18 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-109 exec my-cluster-87f2ccf4-connect-6785794fcf-lj8kj -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 17:23:18 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:23:18 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 17:23:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:23:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged
2022-04-09 17:23:18 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1737600286-1255507072 in namespace namespace-109
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-3cb21d1e-connect is ready
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectIsolatedST:1198] Checking that observed gen. is still on 1 (recreation) and new label is present
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectIsolatedST:1203] Changing deployment strategy to ROLLING_UPDATE
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-3cb21d1e will have desired state: Ready
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-3cb21d1e is in desired state: Ready
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectIsolatedST:1208] Adding another label to Connect resource, pods should be rolled
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-3cb21d1e-connect will be ready
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-3cb21d1e-connect is ready
2022-04-09 17:23:26 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-3cb21d1e-connect to be ready
2022-04-09 17:23:28 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1541545016-30830947 in namespace namespace-109
2022-04-09 17:23:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-cba72165 is in desired state: Ready
2022-04-09 17:23:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector license-source in namespace namespace-114
2022-04-09 17:23:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-09 17:23:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: license-source will have desired state: Ready
2022-04-09 17:23:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnector: license-source is in desired state: Ready
2022-04-09 17:23:35 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:23:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-cba72165-hello-world-consumer in namespace namespace-113
2022-04-09 17:23:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-09 17:23:35 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-cba72165-hello-world-consumer will be in active state
2022-04-09 17:23:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-43a686be-user in namespace namespace-110
2022-04-09 17:23:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-cba72165-hello-world-consumer to finished
2022-04-09 17:23:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-87f2ccf4 in namespace namespace-109
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-2062e614 is in desired state: Ready
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-112 exec my-cluster-2062e614-connect-57f78d7f66-845tq -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:280] Verifying that KafkaConnect pod logs don't contain ERRORs
2022-04-09 17:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:283] Creating FileStreamSink connector via pod my-cluster-2062e614-scraper-6d9c8dcc88-zb5qj with topic my-topic-1346826496-349371495
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-112 exec my-cluster-2062e614-scraper-6d9c8dcc88-zb5qj -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-1346826496-349371495", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-2062e614-connect-api.namespace-112.svc:8083/connectors
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-2062e614-hello-world-producer in namespace namespace-112
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-2062e614-hello-world-consumer in namespace namespace-112
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-2062e614-hello-world-producer will be in active state
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-2062e614-hello-world-consumer will be in active state
2022-04-09 17:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-2062e614-hello-world-producer and consumer my-cluster-2062e614-hello-world-consumer finish
2022-04-09 17:23:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-43a686be in namespace namespace-110
2022-04-09 17:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1541545016-30830947 in namespace namespace-109
2022-04-09 17:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1541545016-30830947 in namespace namespace-109
2022-04-09 17:23:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-87f2ccf4 in namespace namespace-109
2022-04-09 17:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-113 exec my-cluster-cba72165-scraper-58899cf887-f5zkp -- /bin/bash -c curl http://my-cluster-cba72165-connect-api.namespace-113.svc:8083/connectors/license-source
2022-04-09 17:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectAndConnectorFileSinkPlugin
2022-04-09 17:23:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-cba72165 in namespace namespace-113
2022-04-09 17:23:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:23:56 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-110 for test case:testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication
2022-04-09 17:23:58 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:23:58 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-109 for test case:testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged
2022-04-09 17:23:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-cba72165-hello-world-consumer in namespace namespace-113
2022-04-09 17:23:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector license-source in namespace namespace-113
2022-04-09 17:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-2062e614-connect-57f78d7f66-845tq
2022-04-09 17:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-2062e614-connect-57f78d7f66-845tq
2022-04-09 17:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectWithPlainAndScramShaAuthentication
2022-04-09 17:24:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-2062e614 in namespace namespace-112
2022-04-09 17:24:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-cba72165-scraper in namespace namespace-113
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-746478a2 is in desired state: Ready
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ConnectIsolatedST:123] Looks like the connect cluster my-cluster deployed OK
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ConnectIsolatedST:140] Verifying docker image names
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ConnectIsolatedST:152] Docker images verified
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:311] Verifying labels on pod type connect
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:315] Verifying labels for pod: my-cluster-746478a2-connect-58864f7dfc-2tlxq
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:362] Verifying labels for service my-cluster-746478a2-connect-api
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:384] Verifying labels for Config maps
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-746478a2-connect-config
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-746478a2-entity-topic-operator-config
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:407] CM my-cluster-746478a2-entity-topic-operator-config is not related to current test
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-746478a2-entity-user-operator-config
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:407] CM my-cluster-746478a2-entity-user-operator-config is not related to current test
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-746478a2-kafka-config
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-746478a2-zookeeper-config
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:407] CM my-cluster-746478a2-zookeeper-config is not related to current test
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:414] Verifying labels for Service Accounts
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-746478a2-connect
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-746478a2-entity-operator
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-746478a2-kafka
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-746478a2-zookeeper
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployUndeploy
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-746478a2-allow in namespace namespace-114
2022-04-09 17:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-746478a2 in namespace namespace-114
2022-04-09 17:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-2062e614-allow in namespace namespace-112
2022-04-09 17:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-2062e614-hello-world-consumer in namespace namespace-112
2022-04-09 17:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-2062e614-hello-world-producer in namespace namespace-112
2022-04-09 17:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1346826496-349371495 in namespace namespace-112
2022-04-09 17:24:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2062e614-scraper in namespace namespace-112
2022-04-09 17:24:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-746478a2-scraper in namespace namespace-114
2022-04-09 17:24:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication-FINISHED
2022-04-09 17:24:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:24:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:24:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testJvmAndResources-STARTED
2022-04-09 17:24:41 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-3cb21d1e-connect is ready
2022-04-09 17:24:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectIsolatedST:1212] Checking that observed gen. higher (rolling update) and label is changed
2022-04-09 17:24:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:24:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigureDeploymentStrategy
2022-04-09 17:24:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-3cb21d1e in namespace namespace-111
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-115 for test case:testMountingSecretAndConfigMapAsVolumesAndEnvVars
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-115
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-115
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-115
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c34885bd in namespace namespace-115
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-09 17:24:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c34885bd will have desired state: Ready
2022-04-09 17:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged-FINISHED
2022-04-09 17:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication-STARTED
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-116 for test case:testJvmAndResources
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-116
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-116
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-116
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-44e010ce in namespace namespace-116
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-09 17:24:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-44e010ce will have desired state: Ready
2022-04-09 17:24:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-cba72165-allow in namespace namespace-113
2022-04-09 17:24:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-cba72165 in namespace namespace-113
2022-04-09 17:24:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3cb21d1e in namespace namespace-111
2022-04-09 17:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-2062e614-user in namespace namespace-112
2022-04-09 17:24:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:24:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-113 for test case:testKafkaConnectAndConnectorFileSinkPlugin
2022-04-09 17:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-111 for test case:testConfigureDeploymentStrategy
2022-04-09 17:25:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2062e614 in namespace namespace-112
2022-04-09 17:25:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-746478a2 in namespace namespace-114
2022-04-09 17:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-112 for test case:testKafkaConnectWithPlainAndScramShaAuthentication
2022-04-09 17:25:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:25:23 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-114 for test case:testDeployUndeploy
2022-04-09 17:25:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testConfigureDeploymentStrategy-FINISHED
2022-04-09 17:25:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:25:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:25:28 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero-STARTED
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-117 for test case:testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-117
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-117
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-117
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6258d698 in namespace namespace-117
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6258d698 will have desired state: Ready
2022-04-09 17:25:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndConnectorFileSinkPlugin-FINISHED
2022-04-09 17:25:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:25:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:25:42 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithConnectorToZero-STARTED
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-118 for test case:testScaleConnectWithoutConnectorToZero
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-118
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-118
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-118
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-bac73d2e in namespace namespace-118
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-118
2022-04-09 17:25:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bac73d2e will have desired state: Ready
2022-04-09 17:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithPlainAndScramShaAuthentication-FINISHED
2022-04-09 17:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-119 for test case:testScaleConnectWithConnectorToZero
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-119
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-119
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-119
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8f390f75 in namespace namespace-119
2022-04-09 17:26:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-09 17:26:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8f390f75 will have desired state: Ready
2022-04-09 17:26:07 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testDeployUndeploy-FINISHED
2022-04-09 17:26:07 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:26:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-44e010ce is in desired state: Ready
2022-04-09 17:26:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-44e010ce-kafka-clients in namespace namespace-119
2022-04-09 17:26:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-09 17:26:38 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-44e010ce-kafka-clients will be ready
2022-04-09 17:26:41 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-44e010ce-kafka-clients is ready
2022-04-09 17:26:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-44e010ce-scraper in namespace namespace-116
2022-04-09 17:26:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-09 17:26:41 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-44e010ce-scraper will be ready
2022-04-09 17:26:43 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-44e010ce-scraper is ready
2022-04-09 17:26:43 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-44e010ce-scraper to be ready
2022-04-09 17:26:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c34885bd is in desired state: Ready
2022-04-09 17:26:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-c34885bd in namespace namespace-119
2022-04-09 17:26:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-09 17:26:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-c34885bd will have desired state: Ready
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-44e010ce-scraper is ready
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-44e010ce-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-44e010ce-allow in namespace namespace-116
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-44e010ce in namespace namespace-119
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-09 17:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-44e010ce will have desired state: Ready
2022-04-09 17:27:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6258d698 is in desired state: Ready
2022-04-09 17:27:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-6258d698-user in namespace namespace-119
2022-04-09 17:27:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:27:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-6258d698-user will have desired state: Ready
2022-04-09 17:27:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-6258d698-user is in desired state: Ready
2022-04-09 17:27:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-661659756-541251881 in namespace namespace-119
2022-04-09 17:27:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:27:33 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-661659756-541251881 will have desired state: Ready
2022-04-09 17:27:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8f390f75 is in desired state: Ready
2022-04-09 17:27:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-8f390f75 in namespace namespace-119
2022-04-09 17:27:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-09 17:27:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-8f390f75 will have desired state: Ready
2022-04-09 17:27:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-661659756-541251881 is in desired state: Ready
2022-04-09 17:27:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6258d698-scraper in namespace namespace-117
2022-04-09 17:27:34 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:27:34 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6258d698-scraper will be ready
2022-04-09 17:27:37 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6258d698-scraper is ready
2022-04-09 17:27:37 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-6258d698-scraper to be ready
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-6258d698-scraper is ready
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-6258d698-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-6258d698-allow in namespace namespace-117
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-6258d698 in namespace namespace-119
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:27:47 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6258d698 will have desired state: Ready
2022-04-09 17:27:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bac73d2e is in desired state: Ready
2022-04-09 17:27:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-bac73d2e in namespace namespace-119
2022-04-09 17:27:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-118
2022-04-09 17:27:48 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-bac73d2e will have desired state: Ready
2022-04-09 17:27:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-c34885bd is in desired state: Ready
2022-04-09 17:27:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:1148] Check if the ENVs contains desired values
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c printenv MY_CONNECT_SECRET
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c printenv MY_CONNECT_CONFIG_MAP
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c printenv MY_DOTED_CONNECT_SECRET
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c printenv MY_DOTED_CONNECT_CONFIG_MAP
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:1154] Check if volumes contains desired values
2022-04-09 17:27:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c cat external-configuration/connect-config-map/my-key
2022-04-09 17:27:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c cat external-configuration/connect-secret/my-secret-key
2022-04-09 17:27:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c cat external-configuration/connect.config.map/my-key
2022-04-09 17:27:50 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:51 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-c34885bd-connect-697d599796-fkrg7 -- /bin/bash -c cat external-configuration/connect.secret/my-secret-key
2022-04-09 17:27:51 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:27:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:27:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testMountingSecretAndConfigMapAsVolumesAndEnvVars
2022-04-09 17:27:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-c34885bd in namespace namespace-115
2022-04-09 17:28:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c34885bd in namespace namespace-115
2022-04-09 17:28:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-44e010ce is in desired state: Ready
2022-04-09 17:28:04 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-116 exec my-cluster-44e010ce-connect-7d99d76f66-pv2fh -c my-cluster-44e010ce-connect -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-09 17:28:04 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:28:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:28:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testJvmAndResources
2022-04-09 17:28:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-44e010ce-scraper in namespace namespace-116
2022-04-09 17:28:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:28:11 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-115 for test case:testMountingSecretAndConfigMapAsVolumesAndEnvVars
2022-04-09 17:28:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-44e010ce-kafka-clients in namespace namespace-116
2022-04-09 17:28:38 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMountingSecretAndConfigMapAsVolumesAndEnvVars-FINISHED
2022-04-09 17:28:38 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:28:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-44e010ce in namespace namespace-116
2022-04-09 17:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-8f390f75 is in desired state: Ready
2022-04-09 17:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-8f390f75 in namespace namespace-119
2022-04-09 17:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-09 17:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-8f390f75 will have desired state: Ready
2022-04-09 17:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-8f390f75 is in desired state: Ready
2022-04-09 17:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectIsolatedST:934] Scaling KafkaConnect down to zero
2022-04-09 17:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-8f390f75 will have desired state: Ready
2022-04-09 17:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-8f390f75 is in desired state: Ready
2022-04-09 17:28:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-44e010ce-allow in namespace namespace-116
2022-04-09 17:28:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-44e010ce in namespace namespace-116
2022-04-09 17:28:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:28:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectWithConnectorToZero
2022-04-09 17:28:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-8f390f75 in namespace namespace-119
2022-04-09 17:28:54 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6258d698 is in desired state: Ready
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-6258d698-connect-6fb6d9f95-5mv7w -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectIsolatedST:474] Verifying that KafkaConnect pod logs don't contain ERRORs
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectIsolatedST:477] Creating FileStreamSink connector via pod my-cluster-6258d698-scraper-549df698f8-brdl8 with topic my-topic-661659756-541251881
2022-04-09 17:28:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8f390f75 in namespace namespace-119
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-6258d698-scraper-549df698f8-brdl8 -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-661659756-541251881", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-6258d698-connect-api.namespace-117.svc:8083/connectors
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-6258d698-hello-world-producer in namespace namespace-117
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-6258d698-hello-world-consumer in namespace namespace-117
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-09 17:28:55 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-6258d698-hello-world-producer will be in active state
2022-04-09 17:28:56 [ForkJoinPool-3-worker-13] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-6258d698-hello-world-consumer will be in active state
2022-04-09 17:28:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-6258d698-hello-world-producer and consumer my-cluster-6258d698-hello-world-consumer finish
2022-04-09 17:28:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-8f390f75 in namespace namespace-119
2022-04-09 17:29:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-bac73d2e is in desired state: Ready
2022-04-09 17:29:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectIsolatedST:891] Scaling KafkaConnect down to zero
2022-04-09 17:29:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-bac73d2e will have desired state: Ready
2022-04-09 17:29:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-bac73d2e is in desired state: Ready
2022-04-09 17:29:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:29:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-116 for test case:testJvmAndResources
2022-04-09 17:29:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:29:08 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-119 for test case:testScaleConnectWithConnectorToZero
2022-04-09 17:29:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:29:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectWithoutConnectorToZero
2022-04-09 17:29:10 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-bac73d2e in namespace namespace-118
2022-04-09 17:29:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-bac73d2e in namespace namespace-118
2022-04-09 17:29:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testJvmAndResources-FINISHED
2022-04-09 17:29:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:29:16 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-6258d698-connect-6fb6d9f95-5mv7w
2022-04-09 17:29:16 [ForkJoinPool-3-worker-13] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-6258d698-connect-6fb6d9f95-5mv7w
2022-04-09 17:29:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:29:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication
2022-04-09 17:29:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-6258d698 in namespace namespace-117
2022-04-09 17:29:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-661659756-541251881 in namespace namespace-117
2022-04-09 17:29:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-6258d698-hello-world-consumer in namespace namespace-117
2022-04-09 17:29:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-6258d698-hello-world-producer in namespace namespace-117
2022-04-09 17:29:20 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:29:20 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-118 for test case:testScaleConnectWithoutConnectorToZero
2022-04-09 17:29:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-6258d698-allow in namespace namespace-117
2022-04-09 17:29:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-6258d698-user in namespace namespace-117
2022-04-09 17:29:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6258d698-scraper in namespace namespace-117
2022-04-09 17:29:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6258d698 in namespace namespace-117
2022-04-09 17:29:52 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithConnectorToZero-FINISHED
2022-04-09 17:29:52 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:30:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero-FINISHED
2022-04-09 17:30:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:30:06 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:30:06 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-117 for test case:testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication
2022-04-09 17:30:11 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication-FINISHED
2022-04-09 17:30:11 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-04-09 17:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,031.449 s - in io.strimzi.systemtest.connect.ConnectIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectBuilderIsolatedST
2022-04-09 17:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 17:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 17:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 17:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 17:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:30:36 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:30:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:30:36 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:30:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 17:31:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 17:31:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 17:31:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 17:31:41 [ForkJoinPool-3-worker-3] [33mWARN [m [ConnectBuilderIsolatedST:546] For running these tests on K8s you have to have internal registry deployed using `minikube start --insecure-registry '10.0.0.0/24'` and `minikube addons enable registry`
2022-04-09 17:31:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka infra-namespace in namespace infra-namespace
2022-04-09 17:31:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: infra-namespace will have desired state: Ready
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: infra-namespace is in desired state: Ready
2022-04-09 17:33:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:33:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:33:03 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildOtherPluginTypeWithAndWithoutFileName-STARTED
2022-04-09 17:33:03 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildFailsWithWrongChecksumOfArtifact-STARTED
2022-04-09 17:33:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:33:03 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin-STARTED
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildPluginUsingMavenCoordinatesArtifacts-STARTED
2022-04-09 17:33:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:33:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildWithJarTgzAndZip-STARTED
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:33:03 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:33:03 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:33:03 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:33:03 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:33:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-144504209-1011849880 in namespace infra-namespace
2022-04-09 17:33:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1941422706-1114906646 in namespace infra-namespace
2022-04-09 17:33:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-830863609-1585290737 in namespace infra-namespace
2022-04-09 17:33:03 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-324014b6-scraper in namespace infra-namespace
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-871486518-2131265851 in namespace infra-namespace
2022-04-09 17:33:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1941422706-1114906646 will have desired state: Ready
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-19a964fe in namespace infra-namespace
2022-04-09 17:33:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-144504209-1011849880 will have desired state: Ready
2022-04-09 17:33:03 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-830863609-1585290737 will have desired state: Ready
2022-04-09 17:33:03 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-324014b6-scraper will be ready
2022-04-09 17:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-871486518-2131265851 will have desired state: Ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1941422706-1114906646 is in desired state: Ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-144504209-1011849880 is in desired state: Ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b8f6eecd-scraper in namespace infra-namespace
2022-04-09 17:33:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-830863609-1585290737 is in desired state: Ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-6fd69163 in namespace infra-namespace
2022-04-09 17:33:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9df36d9d-scraper in namespace infra-namespace
2022-04-09 17:33:04 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b8f6eecd-scraper will be ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-871486518-2131265851 is in desired state: Ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9df36d9d-scraper will be ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-19a964fe will have desired state: Ready
2022-04-09 17:33:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6fd69163 will have desired state: Ready
2022-04-09 17:33:05 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-324014b6-scraper is ready
2022-04-09 17:33:05 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-324014b6-scraper to be ready
2022-04-09 17:33:06 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b8f6eecd-scraper is ready
2022-04-09 17:33:06 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-b8f6eecd-scraper to be ready
2022-04-09 17:33:06 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9df36d9d-scraper is ready
2022-04-09 17:33:06 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-9df36d9d-scraper to be ready
2022-04-09 17:33:15 [ForkJoinPool-3-worker-13] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-324014b6-scraper is ready
2022-04-09 17:33:15 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-324014b6-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:33:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-324014b6-allow in namespace infra-namespace
2022-04-09 17:33:15 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:33:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-324014b6 in namespace infra-namespace
2022-04-09 17:33:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-324014b6 will have desired state: NotReady
2022-04-09 17:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-b8f6eecd-scraper is ready
2022-04-09 17:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-b8f6eecd-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-b8f6eecd-allow in namespace infra-namespace
2022-04-09 17:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-b8f6eecd in namespace infra-namespace
2022-04-09 17:33:16 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-9df36d9d-scraper is ready
2022-04-09 17:33:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-b8f6eecd will have desired state: Ready
2022-04-09 17:33:16 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-9df36d9d-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:33:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-9df36d9d-allow in namespace infra-namespace
2022-04-09 17:33:16 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:33:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-9df36d9d in namespace infra-namespace
2022-04-09 17:33:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9df36d9d will have desired state: Ready
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-324014b6 is in desired state: NotReady
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectBuilderIsolatedST:186] Checking if KafkaConnect status condition contains message about build failure
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectBuilderIsolatedST:189] Deploying network policies for KafkaConnect
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-324014b6-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-324014b6-allow in namespace infra-namespace
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectBuilderIsolatedST:197] Replacing plugin's checksum with right one
2022-04-09 17:33:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-324014b6 will have desired state: Ready
2022-04-09 17:34:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6fd69163 is in desired state: Ready
2022-04-09 17:34:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-6fd69163 in namespace infra-namespace
2022-04-09 17:34:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-6fd69163 will have desired state: Ready
2022-04-09 17:34:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-6fd69163 is in desired state: Ready
2022-04-09 17:34:53 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:34:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-6fd69163-hello-world-producer in namespace infra-namespace
2022-04-09 17:34:53 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-6fd69163-hello-world-producer will be in active state
2022-04-09 17:34:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-6fd69163-hello-world-producer to finished
2022-04-09 17:35:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9df36d9d is in desired state: Ready
2022-04-09 17:35:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectBuilderIsolatedST:370] Creating EchoSink connector
2022-04-09 17:35:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector echo-sink-connector in namespace infra-namespace
2022-04-09 17:35:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: echo-sink-connector will have desired state: Ready
2022-04-09 17:35:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaConnector: echo-sink-connector is in desired state: Ready
2022-04-09 17:35:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectBuilderIsolatedST:382] Checking that KafkaConnect API contains EchoSink connector and not Camel-Telegram Connector class name
2022-04-09 17:35:04 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-9df36d9d-scraper-78744ffc46-6m8pd -- curl -X GET http://my-cluster-9df36d9d-connect-api:8083/connector-plugins
2022-04-09 17:35:04 [ForkJoinPool-3-worker-11] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:35:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectBuilderIsolatedST:388] Adding one more connector to the KafkaConnect
2022-04-09 17:35:04 [ForkJoinPool-3-worker-1] [32mINFO [m [OpenShiftOnlyCondition:25] testPushIntoImageStream is @OpenShiftOnly, but the running cluster is not OpenShift: Ignoring testPushIntoImageStream
2022-04-09 17:35:04 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-9df36d9d-connect rolling update
2022-04-09 17:35:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-09 17:35:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-b8f6eecd is in desired state: Ready
2022-04-09 17:35:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectBuilderIsolatedST:448] Checking that plugin has correct file name: echo-sink-test.jar
2022-04-09 17:35:12 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-b8f6eecd-connect-77c796768f-56smc -- /bin/bash -c ls plugins/plugin-with-other-type/*
2022-04-09 17:35:12 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:35:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectBuilderIsolatedST:461] Removing file name from the plugin, hash should be used
2022-04-09 17:35:12 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b8f6eecd-connect rolling update
2022-04-09 17:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-19a964fe is in desired state: Ready
2022-04-09 17:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-19a964fe-camel-connector in namespace infra-namespace
2022-04-09 17:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-19a964fe-camel-connector will have desired state: Ready
2022-04-09 17:35:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:189] Message Received message with key 'null' and value '"Hello-world - 99"' found in my-cluster-6fd69163-connect-78b89484bd-d988m log
2022-04-09 17:35:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:35:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildWithJarTgzAndZip
2022-04-09 17:35:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-6fd69163 in namespace infra-namespace
2022-04-09 17:35:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-6fd69163-hello-world-producer in namespace infra-namespace
2022-04-09 17:35:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-6fd69163 in namespace infra-namespace
2022-04-09 17:36:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-144504209-1011849880 in namespace infra-namespace
2022-04-09 17:36:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:36:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildWithJarTgzAndZip-FINISHED
2022-04-09 17:36:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-324014b6 is in desired state: Ready
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectBuilderIsolatedST:215] Checking if KafkaConnect API contains EchoSink connector
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-324014b6-scraper-5fc857489-v2knn -- curl -X GET http://my-cluster-324014b6-connect-api:8083/connector-plugins
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ConnectBuilderIsolatedST:220] Checking if KafkaConnect resource contains EchoSink connector in status
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildFailsWithWrongChecksumOfArtifact
2022-04-09 17:36:16 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-324014b6 in namespace infra-namespace
2022-04-09 17:36:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-324014b6-allow in namespace infra-namespace
2022-04-09 17:36:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-324014b6-scraper in namespace infra-namespace
2022-04-09 17:36:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-324014b6-allow in namespace infra-namespace
2022-04-09 17:36:39 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9df36d9d-connect will be ready
2022-04-09 17:36:39 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9df36d9d-connect is ready
2022-04-09 17:36:47 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b8f6eecd-connect will be ready
2022-04-09 17:36:47 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b8f6eecd-connect is ready
2022-04-09 17:36:49 [ForkJoinPool-3-worker-11] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-9df36d9d-connect rolling update finished
2022-04-09 17:36:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ConnectBuilderIsolatedST:399] Creating Camel-HTTP-Sink connector
2022-04-09 17:36:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector camel-http-connector in namespace infra-namespace
2022-04-09 17:36:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: camel-http-connector will have desired state: Ready
2022-04-09 17:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildFailsWithWrongChecksumOfArtifact-FINISHED
2022-04-09 17:36:56 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b8f6eecd-connect rolling update finished
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ConnectBuilderIsolatedST:468] Checking that plugin has different name than before
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-b8f6eecd-connect-c568747d5-ctfsh -- /bin/bash -c ls plugins/plugin-with-other-type/*
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildOtherPluginTypeWithAndWithoutFileName
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-b8f6eecd-allow in namespace infra-namespace
2022-04-09 17:36:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-b8f6eecd in namespace infra-namespace
2022-04-09 17:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b8f6eecd-scraper in namespace infra-namespace
2022-04-09 17:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1941422706-1114906646 in namespace infra-namespace
2022-04-09 17:37:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-19a964fe-camel-connector is in desired state: Ready
2022-04-09 17:37:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-09 17:37:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-19a964fe-hello-world-consumer in namespace infra-namespace
2022-04-09 17:37:10 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-19a964fe-hello-world-consumer will be in active state
2022-04-09 17:37:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-19a964fe-hello-world-consumer to finished
2022-04-09 17:37:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:37:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildOtherPluginTypeWithAndWithoutFileName-FINISHED
2022-04-09 17:37:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildPluginUsingMavenCoordinatesArtifacts
2022-04-09 17:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-19a964fe-camel-connector in namespace infra-namespace
2022-04-09 17:38:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-19a964fe-hello-world-consumer in namespace infra-namespace
2022-04-09 17:38:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-19a964fe in namespace infra-namespace
2022-04-09 17:38:56 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-871486518-2131265851 in namespace infra-namespace
2022-04-09 17:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildPluginUsingMavenCoordinatesArtifacts-FINISHED
2022-04-09 17:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:40:49 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:384] KafkaConnector(spec=Spec(additionalProperties={}), status=KafkaConnectorStatus(super=Status(conditions=[Condition(status=True, reason=ConnectRestException, message=PUT /connectors/camel-http-connector/config returned 500 (Internal Server Error): Error trying to forward REST request: Failed to find any class that implements Connector and which name matches org.apache.camel.kafkaconnector.http.CamelHttpSinkConnector, available connectors are: PluginDesc{klass=class org.apache.camel.kafkaconnector.CamelSinkConnector, name='org.apache.camel.kafkaconnector.CamelSinkConnector', version='0.9.0', encodedVersion=0.9.0, type=sink, typeName='sink', location='file:/opt/kafka/plugins/connector-from-maven/'}, PluginDesc{klass=class org.apache.camel.kafkaconnector.CamelSourceConnector, name='org.apache.camel.kafkaconnector.CamelSourceConnector', version='0.9.0', encodedVersion=0.9.0, type=source, typeName='source', location='file:/opt/kafka/plugins/connector-from-maven/'}, PluginDesc{klass=class org.apache.camel.kafkaconnector.timer.CamelTimerSourceConnector, name='org.apache.camel.kafkaconnector.timer.CamelTimerSourceConnector', version='0.9.0', encodedVersion=0.9.0, type=source, typeName='source', location='file:/opt/kafka/plugins/connector-from-maven/'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSinkConnector, name='org.apache.kafka.connect.file.FileStreamSinkConnector', version='3.1.0', encodedVersion=3.1.0, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSourceConnector, name='org.apache.kafka.connect.file.FileStreamSourceConnector', version='3.1.0', encodedVersion=3.1.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector, name='org.apache.kafka.connect.mirror.MirrorCheckpointConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector, name='org.apache.kafka.connect.mirror.MirrorHeartbeatConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector, name='org.apache.kafka.connect.mirror.MirrorSourceConnector', version='1', encodedVersion=1, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockConnector, name='org.apache.kafka.connect.tools.MockConnector', version='3.1.0', encodedVersion=3.1.0, type=connector, typeName='connector', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSinkConnector, name='org.apache.kafka.connect.tools.MockSinkConnector', version='3.1.0', encodedVersion=3.1.0, type=sink, typeName='sink', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSourceConnector, name='org.apache.kafka.connect.tools.MockSourceConnector', version='3.1.0', encodedVersion=3.1.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.SchemaSourceConnector, name='org.apache.kafka.connect.tools.SchemaSourceConnector', version='3.1.0', encodedVersion=3.1.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSinkConnector, name='org.apache.kafka.connect.tools.VerifiableSinkConnector', version='3.1.0', encodedVersion=3.1.0, type=source, typeName='source', location='classpath'}, PluginDesc{klass=class org.apache.kafka.connect.tools.VerifiableSourceConnector, name='org.apache.kafka.connect.tools.VerifiableSourceConnector', version='3.1.0', encodedVersion=3.1.0, type=source, typeName='source', location='classpath'}, type=NotReady, lastTransitionTime=2022-04-09T17:37:09.813127Z, additionalProperties={})], observedGeneration=1, additionalProperties={}), connectorStatus=null, tasksMax=2, topics=[]), additionalProperties={}, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=2022-04-09T17:36:49Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=1, labels={strimzi.io/cluster=my-cluster-9df36d9d, test.case=testUpdateConnectWithAnotherPlugin}, managedFields=[], name=camel-http-connector, namespace=infra-namespace, ownerReferences=[], resourceVersion=912522, selfLink=/apis/kafka.strimzi.io/v1beta2/namespaces/infra-namespace/kafkaconnectors/camel-http-connector, uid=f1ad18e8-a10d-4744-9bd9-469f7a2a403f, additionalProperties={}), apiVersion=kafka.strimzi.io/v1beta2, kind=KafkaConnector)
io.strimzi.test.WaitException: Timeout after 240000 ms waiting for KafkaConnector: camel-http-connector will have desired state: Ready
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:435)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:428)
	at io.strimzi.systemtest.resources.ResourceManager.waitForResourceStatus(ResourceManager.java:450)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaConnectorUtils.waitForConnectorStatus(KafkaConnectorUtils.java:68)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaConnectorUtils.waitForConnectorReady(KafkaConnectorUtils.java:72)
	at io.strimzi.systemtest.resources.crd.KafkaConnectorResource.waitForReadiness(KafkaConnectorResource.java:43)
	at io.strimzi.systemtest.resources.crd.KafkaConnectorResource.waitForReadiness(KafkaConnectorResource.java:19)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$waitResourceCondition$2(ResourceManager.java:268)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:142)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.resources.ResourceManager.waitResourceCondition(ResourceManager.java:264)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:226)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:148)
	at io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin(ConnectBuilderIsolatedST.java:400)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
io.strimzi.test.WaitException: Timeout after 240000 ms waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:camel-http-connector
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.resources.ResourceManager.waitResourceCondition(ResourceManager.java:264)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:226)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:148)
	at io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin(ConnectBuilderIsolatedST.java:400)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 17:40:49 [ForkJoinPool-3-worker-11] [1;31mERROR[m [TestExecutionWatcher:28] ConnectBuilderIsolatedST - Exception Timeout after 240000 ms waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:camel-http-connector has been thrown in @Test. Going to collect logs from components.
2022-04-09 17:40:49 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-09 17:40:49 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-09 17:40:49 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-09 17:40:51 [ForkJoinPool-3-worker-11] [33mWARN [m [LogCollector:247] Searching for logs in all pods failed! Some of the logs will not be stored. Exception messagenull
2022-04-09 17:40:51 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-09 17:40:51 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-09 17:40:51 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-09 17:40:52 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-09 17:40:52 [ForkJoinPool-3-worker-11] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-09 17:40:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:40:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateConnectWithAnotherPlugin
2022-04-09 17:40:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-9df36d9d in namespace infra-namespace
2022-04-09 17:40:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9df36d9d-scraper in namespace infra-namespace
2022-04-09 17:40:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-830863609-1585290737 in namespace infra-namespace
2022-04-09 17:40:52 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector camel-http-connector in namespace infra-namespace
2022-04-09 17:40:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector echo-sink-connector in namespace infra-namespace
2022-04-09 17:41:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-9df36d9d-allow in namespace infra-namespace
2022-04-09 17:41:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:41:42 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin-FINISHED
2022-04-09 17:41:42 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:41:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:41:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for ConnectBuilderIsolatedST
2022-04-09 17:41:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka infra-namespace in namespace infra-namespace
2022-04-09 17:41:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 6, Failures: 0, Errors: 1, Skipped: 1, Time elapsed: 700.791 s <<< FAILURE! - in io.strimzi.systemtest.connect.ConnectBuilderIsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin(ExtensionContext)  Time elapsed: 519.219 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 240000 ms waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:camel-http-connector
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.resources.ResourceManager.waitResourceCondition(ResourceManager.java:264)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:226)
	at io.strimzi.systemtest.resources.ResourceManager.createResource(ResourceManager.java:148)
	at io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin(ConnectBuilderIsolatedST.java:400)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST
2022-04-09 17:41:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 17:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 17:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 17:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 17:42:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:17 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:42:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:42:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:42:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:42:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:42:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 17:43:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 17:43:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 17:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 17:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST.backupAndRestore-STARTED
2022-04-09 17:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-30e5b237 in namespace infra-namespace
2022-04-09 17:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-30e5b237 will have desired state: Ready
2022-04-09 17:44:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-30e5b237 is in desired state: Ready
2022-04-09 17:44:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-30e5b237-producer in namespace infra-namespace
2022-04-09 17:44:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-30e5b237-consumer in namespace infra-namespace
2022-04-09 17:44:31 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-30e5b237-producer will be in active state
2022-04-09 17:44:32 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-30e5b237-consumer will be in active state
2022-04-09 17:44:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-30e5b237-producer and consumer my-cluster-30e5b237-consumer finish
2022-04-09 17:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-30e5b237-kafka-0 -- /opt/kafka/bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group my-group
2022-04-09 17:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-30e5b237-producer in namespace infra-namespace
2022-04-09 17:44:51 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-30e5b237-producer will be in active state
2022-04-09 17:44:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-30e5b237-producer to finished
2022-04-09 17:44:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ColdBackupScriptIsolatedST:95] Running backup procedure for infra-namespace/my-cluster-30e5b237
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/cold-backup/run.sh backup -n infra-namespace -c my-cluster-30e5b237 -t /home/ec2-user/strimzi-kafka-operator/systemtest/target/my-cluster-30e5b237.tgz -y
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 17:47:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:11 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:21 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:47:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 17:48:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 17:48:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 17:48:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 17:48:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ColdBackupScriptIsolatedST:109] Running restore procedure for infra-namespace/my-cluster-30e5b237
2022-04-09 17:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/cold-backup/run.sh restore -n infra-namespace -c my-cluster-30e5b237 -s /home/ec2-user/strimzi-kafka-operator/systemtest/target/my-cluster-30e5b237.tgz -y
2022-04-09 17:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-30e5b237 will have desired state: Ready
2022-04-09 17:50:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-30e5b237 is in desired state: Ready
2022-04-09 17:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-30e5b237-kafka-0 -- /opt/kafka/bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group my-group
2022-04-09 17:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-09 17:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-30e5b237-consumer in namespace infra-namespace
2022-04-09 17:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-30e5b237-consumer will be in active state
2022-04-09 17:50:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-30e5b237-consumer to finished
2022-04-09 17:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-30e5b237-consumer in namespace infra-namespace
2022-04-09 17:50:30 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-30e5b237-consumer will be in active state
2022-04-09 17:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-30e5b237-consumer to finished
2022-04-09 17:50:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:50:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for backupAndRestore
2022-04-09 17:50:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-30e5b237-producer in namespace infra-namespace
2022-04-09 17:50:42 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-30e5b237-producer in namespace infra-namespace
2022-04-09 17:50:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-30e5b237-consumer in namespace infra-namespace
2022-04-09 17:50:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-30e5b237 in namespace infra-namespace
2022-04-09 17:50:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-30e5b237-consumer in namespace infra-namespace
2022-04-09 17:50:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-30e5b237-consumer in namespace infra-namespace
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST.backupAndRestore-FINISHED
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ColdBackupScriptIsolatedST is everything deleted.
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 539.835 s - in io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-09 17:50:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:51:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-09 17:51:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-09 17:51:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-09 17:51:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:51:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-09 17:51:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:17 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:51:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:51:17 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:27 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:27 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@12fd93f1
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-09 17:51:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-09 17:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-09 17:52:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-09 17:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:52:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testRestoreOffsetsInConsumerGroup-STARTED
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:52:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2-STARTED
2022-04-09 17:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS-STARTED
2022-04-09 17:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2ToZero-STARTED
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKafkaMirrorMaker2ReflectsConnectorsState-STARTED
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-120 for test case:testRestoreOffsetsInConsumerGroup
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-121 for test case:testKafkaMirrorMaker2ReflectsConnectorsState
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4f9d5e95-source in namespace namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4f9d5e95-target in namespace namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4f9d5e95-source will have desired state: Ready
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-122 for test case:testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-122
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8f6b1684-source in namespace namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8f6b1684-target in namespace namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-121
2022-04-09 17:52:15 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8f6b1684-source will have desired state: Ready
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-122
2022-04-09 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-122
2022-04-09 17:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:52:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-123 for test case:testScaleMirrorMaker2ToZero
2022-04-09 17:52:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-123
2022-04-09 17:52:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-66351af2-source in namespace namespace-122
2022-04-09 17:52:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:52:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-66351af2-source will have desired state: Ready
2022-04-09 17:52:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-123
2022-04-09 17:52:16 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-123
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-124 for test case:testMirrorMaker2
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-124
2022-04-09 17:52:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-967ef532-source in namespace namespace-123
2022-04-09 17:52:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-09 17:52:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-967ef532-source will have desired state: Ready
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-124
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-124
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b1ecb454-source in namespace namespace-124
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-09 17:52:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b1ecb454-source will have desired state: Ready
2022-04-09 17:54:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8f6b1684-source is in desired state: Ready
2022-04-09 17:54:29 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8f6b1684-target will have desired state: Ready
2022-04-09 17:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8f6b1684-target is in desired state: Ready
2022-04-09 17:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-8f6b1684 in namespace namespace-124
2022-04-09 17:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-121
2022-04-09 17:54:38 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:481] Wait for KafkaMirrorMaker2: my-cluster-8f6b1684 will contain desired status message: One or more connectors are in FAILED state
2022-04-09 17:54:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4f9d5e95-source is in desired state: Ready
2022-04-09 17:54:39 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4f9d5e95-target will have desired state: Ready
2022-04-09 17:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b1ecb454-source is in desired state: Ready
2022-04-09 17:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b1ecb454-target in namespace namespace-124
2022-04-09 17:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-09 17:54:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b1ecb454-target will have desired state: Ready
2022-04-09 17:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-967ef532-source is in desired state: Ready
2022-04-09 17:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-967ef532-target in namespace namespace-124
2022-04-09 17:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-09 17:54:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-967ef532-target will have desired state: Ready
2022-04-09 17:55:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-66351af2-source is in desired state: Ready
2022-04-09 17:55:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-66351af2-target in namespace namespace-124
2022-04-09 17:55:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:55:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-66351af2-target will have desired state: Ready
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4f9d5e95-target is in desired state: Ready
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-4f9d5e95-trg-src in namespace namespace-124
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-4f9d5e95-src-trg in namespace namespace-124
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic test-sync-offset-108266502 in namespace namespace-124
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:55:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-4f9d5e95-trg-src will have desired state: Ready
2022-04-09 17:55:46 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:492] KafkaMirrorMaker2: my-cluster-8f6b1684 contains desired message in status: One or more connectors are in FAILED state
2022-04-09 17:55:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-8f6b1684 will have desired state: Ready
2022-04-09 17:55:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-8f6b1684 is in desired state: Ready
2022-04-09 17:55:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:55:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMaker2ReflectsConnectorsState
2022-04-09 17:55:49 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8f6b1684-target in namespace namespace-121
2022-04-09 17:55:59 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-8f6b1684 in namespace namespace-121
2022-04-09 17:56:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-967ef532-target is in desired state: Ready
2022-04-09 17:56:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-967ef532 in namespace namespace-124
2022-04-09 17:56:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-09 17:56:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-967ef532 will have desired state: Ready
2022-04-09 17:56:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b1ecb454-target is in desired state: Ready
2022-04-09 17:56:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-1124840095 in namespace namespace-124
2022-04-09 17:56:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-09 17:56:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-1124840095 will have desired state: Ready
2022-04-09 17:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-1124840095 is in desired state: Ready
2022-04-09 17:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b1ecb454-kafka-clients in namespace namespace-124
2022-04-09 17:56:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-09 17:56:09 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8f6b1684-source in namespace namespace-121
2022-04-09 17:56:17 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:155] Sending messages to - topic availability-topic-source-my-topic-1672952629-1761608139, cluster my-cluster-b1ecb454-source and message count of 100
2022-04-09 17:56:17 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@486886b8, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092, --topic, availability-topic-source-my-topic-1672952629-1761608139], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092', topicName='availability-topic-source-my-topic-1672952629-1761608139', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@298b14eb}
2022-04-09 17:56:17 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092:availability-topic-source-my-topic-1672952629-1761608139 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:56:17 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092 --topic availability-topic-source-my-topic-1672952629-1761608139
2022-04-09 17:56:19 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:56:19 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-121 for test case:testKafkaMirrorMaker2ReflectsConnectorsState
2022-04-09 17:56:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:56:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testStrimziIdentityReplicationPolicy-STARTED
2022-04-09 17:56:20 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:56:20 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2Subresource-STARTED
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-4f9d5e95-trg-src is in desired state: Ready
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-4f9d5e95-src-trg will have desired state: Ready
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-4f9d5e95-src-trg is in desired state: Ready
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: test-sync-offset-108266502 will have desired state: Ready
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: test-sync-offset-108266502 is in desired state: Ready
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1090] Send & receive 100 messages to/from Source cluster.
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-producer-source-my-consumer-group-794449888 in namespace namespace-120
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-source-my-consumer-group-2046547067 in namespace namespace-120
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:56:24 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-producer-source-my-consumer-group-794449888 will be in active state
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-125 for test case:testStrimziIdentityReplicationPolicy
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-125
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-125
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-125
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-41a57801-source in namespace namespace-125
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-09 17:56:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-41a57801-source will have desired state: Ready
2022-04-09 17:56:25 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-source-my-consumer-group-2046547067 will be in active state
2022-04-09 17:56:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-producer-source-my-consumer-group-794449888 to finished
2022-04-09 17:56:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 17:56:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 17:56:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@71dbe049, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1374456775, --group-instance-id, instance282980532, --bootstrap-server, my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092, --topic, availability-topic-source-my-topic-1672952629-1761608139], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092', topicName='availability-topic-source-my-topic-1672952629-1761608139', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1374456775', consumerInstanceId='instance282980532', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@680ee}
2022-04-09 17:56:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092#availability-topic-source-my-topic-1672952629-1761608139 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:56:26 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1374456775 --group-instance-id instance282980532 --bootstrap-server my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092 --topic availability-topic-source-my-topic-1672952629-1761608139
2022-04-09 17:56:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKafkaMirrorMaker2ReflectsConnectorsState-FINISHED
2022-04-09 17:56:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:56:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:56:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testIdentityReplicationPolicy-STARTED
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-126 for test case:testScaleMirrorMaker2Subresource
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-126
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-126
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-126
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dbcf4db2-source in namespace namespace-126
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-09 17:56:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dbcf4db2-source will have desired state: Ready
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:160] Setting topic to availability-topic-target-my-topic-1672952629-1761608139, cluster to my-cluster-b1ecb454-target and changing consumer group
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:168] Sending messages to - topic availability-topic-target-my-topic-1672952629-1761608139, cluster my-cluster-b1ecb454-target and message count of 100
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5b0ee14d, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092, --topic, availability-topic-target-my-topic-1672952629-1761608139], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092', topicName='availability-topic-target-my-topic-1672952629-1761608139', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7d8ae58c}
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092:availability-topic-target-my-topic-1672952629-1761608139 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:56:34 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092 --topic availability-topic-target-my-topic-1672952629-1761608139
2022-04-09 17:56:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-66351af2-target is in desired state: Ready
2022-04-09 17:56:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-a-1844227453 in namespace namespace-126
2022-04-09 17:56:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-a-1844227453 will have desired state: Ready
2022-04-09 17:56:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-source-my-consumer-group-2046547067 to finished
2022-04-09 17:56:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-a-1844227453 is in desired state: Ready
2022-04-09 17:56:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-b-225469898 in namespace namespace-126
2022-04-09 17:56:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-b-225469898 will have desired state: Ready
2022-04-09 17:56:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 17:56:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 17:56:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@41cc6734, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-391108341, --group-instance-id, instance632106366, --bootstrap-server, my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092, --topic, availability-topic-target-my-topic-1672952629-1761608139], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092', topicName='availability-topic-target-my-topic-1672952629-1761608139', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-391108341', consumerInstanceId='instance632106366', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6f3b0b32}
2022-04-09 17:56:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092#availability-topic-target-my-topic-1672952629-1761608139 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:56:38 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-391108341 --group-instance-id instance632106366 --bootstrap-server my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092 --topic availability-topic-target-my-topic-1672952629-1761608139
2022-04-09 17:56:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-b-225469898 is in desired state: Ready
2022-04-09 17:56:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-66351af2-my-user-source in namespace namespace-126
2022-04-09 17:56:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-66351af2-my-user-source will have desired state: Ready
2022-04-09 17:56:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-66351af2-my-user-source is in desired state: Ready
2022-04-09 17:56:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-66351af2-my-user-target in namespace namespace-126
2022-04-09 17:56:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-66351af2-my-user-target will have desired state: Ready
2022-04-09 17:56:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-66351af2-my-user-target is in desired state: Ready
2022-04-09 17:56:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d1e78fc1-kafka-clients in namespace namespace-122
2022-04-09 17:56:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:42 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1098] Send 100 messages to Source cluster.
2022-04-09 17:56:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-producer-source-my-consumer-group-794449888 in namespace namespace-120
2022-04-09 17:56:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:56:42 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-producer-source-my-consumer-group-794449888 will be in active state
2022-04-09 17:56:43 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-producer-source-my-consumer-group-794449888 to finished
2022-04-09 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-b1ecb454 in namespace namespace-126
2022-04-09 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-09 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-b1ecb454 will have desired state: Ready
2022-04-09 17:56:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-131922402-1456384605-test-1 in namespace namespace-126
2022-04-09 17:56:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:50 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-131922402-1456384605-test-1 will have desired state: Ready
2022-04-09 17:56:51 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1105] Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it
2022-04-09 17:56:51 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1107] Receive 100 messages from mirrored topic on Target cluster.
2022-04-09 17:56:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-target-my-consumer-group-744730349 in namespace namespace-120
2022-04-09 17:56:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:56:51 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-target-my-consumer-group-744730349 will be in active state
2022-04-09 17:56:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-target-my-consumer-group-744730349 to finished
2022-04-09 17:56:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-131922402-1456384605-test-1 is in desired state: Ready
2022-04-09 17:56:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-131922402-1456384605-test-2 in namespace namespace-126
2022-04-09 17:56:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:56:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-131922402-1456384605-test-2 will have desired state: Ready
2022-04-09 17:56:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-131922402-1456384605-test-2 is in desired state: Ready
2022-04-09 17:56:52 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 17:56:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@590e3850, messages=[], arguments=[--max-messages, 200, USER=my_cluster_66351af2_my_user_target, --bootstrap-server, my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093, --topic, my-topic-131922402-1456384605-test-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc', podNamespace='namespace-122', bootstrapServer='my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093', topicName='my-topic-131922402-1456384605-test-2', maxMessages=200, kafkaUsername='my-cluster-66351af2-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f6234d7}
2022-04-09 17:56:52 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093:my-topic-131922402-1456384605-test-2 from pod my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc
2022-04-09 17:56:52 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc -n namespace-122 -- /opt/kafka/producer.sh --max-messages 200 USER=my_cluster_66351af2_my_user_target --bootstrap-server my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093 --topic my-topic-131922402-1456384605-test-2
2022-04-09 17:56:57 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 17:56:57 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-09 17:56:57 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2db91118, messages=[], arguments=[--max-messages, 200, --group-id, my-consumer-group-1723541351, --group-instance-id, instance969440791, USER=my_cluster_66351af2_my_user_target, --bootstrap-server, my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093, --topic, my-topic-131922402-1456384605-test-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc', podNamespace='namespace-122', bootstrapServer='my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093', topicName='my-topic-131922402-1456384605-test-2', maxMessages=200, kafkaUsername='my-cluster-66351af2-my-user-target', consumerGroupName='my-consumer-group-1723541351', consumerInstanceId='instance969440791', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4736980b}
2022-04-09 17:56:57 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093:my-topic-131922402-1456384605-test-2 from pod my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc
2022-04-09 17:56:57 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc -n namespace-122 -- /opt/kafka/consumer.sh --max-messages 200 --group-id my-consumer-group-1723541351 --group-instance-id instance969440791 USER=my_cluster_66351af2_my_user_target --bootstrap-server my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093 --topic my-topic-131922402-1456384605-test-2
2022-04-09 17:57:04 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1112] Send 50 messages to Source cluster
2022-04-09 17:57:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-producer-source-my-consumer-group-794449888 in namespace namespace-120
2022-04-09 17:57:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:57:04 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-producer-source-my-consumer-group-794449888 will be in active state
2022-04-09 17:57:05 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-producer-source-my-consumer-group-794449888 to finished
2022-04-09 17:57:06 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 17:57:06 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-09 17:57:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-66351af2 in namespace namespace-126
2022-04-09 17:57:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-09 17:57:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-66351af2 will have desired state: Ready
2022-04-09 17:57:15 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1118] Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it
2022-04-09 17:57:15 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1119] Receive 10 msgs from source cluster
2022-04-09 17:57:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-source-my-consumer-group-2046547067 in namespace namespace-120
2022-04-09 17:57:15 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:57:15 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-source-my-consumer-group-2046547067 will be in active state
2022-04-09 17:57:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-source-my-consumer-group-2046547067 to finished
2022-04-09 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-967ef532 is in desired state: Ready
2022-04-09 17:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:781] Scaling MirrorMaker2 to zero
2022-04-09 17:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleMirrorMaker2ToZero
2022-04-09 17:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-967ef532-target in namespace namespace-123
2022-04-09 17:57:33 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1125] Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it
2022-04-09 17:57:33 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1127] Receive 40 msgs from mirrored topic on Target cluster
2022-04-09 17:57:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-target-my-consumer-group-744730349 in namespace namespace-120
2022-04-09 17:57:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:57:33 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-target-my-consumer-group-744730349 will be in active state
2022-04-09 17:57:34 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-target-my-consumer-group-744730349 to finished
2022-04-09 17:57:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-967ef532 in namespace namespace-123
2022-04-09 17:57:44 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1133] There should be no more messages to read. Try to consume at least 1 message. This client job should fail on timeout.
2022-04-09 17:57:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-target-my-consumer-group-744730349 in namespace namespace-120
2022-04-09 17:57:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:57:44 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-target-my-consumer-group-744730349 will be in active state
2022-04-09 17:57:45 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:mm2-consumer-target-my-consumer-group-744730349 to finish with failure.
2022-04-09 17:57:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-41a57801-source is in desired state: Ready
2022-04-09 17:57:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-41a57801-target in namespace namespace-126
2022-04-09 17:57:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-09 17:57:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-41a57801-target will have desired state: Ready
2022-04-09 17:57:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-967ef532-source in namespace namespace-123
2022-04-09 17:57:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dbcf4db2-source is in desired state: Ready
2022-04-09 17:57:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dbcf4db2-target in namespace namespace-126
2022-04-09 17:57:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-09 17:57:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dbcf4db2-target will have desired state: Ready
2022-04-09 17:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-09 17:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-123 for test case:testScaleMirrorMaker2ToZero
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-b1ecb454 is in desired state: Ready
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:183] Looks like the mirrormaker2 cluster my-cluster deployed OK
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:640] Verifying docker image names
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:653] Docker images verified
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:311] Verifying labels on pod type mirrormaker2
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:315] Verifying labels for pod: my-cluster-b1ecb454-mirrormaker2-6489dbcc7d-w55mr
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:362] Verifying labels for service my-cluster-b1ecb454-mirrormaker2-api
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:384] Verifying labels for Config maps
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-mirrormaker2-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-source-entity-topic-operator-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:407] CM my-cluster-b1ecb454-source-entity-topic-operator-config is not related to current test
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-source-entity-user-operator-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:407] CM my-cluster-b1ecb454-source-entity-user-operator-config is not related to current test
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-source-kafka-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-source-zookeeper-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:407] CM my-cluster-b1ecb454-source-zookeeper-config is not related to current test
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-target-entity-topic-operator-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:407] CM my-cluster-b1ecb454-target-entity-topic-operator-config is not related to current test
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-target-entity-user-operator-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:407] CM my-cluster-b1ecb454-target-entity-user-operator-config is not related to current test
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-target-kafka-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-b1ecb454-target-zookeeper-config
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:407] CM my-cluster-b1ecb454-target-zookeeper-config is not related to current test
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:414] Verifying labels for Service Accounts
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-b1ecb454-source-entity-operator
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-b1ecb454-source-kafka
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-b1ecb454-source-zookeeper
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:198] Setting topic to mirrormaker2-topic-example-1124840095, cluster to my-cluster-b1ecb454-source and changing consumer group
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:206] Sending messages to - topic mirrormaker2-topic-example-1124840095, cluster my-cluster-b1ecb454-source and message count of 100
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7548c054, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092, --topic, mirrormaker2-topic-example-1124840095], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092', topicName='mirrormaker2-topic-example-1124840095', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7ba198b4}
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092:mirrormaker2-topic-example-1124840095 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:58:02 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092 --topic mirrormaker2-topic-example-1124840095
2022-04-09 17:58:05 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-09 17:58:05 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-09 17:58:05 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:210] Consumer in source cluster and topic should receive 100 messages
2022-04-09 17:58:05 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@288b7ace, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-328153488, --group-instance-id, instance2023705797, --bootstrap-server, my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092, --topic, mirrormaker2-topic-example-1124840095], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092', topicName='mirrormaker2-topic-example-1124840095', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-328153488', consumerInstanceId='instance2023705797', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5f983db1}
2022-04-09 17:58:05 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092#mirrormaker2-topic-example-1124840095 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:58:05 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-328153488 --group-instance-id instance2023705797 --bootstrap-server my-cluster-b1ecb454-source-kafka-bootstrap.namespace-124.svc:9092 --topic mirrormaker2-topic-example-1124840095
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:214] Now setting topic to my-cluster-b1ecb454-source.mirrormaker2-topic-example-1124840095 and cluster to my-cluster-b1ecb454-target - the messages should be mirrored
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:222] Consumer in target cluster and topic should receive 100 messages
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6babee9, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-1816018776, --group-instance-id, instance89305754, --bootstrap-server, my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092, --topic, my-cluster-b1ecb454-source.mirrormaker2-topic-example-1124840095], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092', topicName='my-cluster-b1ecb454-source.mirrormaker2-topic-example-1124840095', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1816018776', consumerInstanceId='instance89305754', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1bf245eb}
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092#my-cluster-b1ecb454-source.mirrormaker2-topic-example-1124840095 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-1816018776 --group-instance-id instance89305754 --bootstrap-server my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092 --topic my-cluster-b1ecb454-source.mirrormaker2-topic-example-1124840095
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:227] Changing topic to my-cluster-b1ecb454-source.availability-topic-source-my-topic-1672952629-1761608139
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:233] Check if mm2 mirror automatically created topic
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6b7f2ae4, messages=[], arguments=[--max-messages, 100, --group-id, my-consumer-group-902777450, --group-instance-id, instance960685387, --bootstrap-server, my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092, --topic, my-cluster-b1ecb454-source.availability-topic-source-my-topic-1672952629-1761608139], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54', podNamespace='namespace-124', bootstrapServer='my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092', topicName='my-cluster-b1ecb454-source.availability-topic-source-my-topic-1672952629-1761608139', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-902777450', consumerInstanceId='instance960685387', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@18d17e3a}
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092#my-cluster-b1ecb454-source.availability-topic-source-my-topic-1672952629-1761608139 from pod my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54
2022-04-09 17:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1ecb454-kafka-clients-8fbf67cc5-shd54 -n namespace-124 -- /opt/kafka/consumer.sh --max-messages 100 --group-id my-consumer-group-902777450 --group-instance-id instance960685387 --bootstrap-server my-cluster-b1ecb454-target-kafka-bootstrap.namespace-124.svc:9092 --topic my-cluster-b1ecb454-source.availability-topic-source-my-topic-1672952629-1761608139
2022-04-09 17:58:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-66351af2 is in desired state: Ready
2022-04-09 17:58:18 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1a0e2e88, messages=[], arguments=[--max-messages, 200, USER=my_cluster_66351af2_my_user_source, --bootstrap-server, my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093, --topic, mirrormaker2-topic-example-a-1844227453], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc', podNamespace='namespace-122', bootstrapServer='my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093', topicName='mirrormaker2-topic-example-a-1844227453', maxMessages=200, kafkaUsername='my-cluster-66351af2-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@60ad700d}
2022-04-09 17:58:18 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093:mirrormaker2-topic-example-a-1844227453 from pod my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc
2022-04-09 17:58:18 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc -n namespace-122 -- /opt/kafka/producer.sh --max-messages 200 USER=my_cluster_66351af2_my_user_source --bootstrap-server my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093 --topic mirrormaker2-topic-example-a-1844227453
2022-04-09 17:58:23 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-09 17:58:23 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-09 17:58:23 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@358a1bee, messages=[], arguments=[--max-messages, 200, --group-id, my-consumer-group-1723541351, --group-instance-id, instance1697342889, USER=my_cluster_66351af2_my_user_source, --bootstrap-server, my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093, --topic, mirrormaker2-topic-example-a-1844227453], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc', podNamespace='namespace-122', bootstrapServer='my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093', topicName='mirrormaker2-topic-example-a-1844227453', maxMessages=200, kafkaUsername='my-cluster-66351af2-my-user-source', consumerGroupName='my-consumer-group-1723541351', consumerInstanceId='instance1697342889', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4cf227d5}
2022-04-09 17:58:23 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093:mirrormaker2-topic-example-a-1844227453 from pod my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc
2022-04-09 17:58:23 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc -n namespace-122 -- /opt/kafka/consumer.sh --max-messages 200 --group-id my-consumer-group-1723541351 --group-instance-id instance1697342889 USER=my_cluster_66351af2_my_user_source --bootstrap-server my-cluster-66351af2-source-kafka-bootstrap.namespace-122.svc:9093 --topic mirrormaker2-topic-example-a-1844227453
2022-04-09 17:58:25 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-09 17:58:25 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-09 17:58:25 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:236] Mirrored successful
2022-04-09 17:58:25 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-cluster-b1ecb454-source.mirrormaker2-topic-example-1124840095
2022-04-09 17:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2ToZero-FINISHED
2022-04-09 17:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-09 17:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-09 17:58:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndScramSha512Auth-STARTED
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-127 for test case:testIdentityReplicationPolicy
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-127
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-127
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-127
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7685ac98-source in namespace namespace-127
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-09 17:58:26 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7685ac98-source will have desired state: Ready
2022-04-09 17:58:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 17:58:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-09 17:58:31 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:1551] Consumer in target cluster and topic should receive 200 messages
2022-04-09 17:58:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@553d3b79, messages=[], arguments=[--max-messages, 200, --group-id, my-consumer-group-1723541351, --group-instance-id, instance769735084, USER=my_cluster_66351af2_my_user_target, --bootstrap-server, my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093, --topic, my-cluster-66351af2-source.mirrormaker2-topic-example-a-1844227453], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc', podNamespace='namespace-122', bootstrapServer='my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093', topicName='my-cluster-66351af2-source.mirrormaker2-topic-example-a-1844227453', maxMessages=200, kafkaUsername='my-cluster-66351af2-my-user-target', consumerGroupName='my-consumer-group-1723541351', consumerInstanceId='instance769735084', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3eac384a}
2022-04-09 17:58:31 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093:my-cluster-66351af2-source.mirrormaker2-topic-example-a-1844227453 from pod my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc
2022-04-09 17:58:31 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d1e78fc1-kafka-clients-d894479d5-kfzzc -n namespace-122 -- /opt/kafka/consumer.sh --max-messages 200 --group-id my-consumer-group-1723541351 --group-instance-id instance769735084 USER=my_cluster_66351af2_my_user_target --bootstrap-server my-cluster-66351af2-target-kafka-bootstrap.namespace-122.svc:9093 --topic my-cluster-66351af2-source.mirrormaker2-topic-example-a-1844227453
2022-04-09 17:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-09 17:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-09 17:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:1553] Messages successfully mirrored
2022-04-09 17:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [MirrorMaker2IsolatedST:1567] Renew Clients CA secret for Source cluster via annotation
2022-04-09 17:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:178] Annotating Secret:my-cluster-66351af2-source-clients-ca-cert with annotation strimzi.io/force-renew=true
2022-04-09 17:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-66351af2-source-kafka rolling update
2022-04-09 17:58:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-41a57801-target is in desired state: Ready
2022-04-09 17:58:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-cluster-41a57801 in namespace namespace-127
2022-04-09 17:58:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-09 17:58:59 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-cluster-41a57801 will have desired state: Ready
2022-04-09 17:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dbcf4db2-target is in desired state: Ready
2022-04-09 17:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-dbcf4db2 in namespace namespace-127
2022-04-09 17:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-09 17:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-dbcf4db2 will have desired state: Ready
2022-04-09 17:59:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-cluster-41a57801 is in desired state: Ready
2022-04-09 17:59:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-41a57801-kafka-clients in namespace namespace-125
2022-04-09 17:59:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-09 17:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-09 17:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2
2022-04-09 17:59:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-1124840095 in namespace namespace-124
2022-04-09 17:59:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-41a57801 in namespace namespace-127
2022-04-09 17:59:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-09 17:59:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-41a57801 will have desired state: Ready
2022-04-09 17:59:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7685ac98-source is in desired state: Ready
2022-04-09 17:59:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7685ac98-target in namespace namespace-127
2022-04-09 17:59:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-09 17:59:32 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7685ac98-target will have desired state: Ready
2022-04-09 17:59:46 [ForkJoinPool-3-worker-11] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 121000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.lambda$testRestoreOffsetsInConsumerGroup$13(MirrorMaker2IsolatedST.java:1137)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testRestoreOffsetsInConsumerGroup(MirrorMaker2IsolatedST.java:1137)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-09 17:59:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:100] Client job 'mm2-consumer-target-my-consumer-group-744730349' finished with expected timeout.
2022-04-09 17:59:46 [ForkJoinPool-3-worker-11] [32mINFO [m [MirrorMaker2IsolatedST:1139] As it's Active-Active MM2 mode, there should be no more messages to read from Source cluster topic. This client job should fail on timeout.
2022-04-09 17:59:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-source-my-consumer-group-2046547067 in namespace namespace-120
2022-04-09 17:59:46 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-09 17:59:46 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-source-my-consumer-group-2046547067 will be in active state
2022-04-09 17:59:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:mm2-consumer-source-my-consumer-group-2046547067 to finish with failure.
2022-04-09 18:00:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-dbcf4db2 is in desired state: Ready
2022-04-09 18:00:06 [ForkJoinPool-3-worker-5] [32mINFO [m [MirrorMaker2IsolatedST:675] -------> Scaling KafkaMirrorMaker2 subresource <-------
2022-04-09 18:00:06 [ForkJoinPool-3-worker-5] [32mINFO [m [MirrorMaker2IsolatedST:676] Scaling subresource replicas to 4
2022-04-09 18:00:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-dbcf4db2-mirrormaker2 will be ready
2022-04-09 18:00:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-dbcf4db2-mirrormaker2 is ready
2022-04-09 18:00:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-dbcf4db2-mirrormaker2 to be ready
2022-04-09 18:00:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-66351af2-source-kafka has been successfully rolled
2022-04-09 18:00:15 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-66351af2-source-kafka to be ready
2022-04-09 18:00:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-b1ecb454 in namespace namespace-124
2022-04-09 18:00:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b1ecb454-kafka-clients in namespace namespace-124
2022-04-09 18:00:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-41a57801 is in desired state: Ready
2022-04-09 18:00:39 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMaker2IsolatedST:902] Sending and receiving messages via my-cluster-41a57801-source
2022-04-09 18:00:39 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-09 18:00:39 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@388d12f5, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-41a57801-source-kafka-bootstrap.namespace-125.svc:9092, --topic, my-cluster-41a57801], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-41a57801-kafka-clients-856d676688-glg95', podNamespace='namespace-125', bootstrapServer='my-cluster-41a57801-source-kafka-bootstrap.namespace-125.svc:9092', topicName='my-cluster-41a57801', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@665a81b2}
2022-04-09 18:00:39 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-41a57801-source-kafka-bootstrap.namespace-125.svc:9092:my-cluster-41a57801 from pod my-cluster-41a57801-kafka-clients-856d676688-glg95
2022-04-09 18:00:39 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-41a57801-kafka-clients-856d676688-glg95 -n namespace-125 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-41a57801-source-kafka-bootstrap.namespace-125.svc:9092 --topic my-cluster-41a57801
2022-04-09 18:01:05 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:199] CLI_KAFKA_VERIFIABLE_PRODUCER RETURN code: 1
2022-04-09 18:01:06 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:201] ======STDOUT START=======
2022-04-09 18:01:06 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:202] /tmp/.properties
Starting Producer with configuration:


2022-04-09 18:01:06 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:203] ======STDOUT END======
2022-04-09 18:01:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: false
2022-04-09 18:01:06 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:101] Producer produced -1 messages
2022-04-09 18:01:23 [ForkJoinPool-3-worker-7] [1;31mERROR[m [TestExecutionWatcher:28] MirrorMaker2IsolatedST - Exception Operation: [list]  for kind: [Pod]  with name: [null]  in namespace: [namespace-122]  failed. has been thrown in @Test. Going to collect logs from components.
