[[1;34mINFO[m] Scanning for projects...
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Build Order:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[[1;34mINFO[m] test                                                               [jar]
[[1;34mINFO[m] crd-annotations                                                    [jar]
[[1;34mINFO[m] crd-generator                                                      [jar]
[[1;34mINFO[m] api                                                                [jar]
[[1;34mINFO[m] mockkube                                                           [jar]
[[1;34mINFO[m] config-model                                                       [jar]
[[1;34mINFO[m] certificate-manager                                                [jar]
[[1;34mINFO[m] operator-common                                                    [jar]
[[1;34mINFO[m] systemtest                                                         [jar]
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------------< [0;36mio.strimzi:strimzi[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10][m
[[1;34mINFO[m] [1m--------------------------------[ pom ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mstrimzi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mstrimzi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mstrimzi[0;1m ---[m
[[1;34mINFO[m] Skipping pom project
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--------------------------< [0;36mio.strimzi:test[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding test 0.29.0-SNAPSHOT                                     [2/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/test/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No sources to compile
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/test/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No tests to run.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:crd-annotations[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding crd-annotations 0.29.0-SNAPSHOT                          [3/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-annotations[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-annotations[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:crd-generator[0;1m >----------------------[m
[[1;34mINFO[m] [1mBuilding crd-generator 0.29.0-SNAPSHOT                            [4/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/crd-generator/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 7 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcrd-generator[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-shade-plugin:3.1.0:shade[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6.1 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[[1;34mINFO[m] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[[1;34mINFO[m] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[[1;34mINFO[m] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[[1;34mINFO[m] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[[1;34mINFO[m] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[[1;34mINFO[m] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] Discovered module-info.class. Shading will break its strong encapsulation.
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, generex-1.0.2.jar define 7 overlapping classes: 
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator
[[1;33mWARNING[m]   - com.mifmif.common.regex.Generex
[[1;33mWARNING[m]   - com.mifmif.common.regex.GenerexIterator$Step
[[1;33mWARNING[m]   - com.mifmif.common.regex.Node
[[1;33mWARNING[m]   - com.mifmif.common.regex.Main
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterable
[[1;33mWARNING[m]   - com.mifmif.common.regex.util.Iterator
[[1;33mWARNING[m] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[[1;33mWARNING[m]   - 70 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonInclude
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonIgnore
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSetter
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[[1;33mWARNING[m]   - com.fasterxml.jackson.annotation.JsonSubTypes
[[1;33mWARNING[m]   - 61 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[[1;33mWARNING[m]   - 254 more...
[[1;33mWARNING[m] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] kubernetes-model-storageclass-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 172 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[[1;33mWARNING[m]   - 162 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[[1;33mWARNING[m]   - 102 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-apiextensions-5.12.0.jar define 350 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[[1;33mWARNING[m]   - 340 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-discovery-5.12.0.jar define 88 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 78 more...
[[1;33mWARNING[m] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.WebSocket
[[1;33mWARNING[m]   - okhttp3.Cookie$Builder
[[1;33mWARNING[m]   - okhttp3.internal.http.HttpHeaders
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[[1;33mWARNING[m]   - okhttp3.internal.tls.OkHostnameVerifier
[[1;33mWARNING[m]   - okhttp3.Cache$Entry
[[1;33mWARNING[m]   - okhttp3.internal.http2.Http2Connection$3
[[1;33mWARNING[m]   - okhttp3.internal.ws.RealWebSocket$Streams
[[1;33mWARNING[m]   - okhttp3.CacheControl$Builder
[[1;33mWARNING[m]   - 198 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-metrics-5.12.0.jar define 30 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[[1;33mWARNING[m]   - 20 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-flowcontrol-5.12.0.jar define 132 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[[1;33mWARNING[m]   - 122 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-events-5.12.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] automaton-1.11-8.jar, crd-generator-0.29.0-SNAPSHOT.jar define 25 overlapping classes: 
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonMatcher
[[1;33mWARNING[m]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$Kind
[[1;33mWARNING[m]   - dk.brics.automaton.RunAutomaton
[[1;33mWARNING[m]   - dk.brics.automaton.Automaton
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp
[[1;33mWARNING[m]   - dk.brics.automaton.AutomatonProvider
[[1;33mWARNING[m]   - dk.brics.automaton.RegExp$1
[[1;33mWARNING[m]   - dk.brics.automaton.MinimizationOperations$StateListNode
[[1;33mWARNING[m]   - dk.brics.automaton.State
[[1;33mWARNING[m]   - 15 more...
[[1;33mWARNING[m] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.json.JsonReadFeature
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.Separators
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.TreeNode
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.sym.Name
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.RequestPayload
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[[1;33mWARNING[m]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[[1;33mWARNING[m]   - 114 more...
[[1;33mWARNING[m] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[[1;33mWARNING[m]   - 224 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-coordination-5.12.0.jar define 18 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[[1;33mWARNING[m]   - 8 more...
[[1;33mWARNING[m] zjsonpatch-0.3.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Operation
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.guava.Strings
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.Diff
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[[1;33mWARNING[m]   - io.fabric8.zjsonpatch.JsonPatch
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Plural
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Group
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Singular
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[[1;33mWARNING[m]   - io.fabric8.kubernetes.model.annotation.Version
[[1;33mWARNING[m]   - 6 more...
[[1;33mWARNING[m] kubernetes-model-admissionregistration-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 362 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[[1;33mWARNING[m]   - 352 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[[1;33mWARNING[m]   - okio.ByteString
[[1;33mWARNING[m]   - okio.Source
[[1;33mWARNING[m]   - okio.ForwardingSink
[[1;33mWARNING[m]   - okio.BufferedSource
[[1;33mWARNING[m]   - okio.Util
[[1;33mWARNING[m]   - okio.AsyncTimeout$1
[[1;33mWARNING[m]   - okio.HashingSource
[[1;33mWARNING[m]   - okio.GzipSink
[[1;33mWARNING[m]   - okio.Okio$1
[[1;33mWARNING[m]   - okio.Pipe$PipeSink
[[1;33mWARNING[m]   - 34 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-certificates-5.12.0.jar define 60 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[[1;33mWARNING[m]   - 50 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, jackson-datatype-jsr310-2.13.1.jar define 59 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[[1;33mWARNING[m]   - 49 more...
[[1;33mWARNING[m] crd-annotations-0.29.0-SNAPSHOT.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$Stability
[[1;33mWARNING[m]   - io.strimzi.api.annotations.ApiVersion$1
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedType
[[1;33mWARNING[m]   - io.strimzi.api.annotations.DeprecatedProperty
[[1;33mWARNING[m]   - io.strimzi.api.annotations.VersionRange$VersionParser
[[1;33mWARNING[m]   - io.strimzi.api.annotations.KubeVersion
[[1;33mWARNING[m] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[[1;33mWARNING[m]   - 202 more...
[[1;33mWARNING[m] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$Factory
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Level
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor
[[1;33mWARNING[m]   - okhttp3.logging.package-info
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener
[[1;33mWARNING[m]   - okhttp3.logging.LoggingEventListener$1
[[1;33mWARNING[m]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[[1;33mWARNING[m] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[[1;33mWARNING[m]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[[1;33mWARNING[m]   - 7 more...
[[1;33mWARNING[m] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.StatusBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[[1;33mWARNING[m]   - 2384 more...
[[1;33mWARNING[m] slf4j-api-1.7.36.jar, crd-generator-0.29.0-SNAPSHOT.jar define 34 overlapping classes: 
[[1;33mWARNING[m]   - org.slf4j.helpers.SubstituteLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.NamedLoggerBase
[[1;33mWARNING[m]   - org.slf4j.helpers.NOPMDCAdapter
[[1;33mWARNING[m]   - org.slf4j.MarkerFactory
[[1;33mWARNING[m]   - org.slf4j.helpers.BasicMarker
[[1;33mWARNING[m]   - org.slf4j.spi.LoggerFactoryBinder
[[1;33mWARNING[m]   - org.slf4j.MDC$MDCCloseable
[[1;33mWARNING[m]   - org.slf4j.spi.LocationAwareLogger
[[1;33mWARNING[m]   - org.slf4j.helpers.MessageFormatter
[[1;33mWARNING[m]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[[1;33mWARNING[m]   - 24 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-node-5.12.0.jar define 78 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[[1;33mWARNING[m]   - 68 more...
[[1;33mWARNING[m] jackson-databind-2.12.6.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 700 overlapping classes: 
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.BeanDescription
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.SerializerProvider
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[[1;33mWARNING[m]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[[1;33mWARNING[m]   - 690 more...
[[1;33mWARNING[m] crd-generator-0.29.0-SNAPSHOT.jar, snakeyaml-1.27.jar define 216 overlapping classes: 
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[[1;33mWARNING[m]   - org.yaml.snakeyaml.Yaml$3
[[1;33mWARNING[m]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[[1;33mWARNING[m]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[[1;33mWARNING[m]   - org.yaml.snakeyaml.util.ArrayUtils
[[1;33mWARNING[m]   - org.yaml.snakeyaml.tokens.Token$ID
[[1;33mWARNING[m]   - org.yaml.snakeyaml.reader.StreamReader
[[1;33mWARNING[m]   - 206 more...
[[1;33mWARNING[m] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.CertUtils
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.CustomResource
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.VersionInfo$1
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[[1;33mWARNING[m]   - io.fabric8.kubernetes.client.dsl.Containerable
[[1;33mWARNING[m]   - 526 more...
[[1;33mWARNING[m] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[[1;33mWARNING[m]   - 14 more...
[[1;33mWARNING[m] kubernetes-model-policy-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 162 overlapping classes: 
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[[1;33mWARNING[m]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[[1;33mWARNING[m]   - 152 more...
[[1;33mWARNING[m] maven-shade-plugin has detected that some class files are
[[1;33mWARNING[m] present in two or more JARs. When this happens, only one
[[1;33mWARNING[m] single version of the class is copied to the uber jar.
[[1;33mWARNING[m] Usually this is not harmful and you can skip these warnings,
[[1;33mWARNING[m] otherwise try to manually exclude artifacts based on
[[1;33mWARNING[m] mvn dependency:tree -Ddetail=true and the above output.
[[1;33mWARNING[m] See http://maven.apache.org/plugins/maven-shade-plugin/
[[1;34mINFO[m] Replacing original artifact with shaded artifact.
[[1;34mINFO[m] Replacing /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/ec2-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcrd-generator[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------------< [0;36mio.strimzi:api[0;1m >---------------------------[m
[[1;34mINFO[m] [1mBuilding api 0.29.0-SNAPSHOT                                      [5/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/api/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1)[m @ [36mapi[0;1m ---[m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-crd-co-install-v1-eo)[m @ [36mapi[0;1m ---[m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mexec-maven-plugin:1.6.0:exec[m [1m(generate-doc)[m @ [36mapi[0;1m ---[m
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 99 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-test-compile)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mapi[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/api/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mapi[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mapi[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m------------------------< [0;36mio.strimzi:mockkube[0;1m >-------------------------[m
[[1;34mINFO[m] [1mBuilding mockkube 0.29.0-SNAPSHOT                                 [6/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/mockkube/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mmockkube[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mmockkube[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mmockkube[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m----------------------< [0;36mio.strimzi:config-model[0;1m >-----------------------[m
[[1;34mINFO[m] [1mBuilding config-model 0.29.0-SNAPSHOT                             [7/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/config-model/src/test/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mconfig-model[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mconfig-model[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mconfig-model[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-------------------< [0;36mio.strimzi:certificate-manager[0;1m >-------------------[m
[[1;34mINFO[m] [1mBuilding certificate-manager 0.29.0-SNAPSHOT                      [8/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 1 resource
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36mcertificate-manager[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36mcertificate-manager[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m---------------------< [0;36mio.strimzi:operator-common[0;1m >---------------------[m
[[1;34mINFO[m] [1mBuilding operator-common 0.29.0-SNAPSHOT                          [9/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] skip non existing resourceDirectory /home/ec2-user/strimzi-kafka-operator/operator-common/src/main/resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 9 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36moperator-common[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36moperator-common[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Building jar: /home/ec2-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:test-jar[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36moperator-common[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m-----------------------< [0;36mio.strimzi:systemtest[0;1m >------------------------[m
[[1;34mINFO[m] [1mBuilding systemtest 0.29.0-SNAPSHOT                              [10/10][m
[[1;34mINFO[m] [1m--------------------------------[ jar ]---------------------------------[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:resources[m [1m(default-resources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 3 resources
[[1;34mINFO[m] Copying 2 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:compile[m [1m(default-compile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-resources-plugin:2.6:testResources[m [1m(default-testResources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Using 'UTF-8' encoding to copy filtered resources.
[[1;34mINFO[m] Copying 32 resources
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-compiler-plugin:3.8.1:testCompile[m [1m(default-testCompile)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Nothing to compile - all classes are up to date
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-surefire-plugin:3.0.0-M5:test[m [1m(default-test)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m]  T E S T S
[[1;34mINFO[m] -------------------------------------------------------
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;34mINFO[m] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[[1;34mINFO[m] 
[[1;33mWARNING[m] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/ec2-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-04-06T14-54-18_601-jvmRun1.dumpstream
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-jar-plugin:3.1.0:jar[m [1m(default-jar)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:report[m [1m(report)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping JaCoCo execution due to missing execution data file.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m>>> [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m > [0;1mgenerate-sources[m @ [36msystemtest[0;1m >>>[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-checkstyle-plugin:3.1.2:check[m [1m(validate)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Starting audit...
Audit done.
[[1;34mINFO[m] You have 0 Checkstyle violations.
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-enforcer-plugin:3.0.0-M2:enforce[m [1m(enforce-banned-dependencies)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mjacoco-maven-plugin:0.7.9:prepare-agent[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] surefireArgLine set to -javaagent:/home/ec2-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/ec2-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m<<< [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[0;1m < [0;1mgenerate-sources[m @ [36msystemtest[0;1m <<<[m
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-source-plugin:3.0.1:jar[m [1m(attach-sources)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-javadoc-plugin:3.1.0:jar[m [1m(attach-javadocs)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] Skipping javadoc generation
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:integration-test[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;33mWARNING[m] useSystemClassLoader setting has no effect when not forking
[[1;33mWARNING[m] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.tracing.TracingST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.ListenersST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.listeners.MultipleListenersST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.KafkaST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.kafka.ConfigProviderST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.custom.CustomAuthorizerST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.OpaIntegrationST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.SecurityST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.log.LoggingChangeST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.log.LogSettingST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.FeatureGatesIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.RecoveryIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.HelmChartIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.SpecificIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.specific.DrainCleanerIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectBuilderIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.JmxIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.dump.LogDumpScriptIsolatedST
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-04-06 14:54:36 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.user.UserST
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:219] Used environment variables:
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:220] CONFIG: /home/ec2-user/strimzi-kafka-operator/systemtest/config.json
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SKIP_TEARDOWN: false
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] LB_FINALIZERS: false
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_ORG: strimzi
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_LOG_DIR: /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_REGISTRY: quay.io
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DOCKER_TAG: latest
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_SOURCE_NAME: community-operators
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_FEATURE_GATES: 
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] BRIDGE_IMAGE: latest-released
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Environment:221] OLM_OPERATOR_VERSION: 
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeCluster:87] Using cluster: minikube
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:60] Cluster default namespace is 'default'
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 14:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 14:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 14:54:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 14:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 14:55:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 14:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 14:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: user-st
2022-04-06 14:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: user-st
2022-04-06 14:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-04-06 14:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-04-06 14:55:13 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-04-06 14:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-04-06 14:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-04-06 14:56:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:56:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:56:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-04-06 14:56:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-04-06 14:56:31 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:56:31 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:56:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-631655660-1795017639 in namespace user-st
2022-04-06 14:56:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-319877673-1649871621 in namespace user-st
2022-04-06 14:56:31 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-04-06 14:56:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-631655660-1795017639 will have desired state: Ready
2022-04-06 14:56:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-319877673-1649871621 will have desired state: Ready
2022-04-06 14:56:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-631655660-1795017639 is in desired state: Ready
2022-04-06 14:56:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-319877673-1649871621 is in desired state: Ready
2022-04-06 14:56:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:56:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testUserTemplate
2022-04-06 14:56:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-319877673-1649871621 in namespace user-st
2022-04-06 14:56:35 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-631655660-1795017639
2022-04-06 14:56:35 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 14:56:35 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-631655660-1795017639
2022-04-06 14:56:35 [ForkJoinPool-3-worker-1] [33mWARN [m [KafkaUserUtils:68] KafkaUser my-user-631655660-1795017639 is not deleted yet! Triggering force delete by cmd client!
2022-04-06 14:56:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-631655660-1795017639 deleted
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-631655660-1795017639
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-631655660-1795017639 in namespace user-st
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-614273319-982769242 in namespace user-st
2022-04-06 14:56:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-614273319-982769242 will have desired state: Ready
2022-04-06 14:56:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-614273319-982769242 is in desired state: Ready
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-614273319-982769242
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:50] Secret my-user-614273319-982769242 created
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-614273319-982769242 will have desired state: Ready
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-614273319-982769242 is in desired state: Ready
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-614273319-982769242
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUserUtils:75] KafkaUser my-user-614273319-982769242 deleted
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateUser
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-614273319-982769242 in namespace user-st
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testTlsExternalUser
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-0
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-0
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ee9b7729 in namespace namespace-0
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-04-06 14:56:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ee9b7729 will have desired state: Ready
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-04-06 14:56:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-04-06 14:56:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-04-06 14:56:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-04-06 14:56:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-04-06 14:56:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-04-06 14:56:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-04-06 14:56:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-04-06 14:56:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-04-06 14:56:44 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-04-06 14:56:45 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-04-06 14:56:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:56:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-04-06 14:56:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-04-06 14:56:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-04-06 14:57:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-04-06 14:57:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-04-06 14:57:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-04-06 14:57:19 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-04-06 14:57:19 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 14:57:19 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-04-06 14:57:19 [ForkJoinPool-3-worker-5] [33mWARN [m [KafkaUserUtils:68] KafkaUser encrypted-arnost is not deleted yet! Triggering force delete by cmd client!
2022-04-06 14:57:20 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-04-06 14:57:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-04-06 14:57:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-04-06 14:57:27 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-04-06 14:57:27 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 14:57:27 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-04-06 14:57:27 [ForkJoinPool-3-worker-5] [33mWARN [m [KafkaUserUtils:68] KafkaUser scramed-arnost is not deleted yet! Triggering force delete by cmd client!
2022-04-06 14:57:28 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-04-06 14:57:30 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-1
2022-04-06 14:57:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-1
2022-04-06 14:57:31 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-04-06 14:57:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1f5b74aa in namespace namespace-1
2022-04-06 14:57:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-06 14:57:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1f5b74aa will have desired state: Ready
2022-04-06 14:57:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ee9b7729 is in desired state: Ready
2022-04-06 14:57:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1389461525-700747540 in namespace namespace-0
2022-04-06 14:57:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-0
2022-04-06 14:57:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1389461525-700747540 will have desired state: Ready
2022-04-06 14:57:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1389461525-700747540 is in desired state: Ready
2022-04-06 14:57:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1389461525-700747540 will have desired state: Ready
2022-04-06 14:57:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1389461525-700747540 is in desired state: Ready
2022-04-06 14:57:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:57:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-04-06 14:57:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1389461525-700747540 in namespace namespace-0
2022-04-06 14:58:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ee9b7729 in namespace namespace-0
2022-04-06 14:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 14:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testTlsExternalUser
2022-04-06 14:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1f5b74aa is in desired state: Ready
2022-04-06 14:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-957925818-1939145325 in namespace namespace-1
2022-04-06 14:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-06 14:58:53 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-04-06 14:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-957925818-1939145325 will have desired state: Ready
2022-04-06 14:58:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-04-06 14:58:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 14:58:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-957925818-1939145325 is in desired state: Ready
2022-04-06 14:58:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-1
2022-04-06 14:58:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-06 14:58:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-04-06 14:58:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-04-06 14:58:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-1
2022-04-06 14:58:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-06 14:58:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-04-06 14:58:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-04-06 14:58:56 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:346] Deploying KafkaClients pod for TLS listener
2022-04-06 14:58:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1f5b74aa-tls-kafka-clients in namespace namespace-1
2022-04-06 14:58:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-06 14:58:56 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1f5b74aa-tls-kafka-clients will be ready
2022-04-06 14:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1f5b74aa-tls-kafka-clients is ready
2022-04-06 14:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-04-06 14:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1f5b74aa-plain-kafka-clients in namespace namespace-1
2022-04-06 14:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-1
2022-04-06 14:58:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1f5b74aa-plain-kafka-clients will be ready
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1f5b74aa-plain-kafka-clients is ready
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:357] Checking if user secrets with secret prefixes exists
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:373] Checking if TLS user is able to send messages
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@35767142, messages=[], arguments=[USER=top_secret_encrypted_leopold, --max-messages, 100, --bootstrap-server, my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1f5b74aa-tls-kafka-clients-7899fb9b86-tmgqq', podNamespace='namespace-1', bootstrapServer='my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@76b074b4}
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093:my-topic-2043299187-274155238 from pod my-cluster-1f5b74aa-tls-kafka-clients-7899fb9b86-tmgqq
2022-04-06 14:59:00 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1f5b74aa-tls-kafka-clients-7899fb9b86-tmgqq -n namespace-1 -- /opt/kafka/producer.sh USER=top_secret_encrypted_leopold --max-messages 100 --bootstrap-server my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093 --topic my-topic-2043299187-274155238
2022-04-06 14:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 14:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 14:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@69592a5f, messages=[], arguments=[--group-id, my-consumer-group-1601886726, USER=top_secret_encrypted_leopold, --max-messages, 100, --group-instance-id, instance1718617913, --bootstrap-server, my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1f5b74aa-tls-kafka-clients-7899fb9b86-tmgqq', podNamespace='namespace-1', bootstrapServer='my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-1601886726', consumerInstanceId='instance1718617913', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@82d5120}
2022-04-06 14:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093:my-topic-2043299187-274155238 from pod my-cluster-1f5b74aa-tls-kafka-clients-7899fb9b86-tmgqq
2022-04-06 14:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1f5b74aa-tls-kafka-clients-7899fb9b86-tmgqq -n namespace-1 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1601886726 USER=top_secret_encrypted_leopold --max-messages 100 --group-instance-id instance1718617913 --bootstrap-server my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9093 --topic my-topic-2043299187-274155238
2022-04-06 14:59:11 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 14:59:11 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 14:59:11 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-04-06 14:59:11 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4940018, messages=[], arguments=[USER=top_secret_scramed_leopold, --max-messages, 100, --bootstrap-server, my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1f5b74aa-plain-kafka-clients-56787f456b-p7c6k', podNamespace='namespace-1', bootstrapServer='my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@73aa332d}
2022-04-06 14:59:11 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092:my-topic-2043299187-274155238 from pod my-cluster-1f5b74aa-plain-kafka-clients-56787f456b-p7c6k
2022-04-06 14:59:11 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1f5b74aa-plain-kafka-clients-56787f456b-p7c6k -n namespace-1 -- /opt/kafka/producer.sh USER=top_secret_scramed_leopold --max-messages 100 --bootstrap-server my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092 --topic my-topic-2043299187-274155238
2022-04-06 14:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 14:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 14:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@46f930ac, messages=[], arguments=[--group-id, my-consumer-group-1601886726, USER=top_secret_scramed_leopold, --max-messages, 100, --group-instance-id, instance1185964791, --bootstrap-server, my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1f5b74aa-plain-kafka-clients-56787f456b-p7c6k', podNamespace='namespace-1', bootstrapServer='my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-1601886726', consumerInstanceId='instance1185964791', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66d2e5fb}
2022-04-06 14:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092#my-topic-2043299187-274155238 from pod my-cluster-1f5b74aa-plain-kafka-clients-56787f456b-p7c6k
2022-04-06 14:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1f5b74aa-plain-kafka-clients-56787f456b-p7c6k -n namespace-1 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1601886726 USER=top_secret_scramed_leopold --max-messages 100 --group-instance-id instance1185964791 --bootstrap-server my-cluster-1f5b74aa-kafka-bootstrap.namespace-1.svc:9092 --topic my-topic-2043299187-274155238
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:398] Deleting KafkaUser:scramed-leopold
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [UserST:402] Checking if secrets are deleted
2022-04-06 14:59:41 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-1
2022-04-06 14:59:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-957925818-1939145325 in namespace namespace-1
2022-04-06 14:59:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1f5b74aa-plain-kafka-clients in namespace namespace-1
2022-04-06 14:59:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-1
2022-04-06 14:59:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1f5b74aa in namespace namespace-1
2022-04-06 15:00:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1f5b74aa-tls-kafka-clients in namespace namespace-1
2022-04-06 15:00:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:00:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-04-06 15:00:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-04-06 15:00:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:00:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:00:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for UserST
2022-04-06 15:00:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-04-06 15:01:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 12, Failures: 0, Errors: 0, Skipped: 4, Time elapsed: 402.517 s - in io.strimzi.systemtest.operators.user.UserST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-04-06 15:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-04-06 15:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-04-06 15:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-04-06 15:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-04-06 15:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-04-06 15:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-04-06 15:02:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-04-06 15:02:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:02:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:02:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-04-06 15:02:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-04-06 15:02:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:02:33 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:02:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1202231114-1024194342 in namespace throttling-quota-st
2022-04-06 15:02:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-693505564-2083061960 in namespace throttling-quota-st
2022-04-06 15:02:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1202231114-1024194342 will have desired state: Ready
2022-04-06 15:02:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-693505564-2083061960 will have desired state: Ready
2022-04-06 15:02:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1202231114-1024194342 is in desired state: Ready
2022-04-06 15:02:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-693505564-2083061960 is in desired state: Ready
2022-04-06 15:02:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-16af9a5f-kafka-clients in namespace throttling-quota-st
2022-04-06 15:02:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-e8f1004a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:02:34 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-e8f1004a-kafka-clients will be in active state
2022-04-06 15:02:34 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-16af9a5f-kafka-clients will be in active state
2022-04-06 15:02:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:02:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:06:47 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-16af9a5f-kafka-clients-9cqht log
2022-04-06 15:06:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-e8f1004a-kafka-clients-fw4f4 log
2022-04-06 15:06:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-16af9a5f-kafka-clients in namespace throttling-quota-st
2022-04-06 15:06:53 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-16af9a5f-kafka-clients will be in active state
2022-04-06 15:06:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:06:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-04-06 15:06:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:06:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-04-06 15:06:54 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:06:58 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:06:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1952730571-1634856937 in namespace throttling-quota-st
2022-04-06 15:06:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1952730571-1634856937 will have desired state: Ready
2022-04-06 15:06:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1952730571-1634856937 is in desired state: Ready
2022-04-06 15:06:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:06:59 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-dda7ab8a-kafka-clients will be in active state
2022-04-06 15:06:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:07:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:07:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-04-06 15:07:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-e8f1004a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:07:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1202231114-1024194342 in namespace throttling-quota-st
2022-04-06 15:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-04-06 15:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:07:28 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:07:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-181218024-901789720 in namespace throttling-quota-st
2022-04-06 15:07:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-181218024-901789720 will have desired state: Ready
2022-04-06 15:07:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-181218024-901789720 is in desired state: Ready
2022-04-06 15:07:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-04-06 15:07:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:07:29 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:07:30 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:07:55 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-16af9a5f-kafka-clients-pgdzq log
2022-04-06 15:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job alter-admin-my-cluster-16af9a5f-kafka-clients in namespace throttling-quota-st
2022-04-06 15:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: alter-admin-my-cluster-16af9a5f-kafka-clients will be in active state
2022-04-06 15:08:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:08:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-dda7ab8a-kafka-clients-7pwbt log
2022-04-06 15:08:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job list-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:08:45 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: list-admin-my-cluster-dda7ab8a-kafka-clients will be in active state
2022-04-06 15:08:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:08:48 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-dda7ab8a-kafka-clients-cfzfm log
2022-04-06 15:08:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:08:53 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-dda7ab8a-kafka-clients will be in active state
2022-04-06 15:08:54 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:09:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:09:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-81ad29b4-kafka-clients-wbrq6 log
2022-04-06 15:09:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-04-06 15:09:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:09:05 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:09:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:10:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-dda7ab8a-kafka-clients-98slq log
2022-04-06 15:10:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job list-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:10:29 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: list-admin-my-cluster-dda7ab8a-kafka-clients will be in active state
2022-04-06 15:10:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-dda7ab8a-kafka-clients to finished
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job list-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job list-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-dda7ab8a-kafka-clients in namespace throttling-quota-st
2022-04-06 15:10:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1952730571-1634856937 in namespace throttling-quota-st
2022-04-06 15:10:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:10:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-81ad29b4-kafka-clients-ltdt7 log
2022-04-06 15:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-04-06 15:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:10:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-04-06 15:10:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:10:46 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:10:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:12:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-16af9a5f-kafka-clients-gv97z log
2022-04-06 15:12:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-04-06 15:12:13 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-04-06 15:12:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-04-06 15:12:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:12:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-81ad29b4-kafka-clients-hrhz5 log
2022-04-06 15:12:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-04-06 15:12:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:12:26 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:12:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-16af9a5f-kafka-clients in namespace throttling-quota-st
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job alter-admin-my-cluster-16af9a5f-kafka-clients in namespace throttling-quota-st
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-16af9a5f-kafka-clients in namespace throttling-quota-st
2022-04-06 15:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-693505564-2083061960 in namespace throttling-quota-st
2022-04-06 15:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-04-06 15:13:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:14:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:14:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-81ad29b4-kafka-clients-9srlx log
2022-04-06 15:14:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-04-06 15:14:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:14:06 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: create-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:14:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:15:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:15:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message All topics created found in create-admin-my-cluster-81ad29b4-kafka-clients-shq7j log
2022-04-06 15:15:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:15:46 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:15:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-06 15:19:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-81ad29b4-kafka-clients-4csf6 log
2022-04-06 15:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-81ad29b4-kafka-clients.
2022-04-06 15:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:19:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:21:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-81ad29b4-kafka-clients.
2022-04-06 15:21:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:21:01 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:21:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:22:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-81ad29b4-kafka-clients.
2022-04-06 15:22:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:22:09 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:22:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:23:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-81ad29b4-kafka-clients.
2022-04-06 15:23:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:23:18 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:23:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:24:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-81ad29b4-kafka-clients.
2022-04-06 15:24:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:24:26 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: delete-admin-my-cluster-81ad29b4-kafka-clients will be in active state
2022-04-06 15:24:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-81ad29b4-kafka-clients to finished
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job create-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job delete-admin-my-cluster-81ad29b4-kafka-clients in namespace throttling-quota-st
2022-04-06 15:25:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-181218024-901789720 in namespace throttling-quota-st
2022-04-06 15:25:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:25:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-04-06 15:25:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:25:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ThrottlingQuotaST:353] Tearing down resources after all test
2022-04-06 15:26:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:26:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-04-06 15:26:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-04-06 15:26:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,505.918 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.topic.TopicST
2022-04-06 15:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: topic-st
2022-04-06 15:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: topic-st
2022-04-06 15:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-04-06 15:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:494] Deploying shared Kafka across all test cases in topic-st namespace
2022-04-06 15:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-04-06 15:26:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-04-06 15:27:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-04-06 15:27:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:27:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-04-06 15:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-04-06 15:27:34 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-04-06 15:27:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-553458593-1088005523 in namespace topic-st
2022-04-06 15:27:34 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-553458593-1088005523 will have desired state: Ready
2022-04-06 15:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-04-06 15:27:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-553458593-1088005523 is in desired state: Ready
2022-04-06 15:27:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-553458593-1088005523 will have desired state: NotReady
2022-04-06 15:27:35 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-04-06 15:27:35 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 15:27:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:323] Checking if my-topic-1526156832-481323830 is on topic list
2022-04-06 15:27:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:459] Checking topic my-topic-1526156832-481323830 in Kafka
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-553458593-1088005523 is in desired state: NotReady
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-553458593-1088005523 deletion
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-553458593-1088005523 in namespace topic-st
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-150103242-1747350219 in namespace topic-st
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic my-topic-150103242-1747350219 exists
2022-04-06 15:27:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:459] Checking topic my-topic-150103242-1747350219 in Kafka
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:326] Topic with name my-topic-1526156832-481323830 is not created yet
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:328] Trying to send messages to non-existing topic my-topic-1526156832-481323830
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@74a096d3, messages=[], arguments=[--max-messages, 100, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1526156832-481323830], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-7dd7cb68d4-98wwn', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1526156832-481323830', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3543dbf8}
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-1526156832-481323830 from pod topic-cluster-name-kafka-clients-7dd7cb68d4-98wwn
2022-04-06 15:27:38 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec topic-cluster-name-kafka-clients-7dd7cb68d4-98wwn -n topic-st -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1526156832-481323830
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-150103242-1747350219 will have desired state: NotReady
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-150103242-1747350219 is in desired state: NotReady
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:91] Delete topic my-topic-150103242-1747350219
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-150103242-1747350219 deletion
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-04-06 15:27:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-04-06 15:27:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-04-06 15:27:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:459] Checking topic topic-example-new in Kafka
2022-04-06 15:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 15:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 15:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@42bca807, messages=[], arguments=[--group-id, my-consumer-group-44913428, --max-messages, 100, --group-instance-id, instance384589369, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-1526156832-481323830], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-7dd7cb68d4-98wwn', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1526156832-481323830', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-44913428', consumerInstanceId='instance384589369', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b7fd282}
2022-04-06 15:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-1526156832-481323830 from pod topic-cluster-name-kafka-clients-7dd7cb68d4-98wwn
2022-04-06 15:27:41 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec topic-cluster-name-kafka-clients-7dd7cb68d4-98wwn -n topic-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-44913428 --max-messages 100 --group-instance-id instance384589369 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-1526156832-481323830
2022-04-06 15:27:43 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:27:43 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:27:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic topic-example-new exists
2022-04-06 15:27:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:27:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-04-06 15:27:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-04-06 15:27:46 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 15:27:46 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 15:27:46 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:344] Checking if my-topic-1526156832-481323830 is on topic list
2022-04-06 15:27:46 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:459] Checking topic my-topic-1526156832-481323830 in Kafka
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1526156832-481323830 creation 
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [TopicST:356] Topic successfully created
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-04-06 15:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-150103242-1747350219 in namespace topic-st
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-04-06 15:27:53 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-853130483-507696535 --replication-factor 3 --partitions 3
2022-04-06 15:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:27:56 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-853130483-507696535 creation 
2022-04-06 15:27:57 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:485] Checking in KafkaTopic CR that topic my-topic-853130483-507696535 was created with expected settings
2022-04-06 15:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:27:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:122] Editing topic via Kafka, settings to partitions 5
2022-04-06 15:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-853130483-507696535 --partitions 5
2022-04-06 15:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:28:02 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-853130483-507696535
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-853130483-507696535
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:473] Checking topic my-topic-853130483-507696535 in Kafka topic-cluster-name
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-04-06 15:28:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-04-06 15:28:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-04-06 15:28:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-04-06 15:28:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-04-06 15:28:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-04-06 15:28:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-04-06 15:28:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-04-06 15:28:08 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:459] Checking topic topic-with-replication-to-change in Kafka
2022-04-06 15:28:11 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:28:11 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:28:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-04-06 15:28:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:459] Checking topic another-topic in Kafka
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [TopicST:464] Checking in KafkaTopic CR that topic another-topic exists
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-04-06 15:28:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:28:14 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-04-06 15:28:14 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:28:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:28:29 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-04-06 15:28:29 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-04-06 15:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-36951beb-isolated in namespace topic-st
2022-04-06 15:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-36951beb-isolated will have desired state: Ready
2022-04-06 15:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-36951beb-isolated is in desired state: Ready
2022-04-06 15:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-36951beb-isolated-kafka-clients in namespace topic-st
2022-04-06 15:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-36951beb-isolated-kafka-clients will be ready
2022-04-06 15:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-36951beb-isolated-kafka-clients is ready
2022-04-06 15:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-522493285-491208294 in namespace topic-st
2022-04-06 15:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-522493285-491208294 will have desired state: Ready
2022-04-06 15:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-522493285-491208294 is in desired state: Ready
2022-04-06 15:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 15:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1cd1f4a2, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-522493285-491208294], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-36951beb-isolated-kafka-clients-66659bb4f8-cl2hc', podNamespace='topic-st', bootstrapServer='my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-522493285-491208294', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@323decea}
2022-04-06 15:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-522493285-491208294 from pod my-cluster-36951beb-isolated-kafka-clients-66659bb4f8-cl2hc
2022-04-06 15:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-36951beb-isolated-kafka-clients-66659bb4f8-cl2hc -n topic-st -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-522493285-491208294
2022-04-06 15:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 15:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 15:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:398] Deleting KafkaTopic: my-topic-522493285-491208294
2022-04-06 15:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:400] KafkaTopic my-topic-522493285-491208294 deleted
2022-04-06 15:31:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:404] Wait KafkaTopic my-topic-522493285-491208294 recreation
2022-04-06 15:31:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-522493285-491208294 creation 
2022-04-06 15:31:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TopicST:406] KafkaTopic my-topic-522493285-491208294 recreated
2022-04-06 15:31:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@676e1f90, messages=[], arguments=[--group-id, my-consumer-group-442603606, --max-messages, 100, --group-instance-id, instance346354342, --bootstrap-server, my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092, --topic, my-topic-522493285-491208294], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-36951beb-isolated-kafka-clients-66659bb4f8-cl2hc', podNamespace='topic-st', bootstrapServer='my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-522493285-491208294', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-442603606', consumerInstanceId='instance346354342', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@74898147}
2022-04-06 15:31:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-522493285-491208294 from pod my-cluster-36951beb-isolated-kafka-clients-66659bb4f8-cl2hc
2022-04-06 15:31:29 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-36951beb-isolated-kafka-clients-66659bb4f8-cl2hc -n topic-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-442603606 --max-messages 100 --group-instance-id instance346354342 --bootstrap-server my-cluster-36951beb-isolated-kafka-bootstrap.topic-st.svc:9092 --topic my-topic-522493285-491208294
2022-04-06 15:31:35 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 15:31:35 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 15:31:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:31:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-04-06 15:31:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-36951beb-isolated-kafka-clients in namespace topic-st
2022-04-06 15:31:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-36951beb-isolated in namespace topic-st
2022-04-06 15:31:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-522493285-491208294 in namespace topic-st
2022-04-06 15:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-04-06 15:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for TopicST
2022-04-06 15:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-04-06 15:32:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 387.237 s - in io.strimzi.systemtest.operators.topic.TopicST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.ReconciliationST
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-2
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-2
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-3
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e80fb2b0 in namespace namespace-2
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-04-06 15:32:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e80fb2b0 will have desired state: Ready
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-3
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-96db5894 in namespace namespace-3
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-06 15:32:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-96db5894 will have desired state: Ready
2022-04-06 15:35:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-96db5894 is in desired state: Ready
2022-04-06 15:35:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-04-06 15:35:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-04-06 15:35:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-96db5894 will have desired state: ReconciliationPaused
2022-04-06 15:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-96db5894 is in desired state: ReconciliationPaused
2022-04-06 15:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-96db5894-kafka will have stable 3 replicas
2022-04-06 15:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-06 15:35:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-06 15:35:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-06 15:35:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-06 15:35:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-06 15:35:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-06 15:35:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-06 15:35:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-06 15:35:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-06 15:35:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-06 15:35:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-06 15:35:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-06 15:35:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-06 15:35:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-06 15:35:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-06 15:35:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-06 15:35:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-06 15:35:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-06 15:35:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e80fb2b0 is in desired state: Ready
2022-04-06 15:35:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2023067134-1552335380 in namespace namespace-3
2022-04-06 15:35:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-04-06 15:35:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2023067134-1552335380 will have desired state: Ready
2022-04-06 15:35:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-06 15:35:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2023067134-1552335380 is in desired state: Ready
2022-04-06 15:35:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-04-06 15:35:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2023067134-1552335380 will have desired state: ReconciliationPaused
2022-04-06 15:35:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-06 15:35:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:228] Pod my-cluster-96db5894-kafka has 3 replicas
2022-04-06 15:35:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-04-06 15:35:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-96db5894-kafka to be ready
2022-04-06 15:35:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2023067134-1552335380 is in desired state: ReconciliationPaused
2022-04-06 15:35:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:35:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-04-06 15:35:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:35:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:35:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-04-06 15:35:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:35:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:35:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-04-06 15:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:35:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-04-06 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-04-06 15:36:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-04-06 15:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-04-06 15:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-04-06 15:36:14 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:14 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-04-06 15:36:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-04-06 15:36:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-04-06 15:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-04-06 15:36:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-04-06 15:36:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-04-06 15:36:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-04-06 15:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-04-06 15:36:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:54 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-04-06 15:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-04-06 15:37:04 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:37:04 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:37:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-2 exec my-cluster-e80fb2b0-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-2023067134-1552335380 --describe --bootstrap-server my-cluster-e80fb2b0-kafka-bootstrap:9092
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-2023067134-1552335380
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-e80fb2b0 in namespace namespace-3
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-2
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-04-06 15:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-e80fb2b0 will have desired state: PendingProposal
2022-04-06 15:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-e80fb2b0 is in desired state: PendingProposal
2022-04-06 15:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:164] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-04-06 15:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-e80fb2b0 will have desired state: ProposalReady
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-96db5894 will have desired state: Ready
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-96db5894 is in desired state: Ready
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-96db5894 is ready
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-96db5894-kafka-clients in namespace namespace-3
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-06 15:37:40 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-96db5894-kafka-clients will be ready
2022-04-06 15:37:42 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-96db5894-kafka-clients is ready
2022-04-06 15:37:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-96db5894-scraper in namespace namespace-3
2022-04-06 15:37:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-06 15:37:42 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-96db5894-scraper will be ready
2022-04-06 15:37:51 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-96db5894-scraper is ready
2022-04-06 15:37:51 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-96db5894-scraper to be ready
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-96db5894-scraper is ready
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-96db5894-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-96db5894-allow in namespace namespace-3
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-96db5894 in namespace namespace-3
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-04-06 15:38:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-96db5894 will have desired state: ReconciliationPaused
2022-04-06 15:38:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-96db5894 is in desired state: ReconciliationPaused
2022-04-06 15:38:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-96db5894-connect will have stable 0 replicas
2022-04-06 15:38:02 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-06 15:38:03 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-06 15:38:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-06 15:38:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-06 15:38:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-06 15:38:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-06 15:38:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-06 15:38:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-06 15:38:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-06 15:38:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-06 15:38:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-06 15:38:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-06 15:38:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-06 15:38:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-06 15:38:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-06 15:38:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-06 15:38:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-06 15:38:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-06 15:38:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-06 15:38:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-06 15:38:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:228] Pod my-cluster-96db5894-connect has 0 replicas
2022-04-06 15:38:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-04-06 15:38:21 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-96db5894-connect will be ready
2022-04-06 15:39:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-96db5894-connect is ready
2022-04-06 15:39:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-96db5894-connect to be ready
2022-04-06 15:39:37 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-96db5894-connect is ready
2022-04-06 15:39:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-96db5894 in namespace namespace-3
2022-04-06 15:39:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-3
2022-04-06 15:39:37 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-04-06 15:39:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-96db5894 will have desired state: Ready
2022-04-06 15:39:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-96db5894 is in desired state: Ready
2022-04-06 15:39:39 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:39 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-04-06 15:39:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-96db5894 will have desired state: ReconciliationPaused
2022-04-06 15:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-96db5894 is in desired state: ReconciliationPaused
2022-04-06 15:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-04-06 15:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-04-06 15:39:42 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:42 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:42 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-04-06 15:39:44 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:44 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:44 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-04-06 15:39:45 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:45 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:45 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-04-06 15:39:46 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:46 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:46 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-04-06 15:39:47 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:47 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:47 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-04-06 15:39:48 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:48 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:48 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-04-06 15:39:50 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:50 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:50 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-04-06 15:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-04-06 15:39:52 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:52 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:52 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-04-06 15:39:53 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:53 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:53 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-04-06 15:39:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:54 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-04-06 15:39:56 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:56 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:56 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-04-06 15:39:57 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:57 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:57 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-04-06 15:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-04-06 15:39:59 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:39:59 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:39:59 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-04-06 15:40:01 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:40:01 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:40:01 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-04-06 15:40:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:40:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:40:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894/config
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-3 exec my-cluster-96db5894-connect-9dcf447-xkqhw -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-96db5894/config
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-96db5894-allow in namespace namespace-3
2022-04-06 15:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-96db5894 in namespace namespace-3
2022-04-06 15:40:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-96db5894 in namespace namespace-3
2022-04-06 15:40:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-96db5894-kafka-clients in namespace namespace-3
2022-04-06 15:41:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-96db5894-scraper in namespace namespace-3
2022-04-06 15:41:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-e80fb2b0 is in desired state: ProposalReady
2022-04-06 15:41:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-e80fb2b0 will have desired state: ReconciliationPaused
2022-04-06 15:41:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-e80fb2b0 is in desired state: ReconciliationPaused
2022-04-06 15:41:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): Annotating KafkaRebalance:my-cluster-e80fb2b0 with annotation approve
2022-04-06 15:41:29 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 19 polls
2022-04-06 15:41:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 18 polls
2022-04-06 15:41:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 17 polls
2022-04-06 15:41:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 16 polls
2022-04-06 15:41:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 15 polls
2022-04-06 15:41:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 14 polls
2022-04-06 15:41:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 13 polls
2022-04-06 15:41:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 12 polls
2022-04-06 15:41:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 11 polls
2022-04-06 15:41:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 10 polls
2022-04-06 15:41:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 9 polls
2022-04-06 15:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 8 polls
2022-04-06 15:41:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 7 polls
2022-04-06 15:41:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 6 polls
2022-04-06 15:41:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 5 polls
2022-04-06 15:41:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-96db5894 in namespace namespace-3
2022-04-06 15:41:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 4 polls
2022-04-06 15:41:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 3 polls
2022-04-06 15:41:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 2 polls
2022-04-06 15:41:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status gonna be stable in 1 polls
2022-04-06 15:41:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): KafkaRebalance status is stable for 20 polls intervals
2022-04-06 15:41:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ReconciliationST:178] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-04-06 15:41:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-e80fb2b0 will have desired state: ProposalReady
2022-04-06 15:41:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-e80fb2b0 is in desired state: ProposalReady
2022-04-06 15:41:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-2/my-cluster-e80fb2b0): Annotating KafkaRebalance:my-cluster-e80fb2b0 with annotation approve
2022-04-06 15:41:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-e80fb2b0 will have desired state: Ready
2022-04-06 15:41:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:41:54 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-04-06 15:42:37 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-04-06 15:42:37 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:42:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-e80fb2b0 is in desired state: Ready
2022-04-06 15:42:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:42:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-04-06 15:42:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2023067134-1552335380 in namespace namespace-2
2022-04-06 15:42:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e80fb2b0 in namespace namespace-2
2022-04-06 15:42:56 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-e80fb2b0
2022-04-06 15:43:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-e80fb2b0 in namespace namespace-2
2022-04-06 15:43:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:43:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-04-06 15:43:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-04-06 15:43:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:43:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:43:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-04-06 15:43:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 672.382 s - in io.strimzi.systemtest.operators.ReconciliationST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-04-06 15:44:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-04-06 15:44:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-04-06 15:44:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-04-06 15:44:05 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-04-06 15:44:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-06 15:44:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-04-06 15:45:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-04-06 15:45:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1294180677-270762270 in namespace http-bridge-scram-sha-st
2022-04-06 15:45:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1294180677-270762270 will have desired state: Ready
2022-04-06 15:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1294180677-270762270 is in desired state: Ready
2022-04-06 15:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-04-06 15:45:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-04-06 15:45:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-04-06 15:45:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-06 15:45:16 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-04-06 15:45:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:45:47 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-04-06 15:45:47 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:45:47 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2043299187-274155238 in namespace http-bridge-scram-sha-st
2022-04-06 15:45:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-14280502-1039005138 in namespace http-bridge-scram-sha-st
2022-04-06 15:45:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2043299187-274155238 will have desired state: Ready
2022-04-06 15:45:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-14280502-1039005138 will have desired state: Ready
2022-04-06 15:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2043299187-274155238 is in desired state: Ready
2022-04-06 15:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1301709298 in namespace http-bridge-scram-sha-st
2022-04-06 15:45:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-14280502-1039005138 is in desired state: Ready
2022-04-06 15:45:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1726746626 in namespace http-bridge-scram-sha-st
2022-04-06 15:45:48 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: producer-1726746626 will be in active state
2022-04-06 15:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1301709298 will be in active state
2022-04-06 15:45:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-1726746626 to finished
2022-04-06 15:45:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:45:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-425937150 in namespace http-bridge-scram-sha-st
2022-04-06 15:45:49 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-425937150 will be in active state
2022-04-06 15:45:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer producer-425937150 and consumer consumer-1301709298 finish
2022-04-06 15:46:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:46:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1151311313 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:02 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1151311313 will be in active state
2022-04-06 15:46:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-1151311313 to finished
2022-04-06 15:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-04-06 15:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1301709298 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-425937150 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2043299187-274155238 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:46:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-04-06 15:46:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job producer-1726746626 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1151311313 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-14280502-1039005138 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:46:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-04-06 15:46:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-04-06 15:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:46:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:46:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-04-06 15:46:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-04-06 15:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1294180677-270762270 in namespace http-bridge-scram-sha-st
2022-04-06 15:46:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-06 15:46:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-04-06 15:47:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 190.487 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-04-06 15:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-04-06 15:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-04-06 15:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-04-06 15:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-04-06 15:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-06 15:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-04-06 15:48:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-04-06 15:48:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1223851965-2084245769 in namespace http-bridge-tls-st
2022-04-06 15:48:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1223851965-2084245769 will have desired state: Ready
2022-04-06 15:48:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1223851965-2084245769 is in desired state: Ready
2022-04-06 15:48:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-04-06 15:48:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-04-06 15:48:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-04-06 15:48:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-06 15:48:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-04-06 15:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-678154718-1016450220 in namespace http-bridge-tls-st
2022-04-06 15:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-626416366-1018056694 in namespace http-bridge-tls-st
2022-04-06 15:48:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-678154718-1016450220 will have desired state: Ready
2022-04-06 15:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-626416366-1018056694 will have desired state: Ready
2022-04-06 15:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-678154718-1016450220 is in desired state: Ready
2022-04-06 15:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-34765860 in namespace http-bridge-tls-st
2022-04-06 15:48:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-626416366-1018056694 is in desired state: Ready
2022-04-06 15:48:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-289076680 in namespace http-bridge-tls-st
2022-04-06 15:48:53 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: consumer-289076680 will be in active state
2022-04-06 15:48:53 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-34765860 will be in active state
2022-04-06 15:48:54 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:48:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1486739368 in namespace http-bridge-tls-st
2022-04-06 15:48:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-34765860 to finished
2022-04-06 15:48:54 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: producer-1486739368 will be in active state
2022-04-06 15:48:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:61] Waiting till producer producer-1486739368 and consumer consumer-289076680 finish
2022-04-06 15:49:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:49:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-1830907416 in namespace http-bridge-tls-st
2022-04-06 15:49:05 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-1830907416 will be in active state
2022-04-06 15:49:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-1830907416 to finished
2022-04-06 15:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-04-06 15:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job consumer-289076680 in namespace http-bridge-tls-st
2022-04-06 15:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job producer-1486739368 in namespace http-bridge-tls-st
2022-04-06 15:49:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-626416366-1018056694 in namespace http-bridge-tls-st
2022-04-06 15:49:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:49:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-04-06 15:49:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-34765860 in namespace http-bridge-tls-st
2022-04-06 15:49:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-1830907416 in namespace http-bridge-tls-st
2022-04-06 15:49:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-678154718-1016450220 in namespace http-bridge-tls-st
2022-04-06 15:49:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:49:22 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-04-06 15:49:22 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:49:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:49:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-04-06 15:49:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:49:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:49:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-04-06 15:49:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-04-06 15:49:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1223851965-2084245769 in namespace http-bridge-tls-st
2022-04-06 15:49:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-06 15:49:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-04-06 15:50:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 182.583 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[[1;34mINFO[m] Running io.strimzi.systemtest.tracing.TracingST
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: tracing-st
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: tracing-st
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: tracing-st
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:497] === Applying jaeger operator install files ===
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-cluster-role-binding.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-cluster-role-binding.yaml
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
kind: "ClusterRoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
  namespace: "tracing-st"
roleRef:
  kind: "ClusterRole"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-cluster-role.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-cluster-role.yaml
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRole"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "console.openshift.io"
  resources:
  - "consolelinks"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "namespaces"
  verbs:
  - "get"
  - "list"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  verbs:
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
  - "clusterrolebindings"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-crd.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-crd.yaml
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-operator.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-operator.yaml
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: "jaeger-operator"
  template:
    metadata:
      labels:
        name: "jaeger-operator"
    spec:
      serviceAccountName: "jaeger-operator"
      containers:
      - name: "jaeger-operator"
        image: "jaegertracing/jaeger-operator:1.20.0"
        ports:
        - containerPort: 8383
          name: "http-metrics"
        - containerPort: 8686
          name: "cr-metrics"
        args:
        - "start"
        - "--kafka-provision=no"
        imagePullPolicy: "Always"
        env:
        - name: "WATCH_NAMESPACE"
          value: ""
        - name: "POD_NAME"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.name"
        - name: "POD_NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.namespace"
        - name: "OPERATOR_NAME"
          value: "jaeger-operator"
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-role-binding.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-role-binding.yaml
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
kind: "RoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
roleRef:
  kind: "Role"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-role.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-role.yaml
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "Role"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "image.openshift.io"
  resources:
  - "imagestreams"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:488] Creating jaeger-service-account.yaml from /home/ec2-user/strimzi-kafka-operator/systemtest/../systemtest/src/test/resources/tracing/1.20/operator-files/jaeger-service-account.yaml
2022-04-06 15:50:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st apply -f -
2022-04-06 15:50:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "v1"
kind: "ServiceAccount"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
2022-04-06 15:50:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: jaeger-operator will be ready
2022-04-06 15:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: jaeger-operator is ready
2022-04-06 15:50:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment jaeger-operator to be ready
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment jaeger-operator is ready
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy jaeger-allow in namespace tracing-st
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:524] Network policy for jaeger successfully created
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsConnectService-STARTED
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMaker2Service-STARTED
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testProducerConsumerStreamsConnectService
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-4
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-4
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testProducerConsumerMirrorMaker2Service
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-5
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-5
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 apply -f -
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 apply -f -
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:50:39 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-06 15:50:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-06 15:50:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-06 15:50:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-06 15:50:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-06 15:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-06 15:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-234febc7-kafka-clients in namespace namespace-5
2022-04-06 15:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:50:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-234febc7-kafka-clients will be ready
2022-04-06 15:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-234febc7-kafka-clients is ready
2022-04-06 15:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-234febc7 in namespace namespace-5
2022-04-06 15:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-234febc7 will have desired state: Ready
2022-04-06 15:50:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-06 15:50:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-aeef93e2-kafka-clients in namespace namespace-4
2022-04-06 15:50:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:50:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-aeef93e2-kafka-clients will be ready
2022-04-06 15:51:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-aeef93e2-kafka-clients is ready
2022-04-06 15:51:01 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:51:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-aeef93e2 in namespace namespace-5
2022-04-06 15:51:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:51:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-aeef93e2 will have desired state: Ready
2022-04-06 15:52:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-aeef93e2 is in desired state: Ready
2022-04-06 15:52:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-328693225-2143927892 in namespace namespace-5
2022-04-06 15:52:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:52:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-328693225-2143927892 will have desired state: Ready
2022-04-06 15:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-234febc7 is in desired state: Ready
2022-04-06 15:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-234febc7-target in namespace namespace-5
2022-04-06 15:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:52:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-234febc7-target will have desired state: Ready
2022-04-06 15:52:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-328693225-2143927892 is in desired state: Ready
2022-04-06 15:52:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-616712124-602788140 in namespace namespace-5
2022-04-06 15:52:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:52:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-616712124-602788140 will have desired state: Ready
2022-04-06 15:52:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-616712124-602788140 is in desired state: Ready
2022-04-06 15:52:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-aeef93e2-scraper in namespace namespace-4
2022-04-06 15:52:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:52:16 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-aeef93e2-scraper will be ready
2022-04-06 15:52:18 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-aeef93e2-scraper is ready
2022-04-06 15:52:18 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-aeef93e2-scraper to be ready
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-aeef93e2-scraper is ready
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-aeef93e2-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-aeef93e2-allow in namespace namespace-4
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-aeef93e2 in namespace namespace-5
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:52:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-aeef93e2 will have desired state: Ready
2022-04-06 15:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-234febc7-target is in desired state: Ready
2022-04-06 15:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2066538740-1407390951 in namespace namespace-5
2022-04-06 15:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2066538740-1407390951 will have desired state: Ready
2022-04-06 15:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2066538740-1407390951 is in desired state: Ready
2022-04-06 15:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-cluster-234febc7.my-topic-2066538740-1407390951 in namespace namespace-5
2022-04-06 15:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-cluster-234febc7.my-topic-2066538740-1407390951 will have desired state: Ready
2022-04-06 15:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-cluster-234febc7.my-topic-2066538740-1407390951 is in desired state: Ready
2022-04-06 15:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:177] Setting for kafka source plain bootstrap:my-cluster-234febc7-kafka-bootstrap:9092
2022-04-06 15:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-234febc7-hello-world-producer in namespace namespace-5
2022-04-06 15:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-234febc7-hello-world-producer will be in active state
2022-04-06 15:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingST:186] Setting for kafka target plain bootstrap:my-cluster-234febc7-target-kafka-bootstrap:9092
2022-04-06 15:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-234febc7-hello-world-consumer in namespace namespace-5
2022-04-06 15:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-234febc7-hello-world-consumer will be in active state
2022-04-06 15:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-234febc7 in namespace namespace-5
2022-04-06 15:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-5
2022-04-06 15:53:29 [ForkJoinPool-3-worker-3] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormaker2s' with unstable version 'v1beta2'
2022-04-06 15:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-234febc7 will have desired state: Ready
2022-04-06 15:53:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-aeef93e2 is in desired state: Ready
2022-04-06 15:53:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-aeef93e2 in namespace namespace-5
2022-04-06 15:53:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:53:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-aeef93e2 will have desired state: Ready
2022-04-06 15:53:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-aeef93e2 is in desired state: Ready
2022-04-06 15:53:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-aeef93e2-hello-world-producer in namespace namespace-4
2022-04-06 15:53:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:53:36 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-aeef93e2-hello-world-producer will be in active state
2022-04-06 15:53:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-aeef93e2-hello-world-consumer in namespace namespace-4
2022-04-06 15:53:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-4
2022-04-06 15:53:37 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-aeef93e2-hello-world-consumer will be in active state
2022-04-06 15:53:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-aeef93e2-hello-world-producer and consumer my-cluster-aeef93e2-hello-world-consumer finish
2022-04-06 15:53:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-aeef93e2-kafka-clients-5d89898446-g2prn -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:53:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:53:54 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-06 15:53:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:53:54 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testKafkaBridgeService-STARTED
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-aeef93e2-kafka-clients-5d89898446-g2prn -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer&operation=To_my-topic-328693225-2143927892
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-aeef93e2-kafka-clients-5d89898446-g2prn -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-06 15:53:55 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:53:55 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsService-STARTED
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-aeef93e2-kafka-clients-5d89898446-g2prn -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer&operation=From_my-topic-328693225-2143927892
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-aeef93e2-kafka-clients-5d89898446-g2prn -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:53:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service my-connect is present
2022-04-06 15:53:56 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 exec my-cluster-aeef93e2-kafka-clients-5d89898446-g2prn -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-connect&operation=From_my-topic-328693225-2143927892
2022-04-06 15:53:56 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:53:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:53:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerStreamsConnectService
2022-04-06 15:53:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-aeef93e2-allow in namespace namespace-4
2022-04-06 15:53:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-aeef93e2 in namespace namespace-4
2022-04-06 15:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testKafkaBridgeService
2022-04-06 15:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-6
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-6
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 apply -f -
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:00 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-06 15:54:05 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-06 15:54:05 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-06 15:54:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-aeef93e2-scraper in namespace namespace-4
2022-04-06 15:54:15 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-06 15:54:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-f8f7db26-kafka-clients in namespace namespace-6
2022-04-06 15:54:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-06 15:54:15 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-f8f7db26-kafka-clients will be ready
2022-04-06 15:54:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-f8f7db26-kafka-clients is ready
2022-04-06 15:54:17 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:54:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f8f7db26 in namespace namespace-6
2022-04-06 15:54:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-06 15:54:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f8f7db26 will have desired state: Ready
2022-04-06 15:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-234febc7 is in desired state: Ready
2022-04-06 15:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer&operation=To_my-topic-2066538740-1407390951
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer&operation=From_my-cluster-234febc7.my-topic-2066538740-1407390951
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker2 is present
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker2&operation=From_my-topic-2066538740-1407390951
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker2 is present
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 exec my-cluster-234febc7-kafka-clients-dc9dc5bbb-vnb8h -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker2&operation=To_my-cluster-234febc7.my-topic-2066538740-1407390951
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMaker2Service
2022-04-06 15:54:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-cluster-234febc7.my-topic-2066538740-1407390951 in namespace namespace-5
2022-04-06 15:54:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-aeef93e2-hello-world-producer in namespace namespace-4
2022-04-06 15:54:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-aeef93e2-hello-world-consumer in namespace namespace-4
2022-04-06 15:54:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-aeef93e2 in namespace namespace-4
2022-04-06 15:54:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2066538740-1407390951 in namespace namespace-5
2022-04-06 15:54:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-aeef93e2 in namespace namespace-4
2022-04-06 15:55:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-616712124-602788140 in namespace namespace-4
2022-04-06 15:55:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-328693225-2143927892 in namespace namespace-4
2022-04-06 15:55:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-aeef93e2-kafka-clients in namespace namespace-4
2022-04-06 15:55:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f8f7db26 is in desired state: Ready
2022-04-06 15:55:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-f8f7db26 in namespace namespace-6
2022-04-06 15:55:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-06 15:55:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-f8f7db26 will have desired state: Ready
2022-04-06 15:55:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-f8f7db26 is in desired state: Ready
2022-04-06 15:55:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-944294564-935301373 in namespace namespace-6
2022-04-06 15:55:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-06 15:55:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-944294564-935301373 will have desired state: Ready
2022-04-06 15:55:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-944294564-935301373 is in desired state: Ready
2022-04-06 15:55:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:55:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace namespace-6
2022-04-06 15:55:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-06 15:56:00 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-04-06 15:56:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-f8f7db26-hello-world-consumer in namespace namespace-6
2022-04-06 15:56:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-6
2022-04-06 15:56:01 [ForkJoinPool-3-worker-7] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-f8f7db26-hello-world-consumer will be in active state
2022-04-06 15:56:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:bridge-producer to finished
2022-04-06 15:56:16 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-4 delete -f -
2022-04-06 15:56:16 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:56:16 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:56:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:56:16 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testProducerConsumerStreamsConnectService
2022-04-06 15:56:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsConnectService-FINISHED
2022-04-06 15:56:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:56:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 15:56:28 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMakerService-STARTED
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testProducerConsumerStreamsService
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-7
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-7
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 apply -f -
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:56:30 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-06 15:56:36 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-06 15:56:36 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-06 15:56:46 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-06 15:56:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0657da75-kafka-clients in namespace namespace-7
2022-04-06 15:56:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:56:46 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0657da75-kafka-clients will be ready
2022-04-06 15:56:52 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0657da75-kafka-clients is ready
2022-04-06 15:56:52 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:56:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0657da75 in namespace namespace-7
2022-04-06 15:56:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:56:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0657da75 will have desired state: Ready
2022-04-06 15:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-234febc7-hello-world-consumer in namespace namespace-5
2022-04-06 15:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-234febc7 in namespace namespace-5
2022-04-06 15:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-234febc7-hello-world-producer in namespace namespace-5
2022-04-06 15:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-234febc7 in namespace namespace-5
2022-04-06 15:57:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-234febc7-target in namespace namespace-5
2022-04-06 15:57:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-234febc7-kafka-clients in namespace namespace-5
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-f8f7db26-kafka-clients-cf99978b8-cg44l -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [TracingUtils:47] Jaeger service my-kafka-bridge is present
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 exec my-cluster-f8f7db26-kafka-clients-cf99978b8-cg44l -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-kafka-bridge
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeService
2022-04-06 15:57:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-944294564-935301373 in namespace namespace-6
2022-04-06 15:58:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-f8f7db26 in namespace namespace-6
2022-04-06 15:58:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0657da75 is in desired state: Ready
2022-04-06 15:58:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1464557177-928081008 in namespace namespace-7
2022-04-06 15:58:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:58:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1464557177-928081008 will have desired state: Ready
2022-04-06 15:58:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1464557177-928081008 is in desired state: Ready
2022-04-06 15:58:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-840178909-1961064020 in namespace namespace-7
2022-04-06 15:58:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:58:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-840178909-1961064020 will have desired state: Ready
2022-04-06 15:58:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-840178909-1961064020 is in desired state: Ready
2022-04-06 15:58:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-0657da75-hello-world-producer in namespace namespace-7
2022-04-06 15:58:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:58:08 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-0657da75-hello-world-producer will be in active state
2022-04-06 15:58:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-producer is not present. Present services are ["jaeger-query"].
2022-04-06 15:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-06 15:58:12 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer
2022-04-06 15:58:12 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-0657da75-hello-world-consumer in namespace namespace-7
2022-04-06 15:58:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:58:12 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-0657da75-hello-world-consumer will be in active state
2022-04-06 15:58:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-f8f7db26-hello-world-consumer in namespace namespace-6
2022-04-06 15:58:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace namespace-6
2022-04-06 15:58:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-f8f7db26-kafka-clients in namespace namespace-6
2022-04-06 15:58:13 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:13 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:13 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-06 15:58:14 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:14 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:14 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-06 15:58:15 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:15 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-06 15:58:17 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:17 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:50] Jaeger service hello-world-consumer is not present. Present services are ["jaeger-query","hello-world-producer"].
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 exec my-cluster-0657da75-kafka-clients-66f4fdd996-j489n -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-streams in namespace namespace-7
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-7
2022-04-06 15:58:18 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-streams will be in active state
2022-04-06 15:58:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 15:58:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerStreamsService
2022-04-06 15:58:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-0657da75-hello-world-producer in namespace namespace-7
2022-04-06 15:58:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-840178909-1961064020 in namespace namespace-7
2022-04-06 15:58:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-streams in namespace namespace-7
2022-04-06 15:58:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-0657da75-hello-world-consumer in namespace namespace-7
2022-04-06 15:58:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0657da75 in namespace namespace-7
2022-04-06 15:58:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-5 delete -f -
2022-04-06 15:58:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:58:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:58:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testProducerConsumerMirrorMaker2Service
2022-04-06 15:58:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1464557177-928081008 in namespace namespace-7
2022-04-06 15:58:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0657da75-kafka-clients in namespace namespace-7
2022-04-06 15:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMaker2Service-FINISHED
2022-04-06 15:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-6 delete -f -
2022-04-06 15:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f8f7db26 in namespace namespace-6
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testProducerConsumerMirrorMakerService
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-8
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-8
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingST:531] === Applying jaeger instance install file ===
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 apply -f -
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:58:58 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-jaeger will be ready
2022-04-06 15:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-jaeger is ready
2022-04-06 15:59:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-jaeger to be ready
2022-04-06 15:59:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:59:07 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testKafkaBridgeService
2022-04-06 15:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-7 delete -f -
2022-04-06 15:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 15:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 15:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-jaeger is ready
2022-04-06 15:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-279bb694-kafka-clients in namespace namespace-8
2022-04-06 15:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 15:59:14 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-279bb694-kafka-clients will be ready
2022-04-06 15:59:16 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-279bb694-kafka-clients is ready
2022-04-06 15:59:16 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 15:59:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-279bb694 in namespace namespace-8
2022-04-06 15:59:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 15:59:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-279bb694 will have desired state: Ready
2022-04-06 15:59:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testKafkaBridgeService-FINISHED
2022-04-06 15:59:34 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 15:59:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 15:59:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testProducerConsumerStreamsService
2022-04-06 15:59:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerStreamsService-FINISHED
2022-04-06 15:59:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:00:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-279bb694 is in desired state: Ready
2022-04-06 16:00:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-279bb694-target in namespace namespace-8
2022-04-06 16:00:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 16:00:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-279bb694-target will have desired state: Ready
2022-04-06 16:01:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-279bb694-target is in desired state: Ready
2022-04-06 16:01:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-840466406-2019918534 in namespace namespace-8
2022-04-06 16:01:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 16:01:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-840466406-2019918534 will have desired state: Ready
2022-04-06 16:01:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-840466406-2019918534 is in desired state: Ready
2022-04-06 16:01:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-840466406-2019918534-target in namespace namespace-8
2022-04-06 16:01:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 16:01:48 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-840466406-2019918534-target will have desired state: Ready
2022-04-06 16:01:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-840466406-2019918534-target is in desired state: Ready
2022-04-06 16:01:49 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingST:267] Setting for kafka source plain bootstrap:my-cluster-279bb694-kafka-bootstrap:9092
2022-04-06 16:01:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-279bb694-hello-world-producer in namespace namespace-8
2022-04-06 16:01:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 16:01:49 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-279bb694-hello-world-producer will be in active state
2022-04-06 16:01:50 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingST:276] Setting for kafka target plain bootstrap:my-cluster-279bb694-target-kafka-bootstrap:9092
2022-04-06 16:01:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-279bb694-hello-world-consumer in namespace namespace-8
2022-04-06 16:01:50 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 16:01:50 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-279bb694-hello-world-consumer will be in active state
2022-04-06 16:01:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-279bb694 in namespace namespace-8
2022-04-06 16:01:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-8
2022-04-06 16:01:51 [ForkJoinPool-3-worker-5] [33mWARN [m [VersionUsageUtils:60] The client is using resource type 'kafkamirrormakers' with unstable version 'v1beta2'
2022-04-06 16:01:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-279bb694 will have desired state: Ready
2022-04-06 16:02:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-279bb694 is in desired state: Ready
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-producer is present
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-producer&operation=To_my-topic-840466406-2019918534
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:54 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service hello-world-consumer is present
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=hello-world-consumer&operation=From_my-topic-840466406-2019918534
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker is present
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker&operation=From_my-topic-840466406-2019918534
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/services
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TracingUtils:47] Jaeger service my-mirror-maker is present
2022-04-06 16:02:56 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 exec my-cluster-279bb694-kafka-clients-6d76d9d9d4-c6bdw -- /bin/bash -c curl my-jaeger-query:16686/jaeger/api/traces?service=my-mirror-maker&operation=To_my-topic-840466406-2019918534
2022-04-06 16:02:56 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:02:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:02:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMakerService
2022-04-06 16:02:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-840466406-2019918534-target in namespace namespace-8
2022-04-06 16:02:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-279bb694 in namespace namespace-8
2022-04-06 16:03:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-279bb694-target in namespace namespace-8
2022-04-06 16:03:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-840466406-2019918534 in namespace namespace-8
2022-04-06 16:03:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-279bb694-kafka-clients in namespace namespace-8
2022-04-06 16:03:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-279bb694-hello-world-consumer in namespace namespace-8
2022-04-06 16:03:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-279bb694 in namespace namespace-8
2022-04-06 16:03:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-279bb694-hello-world-producer in namespace namespace-8
2022-04-06 16:03:26 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-8 delete -f -
2022-04-06 16:03:26 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "jaegertracing.io/v1"
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: "allInOne"
  allInOne:
    image: "jaegertracing/all-in-one:1.20"
    options:
      log-level: "debug"
      query:
        base-path: "/jaeger"
  ingress:
    openshift: {}
    resources: {}
    security: "none"
  ui:
    options:
      dependencies:
        menuEnabled: false
      tracking:
        gaID: "UA-000000-2"
      menu:
      - label: "About Jaeger"
        items:
        - label: "Documentation"
          url: "https://www.jaegertracing.io/docs/latest"
  storage:
    options:
      memory:
        max-traces: 100000
2022-04-06 16:03:26 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:03:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:03:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testProducerConsumerMirrorMakerService
2022-04-06 16:04:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.tracing.TracingST.testProducerConsumerMirrorMakerService-FINISHED
2022-04-06 16:04:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:04:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:04:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for TracingST
2022-04-06 16:04:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy jaeger-allow in namespace tracing-st
2022-04-06 16:04:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "v1"
kind: "ServiceAccount"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
2022-04-06 16:04:02 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "Role"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "image.openshift.io"
  resources:
  - "imagestreams"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
kind: "RoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
roleRef:
  kind: "Role"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "apps/v1"
kind: "Deployment"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
spec:
  replicas: 1
  selector:
    matchLabels:
      name: "jaeger-operator"
  template:
    metadata:
      labels:
        name: "jaeger-operator"
    spec:
      serviceAccountName: "jaeger-operator"
      containers:
      - name: "jaeger-operator"
        image: "jaegertracing/jaeger-operator:1.20.0"
        ports:
        - containerPort: 8383
          name: "http-metrics"
        - containerPort: 8686
          name: "cr-metrics"
        args:
        - "start"
        - "--kafka-provision=no"
        imagePullPolicy: "Always"
        env:
        - name: "WATCH_NAMESPACE"
          value: ""
        - name: "POD_NAME"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.name"
        - name: "POD_NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: "metadata.namespace"
        - name: "OPERATOR_NAME"
          value: "jaeger-operator"
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
apiVersion: "rbac.authorization.k8s.io/v1"
kind: "ClusterRole"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
rules:
- apiGroups:
  - "jaegertracing.io"
  resources:
  - "*"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resourceNames:
  - "jaeger-operator"
  resources:
  - "deployments/finalizers"
  verbs:
  - "update"
- apiGroups:
  - ""
  resources:
  - "configmaps"
  - "persistentvolumeclaims"
  - "pods"
  - "secrets"
  - "serviceaccounts"
  - "services"
  - "services/finalizers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  - "daemonsets"
  - "replicasets"
  - "statefulsets"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "extensions"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "networking.k8s.io"
  resources:
  - "ingresses"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "batch"
  resources:
  - "jobs"
  - "cronjobs"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "route.openshift.io"
  resources:
  - "routes"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "console.openshift.io"
  resources:
  - "consolelinks"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "autoscaling"
  resources:
  - "horizontalpodautoscalers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "monitoring.coreos.com"
  resources:
  - "servicemonitors"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "logging.openshift.io"
  resources:
  - "elasticsearches"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "kafka.strimzi.io"
  resources:
  - "kafkas"
  - "kafkausers"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - ""
  resources:
  - "namespaces"
  verbs:
  - "get"
  - "list"
  - "watch"
- apiGroups:
  - "apps"
  resources:
  - "deployments"
  verbs:
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
- apiGroups:
  - "rbac.authorization.k8s.io"
  resources:
  - "clusterrolebindings"
  verbs:
  - "create"
  - "delete"
  - "get"
  - "list"
  - "patch"
  - "update"
  - "watch"
2022-04-06 16:04:03 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:04 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace tracing-st delete -f -
2022-04-06 16:04:04 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Input: ---
kind: "ClusterRoleBinding"
apiVersion: "rbac.authorization.k8s.io/v1"
metadata:
  name: "jaeger-operator"
  namespace: "tracing-st"
subjects:
- kind: "ServiceAccount"
  name: "jaeger-operator"
  namespace: "tracing-st"
roleRef:
  kind: "ClusterRole"
  name: "jaeger-operator"
  apiGroup: "rbac.authorization.k8s.io"
2022-04-06 16:04:04 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:04:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 836.922 s - in io.strimzi.systemtest.tracing.TracingST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
2022-04-06 16:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-api-st
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-api-st
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-api-st
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-STARTED
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-STARTED
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testCruiseControlBasicAPIRequests
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-9
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-9
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-10 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-10
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6d80a983 in namespace namespace-9
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-9
2022-04-06 16:04:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6d80a983 will have desired state: Ready
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-10
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-10
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka cruise-control-api-cluster-name in namespace namespace-10
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-10
2022-04-06 16:04:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: cruise-control-api-cluster-name will have desired state: Ready
2022-04-06 16:06:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6d80a983 is in desired state: Ready
2022-04-06 16:06:01 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:48] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-04-06 16:06:02 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:58] Verifying that Cruise Control REST API is available
2022-04-06 16:06:02 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:66] ----> KAFKA REBALANCE <----
2022-04-06 16:06:02 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:73] Waiting for CC will have for enough metrics to be recorded to make a proposal 
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: cruise-control-api-cluster-name is in desired state: Ready
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlApiST:153] ----> CRUISE CONTROL DEPLOYMENT STATE ENDPOINT <----
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlApiST:157] Verifying that Cruise Control REST API is available using HTTP request without credentials
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka cruise-control-api-cluster-name in namespace namespace-10
2022-04-06 16:06:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-10, for cruise control Kafka cluster cruise-control-api-cluster-name
2022-04-06 16:06:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:06:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-10 for test case:testCruiseControlBasicAPIRequestsWithSecurityDisabled
2022-04-06 16:07:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequestsWithSecurityDisabled-FINISHED
2022-04-06 16:07:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:11:59 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:97] ----> EXECUTION OF STOP PROPOSAL <----
2022-04-06 16:11:59 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:108] ----> USER TASKS <----
2022-04-06 16:12:00 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:126] Verifying that Cruise Control REST API doesn't allow HTTP requests
2022-04-06 16:12:00 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlApiST:132] Verifying that Cruise Control REST API doesn't allow unauthenticated requests
2022-04-06 16:12:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:12:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlBasicAPIRequests
2022-04-06 16:12:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6d80a983 in namespace namespace-9
2022-04-06 16:12:00 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-9, for cruise control Kafka cluster my-cluster-6d80a983
2022-04-06 16:12:10 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:12:10 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testCruiseControlBasicAPIRequests
2022-04-06 16:12:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlApiST.testCruiseControlBasicAPIRequests-FINISHED
2022-04-06 16:12:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:12:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:12:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlApiST is everything deleted.
2022-04-06 16:12:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 524.658 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlApiST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-11 for test case:testCapacityFile
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-11
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-11
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-11
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-12 for test case:testConfigurationFileIsCreated
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-12
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f43ef54c in namespace namespace-11
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-11
2022-04-06 16:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f43ef54c will have desired state: Ready
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-12
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-12
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-662b3bf3 in namespace namespace-12
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-12
2022-04-06 16:12:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-662b3bf3 will have desired state: Ready
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f43ef54c is in desired state: Ready
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-11 exec my-cluster-f43ef54c-cruise-control-7f9b4986c7-p94zh -- /bin/bash -c cat /tmp/capacity.json
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCapacityFile
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f43ef54c in namespace namespace-11
2022-04-06 16:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-11, for cruise control Kafka cluster my-cluster-f43ef54c
2022-04-06 16:15:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:15:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-11 for test case:testCapacityFile
2022-04-06 16:15:33 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:15:33 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-13 for test case:testDeployAndUnDeployCruiseControl
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-13
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-13
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-13
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-383a53e2 in namespace namespace-13
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-13
2022-04-06 16:15:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-383a53e2 will have desired state: Ready
2022-04-06 16:15:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:15:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-662b3bf3 is in desired state: Ready
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-12 exec my-cluster-662b3bf3-cruise-control-5bcd665d59-89vzd -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-662b3bf3 in namespace namespace-12
2022-04-06 16:15:49 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-12, for cruise control Kafka cluster my-cluster-662b3bf3
2022-04-06 16:15:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:15:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-12 for test case:testConfigurationFileIsCreated
2022-04-06 16:16:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-04-06 16:16:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:16:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:16:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-14 for test case:testConfigurationPerformanceOptions
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-14
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-14
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-14
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e5634b50 in namespace namespace-14
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-14
2022-04-06 16:16:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e5634b50 will have desired state: Ready
2022-04-06 16:16:43 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-04-06 16:16:43 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:16:43 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:16:43 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-15 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-15
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-15
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-15
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-00c7eb49 in namespace namespace-15
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-15
2022-04-06 16:16:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-00c7eb49 will have desired state: Ready
2022-04-06 16:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-383a53e2 is in desired state: Ready
2022-04-06 16:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-04-06 16:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-383a53e2-kafka rolling update
2022-04-06 16:18:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e5634b50 is in desired state: Ready
2022-04-06 16:18:22 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-04-06 16:18:22 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-04-06 16:18:22 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e5634b50-cruise-control rolling update
2022-04-06 16:18:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-00c7eb49 is in desired state: Ready
2022-04-06 16:18:29 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-04-06 16:18:29 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-04-06 16:18:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-00c7eb49-cruise-control rolling update
2022-04-06 16:18:34 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-383a53e2-kafka has been successfully rolled
2022-04-06 16:18:34 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-383a53e2-kafka to be ready
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-383a53e2 will have desired state: Ready
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-383a53e2 is in desired state: Ready
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-383a53e2 is ready
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:120] Verifying that my-cluster-383a53e2-cruise-control- pod is not present
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-383a53e2-cruise-control- will have stable 0 replicas
2022-04-06 16:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-06 16:19:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-06 16:19:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-06 16:19:07 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e5634b50-cruise-control will be ready
2022-04-06 16:19:07 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e5634b50-cruise-control is ready
2022-04-06 16:19:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-06 16:19:09 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-06 16:19:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-06 16:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-06 16:19:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-06 16:19:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-06 16:19:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-06 16:19:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-00c7eb49-cruise-control will be ready
2022-04-06 16:19:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-00c7eb49-cruise-control is ready
2022-04-06 16:19:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-06 16:19:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-06 16:19:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-06 16:19:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e5634b50-cruise-control rolling update finished
2022-04-06 16:19:17 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-04-06 16:19:17 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 16:19:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-06 16:19:18 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 16:19:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-06 16:19:19 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 16:19:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-06 16:19:20 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 16:19:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-06 16:19:21 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 16:19:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-06 16:19:22 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 16:19:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-06 16:19:23 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 16:19:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-06 16:19:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:228] Pod my-cluster-383a53e2-cruise-control- has 0 replicas
2022-04-06 16:19:24 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-04-06 16:19:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 16:19:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-00c7eb49-cruise-control rolling update finished
2022-04-06 16:19:25 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-04-06 16:19:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 16:19:25 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 16:19:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 16:19:26 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 16:19:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 16:19:27 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 16:19:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 16:19:28 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 16:19:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 16:19:29 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 16:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 16:19:30 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 16:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 16:19:31 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 16:19:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 16:19:32 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 16:19:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 16:19:33 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 16:19:34 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 16:19:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 16:19:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 16:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 16:19:36 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 16:19:36 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 16:19:37 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 16:19:37 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 16:19:38 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 16:19:38 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 16:19:39 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 16:19:39 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 16:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 16:19:40 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 16:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 16:19:41 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 16:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 16:19:42 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 16:19:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 16:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 16:19:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 16:19:44 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 16:19:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 16:19:45 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 16:19:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 16:19:46 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 16:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 16:19:47 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 16:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 16:19:48 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 16:19:49 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 16:19:49 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 16:19:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 16:19:50 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 16:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 16:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 16:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 16:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 16:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 16:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 16:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 16:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 16:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 16:19:55 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 16:19:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 16:19:56 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 16:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 16:19:57 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 16:19:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 16:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 16:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 16:19:59 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 16:20:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 16:20:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 16:20:01 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 16:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 16:20:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 16:20:02 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 16:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 16:20:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 16:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 16:20:04 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 16:20:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 16:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 16:20:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 16:20:06 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 16:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-e5634b50-kafka-0=30a3d115-ed12-46a7-9fee-950531edfaca, my-cluster-e5634b50-kafka-1=52973c7d-076b-401a-93c4-92aa97158724, my-cluster-e5634b50-kafka-2=84e2583a-ac9b-4de9-85b6-f4042d8da71e} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e5634b50 in namespace namespace-14
2022-04-06 16:20:07 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-14, for cruise control Kafka cluster my-cluster-e5634b50
2022-04-06 16:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 16:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 16:20:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 16:20:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 16:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 16:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 16:20:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-00c7eb49-kafka-2=1d356eb0-7285-4682-9680-ed4e2ae361f4, my-cluster-00c7eb49-kafka-0=6ce5c20f-24ac-4482-926c-7e0248f7943f, my-cluster-00c7eb49-kafka-1=48890239-19a9-4fa7-a213-e1247d1fd711} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-04-06 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-04-06 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-00c7eb49 in namespace namespace-15
2022-04-06 16:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-15, for cruise control Kafka cluster my-cluster-00c7eb49
2022-04-06 16:20:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:20:17 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-14 for test case:testConfigurationPerformanceOptions
2022-04-06 16:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-15 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-04-06 16:21:01 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-04-06 16:21:01 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-16 for test case:testConfigurationReflection
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-16
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-16
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-16
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-42ad2329 in namespace namespace-16
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-16
2022-04-06 16:21:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-42ad2329 will have desired state: Ready
2022-04-06 16:21:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-04-06 16:21:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:21:24 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-383a53e2} has correct cruise control metric reporter properties, null
io.strimzi.test.WaitException: Timeout after 120000 ms waiting for Verify that kafka configuration {cluster-name=my-cluster-383a53e2} has correct cruise control metric reporter properties
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.specific.CruiseControlUtils.verifyCruiseControlMetricReporterConfigurationInKafkaConfigMapIsPresent(CruiseControlUtils.java:83)
	at io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.lambda$testDeployAndUnDeployCruiseControl$1(CruiseControlConfigurationST.java:124)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3082)
	at io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl(CruiseControlConfigurationST.java:124)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-06 16:21:24 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-04-06 16:21:24 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-04-06 16:21:24 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-383a53e2-kafka rolling update
2022-04-06 16:22:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-383a53e2-kafka has been successfully rolled
2022-04-06 16:22:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-383a53e2-kafka to be ready
2022-04-06 16:22:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-42ad2329 is in desired state: Ready
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-16 exec my-cluster-42ad2329-cruise-control-74cd6c9f58-n9dql -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-42ad2329 in namespace namespace-16
2022-04-06 16:22:46 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-16, for cruise control Kafka cluster my-cluster-42ad2329
2022-04-06 16:22:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:22:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-16 for test case:testConfigurationReflection
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-383a53e2 will have desired state: Ready
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-383a53e2 is in desired state: Ready
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-383a53e2 is ready
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-383a53e2 in namespace namespace-13
2022-04-06 16:23:13 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-13, for cruise control Kafka cluster my-cluster-383a53e2
2022-04-06 16:23:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:23:23 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-13 for test case:testDeployAndUnDeployCruiseControl
2022-04-06 16:23:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-04-06 16:23:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:24:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-04-06 16:24:07 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-04-06 16:24:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 673.105 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[[1;34mINFO[m] Running io.strimzi.systemtest.cruisecontrol.CruiseControlST
2022-04-06 16:24:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: cruise-control-st
2022-04-06 16:24:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: cruise-control-st
2022-04-06 16:24:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-st
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlTopicExclusion-STARTED
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage-STARTED
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-17 for test case:testCruiseControlTopicExclusion
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-17
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-17
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-17
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-18 for test case:testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-18
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5023d70d in namespace namespace-17
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-06 16:24:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5023d70d will have desired state: Ready
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-18
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-18
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-19e3c419 in namespace namespace-18
2022-04-06 16:24:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-18
2022-04-06 16:24:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-19e3c419 will have desired state: Ready
2022-04-06 16:25:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-19e3c419 is in desired state: Ready
2022-04-06 16:25:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-19e3c419 in namespace namespace-18
2022-04-06 16:25:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-18
2022-04-06 16:25:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-19e3c419 will have desired state: NotReady
2022-04-06 16:25:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-19e3c419 is in desired state: NotReady
2022-04-06 16:25:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:25:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage
2022-04-06 16:25:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-19e3c419 in namespace namespace-18
2022-04-06 16:26:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-19e3c419 in namespace namespace-18
2022-04-06 16:26:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-18, for cruise control Kafka cluster my-cluster-19e3c419
2022-04-06 16:26:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:26:13 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-18 for test case:testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage
2022-04-06 16:26:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5023d70d is in desired state: Ready
2022-04-06 16:26:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic excluded-topic-1 in namespace namespace-17
2022-04-06 16:26:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-06 16:26:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: excluded-topic-1 will have desired state: Ready
2022-04-06 16:26:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: excluded-topic-1 is in desired state: Ready
2022-04-06 16:26:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic excluded-topic-2 in namespace namespace-17
2022-04-06 16:26:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-06 16:26:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: excluded-topic-2 will have desired state: Ready
2022-04-06 16:26:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: excluded-topic-2 is in desired state: Ready
2022-04-06 16:26:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic included-topic in namespace namespace-17
2022-04-06 16:26:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-06 16:26:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: included-topic will have desired state: Ready
2022-04-06 16:26:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: included-topic is in desired state: Ready
2022-04-06 16:26:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-5023d70d in namespace namespace-18
2022-04-06 16:26:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-17
2022-04-06 16:26:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-5023d70d will have desired state: PendingProposal
2022-04-06 16:26:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-5023d70d is in desired state: PendingProposal
2022-04-06 16:26:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-5023d70d will have desired state: ProposalReady
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancingWithoutSpecifyingJBODStorage-FINISHED
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlReplicaMovementStrategy-STARTED
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-19 for test case:testCruiseControlReplicaMovementStrategy
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-19
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-19
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-19
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-46b97808 in namespace namespace-19
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-06 16:26:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-46b97808 will have desired state: Ready
2022-04-06 16:28:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-46b97808 is in desired state: Ready
2022-04-06 16:28:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-46b97808-kafka-clients in namespace namespace-19
2022-04-06 16:28:40 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-19
2022-04-06 16:28:40 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-46b97808-kafka-clients will be ready
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-46b97808-kafka-clients is ready
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:234] Check for default CruiseControl replicaMovementStrategy in pod configuration file.
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-19 exec my-cluster-46b97808-cruise-control-86bf44d975-gzz8h -c cruise-control -- cat /tmp/cruisecontrol.properties
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:248] Set non-default CruiseControl replicaMovementStrategies to KafkaRebalance resource.
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:252] Verifying that CC pod is rolling, because of change size of disk
2022-04-06 16:28:42 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-46b97808-cruise-control rolling update
2022-04-06 16:29:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-46b97808-cruise-control will be ready
2022-04-06 16:29:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-46b97808-cruise-control is ready
2022-04-06 16:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-46b97808-cruise-control rolling update finished
2022-04-06 16:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-19 exec my-cluster-46b97808-cruise-control-547b745d87-jwrff -c cruise-control -- cat /tmp/cruisecontrol.properties
2022-04-06 16:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 16:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlReplicaMovementStrategy
2022-04-06 16:29:37 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-46b97808-kafka-clients in namespace namespace-19
2022-04-06 16:30:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-46b97808 in namespace namespace-19
2022-04-06 16:30:17 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-19, for cruise control Kafka cluster my-cluster-46b97808
2022-04-06 16:30:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:30:27 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-19 for test case:testCruiseControlReplicaMovementStrategy
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlReplicaMovementStrategy-FINISHED
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithSingleNodeKafka-STARTED
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-20 for test case:testCruiseControlWithSingleNodeKafka
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-20
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-20
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-20
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:169] Deploying single node Kafka with CruiseControl
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-540e79ab in namespace namespace-20
2022-04-06 16:31:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-20
2022-04-06 16:31:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-5023d70d is in desired state: ProposalReady
2022-04-06 16:31:12 [ForkJoinPool-3-worker-1] [32mINFO [m [CruiseControlST:208] Checking status of KafkaRebalance
2022-04-06 16:31:12 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #4(test) KafkaRebalance(cruise-control-st/my-cluster-5023d70d): Annotating KafkaRebalance:my-cluster-5023d70d with annotation approve
2022-04-06 16:31:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-5023d70d will have desired state: Ready
2022-04-06 16:31:14 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:178] Increasing Kafka nodes to 3
2022-04-06 16:31:14 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-540e79ab will have desired state: Ready
2022-04-06 16:32:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-5023d70d is in desired state: Ready
2022-04-06 16:32:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:32:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlTopicExclusion
2022-04-06 16:32:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic excluded-topic-2 in namespace namespace-17
2022-04-06 16:32:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-5023d70d in namespace namespace-17
2022-04-06 16:32:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic included-topic in namespace namespace-17
2022-04-06 16:32:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-540e79ab is in desired state: Ready
2022-04-06 16:32:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:32:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithSingleNodeKafka
2022-04-06 16:32:47 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-540e79ab in namespace namespace-20
2022-04-06 16:32:47 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-20, for cruise control Kafka cluster my-cluster-540e79ab
2022-04-06 16:32:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic excluded-topic-1 in namespace namespace-17
2022-04-06 16:32:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:32:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-20 for test case:testCruiseControlWithSingleNodeKafka
2022-04-06 16:32:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5023d70d in namespace namespace-17
2022-04-06 16:32:58 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-17, for cruise control Kafka cluster my-cluster-5023d70d
2022-04-06 16:33:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:33:08 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-17 for test case:testCruiseControlTopicExclusion
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithSingleNodeKafka-FINISHED
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancing-STARTED
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-21 for test case:testCruiseControlIntraBrokerBalancing
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-21
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-21
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-21
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6c5e9b5c in namespace namespace-21
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-21
2022-04-06 16:33:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6c5e9b5c will have desired state: Ready
2022-04-06 16:33:52 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlTopicExclusion-FINISHED
2022-04-06 16:33:52 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:34:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6c5e9b5c is in desired state: Ready
2022-04-06 16:34:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-6c5e9b5c in namespace namespace-21
2022-04-06 16:34:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-21
2022-04-06 16:34:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-6c5e9b5c will have desired state: PendingProposal
2022-04-06 16:34:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-6c5e9b5c is in desired state: PendingProposal
2022-04-06 16:34:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-6c5e9b5c will have desired state: ProposalReady
2022-04-06 16:40:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-6c5e9b5c is in desired state: ProposalReady
2022-04-06 16:40:57 [ForkJoinPool-3-worker-5] [32mINFO [m [CruiseControlST:292] Checking status of KafkaRebalance
2022-04-06 16:40:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:40:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlIntraBrokerBalancing
2022-04-06 16:40:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-6c5e9b5c in namespace namespace-21
2022-04-06 16:40:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6c5e9b5c in namespace namespace-21
2022-04-06 16:40:57 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-21, for cruise control Kafka cluster my-cluster-6c5e9b5c
2022-04-06 16:41:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:41:07 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-21 for test case:testCruiseControlIntraBrokerBalancing
2022-04-06 16:41:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlIntraBrokerBalancing-FINISHED
2022-04-06 16:41:34 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:41:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:41:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testAutoCreationOfCruiseControlTopics-STARTED
2022-04-06 16:41:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:41:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a1e95383 in namespace cruise-control-st
2022-04-06 16:41:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a1e95383 will have desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a1e95383 is in desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: strimzi.cruisecontrol.metrics will have desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: strimzi.cruisecontrol.metrics is in desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples will have desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples is in desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples will have desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples is in desired state: Ready
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:96] Checking partitions and replicas for strimzi.cruisecontrol.metrics
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:100] Checking partitions and replicas for strimzi.cruisecontrol.modeltrainingsamples
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:104] Checking partitions and replicas for strimzi.cruisecontrol.partitionmetricsamples
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoCreationOfCruiseControlTopics
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a1e95383 in namespace cruise-control-st
2022-04-06 16:43:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-a1e95383
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testAutoCreationOfCruiseControlTopics-FINISHED
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-STARTED
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ce9887cb in namespace cruise-control-st
2022-04-06 16:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ce9887cb will have desired state: Ready
2022-04-06 16:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ce9887cb is in desired state: Ready
2022-04-06 16:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-ce9887cb in namespace cruise-control-st
2022-04-06 16:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: PendingProposal
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: PendingProposal
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): PendingProposal
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:81] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Verifying that KafkaRebalance resource is in PendingProposal state
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: PendingProposal
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: PendingProposal
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:85] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Verifying that KafkaRebalance resource is in ProposalReady state
2022-04-06 16:45:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: ProposalReady
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: ProposalReady
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ProposalReady
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Annotating KafkaRebalance:my-cluster-ce9887cb with annotation approve
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-ce9887cb annotated
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Verifying that annotation triggers the Rebalancing state
2022-04-06 16:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: Rebalancing
2022-04-06 16:50:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: Rebalancing
2022-04-06 16:50:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #5(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Verifying that KafkaRebalance is in the Ready state
2022-04-06 16:50:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: Ready
2022-04-06 16:52:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: Ready
2022-04-06 16:52:11 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:152] Annotating KafkaRebalance: my-cluster-ce9887cb with 'refresh' anno
2022-04-06 16:52:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #6(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Annotating KafkaRebalance:my-cluster-ce9887cb with annotation refresh
2022-04-06 16:52:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: ProposalReady
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: ProposalReady
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [CruiseControlST:156] Trying rebalancing process again
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ProposalReady
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ProposalReady
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): ============================================================================
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Annotating KafkaRebalance:my-cluster-ce9887cb with annotation approve
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-ce9887cb annotated
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Verifying that annotation triggers the Rebalancing state
2022-04-06 16:52:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: Rebalancing
2022-04-06 16:52:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: Rebalancing
2022-04-06 16:52:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #7(test) KafkaRebalance(cruise-control-st/my-cluster-ce9887cb): Verifying that KafkaRebalance is in the Ready state
2022-04-06 16:52:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-ce9887cb will have desired state: Ready
2022-04-06 16:52:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-ce9887cb is in desired state: Ready
2022-04-06 16:52:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 16:52:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithRebalanceResourceAndRefreshAnnotation
2022-04-06 16:52:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-ce9887cb in namespace cruise-control-st
2022-04-06 16:52:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ce9887cb in namespace cruise-control-st
2022-04-06 16:52:18 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-ce9887cb
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithRebalanceResourceAndRefreshAnnotation-FINISHED
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithApiSecurityDisabled-STARTED
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2b868646 in namespace cruise-control-st
2022-04-06 16:52:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2b868646 will have desired state: Ready
2022-04-06 16:55:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2b868646 is in desired state: Ready
2022-04-06 16:55:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-2b868646 in namespace cruise-control-st
2022-04-06 16:55:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-2b868646 will have desired state: PendingProposal
2022-04-06 16:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-2b868646 is in desired state: PendingProposal
2022-04-06 16:55:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-2b868646 will have desired state: ProposalReady
2022-04-06 17:01:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-2b868646 is in desired state: ProposalReady
2022-04-06 17:01:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:01:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCruiseControlWithApiSecurityDisabled
2022-04-06 17:01:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-2b868646 in namespace cruise-control-st
2022-04-06 17:01:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2b868646 in namespace cruise-control-st
2022-04-06 17:01:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace cruise-control-st, for cruise control Kafka cluster my-cluster-2b868646
2022-04-06 17:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlST.testCruiseControlWithApiSecurityDisabled-FINISHED
2022-04-06 17:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context CruiseControlST is everything deleted.
2022-04-06 17:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,263.766 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.ListenersST
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: listeners-st
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: listeners-st
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: listeners-st
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsAuthenticated-STARTED
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testMessagesTlsScramShaWithPredefinedPassword-STARTED
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-22 for test case:testSendMessagesTlsAuthenticated
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-22
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-22
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-22
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-23 for test case:testMessagesTlsScramShaWithPredefinedPassword
2022-04-06 17:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-23
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-db0348a4 in namespace namespace-22
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-22
2022-04-06 17:01:58 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-db0348a4 will have desired state: Ready
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-52ad115a in namespace namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1814341620-520196398 in namespace namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1100718827-365191312 in namespace namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-06 17:01:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-52ad115a will have desired state: Ready
2022-04-06 17:03:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-db0348a4 is in desired state: Ready
2022-04-06 17:03:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1285296438-1581276766 in namespace namespace-23
2022-04-06 17:03:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-22
2022-04-06 17:03:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1285296438-1581276766 will have desired state: Ready
2022-04-06 17:03:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1285296438-1581276766 is in desired state: Ready
2022-04-06 17:03:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1436852079-280858007 in namespace namespace-23
2022-04-06 17:03:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-22
2022-04-06 17:03:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1436852079-280858007 will have desired state: Ready
2022-04-06 17:03:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1436852079-280858007 is in desired state: Ready
2022-04-06 17:03:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-db0348a4-kafka-clients in namespace namespace-23
2022-04-06 17:03:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-22
2022-04-06 17:03:18 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-db0348a4-kafka-clients will be ready
2022-04-06 17:03:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-db0348a4-kafka-clients is ready
2022-04-06 17:03:20 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:03:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ListenersST:221] Checking produced and consumed messages to pod:my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl
2022-04-06 17:03:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@20574519, messages=[], arguments=[USER=my_user_1436852079_280858007, --max-messages, 100, --bootstrap-server, my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093, --topic, my-topic-1285296438-1581276766], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl', podNamespace='namespace-22', bootstrapServer='my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093', topicName='my-topic-1285296438-1581276766', maxMessages=100, kafkaUsername='my-user-1436852079-280858007', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3b1fca5f}
2022-04-06 17:03:20 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093:my-topic-1285296438-1581276766 from pod my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl
2022-04-06 17:03:20 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl -n namespace-22 -- /opt/kafka/producer.sh USER=my_user_1436852079_280858007 --max-messages 100 --bootstrap-server my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093 --topic my-topic-1285296438-1581276766
2022-04-06 17:03:23 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 17:03:23 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 17:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@452f57c7, messages=[], arguments=[--group-id, my-consumer-group-364120213, USER=my_user_1436852079_280858007, --max-messages, 100, --group-instance-id, instance825073561, --bootstrap-server, my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093, --topic, my-topic-1285296438-1581276766], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl', podNamespace='namespace-22', bootstrapServer='my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093', topicName='my-topic-1285296438-1581276766', maxMessages=100, kafkaUsername='my-user-1436852079-280858007', consumerGroupName='my-consumer-group-364120213', consumerInstanceId='instance825073561', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@a738be}
2022-04-06 17:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093:my-topic-1285296438-1581276766 from pod my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl
2022-04-06 17:03:24 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-db0348a4-kafka-clients-999cccd8b-hgzsl -n namespace-22 -- /opt/kafka/consumer.sh --group-id my-consumer-group-364120213 USER=my_user_1436852079_280858007 --max-messages 100 --group-instance-id instance825073561 --bootstrap-server my-cluster-db0348a4-kafka-bootstrap.namespace-22.svc:9093 --topic my-topic-1285296438-1581276766
2022-04-06 17:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 17:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 17:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsAuthenticated
2022-04-06 17:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1436852079-280858007 in namespace namespace-22
2022-04-06 17:03:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-db0348a4-kafka-clients in namespace namespace-22
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-52ad115a is in desired state: Ready
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1814341620-520196398 will have desired state: Ready
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1814341620-520196398 is in desired state: Ready
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1100718827-365191312 will have desired state: Ready
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1100718827-365191312 is in desired state: Ready
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-52ad115a-kafka-clients in namespace namespace-23
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-06 17:03:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-52ad115a-kafka-clients will be ready
2022-04-06 17:03:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-52ad115a-kafka-clients is ready
2022-04-06 17:03:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:03:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@e44b546, messages=[], arguments=[USER=my_user_1814341620_520196398, --max-messages, 100, --bootstrap-server, my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096, --topic, my-topic-1100718827-365191312], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-52ad115a-kafka-clients-55c497949d-k9p6d', podNamespace='namespace-23', bootstrapServer='my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096', topicName='my-topic-1100718827-365191312', maxMessages=100, kafkaUsername='my-user-1814341620-520196398', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6267495f}
2022-04-06 17:03:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096:my-topic-1100718827-365191312 from pod my-cluster-52ad115a-kafka-clients-55c497949d-k9p6d
2022-04-06 17:03:47 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52ad115a-kafka-clients-55c497949d-k9p6d -n namespace-23 -- /opt/kafka/producer.sh USER=my_user_1814341620_520196398 --max-messages 100 --bootstrap-server my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096 --topic my-topic-1100718827-365191312
2022-04-06 17:03:51 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 17:03:51 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 17:03:51 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3893259b, messages=[], arguments=[--group-id, my-consumer-group-164087129, USER=my_user_1814341620_520196398, --max-messages, 100, --group-instance-id, instance1638531078, --bootstrap-server, my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096, --topic, my-topic-1100718827-365191312], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-52ad115a-kafka-clients-55c497949d-k9p6d', podNamespace='namespace-23', bootstrapServer='my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096', topicName='my-topic-1100718827-365191312', maxMessages=100, kafkaUsername='my-user-1814341620-520196398', consumerGroupName='my-consumer-group-164087129', consumerInstanceId='instance1638531078', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4832fadf}
2022-04-06 17:03:51 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096:my-topic-1100718827-365191312 from pod my-cluster-52ad115a-kafka-clients-55c497949d-k9p6d
2022-04-06 17:03:51 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52ad115a-kafka-clients-55c497949d-k9p6d -n namespace-23 -- /opt/kafka/consumer.sh --group-id my-consumer-group-164087129 USER=my_user_1814341620_520196398 --max-messages 100 --group-instance-id instance1638531078 --bootstrap-server my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096 --topic my-topic-1100718827-365191312
2022-04-06 17:03:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 17:03:58 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 17:03:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2213] Changing password in secret: my-cluster-52ad115a-secret, we should be able to send/receive messages
2022-04-06 17:03:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:171] Waiting for user password will be changed to Y29tcGxldGVseV9kaWZmZXJlbnRfc2VjcmV0X3Bhc3N3b3Jk in secret: my-user-1814341620-520196398
2022-04-06 17:04:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1285296438-1581276766 in namespace namespace-22
2022-04-06 17:04:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-db0348a4 in namespace namespace-22
2022-04-06 17:04:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:04:51 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-22 for test case:testSendMessagesTlsAuthenticated
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsAuthenticated-FINISHED
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-STARTED
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-24 for test case:testSendMessagesCustomListenerTlsScramSha
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-24
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-24
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-24
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6bf28409 in namespace namespace-24
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-06 17:05:18 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6bf28409 will have desired state: Ready
2022-04-06 17:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2222] We need to recreate Kafka Clients deployment, so the correct password from secret will be taken
2022-04-06 17:05:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-52ad115a-kafka-clients in namespace namespace-23
2022-04-06 17:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-52ad115a-kafka-clients in namespace namespace-23
2022-04-06 17:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-23
2022-04-06 17:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-52ad115a-kafka-clients will be ready
2022-04-06 17:06:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-52ad115a-kafka-clients is ready
2022-04-06 17:06:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2226] Receiving messages with new password
2022-04-06 17:06:10 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@14982601, messages=[], arguments=[--group-id, my-consumer-group-970609438, USER=my_user_1814341620_520196398, --max-messages, 100, --group-instance-id, instance661627779, --bootstrap-server, my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096, --topic, my-topic-1100718827-365191312], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-52ad115a-kafka-clients-6965db68d6-bbdbg', podNamespace='namespace-23', bootstrapServer='my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096', topicName='my-topic-1100718827-365191312', maxMessages=100, kafkaUsername='my-user-1814341620-520196398', consumerGroupName='my-consumer-group-970609438', consumerInstanceId='instance661627779', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4ba3c9a8}
2022-04-06 17:06:10 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096:my-topic-1100718827-365191312 from pod my-cluster-52ad115a-kafka-clients-6965db68d6-bbdbg
2022-04-06 17:06:10 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-52ad115a-kafka-clients-6965db68d6-bbdbg -n namespace-23 -- /opt/kafka/consumer.sh --group-id my-consumer-group-970609438 USER=my_user_1814341620_520196398 --max-messages 100 --group-instance-id instance661627779 --bootstrap-server my-cluster-52ad115a-kafka-bootstrap.namespace-23.svc:9096 --topic my-topic-1100718827-365191312
2022-04-06 17:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 17:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 17:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMessagesTlsScramShaWithPredefinedPassword
2022-04-06 17:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1100718827-365191312 in namespace namespace-23
2022-04-06 17:06:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-52ad115a-kafka-clients in namespace namespace-23
2022-04-06 17:06:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6bf28409 is in desired state: Ready
2022-04-06 17:06:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1089250715-2108470871 in namespace namespace-24
2022-04-06 17:06:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-06 17:06:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1089250715-2108470871 will have desired state: Ready
2022-04-06 17:06:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1089250715-2108470871 is in desired state: Ready
2022-04-06 17:06:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1565563424-555785438 in namespace namespace-24
2022-04-06 17:06:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-06 17:06:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1565563424-555785438 will have desired state: Ready
2022-04-06 17:06:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1565563424-555785438 is in desired state: Ready
2022-04-06 17:06:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6bf28409-kafka-clients in namespace namespace-24
2022-04-06 17:06:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-24
2022-04-06 17:06:33 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6bf28409-kafka-clients will be ready
2022-04-06 17:06:34 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6bf28409-kafka-clients is ready
2022-04-06 17:06:34 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:06:34 [ForkJoinPool-3-worker-5] [32mINFO [m [ListenersST:442] Checking produced and consumed messages to pod:my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv
2022-04-06 17:06:34 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@691deccb, messages=[], arguments=[USER=my_user_1565563424_555785438, --max-messages, 100, --bootstrap-server, my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122, --topic, my-topic-1089250715-2108470871], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv', podNamespace='namespace-24', bootstrapServer='my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122', topicName='my-topic-1089250715-2108470871', maxMessages=100, kafkaUsername='my-user-1565563424-555785438', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@26a72875}
2022-04-06 17:06:34 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122:my-topic-1089250715-2108470871 from pod my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv
2022-04-06 17:06:34 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv -n namespace-24 -- /opt/kafka/producer.sh USER=my_user_1565563424_555785438 --max-messages 100 --bootstrap-server my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122 --topic my-topic-1089250715-2108470871
2022-04-06 17:06:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 17:06:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 17:06:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7f49727e, messages=[], arguments=[--group-id, my-consumer-group-1453777888, USER=my_user_1565563424_555785438, --max-messages, 100, --group-instance-id, instance2086997597, --bootstrap-server, my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122, --topic, my-topic-1089250715-2108470871], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv', podNamespace='namespace-24', bootstrapServer='my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122', topicName='my-topic-1089250715-2108470871', maxMessages=100, kafkaUsername='my-user-1565563424-555785438', consumerGroupName='my-consumer-group-1453777888', consumerInstanceId='instance2086997597', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6bb74f0a}
2022-04-06 17:06:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122:my-topic-1089250715-2108470871 from pod my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv
2022-04-06 17:06:37 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6bf28409-kafka-clients-7cbc49bf57-2qkfv -n namespace-24 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1453777888 USER=my_user_1565563424_555785438 --max-messages 100 --group-instance-id instance2086997597 --bootstrap-server my-cluster-6bf28409-kafka-bootstrap.namespace-24.svc:9122 --topic my-topic-1089250715-2108470871
2022-04-06 17:06:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 17:06:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 17:06:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:06:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesCustomListenerTlsScramSha
2022-04-06 17:06:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1565563424-555785438 in namespace namespace-24
2022-04-06 17:06:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6bf28409-kafka-clients in namespace namespace-24
2022-04-06 17:07:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-52ad115a-kafka-clients in namespace namespace-23
2022-04-06 17:07:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1814341620-520196398 in namespace namespace-23
2022-04-06 17:07:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-52ad115a in namespace namespace-23
2022-04-06 17:07:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:07:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-23 for test case:testMessagesTlsScramShaWithPredefinedPassword
2022-04-06 17:07:35 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1089250715-2108470871 in namespace namespace-24
2022-04-06 17:07:45 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6bf28409 in namespace namespace-24
2022-04-06 17:07:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:07:55 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-24 for test case:testSendMessagesCustomListenerTlsScramSha
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testMessagesTlsScramShaWithPredefinedPassword-FINISHED
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testAdvertisedHostNamesAppearsInBrokerCerts-STARTED
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-25 for test case:testAdvertisedHostNamesAppearsInBrokerCerts
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-25
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-25
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-25
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b532defb in namespace namespace-25
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-25
2022-04-06 17:08:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b532defb will have desired state: Ready
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesCustomListenerTlsScramSha-FINISHED
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataCrt-STARTED
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-26 for test case:testCertificateWithNonExistingDataCrt
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-26
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-26
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-26
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-2a1fa77e-custom-certificate-server-1
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:50] Secret my-cluster-2a1fa77e-custom-certificate-server-1 created
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2a1fa77e in namespace namespace-26
2022-04-06 17:08:38 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-26
2022-04-06 17:09:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:09:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCertificateWithNonExistingDataCrt
2022-04-06 17:09:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2a1fa77e in namespace namespace-26
2022-04-06 17:09:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:09:13 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-26 for test case:testCertificateWithNonExistingDataCrt
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataCrt-FINISHED
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataKey-STARTED
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-27 for test case:testCertificateWithNonExistingDataKey
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-27
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-27
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-27
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-606ddec4-custom-certificate-server-1
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [SecretUtils:50] Secret my-cluster-606ddec4-custom-certificate-server-1 created
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-606ddec4 in namespace namespace-27
2022-04-06 17:09:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-27
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b532defb is in desired state: Ready
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2338] Encoding my-cluster-b532defb-kafka-0.crt
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2338] Encoding my-cluster-b532defb-kafka-1.crt
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:2338] Encoding my-cluster-b532defb-kafka-2.crt
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAdvertisedHostNamesAppearsInBrokerCerts
2022-04-06 17:09:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b532defb in namespace namespace-25
2022-04-06 17:09:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:09:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-25 for test case:testAdvertisedHostNamesAppearsInBrokerCerts
2022-04-06 17:09:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:09:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testCertificateWithNonExistingDataKey
2022-04-06 17:09:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-606ddec4 in namespace namespace-27
2022-04-06 17:09:53 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:09:53 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-27 for test case:testCertificateWithNonExistingDataKey
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testCertificateWithNonExistingDataKey-FINISHED
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainAnonymous-STARTED
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-28 for test case:testSendMessagesPlainAnonymous
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-28
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-28
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-28
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7e1fddd9 in namespace namespace-28
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-06 17:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7e1fddd9 will have desired state: Ready
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testAdvertisedHostNamesAppearsInBrokerCerts-FINISHED
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testNonExistingCustomCertificate-STARTED
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-29 for test case:testNonExistingCustomCertificate
2022-04-06 17:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-29
2022-04-06 17:10:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-29
2022-04-06 17:10:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-29
2022-04-06 17:10:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-03ac6738 in namespace namespace-29
2022-04-06 17:10:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-29
2022-04-06 17:10:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:10:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testNonExistingCustomCertificate
2022-04-06 17:10:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-03ac6738 in namespace namespace-29
2022-04-06 17:10:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:10:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-29 for test case:testNonExistingCustomCertificate
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testNonExistingCustomCertificate-FINISHED
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-STARTED
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-30 for test case:testSendMessagesTlsScramSha
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-30
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-30
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-30
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-106a3597 in namespace namespace-30
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-30
2022-04-06 17:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-106a3597 will have desired state: Ready
2022-04-06 17:11:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7e1fddd9 is in desired state: Ready
2022-04-06 17:11:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-746613947-1054682436 in namespace namespace-30
2022-04-06 17:11:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-06 17:11:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-746613947-1054682436 will have desired state: Ready
2022-04-06 17:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-746613947-1054682436 is in desired state: Ready
2022-04-06 17:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7e1fddd9-kafka-clients in namespace namespace-30
2022-04-06 17:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-28
2022-04-06 17:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7e1fddd9-kafka-clients will be ready
2022-04-06 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7e1fddd9-kafka-clients is ready
2022-04-06 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [ListenersST:152] Checking produced and consumed messages to pod:my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj
2022-04-06 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@740ad1d6, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092, --topic, my-topic-746613947-1054682436], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj', podNamespace='namespace-28', bootstrapServer='my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092', topicName='my-topic-746613947-1054682436', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@183bb248}
2022-04-06 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092:my-topic-746613947-1054682436 from pod my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj
2022-04-06 17:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj -n namespace-28 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092 --topic my-topic-746613947-1054682436
2022-04-06 17:11:32 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 17:11:32 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 17:11:32 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@60ec63a4, messages=[], arguments=[--group-id, my-consumer-group-180930887, --max-messages, 100, --group-instance-id, instance1267531899, --bootstrap-server, my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092, --topic, my-topic-746613947-1054682436], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj', podNamespace='namespace-28', bootstrapServer='my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092', topicName='my-topic-746613947-1054682436', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-180930887', consumerInstanceId='instance1267531899', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@55ee10fb}
2022-04-06 17:11:32 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092#my-topic-746613947-1054682436 from pod my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj
2022-04-06 17:11:32 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-7e1fddd9-kafka-clients-6b47b558b9-gdwtj -n namespace-28 -- /opt/kafka/consumer.sh --group-id my-consumer-group-180930887 --max-messages 100 --group-instance-id instance1267531899 --bootstrap-server my-cluster-7e1fddd9-kafka-bootstrap.namespace-28.svc:9092 --topic my-topic-746613947-1054682436
2022-04-06 17:11:38 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 17:11:39 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 17:11:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:11:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesPlainAnonymous
2022-04-06 17:11:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-746613947-1054682436 in namespace namespace-28
2022-04-06 17:11:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7e1fddd9-kafka-clients in namespace namespace-28
2022-04-06 17:12:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-106a3597 is in desired state: Ready
2022-04-06 17:12:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1799695187-1533367430 in namespace namespace-30
2022-04-06 17:12:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-30
2022-04-06 17:12:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1799695187-1533367430 will have desired state: Ready
2022-04-06 17:12:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1799695187-1533367430 is in desired state: Ready
2022-04-06 17:12:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1770297514-1869761941 in namespace namespace-30
2022-04-06 17:12:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-30
2022-04-06 17:12:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1770297514-1869761941 will have desired state: Ready
2022-04-06 17:12:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1770297514-1869761941 is in desired state: Ready
2022-04-06 17:12:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-106a3597-kafka-clients in namespace namespace-30
2022-04-06 17:12:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-30
2022-04-06 17:12:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-106a3597-kafka-clients will be ready
2022-04-06 17:12:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-106a3597-kafka-clients is ready
2022-04-06 17:12:19 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:12:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:370] Checking produced and consumed messages to pod:my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm
2022-04-06 17:12:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@922ef11, messages=[], arguments=[USER=my_user_1770297514_1869761941, --max-messages, 100, --bootstrap-server, my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096, --topic, my-topic-1799695187-1533367430], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm', podNamespace='namespace-30', bootstrapServer='my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096', topicName='my-topic-1799695187-1533367430', maxMessages=100, kafkaUsername='my-user-1770297514-1869761941', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2bb2b70}
2022-04-06 17:12:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096:my-topic-1799695187-1533367430 from pod my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm
2022-04-06 17:12:19 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm -n namespace-30 -- /opt/kafka/producer.sh USER=my_user_1770297514_1869761941 --max-messages 100 --bootstrap-server my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096 --topic my-topic-1799695187-1533367430
2022-04-06 17:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 17:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 17:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3059d969, messages=[], arguments=[--group-id, my-consumer-group-15471620, USER=my_user_1770297514_1869761941, --max-messages, 100, --group-instance-id, instance422061029, --bootstrap-server, my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096, --topic, my-topic-1799695187-1533367430], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm', podNamespace='namespace-30', bootstrapServer='my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096', topicName='my-topic-1799695187-1533367430', maxMessages=100, kafkaUsername='my-user-1770297514-1869761941', consumerGroupName='my-consumer-group-15471620', consumerInstanceId='instance422061029', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@149b4a43}
2022-04-06 17:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096:my-topic-1799695187-1533367430 from pod my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm
2022-04-06 17:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-106a3597-kafka-clients-84c5dbdb8f-sjslm -n namespace-30 -- /opt/kafka/consumer.sh --group-id my-consumer-group-15471620 USER=my_user_1770297514_1869761941 --max-messages 100 --group-instance-id instance422061029 --bootstrap-server my-cluster-106a3597-kafka-bootstrap.namespace-30.svc:9096 --topic my-topic-1799695187-1533367430
2022-04-06 17:12:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7e1fddd9 in namespace namespace-28
2022-04-06 17:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 17:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 17:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ListenersST:377] Checking if generated password has 25 characters
2022-04-06 17:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesTlsScramSha
2022-04-06 17:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1770297514-1869761941 in namespace namespace-30
2022-04-06 17:12:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:12:39 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-28 for test case:testSendMessagesPlainAnonymous
2022-04-06 17:12:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-106a3597-kafka-clients in namespace namespace-30
2022-04-06 17:12:44 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:12:44 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainScramSha-STARTED
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-31 for test case:testSendMessagesPlainScramSha
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-31
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-31
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-31
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d637c4aa in namespace namespace-31
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-31
2022-04-06 17:12:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d637c4aa will have desired state: Ready
2022-04-06 17:12:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1799695187-1533367430 in namespace namespace-30
2022-04-06 17:13:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-106a3597 in namespace namespace-30
2022-04-06 17:13:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:13:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-30 for test case:testSendMessagesTlsScramSha
2022-04-06 17:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainAnonymous-FINISHED
2022-04-06 17:13:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:13:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesTlsScramSha-FINISHED
2022-04-06 17:13:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d637c4aa is in desired state: Ready
2022-04-06 17:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-525803543-932466287 in namespace namespace-31
2022-04-06 17:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-31
2022-04-06 17:14:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-525803543-932466287 will have desired state: Ready
2022-04-06 17:14:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-525803543-932466287 is in desired state: Ready
2022-04-06 17:14:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1205932357-1322726748 in namespace namespace-31
2022-04-06 17:14:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-31
2022-04-06 17:14:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1205932357-1322726748 will have desired state: Ready
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1205932357-1322726748 is in desired state: Ready
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,415 INFO Processing override for entityPath: users/my-user-1205932357-1322726748 with config: HashMap(SCRAM-SHA-512 -> [hidden]) (kafka.server.DynamicConfigManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,419 INFO Removing PRODUCE quota for user my-user-1205932357-1322726748 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,422 INFO Removing FETCH quota for user my-user-1205932357-1322726748 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,423 INFO Removing REQUEST quota for user my-user-1205932357-1322726748 (kafka.server.ClientRequestQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,423 INFO Removing CONTROLLER_MUTATION quota for user my-user-1205932357-1322726748 (kafka.server.ControllerMutationQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,640 INFO Processing override for entityPath: users/my-user-1205932357-1322726748 with config: HashMap(SCRAM-SHA-512 -> [hidden]) (kafka.server.DynamicConfigManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,640 INFO Removing PRODUCE quota for user my-user-1205932357-1322726748 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,640 INFO Removing FETCH quota for user my-user-1205932357-1322726748 (kafka.server.ClientQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,640 INFO Removing REQUEST quota for user my-user-1205932357-1322726748 (kafka.server.ClientRequestQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:273] Broker pod log line about user my-user-1205932357-1322726748: 2022-04-06 17:14:15,640 INFO Removing CONTROLLER_MUTATION quota for user my-user-1205932357-1322726748 (kafka.server.ControllerMutationQuotaManager) [/config/changes-event-process-thread]
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d637c4aa-kafka-clients in namespace namespace-31
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-31
2022-04-06 17:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d637c4aa-kafka-clients will be ready
2022-04-06 17:14:18 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d637c4aa-kafka-clients is ready
2022-04-06 17:14:18 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:14:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ListenersST:296] Checking produced and consumed messages to pod:my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8
2022-04-06 17:14:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@391f8e12, messages=[], arguments=[USER=my_user_1205932357_1322726748, --max-messages, 100, --bootstrap-server, my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095, --topic, my-topic-525803543-932466287], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8', podNamespace='namespace-31', bootstrapServer='my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095', topicName='my-topic-525803543-932466287', maxMessages=100, kafkaUsername='my-user-1205932357-1322726748', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a15d4b5}
2022-04-06 17:14:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095:my-topic-525803543-932466287 from pod my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8
2022-04-06 17:14:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8 -n namespace-31 -- /opt/kafka/producer.sh USER=my_user_1205932357_1322726748 --max-messages 100 --bootstrap-server my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095 --topic my-topic-525803543-932466287
2022-04-06 17:14:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 17:14:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 17:14:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5cabc007, messages=[], arguments=[--group-id, my-consumer-group-363500184, USER=my_user_1205932357_1322726748, --max-messages, 100, --group-instance-id, instance1514648565, --bootstrap-server, my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095, --topic, my-topic-525803543-932466287], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8', podNamespace='namespace-31', bootstrapServer='my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095', topicName='my-topic-525803543-932466287', maxMessages=100, kafkaUsername='my-user-1205932357-1322726748', consumerGroupName='my-consumer-group-363500184', consumerInstanceId='instance1514648565', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@26dd0199}
2022-04-06 17:14:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095#my-topic-525803543-932466287 from pod my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8
2022-04-06 17:14:20 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d637c4aa-kafka-clients-77db78fb99-p68d8 -n namespace-31 -- /opt/kafka/consumer.sh --group-id my-consumer-group-363500184 USER=my_user_1205932357_1322726748 --max-messages 100 --group-instance-id instance1514648565 --bootstrap-server my-cluster-d637c4aa-kafka-bootstrap.namespace-31.svc:9095 --topic my-topic-525803543-932466287
2022-04-06 17:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 17:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 17:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testSendMessagesPlainScramSha
2022-04-06 17:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1205932357-1322726748 in namespace namespace-31
2022-04-06 17:14:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-525803543-932466287 in namespace namespace-31
2022-04-06 17:14:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d637c4aa-kafka-clients in namespace namespace-31
2022-04-06 17:14:36 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d637c4aa in namespace namespace-31
2022-04-06 17:15:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:15:26 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-31 for test case:testSendMessagesPlainScramSha
2022-04-06 17:15:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.ListenersST.testSendMessagesPlainScramSha-FINISHED
2022-04-06 17:15:32 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:15:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:15:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ListenersST is everything deleted.
2022-04-06 17:15:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 821.361 s - in io.strimzi.systemtest.kafka.listeners.ListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.listeners.MultipleListenersST
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:294] Starting to generate test cases for multiple listeners
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating INTERNAL listener
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type INTERNAL -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@5ac385c0, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@a91b93ce]
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating ROUTE listener
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type ROUTE -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@8dc09f69]
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating LOADBALANCER listener
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type LOADBALANCER -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@bbf4c2d, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@5a175a3b]
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating NODEPORT listener
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type NODEPORT -> [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@fc7a8063, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@4ad28e71, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@8c2a7a8d, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@cd8266a9]
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:300] Generating INGRESS listener
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:363] Generating listeners with type INGRESS -> []
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:367] Finished with generation of test cases for multiple listeners
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: multiple-listeners-st
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-listeners-st
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: multiple-listeners-st
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.listeners.MultipleListenersST.testMultipleInternal-STARTED
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:163] This is listeners [io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@5ac385c0, io.strimzi.api.kafka.model.listener.arraylistener.GenericKafkaListener@a91b93ce], which will verified.
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-87e8ecec in namespace multiple-listeners-st
2022-04-06 17:15:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-87e8ecec will have desired state: Ready
2022-04-06 17:16:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-87e8ecec is in desired state: Ready
2022-04-06 17:16:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-900119952-321411128 in namespace multiple-listeners-st
2022-04-06 17:16:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-900119952-321411128 will have desired state: Ready
2022-04-06 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-900119952-321411128 is in desired state: Ready
2022-04-06 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-938998480-689147074 in namespace multiple-listeners-st
2022-04-06 17:16:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-938998480-689147074 will have desired state: Ready
2022-04-06 17:16:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-938998480-689147074 is in desired state: Ready
2022-04-06 17:16:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-87e8ecec-kafka-clients-tls in namespace multiple-listeners-st
2022-04-06 17:16:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-87e8ecec-kafka-clients-tls will be ready
2022-04-06 17:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-87e8ecec-kafka-clients-tls is ready
2022-04-06 17:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:252] Checking produced and consumed messages to pod:my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82
2022-04-06 17:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1618465909-1248239816 in namespace multiple-listeners-st
2022-04-06 17:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1618465909-1248239816 will have desired state: Ready
2022-04-06 17:16:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1618465909-1248239816 is in desired state: Ready
2022-04-06 17:16:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:127] Sending messages to - topic my-topic-938998480-689147074, cluster my-cluster-87e8ecec and message count of 100
2022-04-06 17:16:53 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@13d1a3d1, messages=[], arguments=[USER=my_user_900119952_321411128, --max-messages, 100, --bootstrap-server, my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900, --topic, my-topic-1618465909-1248239816], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900', topicName='my-topic-1618465909-1248239816', maxMessages=100, kafkaUsername='my-user-900119952-321411128', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2c82c56f}
2022-04-06 17:16:53 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900:my-topic-1618465909-1248239816 from pod my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82
2022-04-06 17:16:53 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82 -n multiple-listeners-st -- /opt/kafka/producer.sh USER=my_user_900119952_321411128 --max-messages 100 --bootstrap-server my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900 --topic my-topic-1618465909-1248239816
2022-04-06 17:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 17:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 17:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2b64527b, messages=[], arguments=[--group-id, my-consumer-group-595960866, USER=my_user_900119952_321411128, --max-messages, 100, --group-instance-id, instance95770426, --bootstrap-server, my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900, --topic, my-topic-1618465909-1248239816], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900', topicName='my-topic-1618465909-1248239816', maxMessages=100, kafkaUsername='my-user-900119952-321411128', consumerGroupName='my-consumer-group-595960866', consumerInstanceId='instance95770426', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11775487}
2022-04-06 17:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900:my-topic-1618465909-1248239816 from pod my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82
2022-04-06 17:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-87e8ecec-kafka-clients-tls-59cb6d8dd9-hnq82 -n multiple-listeners-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-595960866 USER=my_user_900119952_321411128 --max-messages 100 --group-instance-id instance95770426 --bootstrap-server my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13900 --topic my-topic-1618465909-1248239816
2022-04-06 17:17:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 17:17:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 17:17:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:133] Sent 100 and received 100
2022-04-06 17:17:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-399879584-1532739506 in namespace multiple-listeners-st
2022-04-06 17:17:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-399879584-1532739506 will have desired state: Ready
2022-04-06 17:17:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-399879584-1532739506 is in desired state: Ready
2022-04-06 17:17:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-87e8ecec-kafka-clients-plain in namespace multiple-listeners-st
2022-04-06 17:17:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-87e8ecec-kafka-clients-plain will be ready
2022-04-06 17:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-87e8ecec-kafka-clients-plain is ready
2022-04-06 17:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleListenersST:274] Checking produced and consumed messages to pod:my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw
2022-04-06 17:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@13c6f910, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901, --topic, my-topic-399879584-1532739506], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901', topicName='my-topic-399879584-1532739506', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3535b3ed}
2022-04-06 17:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901:my-topic-399879584-1532739506 from pod my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw
2022-04-06 17:17:06 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw -n multiple-listeners-st -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901 --topic my-topic-399879584-1532739506
2022-04-06 17:17:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 17:17:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 17:17:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@31c03e75, messages=[], arguments=[--group-id, my-consumer-group-575123548, --max-messages, 100, --group-instance-id, instance1165103021, --bootstrap-server, my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901, --topic, my-topic-399879584-1532739506], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw', podNamespace='multiple-listeners-st', bootstrapServer='my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901', topicName='my-topic-399879584-1532739506', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-575123548', consumerInstanceId='instance1165103021', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@383e5483}
2022-04-06 17:17:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901#my-topic-399879584-1532739506 from pod my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw
2022-04-06 17:17:09 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-87e8ecec-kafka-clients-plain-5cc58f7bcc-2dlgw -n multiple-listeners-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-575123548 --max-messages 100 --group-instance-id instance1165103021 --bootstrap-server my-cluster-87e8ecec-kafka-bootstrap.multiple-listeners-st.svc:13901 --topic my-topic-399879584-1532739506
2022-04-06 17:17:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 17:17:14 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 17:17:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:17:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMultipleInternal
2022-04-06 17:17:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1618465909-1248239816 in namespace multiple-listeners-st
2022-04-06 17:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-900119952-321411128 in namespace multiple-listeners-st
2022-04-06 17:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-938998480-689147074 in namespace multiple-listeners-st
2022-04-06 17:17:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-87e8ecec-kafka-clients-tls in namespace multiple-listeners-st
2022-04-06 17:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-87e8ecec in namespace multiple-listeners-st
2022-04-06 17:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-87e8ecec-kafka-clients-plain in namespace multiple-listeners-st
2022-04-06 17:18:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-399879584-1532739506 in namespace multiple-listeners-st
2022-04-06 17:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.listeners.MultipleListenersST.testMultipleInternal-FINISHED
2022-04-06 17:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MultipleListenersST is everything deleted.
2022-04-06 17:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 172.599 s - in io.strimzi.systemtest.kafka.listeners.MultipleListenersST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: dynamic-conf-st
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: dynamic-conf-st
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: dynamic-conf-st
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testSimpleDynamicConfiguration-STARTED
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-243512a5 in namespace dynamic-conf-st
2022-04-06 17:18:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-243512a5 will have desired state: Ready
2022-04-06 17:19:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-243512a5 is in desired state: Ready
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-243512a5-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-243512a5-kafka are stable
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:19:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:19:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:19:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:19:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:19:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:19:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:19:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:19:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:19:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:19:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:19:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:19:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:19:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:19:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:19:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:19:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:19:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:19:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:19:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:19:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:19:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:19:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:19:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:19:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:19:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:19:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:19:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:19:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:20:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:20:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:20:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:20:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:20:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:20:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:20:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:20:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:20:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:20:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:20:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:20:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:20:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:20:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:20:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:20:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:20:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:20:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:20:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:20:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:20:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:20:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:20:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:20:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:20:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:20:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:20:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:20:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:20:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:20:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:20:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:20:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:20:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:20:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:20:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:20:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:20:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:20:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:20:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:20:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:20:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:20:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:20:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:20:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:20:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:20:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:20:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:20:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:20:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:20:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:20:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:20:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:20:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:20:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-243512a5-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:20:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-243512a5-kafka-0 ,my-cluster-243512a5-kafka-1 ,my-cluster-243512a5-kafka-2
2022-04-06 17:20:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-243512a5-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:20:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:20:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:102] Verify values after update
2022-04-06 17:20:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:20:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSimpleDynamicConfiguration
2022-04-06 17:20:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-243512a5 in namespace dynamic-conf-st
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testSimpleDynamicConfiguration-FINISHED
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testUpdateToExternalListenerCausesRollingRestart-STARTED
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dfd70ecf in namespace dynamic-conf-st
2022-04-06 17:20:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfd70ecf will have desired state: Ready
2022-04-06 17:21:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfd70ecf is in desired state: Ready
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-dfd70ecf-kafka are stable
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:22:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:22:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:22:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:22:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:22:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:22:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:22:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:22:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:22:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:22:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:22:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:22:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:22:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:22:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:22:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:22:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:22:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:22:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:22:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:22:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:22:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:22:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:22:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:22:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:22:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:22:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:22:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:22:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:22:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:22:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:22:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:22:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:22:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:22:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:22:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:22:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:22:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:22:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:22:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:22:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:22:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:22:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:22:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:22:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:22:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:22:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:22:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:22:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:22:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:22:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:22:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:22:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:22:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:22:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:22:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:22:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:22:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:22:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:22:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:22:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:22:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:22:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:22:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:22:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:22:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:22:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:22:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:22:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:22:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:22:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:22:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:22:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:22:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:22:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:22:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:22:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:22:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:22:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:22:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:22:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:22:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:22:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:22:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:22:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:22:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:22:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:22:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:22:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:22:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:22:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:22:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:22:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:22:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:22:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:22:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:22:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:22:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:22:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:22:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:22:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-dfd70ecf-kafka-0 ,my-cluster-dfd70ecf-kafka-1 ,my-cluster-dfd70ecf-kafka-2
2022-04-06 17:22:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:22:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:22:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:163] Updating listeners of Kafka cluster
2022-04-06 17:22:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-dfd70ecf-kafka rolling update
2022-04-06 17:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-dfd70ecf-kafka has been successfully rolled
2022-04-06 17:23:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-dfd70ecf-kafka to be ready
2022-04-06 17:24:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfd70ecf will have desired state: Ready
2022-04-06 17:24:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfd70ecf is in desired state: Ready
2022-04-06 17:24:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-dfd70ecf is ready
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-dfd70ecf-kafka are stable
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:24:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:24:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:24:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:24:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:24:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:24:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:24:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:24:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:24:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:24:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:24:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:24:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:24:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:24:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:24:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:24:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:24:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:24:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:24:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:24:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:24:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:24:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:24:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:24:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:24:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:24:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:24:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:24:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:24:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:24:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:24:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:24:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:24:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:24:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:24:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:24:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:24:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:24:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:24:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:24:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:24:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:24:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:24:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:24:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:24:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:24:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:24:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:24:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:24:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:24:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:24:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:24:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:24:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:24:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:24:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:24:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:24:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:25:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:25:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:25:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:25:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:25:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:25:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:25:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:25:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:25:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:25:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:25:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:25:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:25:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:25:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:25:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:25:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:25:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:25:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:25:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:25:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:25:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:25:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:25:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:25:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:25:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:25:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:25:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:25:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:25:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:25:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:25:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:25:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:25:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:25:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:25:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:25:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:25:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:25:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:25:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:25:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:25:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:25:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:25:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:25:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:25:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-dfd70ecf-kafka-0 ,my-cluster-dfd70ecf-kafka-1 ,my-cluster-dfd70ecf-kafka-2
2022-04-06 17:25:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:25:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-dfd70ecf-kafka are stable
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:25:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:25:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:25:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:25:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:25:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:25:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:25:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:25:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:25:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:25:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:25:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:25:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:25:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:25:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:25:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:25:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:25:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:25:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:25:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:25:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:25:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:25:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:25:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:25:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:25:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:25:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:25:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:25:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:25:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:25:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:25:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:25:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:25:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:25:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:25:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:25:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:25:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:25:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:25:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:25:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:25:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:25:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:25:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:25:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:25:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:25:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:25:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:25:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:25:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:25:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:25:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:25:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:25:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:25:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:25:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:25:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:25:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:25:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:25:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:25:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:25:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:25:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:25:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:25:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:25:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:26:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:26:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:26:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:26:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:26:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:26:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:26:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:26:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:26:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:26:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:26:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:26:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:26:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:26:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:26:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:26:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:26:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:26:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:26:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:26:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:26:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:26:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:26:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:26:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:26:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:26:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:26:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:26:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:26:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:26:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:26:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:26:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:26:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:26:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:26:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-dfd70ecf-kafka-0 ,my-cluster-dfd70ecf-kafka-1 ,my-cluster-dfd70ecf-kafka-2
2022-04-06 17:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:214] Updating listeners of Kafka cluster
2022-04-06 17:26:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-dfd70ecf-kafka rolling update
2022-04-06 17:27:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-dfd70ecf-kafka has been successfully rolled
2022-04-06 17:27:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-dfd70ecf-kafka to be ready
2022-04-06 17:27:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfd70ecf will have desired state: Ready
2022-04-06 17:27:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfd70ecf is in desired state: Ready
2022-04-06 17:27:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-dfd70ecf is ready
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfST:380] Updating configuration of Kafka cluster
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-dfd70ecf-kafka are stable
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:28:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 17:28:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:28:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:28:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 17:28:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:28:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:28:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 17:28:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:28:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:28:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 17:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 17:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 17:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 17:28:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:28:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:28:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 17:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:28:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 17:28:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:28:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:28:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 17:28:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:28:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:28:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 17:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:28:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 17:28:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:28:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:28:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 17:28:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:28:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:28:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 17:28:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:28:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:28:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 17:28:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:28:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:28:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 17:28:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:28:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:28:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 17:28:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:28:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:28:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 17:28:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:28:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:28:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 17:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 17:28:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:28:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:28:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 17:28:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:28:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:28:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 17:28:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:28:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:28:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 17:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 17:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 17:28:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:28:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:28:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 17:28:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:28:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:28:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 17:28:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:28:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:28:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 17:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:28:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 17:28:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:28:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:28:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:28:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 17:28:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:28:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:28:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 17:28:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:28:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:28:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 17:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 17:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:28:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 17:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:28:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 17:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:28:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 17:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:28:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 17:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:28:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 17:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:28:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 17:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 17:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:28:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 17:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 17:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 17:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:28:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 17:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:28:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 17:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:28:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 17:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:28:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 17:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:28:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 17:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-dfd70ecf-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 17:28:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-dfd70ecf-kafka-0 ,my-cluster-dfd70ecf-kafka-1 ,my-cluster-dfd70ecf-kafka-2
2022-04-06 17:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-st exec my-cluster-dfd70ecf-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateToExternalListenerCausesRollingRestart
2022-04-06 17:28:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-dfd70ecf in namespace dynamic-conf-st
2022-04-06 17:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST.testUpdateToExternalListenerCausesRollingRestart-FINISHED
2022-04-06 17:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context DynamicConfST is everything deleted.
2022-04-06 17:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 660.021 s - in io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST
2022-04-06 17:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: dynamic-conf-shared-st
2022-04-06 17:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: dynamic-conf-shared-st
2022-04-06 17:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: dynamic-conf-shared-st
2022-04-06 17:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DynamicConfSharedST:218] Deploying shared Kafka across all test cases!
2022-04-06 17:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka dynamic-configuration-shared-cluster-name in namespace dynamic-conf-shared-st
2022-04-06 17:29:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: dynamic-configuration-shared-cluster-name will have desired state: Ready
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: dynamic-configuration-shared-cluster-name is in desired state: Ready
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST.testDynConfiguration-STARTED
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:334] Kafka config {advertised.listeners=io.strimzi.kafka.config.model.ConfigModel@4f746b02, alter.config.policy.class.name=io.strimzi.kafka.config.model.ConfigModel@61299f65, alter.log.dirs.replication.quota.window.num=io.strimzi.kafka.config.model.ConfigModel@1e12c460, alter.log.dirs.replication.quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@531a340b, authorizer.class.name=io.strimzi.kafka.config.model.ConfigModel@3f5a6911, auto.create.topics.enable=io.strimzi.kafka.config.model.ConfigModel@12a50b28, auto.leader.rebalance.enable=io.strimzi.kafka.config.model.ConfigModel@1a9cfcd8, background.threads=io.strimzi.kafka.config.model.ConfigModel@18a429ec, broker.heartbeat.interval.ms=io.strimzi.kafka.config.model.ConfigModel@745ac908, broker.id=io.strimzi.kafka.config.model.ConfigModel@4e9c70e0, broker.id.generation.enable=io.strimzi.kafka.config.model.ConfigModel@331b3fd7, broker.rack=io.strimzi.kafka.config.model.ConfigModel@a498b11, broker.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@28f911d9, client.quota.callback.class=io.strimzi.kafka.config.model.ConfigModel@54f6f56, compression.type=io.strimzi.kafka.config.model.ConfigModel@6fc00daf, connection.failed.authentication.delay.ms=io.strimzi.kafka.config.model.ConfigModel@29971245, connections.max.idle.ms=io.strimzi.kafka.config.model.ConfigModel@4980b977, connections.max.reauth.ms=io.strimzi.kafka.config.model.ConfigModel@37cb5ace, control.plane.listener.name=io.strimzi.kafka.config.model.ConfigModel@580984ed, controlled.shutdown.enable=io.strimzi.kafka.config.model.ConfigModel@499db093, controlled.shutdown.max.retries=io.strimzi.kafka.config.model.ConfigModel@59ae3d6, controlled.shutdown.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@68eac481, controller.listener.names=io.strimzi.kafka.config.model.ConfigModel@7cd01b9d, controller.quorum.append.linger.ms=io.strimzi.kafka.config.model.ConfigModel@58ed35bc, controller.quorum.election.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@a3bf924, controller.quorum.election.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@344bea2e, controller.quorum.fetch.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@483b13c9, controller.quorum.request.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@79ca8fa7, controller.quorum.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@2c6dbb63, controller.quota.window.num=io.strimzi.kafka.config.model.ConfigModel@471c2e5a, controller.quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@5729c82b, controller.socket.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@7a33c2de, create.topic.policy.class.name=io.strimzi.kafka.config.model.ConfigModel@3b44c858, default.replication.factor=io.strimzi.kafka.config.model.ConfigModel@1d0c5f94, delegation.token.expiry.check.interval.ms=io.strimzi.kafka.config.model.ConfigModel@47da48e, delegation.token.expiry.time.ms=io.strimzi.kafka.config.model.ConfigModel@603cb3ed, delegation.token.master.key=io.strimzi.kafka.config.model.ConfigModel@7d27b1ba, delegation.token.max.lifetime.ms=io.strimzi.kafka.config.model.ConfigModel@2a8b175, delegation.token.secret.key=io.strimzi.kafka.config.model.ConfigModel@6b183c7a, delete.records.purgatory.purge.interval.requests=io.strimzi.kafka.config.model.ConfigModel@6e3e592f, delete.topic.enable=io.strimzi.kafka.config.model.ConfigModel@2ed49f33, fetch.max.bytes=io.strimzi.kafka.config.model.ConfigModel@69b0a085, fetch.purgatory.purge.interval.requests=io.strimzi.kafka.config.model.ConfigModel@4361ba25, group.initial.rebalance.delay.ms=io.strimzi.kafka.config.model.ConfigModel@5f0a6447, group.max.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@55ad5592, group.max.size=io.strimzi.kafka.config.model.ConfigModel@39f5632a, group.min.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@770b56a3, initial.broker.registration.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@7baf3c59, inter.broker.listener.name=io.strimzi.kafka.config.model.ConfigModel@4491f77b, inter.broker.protocol.version=io.strimzi.kafka.config.model.ConfigModel@566f20f0, kafka.metrics.polling.interval.secs=io.strimzi.kafka.config.model.ConfigModel@7b7cdad0, kafka.metrics.reporters=io.strimzi.kafka.config.model.ConfigModel@79337c12, leader.imbalance.check.interval.seconds=io.strimzi.kafka.config.model.ConfigModel@32d26660, leader.imbalance.per.broker.percentage=io.strimzi.kafka.config.model.ConfigModel@1d95a86a, listener.security.protocol.map=io.strimzi.kafka.config.model.ConfigModel@225b52, listeners=io.strimzi.kafka.config.model.ConfigModel@629364a8, log.cleaner.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@43d3effc, log.cleaner.dedupe.buffer.size=io.strimzi.kafka.config.model.ConfigModel@73b5d914, log.cleaner.delete.retention.ms=io.strimzi.kafka.config.model.ConfigModel@615fe4fb, log.cleaner.enable=io.strimzi.kafka.config.model.ConfigModel@18107a5c, log.cleaner.io.buffer.load.factor=io.strimzi.kafka.config.model.ConfigModel@44e0caad, log.cleaner.io.buffer.size=io.strimzi.kafka.config.model.ConfigModel@1940d65c, log.cleaner.io.max.bytes.per.second=io.strimzi.kafka.config.model.ConfigModel@7a691b3, log.cleaner.max.compaction.lag.ms=io.strimzi.kafka.config.model.ConfigModel@1c7fd981, log.cleaner.min.cleanable.ratio=io.strimzi.kafka.config.model.ConfigModel@5e08dd01, log.cleaner.min.compaction.lag.ms=io.strimzi.kafka.config.model.ConfigModel@41e512aa, log.cleaner.threads=io.strimzi.kafka.config.model.ConfigModel@727d382c, log.cleanup.policy=io.strimzi.kafka.config.model.ConfigModel@10ac4433, log.dir=io.strimzi.kafka.config.model.ConfigModel@85463f2, log.dirs=io.strimzi.kafka.config.model.ConfigModel@614d4010, log.flush.interval.messages=io.strimzi.kafka.config.model.ConfigModel@7aeab84d, log.flush.interval.ms=io.strimzi.kafka.config.model.ConfigModel@895f6ff, log.flush.offset.checkpoint.interval.ms=io.strimzi.kafka.config.model.ConfigModel@23f54fec, log.flush.scheduler.interval.ms=io.strimzi.kafka.config.model.ConfigModel@2833c642, log.flush.start.offset.checkpoint.interval.ms=io.strimzi.kafka.config.model.ConfigModel@7b42bd9, log.index.interval.bytes=io.strimzi.kafka.config.model.ConfigModel@203107de, log.index.size.max.bytes=io.strimzi.kafka.config.model.ConfigModel@2932b61b, log.message.downconversion.enable=io.strimzi.kafka.config.model.ConfigModel@1f0564a9, log.message.format.version=io.strimzi.kafka.config.model.ConfigModel@39496e73, log.message.timestamp.difference.max.ms=io.strimzi.kafka.config.model.ConfigModel@116e8173, log.message.timestamp.type=io.strimzi.kafka.config.model.ConfigModel@2d507f2b, log.preallocate=io.strimzi.kafka.config.model.ConfigModel@19447604, log.retention.bytes=io.strimzi.kafka.config.model.ConfigModel@6b8cb9a9, log.retention.check.interval.ms=io.strimzi.kafka.config.model.ConfigModel@e0cd8b2, log.retention.hours=io.strimzi.kafka.config.model.ConfigModel@200b577f, log.retention.minutes=io.strimzi.kafka.config.model.ConfigModel@1ac5e12c, log.retention.ms=io.strimzi.kafka.config.model.ConfigModel@140a422c, log.roll.hours=io.strimzi.kafka.config.model.ConfigModel@7cabc157, log.roll.jitter.hours=io.strimzi.kafka.config.model.ConfigModel@6e1d112, log.roll.jitter.ms=io.strimzi.kafka.config.model.ConfigModel@33988c63, log.roll.ms=io.strimzi.kafka.config.model.ConfigModel@75c5b31e, log.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@57466588, log.segment.delete.delay.ms=io.strimzi.kafka.config.model.ConfigModel@5fa74733, max.connection.creation.rate=io.strimzi.kafka.config.model.ConfigModel@28ab3330, max.connections=io.strimzi.kafka.config.model.ConfigModel@70ae2878, max.connections.per.ip=io.strimzi.kafka.config.model.ConfigModel@14a069c0, max.connections.per.ip.overrides=io.strimzi.kafka.config.model.ConfigModel@71db2ab8, max.incremental.fetch.session.cache.slots=io.strimzi.kafka.config.model.ConfigModel@98b500, message.max.bytes=io.strimzi.kafka.config.model.ConfigModel@174a083, metadata.log.dir=io.strimzi.kafka.config.model.ConfigModel@168d3704, metadata.log.max.record.bytes.between.snapshots=io.strimzi.kafka.config.model.ConfigModel@3e2b4187, metadata.log.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@7b5ed3f7, metadata.log.segment.min.bytes=io.strimzi.kafka.config.model.ConfigModel@5f0a04a7, metadata.log.segment.ms=io.strimzi.kafka.config.model.ConfigModel@20c03d55, metadata.max.retention.bytes=io.strimzi.kafka.config.model.ConfigModel@440367b3, metadata.max.retention.ms=io.strimzi.kafka.config.model.ConfigModel@49cfd5af, metric.reporters=io.strimzi.kafka.config.model.ConfigModel@466fdf0d, metrics.num.samples=io.strimzi.kafka.config.model.ConfigModel@67646800, metrics.recording.level=io.strimzi.kafka.config.model.ConfigModel@3885a5a0, metrics.sample.window.ms=io.strimzi.kafka.config.model.ConfigModel@f507db2, min.insync.replicas=io.strimzi.kafka.config.model.ConfigModel@37ff5ce2, node.id=io.strimzi.kafka.config.model.ConfigModel@739f1675, num.io.threads=io.strimzi.kafka.config.model.ConfigModel@66a21503, num.network.threads=io.strimzi.kafka.config.model.ConfigModel@6a935988, num.partitions=io.strimzi.kafka.config.model.ConfigModel@27460074, num.recovery.threads.per.data.dir=io.strimzi.kafka.config.model.ConfigModel@2ce18657, num.replica.alter.log.dirs.threads=io.strimzi.kafka.config.model.ConfigModel@5a1d84c5, num.replica.fetchers=io.strimzi.kafka.config.model.ConfigModel@2ab2d917, offset.metadata.max.bytes=io.strimzi.kafka.config.model.ConfigModel@3d603995, offsets.commit.required.acks=io.strimzi.kafka.config.model.ConfigModel@1d93c350, offsets.commit.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@2b9e478b, offsets.load.buffer.size=io.strimzi.kafka.config.model.ConfigModel@1758498e, offsets.retention.check.interval.ms=io.strimzi.kafka.config.model.ConfigModel@2b487661, offsets.retention.minutes=io.strimzi.kafka.config.model.ConfigModel@28c2a91b, offsets.topic.compression.codec=io.strimzi.kafka.config.model.ConfigModel@71390197, offsets.topic.num.partitions=io.strimzi.kafka.config.model.ConfigModel@24f83207, offsets.topic.replication.factor=io.strimzi.kafka.config.model.ConfigModel@4cebb387, offsets.topic.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@71dc5170, password.encoder.cipher.algorithm=io.strimzi.kafka.config.model.ConfigModel@48d0d365, password.encoder.iterations=io.strimzi.kafka.config.model.ConfigModel@2d5db872, password.encoder.key.length=io.strimzi.kafka.config.model.ConfigModel@3cbb51d5, password.encoder.keyfactory.algorithm=io.strimzi.kafka.config.model.ConfigModel@2f7fd0e3, password.encoder.old.secret=io.strimzi.kafka.config.model.ConfigModel@61fa7592, password.encoder.secret=io.strimzi.kafka.config.model.ConfigModel@2124cff7, principal.builder.class=io.strimzi.kafka.config.model.ConfigModel@47f707ff, process.roles=io.strimzi.kafka.config.model.ConfigModel@165e2f2a, producer.purgatory.purge.interval.requests=io.strimzi.kafka.config.model.ConfigModel@24ba8e96, queued.max.request.bytes=io.strimzi.kafka.config.model.ConfigModel@23e6a681, queued.max.requests=io.strimzi.kafka.config.model.ConfigModel@5c5195fd, quota.window.num=io.strimzi.kafka.config.model.ConfigModel@3b11c622, quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@4b2f31e8, remote.log.index.file.cache.total.size.bytes=io.strimzi.kafka.config.model.ConfigModel@5deb54a2, remote.log.manager.task.interval.ms=io.strimzi.kafka.config.model.ConfigModel@dc5cc9f, remote.log.manager.task.retry.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@38503bd3, remote.log.manager.task.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@a42fd40, remote.log.manager.task.retry.jitter=io.strimzi.kafka.config.model.ConfigModel@d8bf6ee, remote.log.manager.thread.pool.size=io.strimzi.kafka.config.model.ConfigModel@2e0972f4, remote.log.metadata.manager.class.name=io.strimzi.kafka.config.model.ConfigModel@17be4a45, remote.log.metadata.manager.class.path=io.strimzi.kafka.config.model.ConfigModel@a0b0d0f, remote.log.metadata.manager.impl.prefix=io.strimzi.kafka.config.model.ConfigModel@58e5a626, remote.log.metadata.manager.listener.name=io.strimzi.kafka.config.model.ConfigModel@3d9ee11c, remote.log.reader.max.pending.tasks=io.strimzi.kafka.config.model.ConfigModel@58177888, remote.log.reader.threads=io.strimzi.kafka.config.model.ConfigModel@aa675f0, remote.log.storage.manager.class.name=io.strimzi.kafka.config.model.ConfigModel@7f073614, remote.log.storage.manager.class.path=io.strimzi.kafka.config.model.ConfigModel@35dbaee9, remote.log.storage.manager.impl.prefix=io.strimzi.kafka.config.model.ConfigModel@566b43e2, remote.log.storage.system.enable=io.strimzi.kafka.config.model.ConfigModel@6f5bb52a, replica.fetch.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@29aa9943, replica.fetch.max.bytes=io.strimzi.kafka.config.model.ConfigModel@41eed679, replica.fetch.min.bytes=io.strimzi.kafka.config.model.ConfigModel@4d96e9f5, replica.fetch.response.max.bytes=io.strimzi.kafka.config.model.ConfigModel@7fd6e788, replica.fetch.wait.max.ms=io.strimzi.kafka.config.model.ConfigModel@2cb09b0, replica.high.watermark.checkpoint.interval.ms=io.strimzi.kafka.config.model.ConfigModel@35214a8e, replica.lag.time.max.ms=io.strimzi.kafka.config.model.ConfigModel@4f82c49e, replica.selector.class=io.strimzi.kafka.config.model.ConfigModel@7a09041, replica.socket.receive.buffer.bytes=io.strimzi.kafka.config.model.ConfigModel@5d7c9f8c, replica.socket.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@62b6db97, replication.quota.window.num=io.strimzi.kafka.config.model.ConfigModel@3bc9556b, replication.quota.window.size.seconds=io.strimzi.kafka.config.model.ConfigModel@38e8e8cb, request.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@1d190a5, reserved.broker.max.id=io.strimzi.kafka.config.model.ConfigModel@55ef896b, sasl.client.callback.handler.class=io.strimzi.kafka.config.model.ConfigModel@24fcfcb0, sasl.enabled.mechanisms=io.strimzi.kafka.config.model.ConfigModel@5b2df654, sasl.jaas.config=io.strimzi.kafka.config.model.ConfigModel@5674012c, sasl.kerberos.kinit.cmd=io.strimzi.kafka.config.model.ConfigModel@7f796e58, sasl.kerberos.min.time.before.relogin=io.strimzi.kafka.config.model.ConfigModel@3c23047b, sasl.kerberos.principal.to.local.rules=io.strimzi.kafka.config.model.ConfigModel@808cafd, sasl.kerberos.service.name=io.strimzi.kafka.config.model.ConfigModel@7e03c7b1, sasl.kerberos.ticket.renew.jitter=io.strimzi.kafka.config.model.ConfigModel@4abc7d87, sasl.kerberos.ticket.renew.window.factor=io.strimzi.kafka.config.model.ConfigModel@6e98a877, sasl.login.callback.handler.class=io.strimzi.kafka.config.model.ConfigModel@2009d87b, sasl.login.class=io.strimzi.kafka.config.model.ConfigModel@40d7bd84, sasl.login.connect.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@692ea05c, sasl.login.read.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@425e761e, sasl.login.refresh.buffer.seconds=io.strimzi.kafka.config.model.ConfigModel@2c7ab42e, sasl.login.refresh.min.period.seconds=io.strimzi.kafka.config.model.ConfigModel@51bc07b8, sasl.login.refresh.window.factor=io.strimzi.kafka.config.model.ConfigModel@4d0c6193, sasl.login.refresh.window.jitter=io.strimzi.kafka.config.model.ConfigModel@7c5310aa, sasl.login.retry.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@5f100ca, sasl.login.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@34c1460b, sasl.mechanism.controller.protocol=io.strimzi.kafka.config.model.ConfigModel@1a6fa9f9, sasl.mechanism.inter.broker.protocol=io.strimzi.kafka.config.model.ConfigModel@7e5d3c5e, sasl.oauthbearer.clock.skew.seconds=io.strimzi.kafka.config.model.ConfigModel@7327840f, sasl.oauthbearer.expected.audience=io.strimzi.kafka.config.model.ConfigModel@4d68657e, sasl.oauthbearer.expected.issuer=io.strimzi.kafka.config.model.ConfigModel@2a2f3d4b, sasl.oauthbearer.jwks.endpoint.refresh.ms=io.strimzi.kafka.config.model.ConfigModel@3d875b9c, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms=io.strimzi.kafka.config.model.ConfigModel@72cfb7df, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms=io.strimzi.kafka.config.model.ConfigModel@7c027f6d, sasl.oauthbearer.jwks.endpoint.url=io.strimzi.kafka.config.model.ConfigModel@2f14d0c3, sasl.oauthbearer.scope.claim.name=io.strimzi.kafka.config.model.ConfigModel@28c82506, sasl.oauthbearer.sub.claim.name=io.strimzi.kafka.config.model.ConfigModel@27361e8f, sasl.oauthbearer.token.endpoint.url=io.strimzi.kafka.config.model.ConfigModel@48a0e413, sasl.server.callback.handler.class=io.strimzi.kafka.config.model.ConfigModel@413ba70c, security.inter.broker.protocol=io.strimzi.kafka.config.model.ConfigModel@28b24a38, security.providers=io.strimzi.kafka.config.model.ConfigModel@3f67fbc7, socket.connection.setup.timeout.max.ms=io.strimzi.kafka.config.model.ConfigModel@70281044, socket.connection.setup.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@6c37da8a, socket.receive.buffer.bytes=io.strimzi.kafka.config.model.ConfigModel@56eebebe, socket.request.max.bytes=io.strimzi.kafka.config.model.ConfigModel@56d7ce67, socket.send.buffer.bytes=io.strimzi.kafka.config.model.ConfigModel@664d4ec9, ssl.cipher.suites=io.strimzi.kafka.config.model.ConfigModel@347c3b00, ssl.client.auth=io.strimzi.kafka.config.model.ConfigModel@1744687f, ssl.enabled.protocols=io.strimzi.kafka.config.model.ConfigModel@7f883806, ssl.endpoint.identification.algorithm=io.strimzi.kafka.config.model.ConfigModel@733f0a38, ssl.engine.factory.class=io.strimzi.kafka.config.model.ConfigModel@264e8b64, ssl.key.password=io.strimzi.kafka.config.model.ConfigModel@127d1842, ssl.keymanager.algorithm=io.strimzi.kafka.config.model.ConfigModel@2c0b5a03, ssl.keystore.certificate.chain=io.strimzi.kafka.config.model.ConfigModel@276efb70, ssl.keystore.key=io.strimzi.kafka.config.model.ConfigModel@75294152, ssl.keystore.location=io.strimzi.kafka.config.model.ConfigModel@f6cf8bd, ssl.keystore.password=io.strimzi.kafka.config.model.ConfigModel@79c17011, ssl.keystore.type=io.strimzi.kafka.config.model.ConfigModel@2924a122, ssl.principal.mapping.rules=io.strimzi.kafka.config.model.ConfigModel@16384685, ssl.protocol=io.strimzi.kafka.config.model.ConfigModel@7e79edef, ssl.provider=io.strimzi.kafka.config.model.ConfigModel@cca5bc9, ssl.secure.random.implementation=io.strimzi.kafka.config.model.ConfigModel@6bd6c3, ssl.trustmanager.algorithm=io.strimzi.kafka.config.model.ConfigModel@5433b425, ssl.truststore.certificates=io.strimzi.kafka.config.model.ConfigModel@65943dd9, ssl.truststore.location=io.strimzi.kafka.config.model.ConfigModel@3b5e26b1, ssl.truststore.password=io.strimzi.kafka.config.model.ConfigModel@65dd32d7, ssl.truststore.type=io.strimzi.kafka.config.model.ConfigModel@2c8630d5, transaction.abort.timed.out.transaction.cleanup.interval.ms=io.strimzi.kafka.config.model.ConfigModel@68b8aa69, transaction.max.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@22197261, transaction.remove.expired.transaction.cleanup.interval.ms=io.strimzi.kafka.config.model.ConfigModel@2edeb77f, transaction.state.log.load.buffer.size=io.strimzi.kafka.config.model.ConfigModel@33433ae9, transaction.state.log.min.isr=io.strimzi.kafka.config.model.ConfigModel@1e85350a, transaction.state.log.num.partitions=io.strimzi.kafka.config.model.ConfigModel@77e6c6c1, transaction.state.log.replication.factor=io.strimzi.kafka.config.model.ConfigModel@2cd8f20d, transaction.state.log.segment.bytes=io.strimzi.kafka.config.model.ConfigModel@666010d, transactional.id.expiration.ms=io.strimzi.kafka.config.model.ConfigModel@317c59ae, unclean.leader.election.enable=io.strimzi.kafka.config.model.ConfigModel@706d518e, zookeeper.clientCnxnSocket=io.strimzi.kafka.config.model.ConfigModel@1b4e8d14, zookeeper.connect=io.strimzi.kafka.config.model.ConfigModel@2d968919, zookeeper.connection.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@7e74042c, zookeeper.max.in.flight.requests=io.strimzi.kafka.config.model.ConfigModel@648fdb4d, zookeeper.session.timeout.ms=io.strimzi.kafka.config.model.ConfigModel@bc6cce5, zookeeper.set.acl=io.strimzi.kafka.config.model.ConfigModel@120142c5, zookeeper.ssl.cipher.suites=io.strimzi.kafka.config.model.ConfigModel@4c0a61cd, zookeeper.ssl.client.enable=io.strimzi.kafka.config.model.ConfigModel@56c59eb2, zookeeper.ssl.crl.enable=io.strimzi.kafka.config.model.ConfigModel@6bd9d16b, zookeeper.ssl.enabled.protocols=io.strimzi.kafka.config.model.ConfigModel@7356b1bf, zookeeper.ssl.endpoint.identification.algorithm=io.strimzi.kafka.config.model.ConfigModel@6ccdb211, zookeeper.ssl.keystore.location=io.strimzi.kafka.config.model.ConfigModel@16879134, zookeeper.ssl.keystore.password=io.strimzi.kafka.config.model.ConfigModel@33f05806, zookeeper.ssl.keystore.type=io.strimzi.kafka.config.model.ConfigModel@75eddc8e, zookeeper.ssl.ocsp.enable=io.strimzi.kafka.config.model.ConfigModel@7431b518, zookeeper.ssl.protocol=io.strimzi.kafka.config.model.ConfigModel@5e95949b, zookeeper.ssl.truststore.location=io.strimzi.kafka.config.model.ConfigModel@6f94c727, zookeeper.ssl.truststore.password=io.strimzi.kafka.config.model.ConfigModel@74866ff7, zookeeper.ssl.truststore.type=io.strimzi.kafka.config.model.ConfigModel@d668d0a, zookeeper.sync.time.ms=io.strimzi.kafka.config.model.ConfigModel@7fa5f171}
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:336] Number of all kafka configs 261
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:356] Number of dynamic-configs 40
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:364] Number of forbidden-exception-configs 7
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:371] Size of dynamic-configs with forbidden-exception-configs 46
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] compression.type -> CLUSTER_WIDE:STRING
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.min.compaction.lag.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.retention.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connections.per.ip.overrides -> CLUSTER_WIDE:STRING
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] metric.reporters -> CLUSTER_WIDE:LIST
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.flush.interval.messages -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.message.timestamp.difference.max.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.flush.interval.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] principal.builder.class -> PER_BROKER:CLASS
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.delete.retention.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.io.buffer.size -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] min.insync.replicas -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] zookeeper.connection.timeout.ms -> READ_ONLY:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.threads -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.io.max.bytes.per.second -> CLUSTER_WIDE:DOUBLE
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.min.cleanable.ratio -> CLUSTER_WIDE:DOUBLE
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.recovery.threads.per.data.dir -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.retention.bytes -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.network.threads -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleanup.policy -> CLUSTER_WIDE:LIST
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.message.timestamp.type -> CLUSTER_WIDE:STRING
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.preallocate -> CLUSTER_WIDE:BOOLEAN
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.roll.jitter.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connections -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connections.per.ip -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] background.threads -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.message.downconversion.enable -> CLUSTER_WIDE:BOOLEAN
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] message.max.bytes -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] ssl.protocol -> PER_BROKER:STRING
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.partitions -> READ_ONLY:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.io.threads -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] ssl.enabled.protocols -> PER_BROKER:LIST
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] max.connection.creation.rate -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.roll.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] ssl.cipher.suites -> PER_BROKER:LIST
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] unclean.leader.election.enable -> CLUSTER_WIDE:BOOLEAN
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.index.interval.bytes -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.backoff.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.segment.bytes -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.io.buffer.load.factor -> CLUSTER_WIDE:DOUBLE
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.index.size.max.bytes -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] zookeeper.connect -> READ_ONLY:STRING
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.segment.delete.delay.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.max.compaction.lag.ms -> CLUSTER_WIDE:LONG
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] num.replica.fetchers -> CLUSTER_WIDE:INT
2022-04-06 17:31:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:373] log.cleaner.dedupe.buffer.size -> CLUSTER_WIDE:LONG
2022-04-06 17:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:234] Kafka config before updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3}'
2022-04-06 17:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:238] Kafka config after updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3, log.cleanup.policy=compact}'
2022-04-06 17:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-06 17:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-06 17:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:262] Dynamic Configuration in Kafka CR is log.cleanup.policy=compact and expected is log.cleanup.policy=compact
2022-04-06 17:32:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:32:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:32:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-1 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:32:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-2 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:234] Kafka config before updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.cleanup.policy=compact, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3}'
2022-04-06 17:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:238] Kafka config after updating '{default.replication.factor=3, inter.broker.protocol.version=3.1, log.cleanup.policy=compact, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3, compression.type=gzip}'
2022-04-06 17:32:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-06 17:33:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-06 17:33:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:262] Dynamic Configuration in Kafka CR is compression.type=gzip and expected is compression.type=gzip
2022-04-06 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:33:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:33:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-1 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:33:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-2 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:234] Kafka config before updating '{compression.type=gzip, default.replication.factor=3, inter.broker.protocol.version=3.1, log.cleanup.policy=compact, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3}'
2022-04-06 17:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:238] Kafka config after updating '{compression.type=gzip, default.replication.factor=3, inter.broker.protocol.version=3.1, log.cleanup.policy=compact, log.message.format.version=3.1, min.insync.replicas=2, offsets.topic.replication.factor=3, transaction.state.log.min.isr=2, transaction.state.log.replication.factor=3, log.message.timestamp.difference.max.ms=440}'
2022-04-06 17:33:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-06 17:34:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-06 17:34:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:262] Dynamic Configuration in Kafka CR is log.message.timestamp.difference.max.ms=440 and expected is log.message.timestamp.difference.max.ms=440
2022-04-06 17:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-1 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:34:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace dynamic-conf-shared-st exec dynamic-configuration-shared-cluster-name-kafka-2 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --entity-type brokers --entity-name 0 --describe
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testDynConfiguration is everything deleted.
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST.testDynConfiguration-FINISHED
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for DynamicConfSharedST
2022-04-06 17:34:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka dynamic-configuration-shared-cluster-name in namespace dynamic-conf-shared-st
2022-04-06 17:35:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 378.014 s - in io.strimzi.systemtest.kafka.dynamicconfiguration.DynamicConfSharedST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.KafkaST
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: kafka-st
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: kafka-st
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: kafka-st
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsFalse-STARTED
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaOffsetsReplicationFactorHigherThanReplicas-STARTED
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-32 for test case:testKafkaJBODDeleteClaimsFalse
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-32
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-32
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-32
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-33 for test case:testKafkaOffsetsReplicationFactorHigherThanReplicas
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-33
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0cd8acb8 in namespace namespace-32
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-32
2022-04-06 17:35:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0cd8acb8 will have desired state: Ready
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-33
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-33
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-68a0ce43 in namespace namespace-33
2022-04-06 17:35:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-33
2022-04-06 17:35:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:35:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaOffsetsReplicationFactorHigherThanReplicas
2022-04-06 17:35:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-68a0ce43 in namespace namespace-33
2022-04-06 17:36:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:36:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-33 for test case:testKafkaOffsetsReplicationFactorHigherThanReplicas
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaOffsetsReplicationFactorHigherThanReplicas-FINISHED
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserOperator-STARTED
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-34 for test case:testEntityOperatorWithoutUserOperator
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-34
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-34
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-34
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:787] Deploying Kafka cluster without UO in EO
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8f72c824 in namespace namespace-34
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-34
2022-04-06 17:36:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8f72c824 will have desired state: Ready
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0cd8acb8 is in desired state: Ready
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-0cd8acb8-kafka-0
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-0cd8acb8-kafka-1
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-0cd8acb8-kafka-0
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-0cd8acb8-kafka-1
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1717] Checking PVC names included in JBOD array
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1724] Checking PVC on Kafka pods
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 0
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 0
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 1
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 1
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:930] Deleting cluster
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:933] Waiting for PVC deletion
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaJBODDeleteClaimsFalse
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0cd8acb8 in namespace namespace-32
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:37:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-32 for test case:testKafkaJBODDeleteClaimsFalse
2022-04-06 17:37:08 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:37:08 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutTopicOperator-STARTED
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-35 for test case:testEntityOperatorWithoutTopicOperator
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-35
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-35
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-35
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:757] Deploying Kafka cluster without TO in EO
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8184f0dc in namespace namespace-35
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-35
2022-04-06 17:37:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8184f0dc will have desired state: Ready
2022-04-06 17:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8f72c824 is in desired state: Ready
2022-04-06 17:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 78 seconds
2022-04-06 17:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-06 17:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testEntityOperatorWithoutUserOperator
2022-04-06 17:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8f72c824 in namespace namespace-34
2022-04-06 17:37:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:37:37 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-34 for test case:testEntityOperatorWithoutUserOperator
2022-04-06 17:37:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsFalse-FINISHED
2022-04-06 17:37:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:37:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:37:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrue-STARTED
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-36 for test case:testKafkaJBODDeleteClaimsTrue
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-36
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-36
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-36
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5a51c05b in namespace namespace-36
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-36
2022-04-06 17:37:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5a51c05b will have desired state: Ready
2022-04-06 17:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserOperator-FINISHED
2022-04-06 17:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testTopicWithoutLabels-STARTED
2022-04-06 17:38:09 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:38:09 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-37 for test case:testTopicWithoutLabels
2022-04-06 17:38:09 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-37
2022-04-06 17:38:09 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-37
2022-04-06 17:38:09 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-37
2022-04-06 17:38:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-aa006c45 in namespace namespace-37
2022-04-06 17:38:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-37
2022-04-06 17:38:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-aa006c45 will have desired state: Ready
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5a51c05b is in desired state: Ready
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-5a51c05b-kafka-0
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-5a51c05b-kafka-1
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-5a51c05b-kafka-0
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-5a51c05b-kafka-1
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1717] Checking PVC names included in JBOD array
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1724] Checking PVC on Kafka pods
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 0
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 0
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 1
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 1
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:906] Deleting cluster
2022-04-06 17:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:909] Waiting for PVC deletion
2022-04-06 17:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8184f0dc is in desired state: Ready
2022-04-06 17:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 127 seconds
2022-04-06 17:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-06 17:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testEntityOperatorWithoutTopicOperator
2022-04-06 17:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8184f0dc in namespace namespace-35
2022-04-06 17:39:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-aa006c45 is in desired state: Ready
2022-04-06 17:39:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic topic-without-labels in namespace namespace-37
2022-04-06 17:39:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-37
2022-04-06 17:39:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-37 exec my-cluster-aa006c45-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 17:39:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:39:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic topic-without-labels deletion
2022-04-06 17:39:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:39:31 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-35 for test case:testEntityOperatorWithoutTopicOperator
2022-04-06 17:39:33 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-37 exec my-cluster-aa006c45-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 17:39:33 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:39:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:39:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicWithoutLabels
2022-04-06 17:39:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic topic-without-labels in namespace namespace-37
2022-04-06 17:39:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-aa006c45 in namespace namespace-37
2022-04-06 17:39:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:39:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-37 for test case:testTopicWithoutLabels
2022-04-06 17:39:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:39:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaJBODDeleteClaimsTrue
2022-04-06 17:39:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5a51c05b in namespace namespace-36
2022-04-06 17:39:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:39:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-36 for test case:testKafkaJBODDeleteClaimsTrue
2022-04-06 17:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutTopicOperator-FINISHED
2022-04-06 17:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testRemoveTopicOperatorFromEntityOperator-STARTED
2022-04-06 17:39:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrue-FINISHED
2022-04-06 17:39:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:39:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:39:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testCustomAndUpdatedValues-STARTED
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-38 for test case:testRemoveTopicOperatorFromEntityOperator
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-38
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-38
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-38
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:623] Deploying Kafka cluster my-cluster-27165539
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-27165539 in namespace namespace-38
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-38
2022-04-06 17:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-27165539 will have desired state: Ready
2022-04-06 17:40:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testTopicWithoutLabels-FINISHED
2022-04-06 17:40:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:40:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:40:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserAndTopicOperators-STARTED
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-39 for test case:testCustomAndUpdatedValues
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-39
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-39
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-39
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9061591f in namespace namespace-39
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-39
2022-04-06 17:40:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9061591f will have desired state: Ready
2022-04-06 17:41:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-27165539 is in desired state: Ready
2022-04-06 17:41:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-27165539-entity-operator-649cddfb56-grsgn will be deleted
2022-04-06 17:41:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:171] Pod my-cluster-27165539-entity-operator-649cddfb56-grsgn deleted
2022-04-06 17:41:25 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-27165539-entity-operator will be ready
2022-04-06 17:41:39 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-27165539-entity-operator is ready
2022-04-06 17:41:39 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-27165539-entity-operator to be ready
2022-04-06 17:41:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-27165539-entity-operator is ready
2022-04-06 17:41:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:201] Wait until Pod my-cluster-27165539-entity-operator will have 1 containers
2022-04-06 17:41:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:205] Pod my-cluster-27165539-entity-operator has 1 containers
2022-04-06 17:41:49 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-27165539-entity-operator-cb7f9c57b-xbfdp will be deleted
2022-04-06 17:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:171] Pod my-cluster-27165539-entity-operator-cb7f9c57b-xbfdp deleted
2022-04-06 17:42:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-27165539-entity-operator will be ready
2022-04-06 17:42:22 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-27165539-entity-operator is ready
2022-04-06 17:42:22 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-27165539-entity-operator to be ready
2022-04-06 17:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-27165539-entity-operator is ready
2022-04-06 17:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testRemoveTopicOperatorFromEntityOperator
2022-04-06 17:42:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-27165539 in namespace namespace-38
2022-04-06 17:42:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:42:42 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-38 for test case:testRemoveTopicOperatorFromEntityOperator
2022-04-06 17:43:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testRemoveTopicOperatorFromEntityOperator-FINISHED
2022-04-06 17:43:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:43:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:43:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserAndTopicOperatorsFromEntityOperator-STARTED
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-40 for test case:testEntityOperatorWithoutUserAndTopicOperators
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-40
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-40
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-40
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:814] Deploying Kafka cluster without UO and TO in EO
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c592296b in namespace namespace-40
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-40
2022-04-06 17:43:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c592296b will have desired state: Ready
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9061591f is in desired state: Ready
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:290] Verify values before update
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-kafka in pod name
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container kafka
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1661] Checking kafka configuration
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-0 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-1 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-2 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-kafka
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container kafka
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-0 -- cat /tmp/strimzi.properties
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:308] Testing Zookeepers
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-zookeeper in pod name
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container zookeeper
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-9061591f-zookeeper
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container zookeeper
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-zookeeper
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container zookeeper
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:315] Checking configuration of TO and UO
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-entity-operator in pod name
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container topic-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-entity-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container topic-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-entity-operator in pod name
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container user-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-entity-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container user-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-entity-operator in pod name
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container tls-sidecar
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-entity-operator
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container tls-sidecar
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:326] Updating configuration of Kafka cluster
2022-04-06 17:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-9061591f-zookeeper rolling update
2022-04-06 17:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c592296b is in desired state: Ready
2022-04-06 17:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 50 seconds
2022-04-06 17:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-06 17:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testEntityOperatorWithoutUserAndTopicOperators
2022-04-06 17:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c592296b in namespace namespace-40
2022-04-06 17:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-40 for test case:testEntityOperatorWithoutUserAndTopicOperators
2022-04-06 17:44:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEntityOperatorWithoutUserAndTopicOperators-FINISHED
2022-04-06 17:44:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:44:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:44:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testConsumerOffsetFiles-STARTED
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-41 for test case:testRemoveUserAndTopicOperatorsFromEntityOperator
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-41
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-41
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-41
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-74f1c3c6 in namespace namespace-41
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-41
2022-04-06 17:44:51 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-74f1c3c6 will have desired state: Ready
2022-04-06 17:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-9061591f-zookeeper has been successfully rolled
2022-04-06 17:45:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-9061591f-zookeeper to be ready
2022-04-06 17:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9061591f will have desired state: Ready
2022-04-06 17:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9061591f is in desired state: Ready
2022-04-06 17:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-9061591f is ready
2022-04-06 17:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-9061591f-kafka rolling update
2022-04-06 17:46:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-74f1c3c6 is in desired state: Ready
2022-04-06 17:46:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-74f1c3c6-entity-operator will have stable 0 replicas
2022-04-06 17:46:05 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:06 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:07 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:08 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:10 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:11 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:12 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:14 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:15 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:16 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:17 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:18 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:19 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:20 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:21 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:22 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-06 17:46:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-06 17:46:24 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-06 17:46:25 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-06 17:46:26 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-06 17:46:27 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-06 17:46:28 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-06 17:46:29 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-06 17:46:30 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-06 17:46:31 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-06 17:46:32 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-06 17:46:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-06 17:46:34 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-06 17:46:35 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-06 17:46:36 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-06 17:46:37 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-06 17:46:38 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-06 17:46:39 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-06 17:46:40 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-06 17:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-06 17:46:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-06 17:46:42 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:228] Pod my-cluster-74f1c3c6-entity-operator has 0 replicas
2022-04-06 17:46:42 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-74f1c3c6-entity-operator will be ready
2022-04-06 17:47:03 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-74f1c3c6-entity-operator is ready
2022-04-06 17:47:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 132 seconds
2022-04-06 17:47:03 [ForkJoinPool-3-worker-5] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-06 17:47:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:47:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testRemoveUserAndTopicOperatorsFromEntityOperator
2022-04-06 17:47:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-74f1c3c6 in namespace namespace-41
2022-04-06 17:47:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:47:13 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-41 for test case:testRemoveUserAndTopicOperatorsFromEntityOperator
2022-04-06 17:47:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-9061591f-kafka has been successfully rolled
2022-04-06 17:47:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-9061591f-kafka to be ready
2022-04-06 17:47:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:47:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testAppDomainLabels-STARTED
2022-04-06 17:47:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserAndTopicOperatorsFromEntityOperator-FINISHED
2022-04-06 17:47:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:47:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:47:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserOperatorFromEntityOperator-STARTED
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-42 for test case:testConsumerOffsetFiles
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-42
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-42
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-42
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2f3c364b in namespace namespace-42
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-42
2022-04-06 17:47:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2f3c364b will have desired state: Ready
2022-04-06 17:48:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9061591f will have desired state: Ready
2022-04-06 17:48:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9061591f is in desired state: Ready
2022-04-06 17:48:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-9061591f is ready
2022-04-06 17:48:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-9061591f-entity-operator rolling update
2022-04-06 17:48:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9061591f-entity-operator will be ready
2022-04-06 17:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2f3c364b is in desired state: Ready
2022-04-06 17:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1236026990-446651802 in namespace namespace-42
2022-04-06 17:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-42
2022-04-06 17:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1236026990-446651802 will have desired state: Ready
2022-04-06 17:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1236026990-446651802 is in desired state: Ready
2022-04-06 17:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2f3c364b-kafka-clients in namespace namespace-42
2022-04-06 17:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-42
2022-04-06 17:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2f3c364b-kafka-clients will be ready
2022-04-06 17:49:17 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2f3c364b-kafka-clients is ready
2022-04-06 17:49:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:49:17 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1415] Executing command cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V in my-cluster-2f3c364b-kafka-0
2022-04-06 17:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-42 exec my-cluster-2f3c364b-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V
2022-04-06 17:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1422] Result: 
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99

2022-04-06 17:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3d65398c, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092, --topic, my-topic-1236026990-446651802], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2f3c364b-kafka-clients-6f9d459dd6-grhw6', podNamespace='namespace-42', bootstrapServer='my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092', topicName='my-topic-1236026990-446651802', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2b88520a}
2022-04-06 17:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092:my-topic-1236026990-446651802 from pod my-cluster-2f3c364b-kafka-clients-6f9d459dd6-grhw6
2022-04-06 17:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2f3c364b-kafka-clients-6f9d459dd6-grhw6 -n namespace-42 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092 --topic my-topic-1236026990-446651802
2022-04-06 17:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 17:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 17:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1af3909d, messages=[], arguments=[--group-id, my-consumer-group-31355974, --max-messages, 100, --group-instance-id, instance709360712, --bootstrap-server, my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092, --topic, my-topic-1236026990-446651802], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2f3c364b-kafka-clients-6f9d459dd6-grhw6', podNamespace='namespace-42', bootstrapServer='my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092', topicName='my-topic-1236026990-446651802', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-31355974', consumerInstanceId='instance709360712', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@46f80d30}
2022-04-06 17:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092#my-topic-1236026990-446651802 from pod my-cluster-2f3c364b-kafka-clients-6f9d459dd6-grhw6
2022-04-06 17:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2f3c364b-kafka-clients-6f9d459dd6-grhw6 -n namespace-42 -- /opt/kafka/consumer.sh --group-id my-consumer-group-31355974 --max-messages 100 --group-instance-id instance709360712 --bootstrap-server my-cluster-2f3c364b-kafka-bootstrap.namespace-42.svc:9092 --topic my-topic-1236026990-446651802
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1429] Executing command cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V in my-cluster-2f3c364b-kafka-0
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-42 exec my-cluster-2f3c364b-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/;ls -1 | sed -n "s#__consumer_offsets-\([0-9]*\)#\1#p" | sort -V
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testConsumerOffsetFiles
2022-04-06 17:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1236026990-446651802 in namespace namespace-42
2022-04-06 17:49:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2f3c364b-kafka-clients in namespace namespace-42
2022-04-06 17:49:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9061591f-entity-operator is ready
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-9061591f-entity-operator rolling update finished
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9061591f will have desired state: Ready
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9061591f is in desired state: Ready
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:386] Verify values after update
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-kafka in pod name
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container kafka
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1661] Checking kafka configuration
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-0 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-1 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-2 -- /bin/bash -c cat /tmp/strimzi.properties
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-kafka
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container kafka
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-39 exec my-cluster-9061591f-kafka-0 -- cat /tmp/strimzi.properties
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:404] Testing Zookeepers
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-zookeeper in pod name
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container zookeeper
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-9061591f-zookeeper
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container zookeeper
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-zookeeper
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container zookeeper
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:410] Getting entity operator to check configuration of TO and UO
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-entity-operator in pod name
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container topic-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-entity-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container topic-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-entity-operator in pod name
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container user-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-entity-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container user-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-9061591f-entity-operator in pod name
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container tls-sidecar
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-9061591f-entity-operator
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container tls-sidecar
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-06 17:49:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9061591f in namespace namespace-39
2022-04-06 17:50:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:50:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-39 for test case:testCustomAndUpdatedValues
2022-04-06 17:50:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2f3c364b in namespace namespace-42
2022-04-06 17:50:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:50:26 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-42 for test case:testConsumerOffsetFiles
2022-04-06 17:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testCustomAndUpdatedValues-FINISHED
2022-04-06 17:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testReadOnlyRootFileSystem-STARTED
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-43 for test case:testAppDomainLabels
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-43
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-43
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-43
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fcc32407 in namespace namespace-43
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-43
2022-04-06 17:50:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fcc32407 will have desired state: Ready
2022-04-06 17:50:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testConsumerOffsetFiles-FINISHED
2022-04-06 17:50:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:50:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:50:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testUOListeningOnlyUsersInSameCluster-STARTED
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-44 for test case:testReadOnlyRootFileSystem
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-44
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-44
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-44
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f04dbaa0 in namespace namespace-44
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-44
2022-04-06 17:50:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f04dbaa0 will have desired state: Ready
2022-04-06 17:52:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fcc32407 is in desired state: Ready
2022-04-06 17:52:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1450467083-1463898483 in namespace namespace-44
2022-04-06 17:52:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-43
2022-04-06 17:52:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1450467083-1463898483 will have desired state: Ready
2022-04-06 17:52:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1450467083-1463898483 is in desired state: Ready
2022-04-06 17:52:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-fcc32407-kafka-clients in namespace namespace-44
2022-04-06 17:52:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-43
2022-04-06 17:52:02 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-fcc32407-kafka-clients will be ready
2022-04-06 17:52:05 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-fcc32407-kafka-clients is ready
2022-04-06 17:52:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:52:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1220] ---> PODS <---
2022-04-06 17:52:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1232] ---> STATEFUL SETS <---
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1236] Getting labels from stateful set of kafka resource
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-kafka, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1241] Getting labels from stateful set of zookeeper resource
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-zookeeper, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1244] ---> SERVICES <---
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-fcc32407-kafka-bootstrap service
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/discovery=true, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-kafka, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-fcc32407-kafka-brokers service
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-kafka, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-fcc32407-zookeeper-client service
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-zookeeper-client, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1251] Getting labels from my-cluster-fcc32407-zookeeper-nodes service
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1765] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-zookeeper, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1255] ---> SECRETS <---
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-clients-ca secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-clients-ca-cert secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-cluster-ca secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-cluster-ca-cert secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-cluster-operator-certs secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-entity-topic-operator-certs secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-topic-operator, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-entity-user-operator-certs secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-user-operator, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-kafka-brokers secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1262] Getting labels from my-cluster-fcc32407-zookeeper-nodes secret
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1266] ---> CONFIG MAPS <---
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-fcc32407-entity-topic-operator-config config map
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-topic-operator, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-fcc32407-entity-user-operator-config config map
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=entity-user-operator, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-fcc32407-kafka-config config map
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-fcc32407-kafka, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1271] Getting labels from my-cluster-fcc32407-zookeeper-config config map
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1772] Verifying labels {app.kubernetes.io/instance=my-cluster-fcc32407, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-fcc32407, strimzi.io/cluster=my-cluster-fcc32407, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testAppDomainLabels}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6db957d4, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092, --topic, my-topic-1450467083-1463898483], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fcc32407-kafka-clients-5d48479648-s74b9', podNamespace='namespace-43', bootstrapServer='my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092', topicName='my-topic-1450467083-1463898483', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6633bb07}
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092:my-topic-1450467083-1463898483 from pod my-cluster-fcc32407-kafka-clients-5d48479648-s74b9
2022-04-06 17:52:06 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fcc32407-kafka-clients-5d48479648-s74b9 -n namespace-43 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092 --topic my-topic-1450467083-1463898483
2022-04-06 17:52:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 17:52:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 17:52:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@d4877b1, messages=[], arguments=[--group-id, my-consumer-group-1550836770, --max-messages, 100, --group-instance-id, instance1731435890, --bootstrap-server, my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092, --topic, my-topic-1450467083-1463898483], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fcc32407-kafka-clients-5d48479648-s74b9', podNamespace='namespace-43', bootstrapServer='my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092', topicName='my-topic-1450467083-1463898483', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1550836770', consumerInstanceId='instance1731435890', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@765811fe}
2022-04-06 17:52:09 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092#my-topic-1450467083-1463898483 from pod my-cluster-fcc32407-kafka-clients-5d48479648-s74b9
2022-04-06 17:52:09 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fcc32407-kafka-clients-5d48479648-s74b9 -n namespace-43 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1550836770 --max-messages 100 --group-instance-id instance1731435890 --bootstrap-server my-cluster-fcc32407-kafka-bootstrap.namespace-43.svc:9092 --topic my-topic-1450467083-1463898483
2022-04-06 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testAppDomainLabels
2022-04-06 17:52:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1450467083-1463898483 in namespace namespace-43
2022-04-06 17:52:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-fcc32407-kafka-clients in namespace namespace-43
2022-04-06 17:53:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fcc32407 in namespace namespace-43
2022-04-06 17:53:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:53:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-43 for test case:testAppDomainLabels
2022-04-06 17:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f04dbaa0 is in desired state: Ready
2022-04-06 17:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f04dbaa0 will have desired state: Ready
2022-04-06 17:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f04dbaa0 is in desired state: Ready
2022-04-06 17:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1609606534-887272880 in namespace namespace-44
2022-04-06 17:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-44
2022-04-06 17:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1609606534-887272880 will have desired state: Ready
2022-04-06 17:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1609606534-887272880 is in desired state: Ready
2022-04-06 17:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-f04dbaa0-kafka-clients in namespace namespace-44
2022-04-06 17:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-44
2022-04-06 17:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-f04dbaa0-kafka-clients will be ready
2022-04-06 17:53:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-f04dbaa0-kafka-clients is ready
2022-04-06 17:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 17:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1652] Checking produced and consumed messages to pod:my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq
2022-04-06 17:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@668fd935, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092, --topic, my-topic-1609606534-887272880], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq', podNamespace='namespace-44', bootstrapServer='my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092', topicName='my-topic-1609606534-887272880', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@31a98631}
2022-04-06 17:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092:my-topic-1609606534-887272880 from pod my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq
2022-04-06 17:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq -n namespace-44 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092 --topic my-topic-1609606534-887272880
2022-04-06 17:53:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 17:53:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 17:53:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@20b2e2b3, messages=[], arguments=[--group-id, my-consumer-group-762330339, --max-messages, 100, --group-instance-id, instance1880583059, --bootstrap-server, my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092, --topic, my-topic-1609606534-887272880], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq', podNamespace='namespace-44', bootstrapServer='my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092', topicName='my-topic-1609606534-887272880', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-762330339', consumerInstanceId='instance1880583059', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@15d67143}
2022-04-06 17:53:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092#my-topic-1609606534-887272880 from pod my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq
2022-04-06 17:53:38 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-f04dbaa0-kafka-clients-7dfd5446cf-spfzq -n namespace-44 -- /opt/kafka/consumer.sh --group-id my-consumer-group-762330339 --max-messages 100 --group-instance-id instance1880583059 --bootstrap-server my-cluster-f04dbaa0-kafka-bootstrap.namespace-44.svc:9092 --topic my-topic-1609606534-887272880
2022-04-06 17:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 17:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 17:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReadOnlyRootFileSystem
2022-04-06 17:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1609606534-887272880 in namespace namespace-44
2022-04-06 17:53:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-f04dbaa0-kafka-clients in namespace namespace-44
2022-04-06 17:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testAppDomainLabels-FINISHED
2022-04-06 17:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:53:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrueFalse-STARTED
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-45 for test case:testRemoveUserOperatorFromEntityOperator
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-45
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-45
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-45
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:669] Deploying Kafka cluster my-cluster-34935360
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-34935360 in namespace namespace-45
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-45
2022-04-06 17:54:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-34935360 will have desired state: Ready
2022-04-06 17:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f04dbaa0 in namespace namespace-44
2022-04-06 17:54:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-44, for cruise control Kafka cluster my-cluster-f04dbaa0
2022-04-06 17:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-44 for test case:testReadOnlyRootFileSystem
2022-04-06 17:55:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-34935360 is in desired state: Ready
2022-04-06 17:55:23 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-34935360-entity-operator-75cdfc67d8-7pv4j will be deleted
2022-04-06 17:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testReadOnlyRootFileSystem-FINISHED
2022-04-06 17:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:55:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testForTopicOperator-STARTED
2022-04-06 17:55:28 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:55:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-46 for test case:testUOListeningOnlyUsersInSameCluster
2022-04-06 17:55:28 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-46
2022-04-06 17:55:29 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-46
2022-04-06 17:55:29 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-46
2022-04-06 17:55:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1 in namespace namespace-46
2022-04-06 17:55:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-46
2022-04-06 17:55:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1 will have desired state: Ready
2022-04-06 17:55:33 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:171] Pod my-cluster-34935360-entity-operator-75cdfc67d8-7pv4j deleted
2022-04-06 17:55:33 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-34935360-entity-operator will be ready
2022-04-06 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1 is in desired state: Ready
2022-04-06 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2 in namespace namespace-46
2022-04-06 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-46
2022-04-06 17:56:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2 will have desired state: Ready
2022-04-06 17:57:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-34935360-entity-operator is ready
2022-04-06 17:57:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-34935360-entity-operator to be ready
2022-04-06 17:57:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-34935360-entity-operator is ready
2022-04-06 17:57:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:201] Wait until Pod my-cluster-34935360-entity-operator will have 2 containers
2022-04-06 17:57:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:205] Pod my-cluster-34935360-entity-operator has 2 containers
2022-04-06 17:57:59 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-34935360-entity-operator-f54ccc7c5-p67pl will be deleted
2022-04-06 17:58:09 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:171] Pod my-cluster-34935360-entity-operator-f54ccc7c5-p67pl deleted
2022-04-06 17:58:09 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-34935360-entity-operator will be ready
2022-04-06 17:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2 is in desired state: Ready
2022-04-06 17:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1665426597-678679743 in namespace namespace-46
2022-04-06 17:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-46
2022-04-06 17:58:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1665426597-678679743 will have desired state: Ready
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1665426597-678679743 is in desired state: Ready
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1292] Verifying that user my-user-1665426597-678679743 in cluster my-cluster-1 is created
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1297] Verifying that user my-user-1665426597-678679743 in cluster my-cluster-2 is not created
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1302] Verifying that user belongs to my-cluster-1 cluster
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testUOListeningOnlyUsersInSameCluster
2022-04-06 17:58:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2 in namespace namespace-46
2022-04-06 17:58:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1665426597-678679743 in namespace namespace-46
2022-04-06 17:58:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1 in namespace namespace-46
2022-04-06 17:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:58:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-46 for test case:testUOListeningOnlyUsersInSameCluster
2022-04-06 17:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-34935360-entity-operator is ready
2022-04-06 17:58:53 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-34935360-entity-operator to be ready
2022-04-06 17:59:03 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-34935360-entity-operator is ready
2022-04-06 17:59:03 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 301 seconds
2022-04-06 17:59:03 [ForkJoinPool-3-worker-5] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-06 17:59:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 17:59:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testRemoveUserOperatorFromEntityOperator
2022-04-06 17:59:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-34935360 in namespace namespace-45
2022-04-06 17:59:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 17:59:13 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-45 for test case:testRemoveUserOperatorFromEntityOperator
2022-04-06 17:59:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testUOListeningOnlyUsersInSameCluster-FINISHED
2022-04-06 17:59:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:59:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:59:25 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testLabelsAndAnnotationForPVC-STARTED
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-47 for test case:testForTopicOperator
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-47
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-47
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-47
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ddadf5ef in namespace namespace-47
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-47
2022-04-06 17:59:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ddadf5ef will have desired state: Ready
2022-04-06 17:59:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testRemoveUserOperatorFromEntityOperator-FINISHED
2022-04-06 17:59:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 17:59:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 17:59:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testMessagesAreStoredInDisk-STARTED
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-48 for test case:testKafkaJBODDeleteClaimsTrueFalse
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-48
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-48
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-48
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6465aabb in namespace namespace-48
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-48
2022-04-06 17:59:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6465aabb will have desired state: Ready
2022-04-06 18:01:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ddadf5ef is in desired state: Ready
2022-04-06 18:01:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-271975802-1704787488 in namespace namespace-48
2022-04-06 18:01:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-47
2022-04-06 18:01:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-271975802-1704787488 will have desired state: Ready
2022-04-06 18:01:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-271975802-1704787488 is in desired state: Ready
2022-04-06 18:01:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-271975802-1704787488 will have desired state: Ready
2022-04-06 18:01:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-271975802-1704787488 is in desired state: Ready
2022-04-06 18:01:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 18:01:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6465aabb is in desired state: Ready
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-6465aabb-kafka-0
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-0-my-cluster-6465aabb-kafka-1
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-6465aabb-kafka-0
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1710] Checking labels for volume:data-1-my-cluster-6465aabb-kafka-1
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1717] Checking PVC names included in JBOD array
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1724] Checking PVC on Kafka pods
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 0
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 0
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1729] Getting list of mounted data sources and PVCs on Kafka pod 1
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1737] Verifying mounted data sources and PVCs on Kafka pod 1
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:882] Deleting cluster
2022-04-06 18:01:46 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:885] Waiting for PVC deletion
2022-04-06 18:01:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic topic-from-cli --replication-factor 1 --partitions 1
2022-04-06 18:01:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:01:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic topic-from-cli creation 
2022-04-06 18:01:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 18:01:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-271975802-1704787488 --partitions 2
2022-04-06 18:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ddadf5ef will have desired state: Ready
2022-04-06 18:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ddadf5ef is in desired state: Ready
2022-04-06 18:01:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-271975802-1704787488
2022-04-06 18:01:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:01:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ddadf5ef will have desired state: Ready
2022-04-06 18:01:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ddadf5ef is in desired state: Ready
2022-04-06 18:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic topic-from-cli
2022-04-06 18:02:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:02:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic my-topic-271975802-1704787488
2022-04-06 18:02:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:02:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-271975802-1704787488 deletion
2022-04-06 18:02:03 [ForkJoinPool-3-worker-3] [33mWARN [m [KafkaTopicUtils:110] KafkaTopic my-topic-271975802-1704787488 is not deleted yet! Triggering force delete by cmd client!
2022-04-06 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaJBODDeleteClaimsTrueFalse
2022-04-06 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6465aabb in namespace namespace-48
2022-04-06 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:02:06 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-48 for test case:testKafkaJBODDeleteClaimsTrueFalse
2022-04-06 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testKafkaJBODDeleteClaimsTrueFalse-FINISHED
2022-04-06 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:02:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testLabelModificationDoesNotBreakCluster-STARTED
2022-04-06 18:02:11 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:02:11 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-49 for test case:testMessagesAreStoredInDisk
2022-04-06 18:02:11 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-49
2022-04-06 18:02:12 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-49
2022-04-06 18:02:12 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-49
2022-04-06 18:02:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-10593199 in namespace namespace-49
2022-04-06 18:02:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-49
2022-04-06 18:02:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-10593199 will have desired state: Ready
2022-04-06 18:02:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-47 exec my-cluster-ddadf5ef-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-06 18:02:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:02:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:02:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testForTopicOperator
2022-04-06 18:02:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-271975802-1704787488 in namespace namespace-47
2022-04-06 18:02:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ddadf5ef in namespace namespace-47
2022-04-06 18:02:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:02:26 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-47 for test case:testForTopicOperator
2022-04-06 18:03:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testForTopicOperator-FINISHED
2022-04-06 18:03:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:03:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:03:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testPersistentStorageSize-STARTED
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-50 for test case:testLabelsAndAnnotationForPVC
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-50
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-50
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-50
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-074c89da in namespace namespace-50
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-50
2022-04-06 18:03:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-074c89da will have desired state: Ready
2022-04-06 18:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-10593199 is in desired state: Ready
2022-04-06 18:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1999537273-1597681813 in namespace namespace-50
2022-04-06 18:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-49
2022-04-06 18:03:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1999537273-1597681813 will have desired state: Ready
2022-04-06 18:03:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1999537273-1597681813 is in desired state: Ready
2022-04-06 18:03:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-10593199-kafka-clients in namespace namespace-50
2022-04-06 18:03:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-49
2022-04-06 18:03:32 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-10593199-kafka-clients will be ready
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-10593199-kafka-clients is ready
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-10593199-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0; ls -1
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-10593199-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0; ls -1 | sed -n '/my-topic-1999537273-1597681813/p'
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:03:34 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1344] Executing command cd /var/lib/kafka/data/kafka-log0/my-topic-1999537273-1597681813-0
/;cat 00000000000000000000.log in my-cluster-10593199-kafka-0
2022-04-06 18:03:35 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-10593199-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/my-topic-1999537273-1597681813-0
/;cat 00000000000000000000.log
2022-04-06 18:03:35 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:03:35 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1348] Topic my-topic-1999537273-1597681813 is present in kafka broker my-cluster-10593199-kafka-0 with no data
2022-04-06 18:03:35 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1fbb3f15, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092, --topic, my-topic-1999537273-1597681813], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-10593199-kafka-clients-84c44b9685-mnfm2', podNamespace='namespace-49', bootstrapServer='my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092', topicName='my-topic-1999537273-1597681813', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3c1a15b8}
2022-04-06 18:03:35 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092:my-topic-1999537273-1597681813 from pod my-cluster-10593199-kafka-clients-84c44b9685-mnfm2
2022-04-06 18:03:35 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-10593199-kafka-clients-84c44b9685-mnfm2 -n namespace-49 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092 --topic my-topic-1999537273-1597681813
2022-04-06 18:03:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:03:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:03:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@61557d8, messages=[], arguments=[--group-id, my-consumer-group-99226424, --max-messages, 100, --group-instance-id, instance861289875, --bootstrap-server, my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092, --topic, my-topic-1999537273-1597681813], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-10593199-kafka-clients-84c44b9685-mnfm2', podNamespace='namespace-49', bootstrapServer='my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092', topicName='my-topic-1999537273-1597681813', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-99226424', consumerInstanceId='instance861289875', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@41080603}
2022-04-06 18:03:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092#my-topic-1999537273-1597681813 from pod my-cluster-10593199-kafka-clients-84c44b9685-mnfm2
2022-04-06 18:03:37 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-10593199-kafka-clients-84c44b9685-mnfm2 -n namespace-49 -- /opt/kafka/consumer.sh --group-id my-consumer-group-99226424 --max-messages 100 --group-instance-id instance861289875 --bootstrap-server my-cluster-10593199-kafka-bootstrap.namespace-49.svc:9092 --topic my-topic-1999537273-1597681813
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1355] Executing command cd /var/lib/kafka/data/kafka-log0/my-topic-1999537273-1597681813-0
/;cat 00000000000000000000.log in my-cluster-10593199-kafka-0
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-10593199-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/my-topic-1999537273-1597681813-0
/;cat 00000000000000000000.log
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1364] Deleting kafka pod my-cluster-10593199-kafka-0
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1364] Deleting kafka pod my-cluster-10593199-kafka-clients-84c44b9685-mnfm2
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1368] Wait for kafka to rolling restart ...
2022-04-06 18:03:43 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-10593199-kafka rolling update
2022-04-06 18:03:53 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-10593199-kafka has been successfully rolled
2022-04-06 18:03:53 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of my-cluster-10593199-kafka to be ready
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-074c89da is in desired state: Ready
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1510] Check if Kubernetes labels are applied
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1517] Kubernetes labels are correctly set and present
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-0-my-cluster-074c89da-kafka-0 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-0-my-cluster-074c89da-kafka-1 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-0-my-cluster-074c89da-kafka-2 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-1-my-cluster-074c89da-kafka-0 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-1-my-cluster-074c89da-kafka-1 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-1-my-cluster-074c89da-kafka-2 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1525] Verifying that PVC label data-my-cluster-074c89da-zookeeper-0 - testValue = testValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1535] Replacing kafka && zookeeper labels and annotations from testKey to editedTestValue
2022-04-06 18:04:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PersistentVolumeClaimUtils:30] Wait until PVC labels will change {testKey=editedTestValue}
2022-04-06 18:04:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-10593199 will have desired state: Ready
2022-04-06 18:04:19 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-10593199 is in desired state: Ready
2022-04-06 18:04:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-10593199 is ready
2022-04-06 18:04:19 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:1371] Executing command cd /var/lib/kafka/data/kafka-log0/my-topic-1999537273-1597681813-0
/;cat 00000000000000000000.log in my-cluster-10593199-kafka-0
2022-04-06 18:04:20 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-49 exec my-cluster-10593199-kafka-0 -- /bin/bash -c cd /var/lib/kafka/data/kafka-log0/my-topic-1999537273-1597681813-0
/;cat 00000000000000000000.log
2022-04-06 18:04:20 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:04:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:04:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testMessagesAreStoredInDisk
2022-04-06 18:04:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1999537273-1597681813 in namespace namespace-49
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PersistentVolumeClaimUtils:46] PVC labels has changed {testKey=editedTestValue}
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PersistentVolumeClaimUtils:50] Wait until PVC annotation will change {testKey=editedTestValue}
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PersistentVolumeClaimUtils:66] PVC annotation has changed {testKey=editedTestValue}
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-074c89da will have desired state: Ready
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-074c89da is in desired state: Ready
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1549] [PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:33Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-0-my-cluster-074c89da-kafka-0, namespace=namespace-50, ownerReferences=[], resourceVersion=43124, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-0-my-cluster-074c89da-kafka-0, uid=b4f9534d-865c-49aa-b87e-5a637dddf415, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=20Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-b4f9534d-865c-49aa-b87e-5a637dddf415, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=20Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:33Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-0-my-cluster-074c89da-kafka-1, namespace=namespace-50, ownerReferences=[], resourceVersion=43128, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-0-my-cluster-074c89da-kafka-1, uid=65dfcc4a-dd81-4046-9159-508fbda99579, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=20Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-65dfcc4a-dd81-4046-9159-508fbda99579, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=20Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:33Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-0-my-cluster-074c89da-kafka-2, namespace=namespace-50, ownerReferences=[], resourceVersion=43131, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-0-my-cluster-074c89da-kafka-2, uid=644001ce-0aae-47a8-a052-9d2f31dfb779, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=20Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-644001ce-0aae-47a8-a052-9d2f31dfb779, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=20Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=true, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:33Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-1-my-cluster-074c89da-kafka-0, namespace=namespace-50, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-074c89da, uid=3d14ff8e-ccf8-431a-8a7f-f636e8acebd8, additionalProperties={})], resourceVersion=43126, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-1-my-cluster-074c89da-kafka-0, uid=3f41b656-f8a7-46fb-9e04-d41b41108a7d, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=10Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-3f41b656-f8a7-46fb-9e04-d41b41108a7d, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=10Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=true, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:33Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-1-my-cluster-074c89da-kafka-1, namespace=namespace-50, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-074c89da, uid=3d14ff8e-ccf8-431a-8a7f-f636e8acebd8, additionalProperties={})], resourceVersion=43129, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-1-my-cluster-074c89da-kafka-1, uid=c148e7be-8821-4633-8154-31d0549fbc52, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=10Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-c148e7be-8821-4633-8154-31d0549fbc52, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=10Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=true, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:33Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-kafka, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-1-my-cluster-074c89da-kafka-2, namespace=namespace-50, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-074c89da, uid=3d14ff8e-ccf8-431a-8a7f-f636e8acebd8, additionalProperties={})], resourceVersion=43130, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-1-my-cluster-074c89da-kafka-2, uid=b7c63d09-c03b-401d-b3a7-4588c2a65e57, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=10Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-b7c63d09-c03b-401d-b3a7-4588c2a65e57, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=10Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={}), PersistentVolumeClaim(apiVersion=v1, kind=PersistentVolumeClaim, metadata=ObjectMeta(annotations={pv.kubernetes.io/bind-completed=yes, strimzi.io/delete-claim=false, testKey=editedTestValue}, clusterName=null, creationTimestamp=2022-04-06T18:03:06Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[kubernetes.io/pvc-protection], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-074c89da, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=zookeeper, app.kubernetes.io/part-of=strimzi-my-cluster-074c89da, strimzi.io/cluster=my-cluster-074c89da, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-074c89da-zookeeper, test.case=testLabelsAndAnnotationForPVC, testKey=editedTestValue}, managedFields=[], name=data-my-cluster-074c89da-zookeeper-0, namespace=namespace-50, ownerReferences=[], resourceVersion=43112, selfLink=/api/v1/namespaces/namespace-50/persistentvolumeclaims/data-my-cluster-074c89da-zookeeper-0, uid=9dd9ba6d-e765-476a-9276-1f5e76cffb12, additionalProperties={}), spec=PersistentVolumeClaimSpec(accessModes=[ReadWriteOnce], dataSource=null, dataSourceRef=null, resources=ResourceRequirements(limits=null, requests={storage=3Gi}, additionalProperties={}), selector=null, storageClassName=standard, volumeMode=Filesystem, volumeName=pvc-9dd9ba6d-e765-476a-9276-1f5e76cffb12, additionalProperties={}), status=PersistentVolumeClaimStatus(accessModes=[ReadWriteOnce], allocatedResources=null, capacity={storage=3Gi}, conditions=[], phase=Bound, resizeStatus=null, additionalProperties={}), additionalProperties={})]
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-0-my-cluster-074c89da-kafka-0 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-0-my-cluster-074c89da-kafka-1 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-0-my-cluster-074c89da-kafka-2 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-1-my-cluster-074c89da-kafka-0 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-1-my-cluster-074c89da-kafka-1 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-1-my-cluster-074c89da-kafka-2 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:1554] Verifying replaced PVC label data-my-cluster-074c89da-zookeeper-0 - testValue = editedTestValue
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testLabelsAndAnnotationForPVC
2022-04-06 18:04:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-074c89da in namespace namespace-50
2022-04-06 18:04:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-10593199-kafka-clients in namespace namespace-49
2022-04-06 18:04:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:04:31 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-50 for test case:testLabelsAndAnnotationForPVC
2022-04-06 18:04:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testLabelsAndAnnotationForPVC-FINISHED
2022-04-06 18:04:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:04:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:04:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testEODeletion-STARTED
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-51 for test case:testPersistentStorageSize
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-51
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-51
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-51
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-17cd5c2f in namespace namespace-51
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-51
2022-04-06 18:04:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-17cd5c2f will have desired state: Ready
2022-04-06 18:05:20 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-10593199 in namespace namespace-49
2022-04-06 18:05:30 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:05:30 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-49 for test case:testMessagesAreStoredInDisk
2022-04-06 18:05:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testMessagesAreStoredInDisk-FINISHED
2022-04-06 18:05:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:05:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:05:35 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.KafkaST.testJvmAndResources-STARTED
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-52 for test case:testLabelModificationDoesNotBreakCluster
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-52
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-52
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-52
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-25764cc3 in namespace namespace-52
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-52
2022-04-06 18:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25764cc3 will have desired state: Ready
2022-04-06 18:06:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-17cd5c2f is in desired state: Ready
2022-04-06 18:06:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-12920862-1402921330 in namespace namespace-52
2022-04-06 18:06:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-51
2022-04-06 18:06:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-12920862-1402921330 will have desired state: Ready
2022-04-06 18:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-12920862-1402921330 is in desired state: Ready
2022-04-06 18:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-17cd5c2f-kafka-clients in namespace namespace-52
2022-04-06 18:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-51
2022-04-06 18:06:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-17cd5c2f-kafka-clients will be ready
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-17cd5c2f-kafka-clients is ready
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1694] Checking volume data-0-my-cluster-17cd5c2f-kafka-0 and size of storage 70Gi
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1694] Checking volume data-0-my-cluster-17cd5c2f-kafka-1 and size of storage 70Gi
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1694] Checking volume data-1-my-cluster-17cd5c2f-kafka-0 and size of storage 20Gi
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:1694] Checking volume data-1-my-cluster-17cd5c2f-kafka-1 and size of storage 20Gi
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaST:983] Checking produced and consumed messages to pod:my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@422ae6cb, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092, --topic, my-topic-12920862-1402921330], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb', podNamespace='namespace-51', bootstrapServer='my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092', topicName='my-topic-12920862-1402921330', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@53d6c2f7}
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092:my-topic-12920862-1402921330 from pod my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb
2022-04-06 18:06:20 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb -n namespace-51 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092 --topic my-topic-12920862-1402921330
2022-04-06 18:06:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:06:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:06:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@9a94301, messages=[], arguments=[--group-id, my-consumer-group-1519663193, --max-messages, 100, --group-instance-id, instance569478345, --bootstrap-server, my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092, --topic, my-topic-12920862-1402921330], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb', podNamespace='namespace-51', bootstrapServer='my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092', topicName='my-topic-12920862-1402921330', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1519663193', consumerInstanceId='instance569478345', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3ff8ee0c}
2022-04-06 18:06:22 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092#my-topic-12920862-1402921330 from pod my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb
2022-04-06 18:06:22 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-17cd5c2f-kafka-clients-54549dd65-7p8cb -n namespace-51 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1519663193 --max-messages 100 --group-instance-id instance569478345 --bootstrap-server my-cluster-17cd5c2f-kafka-bootstrap.namespace-51.svc:9092 --topic my-topic-12920862-1402921330
2022-04-06 18:06:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:06:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:06:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:06:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testPersistentStorageSize
2022-04-06 18:06:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-12920862-1402921330 in namespace namespace-51
2022-04-06 18:06:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-17cd5c2f-kafka-clients in namespace namespace-51
2022-04-06 18:06:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25764cc3 is in desired state: Ready
2022-04-06 18:06:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-178412413-273707884 in namespace namespace-52
2022-04-06 18:06:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-52
2022-04-06 18:06:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-178412413-273707884 will have desired state: Ready
2022-04-06 18:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-178412413-273707884 is in desired state: Ready
2022-04-06 18:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-25764cc3-kafka-clients in namespace namespace-52
2022-04-06 18:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-52
2022-04-06 18:06:49 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-25764cc3-kafka-clients will be ready
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-25764cc3-kafka-clients is ready
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1078] Waiting for kafka stateful set labels changed {label-name-1=name-of-the-label-1, label-name-2=name-of-the-label-2}
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-1 -> name-of-the-label-1
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-2 -> name-of-the-label-2
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1081] Getting labels from stateful set resource
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1084] Verifying default labels in the Kafka CR
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1095] Setting new values of labels from name-of-the-label-1 to new-name-of-the-label-1 | from name-of-the-label-2 to new-name-of-the-label-2 and adding one label-name-3 with value name-of-the-label-3
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1098] Edit kafka labels in Kafka CR
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1109] Waiting for kafka service labels changed {label-name-1=new-name-of-the-label-1, label-name-2=new-name-of-the-label-2, label-name-3=name-of-the-label-3}
2022-04-06 18:06:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ServiceUtils:33] Waiting for Service label change label-name-1 -> new-name-of-the-label-1
2022-04-06 18:07:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-17cd5c2f in namespace namespace-51
2022-04-06 18:07:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ServiceUtils:33] Waiting for Service label change label-name-2 -> new-name-of-the-label-2
2022-04-06 18:07:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ServiceUtils:33] Waiting for Service label change label-name-3 -> name-of-the-label-3
2022-04-06 18:07:19 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1112] Verifying kafka labels via services
2022-04-06 18:07:19 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1118] Waiting for Kafka ConfigMap my-cluster-25764cc3-kafka-config in namespace namespace-52 to have new labels: {label-name-1=new-name-of-the-label-1, label-name-2=new-name-of-the-label-2, label-name-3=name-of-the-label-3}
2022-04-06 18:07:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:42] Waiting for ConfigMap my-cluster-25764cc3-kafka-config label change label-name-1 -> new-name-of-the-label-1
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:42] Waiting for ConfigMap my-cluster-25764cc3-kafka-config label change label-name-2 -> new-name-of-the-label-2
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:42] Waiting for ConfigMap my-cluster-25764cc3-kafka-config label change label-name-3 -> name-of-the-label-3
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1121] Verifying Kafka labels on ConfigMap my-cluster-25764cc3-kafka-config in namespace namespace-52
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1127] Waiting for kafka stateful set labels changed {label-name-1=new-name-of-the-label-1, label-name-2=new-name-of-the-label-2, label-name-3=name-of-the-label-3}
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-1 -> new-name-of-the-label-1
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-2 -> new-name-of-the-label-2
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:85] Waiting for Stateful set label change label-name-3 -> name-of-the-label-3
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1130] Verifying kafka labels via stateful set
2022-04-06 18:07:20 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-25764cc3-kafka rolling update
2022-04-06 18:07:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:07:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-51 for test case:testPersistentStorageSize
2022-04-06 18:08:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testPersistentStorageSize-FINISHED
2022-04-06 18:08:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-53 for test case:testJvmAndResources
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-53
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-53
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-53
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0b3f5e66 in namespace namespace-53
2022-04-06 18:08:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-53
2022-04-06 18:08:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0b3f5e66 will have desired state: Ready
2022-04-06 18:09:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0b3f5e66 is in desired state: Ready
2022-04-06 18:09:22 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-53 exec my-cluster-0b3f5e66-kafka-0 -c kafka -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-06 18:09:22 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:09:22 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-53 exec my-cluster-0b3f5e66-zookeeper-0 -c zookeeper -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-06 18:09:22 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-53 exec my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62 -c topic-operator -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-53 exec my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62 -c user-operator -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:533] Check if -D java options are present in topic-operator
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:533] Check if -D java options are present in user-operator
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaST:552] Checking no rolling update for Kafka cluster
2022-04-06 18:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 18:09:24 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 18:09:25 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 18:09:26 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 18:09:27 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 18:09:28 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 18:09:29 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 18:09:30 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 18:09:30 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-25764cc3-kafka has been successfully rolled
2022-04-06 18:09:30 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-25764cc3-kafka to be ready
2022-04-06 18:09:31 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 18:09:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 18:09:33 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 18:09:34 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 18:09:35 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 18:09:36 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 18:09:37 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 18:09:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 18:09:39 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 18:09:40 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 18:09:41 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 18:09:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 18:09:43 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 18:09:44 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 18:09:45 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 18:09:46 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 18:09:47 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 18:09:48 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 18:09:49 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 18:09:50 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 18:09:51 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 18:09:52 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 18:09:53 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 18:09:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 18:09:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 18:09:56 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25764cc3 will have desired state: Ready
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25764cc3 is in desired state: Ready
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-25764cc3 is ready
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1136] Verifying via kafka pods
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1143] Removing labels: label-name-1 -> new-name-of-the-label-1, label-name-2 -> new-name-of-the-label-2, label-name-3 -> name-of-the-label-3
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1155] Waiting for kafka service labels deletion {app.kubernetes.io/instance=my-cluster-25764cc3, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-25764cc3, controller-revision-hash=my-cluster-25764cc3-kafka-546887556f, statefulset.kubernetes.io/pod-name=my-cluster-25764cc3-kafka-0, strimzi.io/cluster=my-cluster-25764cc3, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-25764cc3-kafka, test.case=testLabelModificationDoesNotBreakCluster}
2022-04-06 18:09:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ServiceUtils:44] Service label label-name-1 change to null
2022-04-06 18:09:57 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 18:09:58 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 18:09:59 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 18:10:00 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 18:10:01 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 18:10:02 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 18:10:03 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 18:10:04 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 18:10:05 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 18:10:06 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 18:10:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 18:10:08 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 18:10:09 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 18:10:10 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 18:10:11 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 18:10:12 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 18:10:13 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-zookeeper-0=078f74c7-96ad-4703-8223-0fc1dc34b567} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 18:10:13 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 18:10:14 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 18:10:15 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 18:10:16 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 18:10:17 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 18:10:18 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 18:10:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 18:10:20 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 18:10:21 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 18:10:22 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 18:10:23 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 18:10:24 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 18:10:25 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 18:10:26 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 18:10:27 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 18:10:28 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 18:10:29 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 18:10:30 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 18:10:31 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 18:10:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 18:10:33 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 18:10:34 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 18:10:35 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 18:10:36 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 18:10:37 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 18:10:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 18:10:39 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 18:10:40 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 18:10:41 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 18:10:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 18:10:43 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 18:10:44 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 18:10:45 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 18:10:46 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 18:10:47 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 18:10:48 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 18:10:49 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 18:10:50 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 18:10:51 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 18:10:52 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 18:10:53 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 18:10:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 18:10:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 18:10:56 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 18:10:57 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 18:10:58 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 18:10:59 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 18:11:00 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 18:11:01 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 18:11:02 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 18:11:03 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-0b3f5e66-kafka-0=f0103c96-ffd4-4bca-97c1-ba57549fbee7} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 18:11:03 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 50
2022-04-06 18:11:04 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 49
2022-04-06 18:11:05 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 48
2022-04-06 18:11:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 47
2022-04-06 18:11:07 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 46
2022-04-06 18:11:08 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 45
2022-04-06 18:11:09 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 44
2022-04-06 18:11:10 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 43
2022-04-06 18:11:11 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 42
2022-04-06 18:11:12 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 41
2022-04-06 18:11:13 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 40
2022-04-06 18:11:14 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 39
2022-04-06 18:11:15 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 38
2022-04-06 18:11:16 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 37
2022-04-06 18:11:17 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 36
2022-04-06 18:11:18 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 35
2022-04-06 18:11:19 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 34
2022-04-06 18:11:20 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 33
2022-04-06 18:11:21 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 32
2022-04-06 18:11:22 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 31
2022-04-06 18:11:23 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 30
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ServiceUtils:44] Service label label-name-2 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ServiceUtils:44] Service label label-name-3 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1158] Verifying kafka labels via services
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1164] Waiting for Kafka ConfigMap my-cluster-25764cc3-kafka-config in namespace namespace-52 to have labels removed: [label-name-1, label-name-2, label-name-3]
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:53] Waiting for ConfigMap my-cluster-25764cc3-kafka-config label label-name-1 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:60] ConfigMap my-cluster-25764cc3-kafka-config label label-name-1 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:53] Waiting for ConfigMap my-cluster-25764cc3-kafka-config label label-name-2 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 29
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:60] ConfigMap my-cluster-25764cc3-kafka-config label label-name-2 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:53] Waiting for ConfigMap my-cluster-25764cc3-kafka-config label label-name-3 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ConfigMapUtils:60] ConfigMap my-cluster-25764cc3-kafka-config label label-name-3 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1167] Verifying Kafka labels on ConfigMap my-cluster-25764cc3-kafka-config in namespace namespace-52
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1173] Waiting for kafka stateful set labels changed {app.kubernetes.io/instance=my-cluster-25764cc3, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-25764cc3, controller-revision-hash=my-cluster-25764cc3-kafka-546887556f, statefulset.kubernetes.io/pod-name=my-cluster-25764cc3-kafka-0, strimzi.io/cluster=my-cluster-25764cc3, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-25764cc3-kafka, test.case=testLabelModificationDoesNotBreakCluster}
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:96] Waiting for StatefulSet label label-name-1 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:101] StatefulSet label label-name-1 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:96] Waiting for StatefulSet label label-name-2 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:101] StatefulSet label label-name-2 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:96] Waiting for StatefulSet label label-name-3 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [StatefulSetUtils:101] StatefulSet label label-name-3 change to null
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1176] Verifying kafka labels via stateful set
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-25764cc3-kafka rolling update
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-25764cc3-kafka has been successfully rolled
2022-04-06 18:11:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-25764cc3-kafka to be ready
2022-04-06 18:11:25 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 28
2022-04-06 18:11:26 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 27
2022-04-06 18:11:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 26
2022-04-06 18:11:28 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 25
2022-04-06 18:11:29 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 24
2022-04-06 18:11:30 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 23
2022-04-06 18:11:31 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 22
2022-04-06 18:11:33 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 21
2022-04-06 18:11:34 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 20
2022-04-06 18:11:35 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 19
2022-04-06 18:11:36 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 18
2022-04-06 18:11:37 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 17
2022-04-06 18:11:38 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 16
2022-04-06 18:11:39 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 15
2022-04-06 18:11:40 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 14
2022-04-06 18:11:41 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 13
2022-04-06 18:11:42 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 12
2022-04-06 18:11:43 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 11
2022-04-06 18:11:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 10
2022-04-06 18:11:45 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 9
2022-04-06 18:11:46 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 8
2022-04-06 18:11:47 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 7
2022-04-06 18:11:48 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 6
2022-04-06 18:11:49 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 5
2022-04-06 18:11:50 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 4
2022-04-06 18:11:51 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 3
2022-04-06 18:11:52 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 2
2022-04-06 18:11:53 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 1
2022-04-06 18:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:80] {my-cluster-0b3f5e66-entity-operator-fc76c564c-fhj62=98765724-3cf3-474f-a102-47f352472086} pods not rolling waiting, remaining seconds for stability 0
2022-04-06 18:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testJvmAndResources
2022-04-06 18:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0b3f5e66 in namespace namespace-53
2022-04-06 18:12:04 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:12:04 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-53 for test case:testJvmAndResources
2022-04-06 18:12:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testJvmAndResources-FINISHED
2022-04-06 18:12:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:12:18 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:12:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-54 for test case:testEODeletion
2022-04-06 18:12:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-54
2022-04-06 18:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-54
2022-04-06 18:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-54
2022-04-06 18:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c3b0e8ec in namespace namespace-54
2022-04-06 18:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-54
2022-04-06 18:12:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c3b0e8ec will have desired state: Ready
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25764cc3 will have desired state: Ready
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25764cc3 is in desired state: Ready
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-25764cc3 is ready
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1181] Waiting for kafka pod labels deletion {app.kubernetes.io/instance=my-cluster-25764cc3, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=kafka, app.kubernetes.io/part-of=strimzi-my-cluster-25764cc3, controller-revision-hash=my-cluster-25764cc3-kafka-546887556f, statefulset.kubernetes.io/pod-name=my-cluster-25764cc3-kafka-0, strimzi.io/cluster=my-cluster-25764cc3, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-25764cc3-kafka, test.case=testLabelModificationDoesNotBreakCluster}
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:262] Waiting for Pod label label-name-1 change to null
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:267] Pod label label-name-1 changed to null
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:262] Waiting for Pod label label-name-2 change to null
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:267] Pod label label-name-2 changed to null
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:262] Waiting for Pod label label-name-3 change to null
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:267] Pod label label-name-3 changed to null
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaST:1186] Verifying via kafka pods
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@325376b0, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092, --topic, my-topic-178412413-273707884], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-25764cc3-kafka-clients-5cffc9488c-mklpv', podNamespace='namespace-52', bootstrapServer='my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092', topicName='my-topic-178412413-273707884', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3ec3eb9c}
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092:my-topic-178412413-273707884 from pod my-cluster-25764cc3-kafka-clients-5cffc9488c-mklpv
2022-04-06 18:13:16 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-25764cc3-kafka-clients-5cffc9488c-mklpv -n namespace-52 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092 --topic my-topic-178412413-273707884
2022-04-06 18:13:20 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:13:20 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:13:20 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7a96752a, messages=[], arguments=[--group-id, my-consumer-group-2080741738, --max-messages, 100, --group-instance-id, instance531507178, --bootstrap-server, my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092, --topic, my-topic-178412413-273707884], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-25764cc3-kafka-clients-5cffc9488c-mklpv', podNamespace='namespace-52', bootstrapServer='my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092', topicName='my-topic-178412413-273707884', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2080741738', consumerInstanceId='instance531507178', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6cdd53c2}
2022-04-06 18:13:20 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092#my-topic-178412413-273707884 from pod my-cluster-25764cc3-kafka-clients-5cffc9488c-mklpv
2022-04-06 18:13:20 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-25764cc3-kafka-clients-5cffc9488c-mklpv -n namespace-52 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2080741738 --max-messages 100 --group-instance-id instance531507178 --bootstrap-server my-cluster-25764cc3-kafka-bootstrap.namespace-52.svc:9092 --topic my-topic-178412413-273707884
2022-04-06 18:13:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:13:25 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:13:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:13:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testLabelModificationDoesNotBreakCluster
2022-04-06 18:13:25 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-178412413-273707884 in namespace namespace-52
2022-04-06 18:13:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c3b0e8ec is in desired state: Ready
2022-04-06 18:13:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:122] Setting entity operator to null
2022-04-06 18:13:31 [ForkJoinPool-3-worker-1] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-c3b0e8ec-entity-operator is not deleted yet! Triggering force delete by cmd client!
2022-04-06 18:13:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-25764cc3-kafka-clients in namespace namespace-52
2022-04-06 18:13:36 [ForkJoinPool-3-worker-1] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-c3b0e8ec-entity-operator is not deleted yet! Triggering force delete by cmd client!
2022-04-06 18:13:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:154] Waiting when Pod my-cluster-c3b0e8ec-entity-operator-7ccf9bc4bd-q5nqp will be deleted
2022-04-06 18:13:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:171] Pod my-cluster-c3b0e8ec-entity-operator-7ccf9bc4bd-q5nqp deleted
2022-04-06 18:13:41 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaST:130] Entity operator was deleted
2022-04-06 18:13:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:13:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testEODeletion
2022-04-06 18:13:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c3b0e8ec in namespace namespace-54
2022-04-06 18:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-54 for test case:testEODeletion
2022-04-06 18:14:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-25764cc3 in namespace namespace-52
2022-04-06 18:14:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:14:26 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-52 for test case:testLabelModificationDoesNotBreakCluster
2022-04-06 18:14:35 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testEODeletion-FINISHED
2022-04-06 18:14:35 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:15:09 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.KafkaST.testLabelModificationDoesNotBreakCluster-FINISHED
2022-04-06 18:15:09 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:15:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:15:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context KafkaST is everything deleted.
2022-04-06 18:15:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 23, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,366.5 s - in io.strimzi.systemtest.kafka.KafkaST
[[1;34mINFO[m] Running io.strimzi.systemtest.kafka.ConfigProviderST
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: config-provider-st
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: config-provider-st
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: config-provider-st
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.kafka.ConfigProviderST.testConnectWithConnectorUsingConfigAndEnvProvider-STARTED
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-55 for test case:testConnectWithConnectorUsingConfigAndEnvProvider
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-55
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-55
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-55
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9497c409 in namespace namespace-55
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-06 18:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9497c409 will have desired state: Ready
2022-04-06 18:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9497c409 is in desired state: Ready
2022-04-06 18:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-9497c409 in namespace namespace-55
2022-04-06 18:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-06 18:16:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9497c409 will have desired state: Ready
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9497c409 is in desired state: Ready
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigProviderST:100] Creating needed RoleBinding and Role for Kubernetes Config Provider
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding connector-config-rb in namespace namespace-55
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-9497c409 in namespace namespace-55
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-06 18:17:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-9497c409 will have desired state: Ready
2022-04-06 18:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-9497c409 is in desired state: Ready
2022-04-06 18:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 18:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-my-consumer-group-1164545986 in namespace namespace-55
2022-04-06 18:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-55
2022-04-06 18:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-my-consumer-group-1164545986 will be in active state
2022-04-06 18:17:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-9497c409-connect-8475d8f76-b4fzt
2022-04-06 18:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-9497c409-connect-8475d8f76-b4fzt
2022-04-06 18:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testConnectWithConnectorUsingConfigAndEnvProvider
2022-04-06 18:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding connector-config-rb in namespace namespace-55
2022-04-06 18:17:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-9497c409 in namespace namespace-55
2022-04-06 18:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-my-consumer-group-1164545986 in namespace namespace-55
2022-04-06 18:17:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-9497c409 in namespace namespace-55
2022-04-06 18:17:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9497c409 in namespace namespace-55
2022-04-06 18:17:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:17:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-55 for test case:testConnectWithConnectorUsingConfigAndEnvProvider
2022-04-06 18:18:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.kafka.ConfigProviderST.testConnectWithConnectorUsingConfigAndEnvProvider-FINISHED
2022-04-06 18:18:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:18:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:18:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ConfigProviderST is everything deleted.
2022-04-06 18:18:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 210.47 s - in io.strimzi.systemtest.kafka.ConfigProviderST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.custom.CustomAuthorizerST
2022-04-06 18:18:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: custom-authorizer-st
2022-04-06 18:18:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: custom-authorizer-st
2022-04-06 18:18:45 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: custom-authorizer-st
2022-04-06 18:18:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka custom-authorizer in namespace custom-authorizer-st
2022-04-06 18:18:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: custom-authorizer will have desired state: Ready
2022-04-06 18:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: custom-authorizer is in desired state: Ready
2022-04-06 18:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclRuleReadAndWrite-STARTED
2022-04-06 18:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclWithSuperUser-STARTED
2022-04-06 18:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1665480903-79736535 in namespace custom-authorizer-st
2022-04-06 18:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1260359742-597205446 in namespace custom-authorizer-st
2022-04-06 18:19:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1260359742-597205446 will have desired state: Ready
2022-04-06 18:19:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1665480903-79736535 will have desired state: Ready
2022-04-06 18:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1260359742-597205446 is in desired state: Ready
2022-04-06 18:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser sre-admin in namespace custom-authorizer-st
2022-04-06 18:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1665480903-79736535 is in desired state: Ready
2022-04-06 18:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser kafka-user-write in namespace custom-authorizer-st
2022-04-06 18:19:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: kafka-user-write will have desired state: Ready
2022-04-06 18:19:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: sre-admin will have desired state: Ready
2022-04-06 18:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: kafka-user-write is in desired state: Ready
2022-04-06 18:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser kafka-user-read in namespace custom-authorizer-st
2022-04-06 18:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: sre-admin is in desired state: Ready
2022-04-06 18:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a521a7b6-kafka-clients in namespace custom-authorizer-st
2022-04-06 18:19:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: kafka-user-read will have desired state: Ready
2022-04-06 18:19:53 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a521a7b6-kafka-clients will be ready
2022-04-06 18:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: kafka-user-read is in desired state: Ready
2022-04-06 18:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [CustomAuthorizerST:105] Checking KafkaUser kafka-user-write that is able to send messages to topic 'my-topic-1665480903-79736535'
2022-04-06 18:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a521a7b6-kafka-clients is ready
2022-04-06 18:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [CustomAuthorizerST:173] Checking kafka super user:sre-admin that is able to send messages to topic:my-topic-1260359742-597205446
2022-04-06 18:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6063499c-kafka-clients in namespace custom-authorizer-st
2022-04-06 18:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@db37f97, messages=[], arguments=[USER=sre_admin, --max-messages, 100, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1260359742-597205446], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a521a7b6-kafka-clients-78c4486d5d-t7djc', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1260359742-597205446', maxMessages=100, kafkaUsername='sre-admin', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@d3d22ea}
2022-04-06 18:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1260359742-597205446 from pod my-cluster-a521a7b6-kafka-clients-78c4486d5d-t7djc
2022-04-06 18:19:54 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a521a7b6-kafka-clients-78c4486d5d-t7djc -n custom-authorizer-st -- /opt/kafka/producer.sh USER=sre_admin --max-messages 100 --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1260359742-597205446
2022-04-06 18:19:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6063499c-kafka-clients will be ready
2022-04-06 18:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6063499c-kafka-clients is ready
2022-04-06 18:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@c04a49a, messages=[], arguments=[USER=kafka_user_write, --max-messages, 500, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1665480903-79736535], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1665480903-79736535', maxMessages=500, kafkaUsername='kafka-user-write', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@246dcbe0}
2022-04-06 18:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 500 messages to custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1665480903-79736535 from pod my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9
2022-04-06 18:19:55 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9 -n custom-authorizer-st -- /opt/kafka/producer.sh USER=kafka_user_write --max-messages 500 --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1665480903-79736535
2022-04-06 18:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 18:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [CustomAuthorizerST:187] Checking kafka super user:sre-admin that is able to read messages to topic:my-topic-2043299187-274155238 regardless that we configured Acls with only write operation
2022-04-06 18:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@16e2a752, messages=[], arguments=[--group-id, my-consumer-group-2007128061, USER=sre_admin, --max-messages, 100, --group-instance-id, instance1424324796, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1260359742-597205446], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a521a7b6-kafka-clients-78c4486d5d-t7djc', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1260359742-597205446', maxMessages=100, kafkaUsername='sre-admin', consumerGroupName='my-consumer-group-2007128061', consumerInstanceId='instance1424324796', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@722ce18f}
2022-04-06 18:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1260359742-597205446 from pod my-cluster-a521a7b6-kafka-clients-78c4486d5d-t7djc
2022-04-06 18:19:58 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-a521a7b6-kafka-clients-78c4486d5d-t7djc -n custom-authorizer-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-2007128061 USER=sre_admin --max-messages 100 --group-instance-id instance1424324796 --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1260359742-597205446
2022-04-06 18:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 500 messages
2022-04-06 18:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@45bb13d7, messages=[], arguments=[--group-id, my-consumer-group-1367553470, USER=kafka_user_write, --max-messages, 500, --group-instance-id, instance743609332, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1665480903-79736535], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1665480903-79736535', maxMessages=500, kafkaUsername='kafka-user-write', consumerGroupName='my-consumer-group-1367553470', consumerInstanceId='instance743609332', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@f43eb14}
2022-04-06 18:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 500 messages from custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1665480903-79736535 from pod my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9
2022-04-06 18:19:59 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9 -n custom-authorizer-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-1367553470 USER=kafka_user_write --max-messages 500 --group-instance-id instance743609332 --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1665480903-79736535
2022-04-06 18:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 0 messages
2022-04-06 18:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@23f3d0f7, messages=[], arguments=[--group-id, consumer-group-name-1, USER=kafka_user_read, --max-messages, 500, --group-instance-id, instance1587640037, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1665480903-79736535], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1665480903-79736535', maxMessages=500, kafkaUsername='kafka-user-read', consumerGroupName='consumer-group-name-1', consumerInstanceId='instance1587640037', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6cb25f8e}
2022-04-06 18:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 500 messages from custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1665480903-79736535 from pod my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9
2022-04-06 18:20:03 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9 -n custom-authorizer-st -- /opt/kafka/consumer.sh --group-id consumer-group-name-1 USER=kafka_user_read --max-messages 500 --group-instance-id instance1587640037 --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1665480903-79736535
2022-04-06 18:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testAclWithSuperUser
2022-04-06 18:20:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser sre-admin in namespace custom-authorizer-st
2022-04-06 18:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 500 messages
2022-04-06 18:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [CustomAuthorizerST:137] Checking KafkaUser kafka-user-read that is not able to send messages to topic 'my-topic-1665480903-79736535'
2022-04-06 18:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2ec00cd5, messages=[], arguments=[USER=kafka_user_read, --max-messages, 500, --bootstrap-server, custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093, --topic, my-topic-1665480903-79736535], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9', podNamespace='custom-authorizer-st', bootstrapServer='custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093', topicName='my-topic-1665480903-79736535', maxMessages=500, kafkaUsername='kafka-user-read', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@33f894d6}
2022-04-06 18:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 500 messages to custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093:my-topic-1665480903-79736535 from pod my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9
2022-04-06 18:20:09 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6063499c-kafka-clients-7f44d79f66-pkvb9 -n custom-authorizer-st -- /opt/kafka/producer.sh USER=kafka_user_read --max-messages 500 --bootstrap-server custom-authorizer-kafka-bootstrap.custom-authorizer-st.svc:9093 --topic my-topic-1665480903-79736535
2022-04-06 18:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced -1 messages
2022-04-06 18:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAclRuleReadAndWrite
2022-04-06 18:20:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser kafka-user-read in namespace custom-authorizer-st
2022-04-06 18:20:15 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a521a7b6-kafka-clients in namespace custom-authorizer-st
2022-04-06 18:20:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6063499c-kafka-clients in namespace custom-authorizer-st
2022-04-06 18:20:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1260359742-597205446 in namespace custom-authorizer-st
2022-04-06 18:21:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser kafka-user-write in namespace custom-authorizer-st
2022-04-06 18:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclWithSuperUser-FINISHED
2022-04-06 18:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:21:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1665480903-79736535 in namespace custom-authorizer-st
2022-04-06 18:21:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:21:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.custom.CustomAuthorizerST.testAclRuleReadAndWrite-FINISHED
2022-04-06 18:21:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:21:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:21:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for CustomAuthorizerST
2022-04-06 18:21:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka custom-authorizer in namespace custom-authorizer-st
2022-04-06 18:21:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 169.139 s - in io.strimzi.systemtest.security.custom.CustomAuthorizerST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.OpaIntegrationST
2022-04-06 18:21:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: opa-integration-st
2022-04-06 18:21:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: opa-integration-st
2022-04-06 18:21:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: opa-integration-st
2022-04-06 18:21:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka opa-cluster in namespace opa-integration-st
2022-04-06 18:21:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: opa-cluster will have desired state: Ready
2022-04-06 18:22:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: opa-cluster is in desired state: Ready
2022-04-06 18:22:56 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:22:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:22:56 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorizationSuperUser-STARTED
2022-04-06 18:22:56 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorization-STARTED
2022-04-06 18:22:56 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:22:56 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:22:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser good-user in namespace opa-integration-st
2022-04-06 18:22:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-30087359-935506946 in namespace opa-integration-st
2022-04-06 18:22:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-30087359-935506946 will have desired state: Ready
2022-04-06 18:22:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: good-user will have desired state: Ready
2022-04-06 18:22:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-30087359-935506946 is in desired state: Ready
2022-04-06 18:22:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser arnost in namespace opa-integration-st
2022-04-06 18:22:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: good-user is in desired state: Ready
2022-04-06 18:22:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bad-user in namespace opa-integration-st
2022-04-06 18:22:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: arnost will have desired state: Ready
2022-04-06 18:22:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bad-user will have desired state: Ready
2022-04-06 18:22:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: arnost is in desired state: Ready
2022-04-06 18:22:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-849c1a2a-kafka-clients in namespace opa-integration-st
2022-04-06 18:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: bad-user is in desired state: Ready
2022-04-06 18:22:58 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-849c1a2a-kafka-clients will be ready
2022-04-06 18:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-2d42e20c-kafka-clients in namespace opa-integration-st
2022-04-06 18:22:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-2d42e20c-kafka-clients will be ready
2022-04-06 18:23:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-2d42e20c-kafka-clients is ready
2022-04-06 18:23:00 [ForkJoinPool-3-worker-3] [32mINFO [m [OpaIntegrationST:72] Checking KafkaUser good-user that is able to send and receive messages to/from topic 'my-topic-45137285-1662608141'
2022-04-06 18:23:00 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@119669b2, messages=[], arguments=[USER=good_user, --max-messages, 100, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-45137285-1662608141], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-45137285-1662608141', maxMessages=100, kafkaUsername='good-user', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3472462a}
2022-04-06 18:23:00 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-45137285-1662608141 from pod my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2
2022-04-06 18:23:00 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2 -n opa-integration-st -- /opt/kafka/producer.sh USER=good_user --max-messages 100 --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-45137285-1662608141
2022-04-06 18:23:01 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-849c1a2a-kafka-clients is ready
2022-04-06 18:23:01 [ForkJoinPool-3-worker-7] [32mINFO [m [OpaIntegrationST:120] Checking KafkaUser good-user that is able to send and receive messages to/from topic 'my-topic-30087359-935506946'
2022-04-06 18:23:01 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@287507e2, messages=[], arguments=[USER=arnost, --max-messages, 100, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-30087359-935506946], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-849c1a2a-kafka-clients-546846654b-nq7lc', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-30087359-935506946', maxMessages=100, kafkaUsername='arnost', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2baeedf2}
2022-04-06 18:23:01 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-30087359-935506946 from pod my-cluster-849c1a2a-kafka-clients-546846654b-nq7lc
2022-04-06 18:23:01 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-849c1a2a-kafka-clients-546846654b-nq7lc -n opa-integration-st -- /opt/kafka/producer.sh USER=arnost --max-messages 100 --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-30087359-935506946
2022-04-06 18:23:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:23:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 18:23:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@498243b0, messages=[], arguments=[--group-id, consumer-group-name-2, USER=arnost, --max-messages, 100, --group-instance-id, instance425839147, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-30087359-935506946], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-849c1a2a-kafka-clients-546846654b-nq7lc', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-30087359-935506946', maxMessages=100, kafkaUsername='arnost', consumerGroupName='consumer-group-name-2', consumerInstanceId='instance425839147', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@151ffd9c}
2022-04-06 18:23:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-30087359-935506946 from pod my-cluster-849c1a2a-kafka-clients-546846654b-nq7lc
2022-04-06 18:23:04 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-849c1a2a-kafka-clients-546846654b-nq7lc -n opa-integration-st -- /opt/kafka/consumer.sh --group-id consumer-group-name-2 USER=arnost --max-messages 100 --group-instance-id instance425839147 --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-30087359-935506946
2022-04-06 18:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 18:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6a18c4f8, messages=[], arguments=[--group-id, my-consumer-group-1774700348, USER=good_user, --max-messages, 100, --group-instance-id, instance765810650, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-45137285-1662608141], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-45137285-1662608141', maxMessages=100, kafkaUsername='good-user', consumerGroupName='my-consumer-group-1774700348', consumerInstanceId='instance765810650', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4cfd885b}
2022-04-06 18:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-45137285-1662608141 from pod my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2
2022-04-06 18:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2 -n opa-integration-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-1774700348 USER=good_user --max-messages 100 --group-instance-id instance765810650 --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-45137285-1662608141
2022-04-06 18:23:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:23:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:23:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:23:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testOpaAuthorizationSuperUser
2022-04-06 18:23:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser arnost in namespace opa-integration-st
2022-04-06 18:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [OpaIntegrationST:89] Checking KafkaUser bad-user that is not able to send or receive messages to/from topic 'my-topic-45137285-1662608141'
2022-04-06 18:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@15596c90, messages=[], arguments=[USER=bad_user, --max-messages, 100, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-45137285-1662608141], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-45137285-1662608141', maxMessages=100, kafkaUsername='bad-user', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4861ebf0}
2022-04-06 18:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-45137285-1662608141 from pod my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2
2022-04-06 18:23:12 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2 -n opa-integration-st -- /opt/kafka/producer.sh USER=bad_user --max-messages 100 --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-45137285-1662608141
2022-04-06 18:23:15 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:23:15 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced -1 messages
2022-04-06 18:23:15 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1955bc74, messages=[], arguments=[--group-id, my-consumer-group-1774700348, USER=bad_user, --max-messages, 100, --group-instance-id, instance1426040669, --bootstrap-server, opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093, --topic, my-topic-45137285-1662608141], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2', podNamespace='opa-integration-st', bootstrapServer='opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093', topicName='my-topic-45137285-1662608141', maxMessages=100, kafkaUsername='bad-user', consumerGroupName='my-consumer-group-1774700348', consumerInstanceId='instance1426040669', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@35c68130}
2022-04-06 18:23:15 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093:my-topic-45137285-1662608141 from pod my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2
2022-04-06 18:23:15 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-2d42e20c-kafka-clients-85858cd64-xddd2 -n opa-integration-st -- /opt/kafka/consumer.sh --group-id my-consumer-group-1774700348 USER=bad_user --max-messages 100 --group-instance-id instance1426040669 --bootstrap-server opa-cluster-kafka-bootstrap.opa-integration-st.svc:9093 --topic my-topic-45137285-1662608141
2022-04-06 18:23:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-849c1a2a-kafka-clients in namespace opa-integration-st
2022-04-06 18:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 0 messages
2022-04-06 18:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testOpaAuthorization
2022-04-06 18:23:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bad-user in namespace opa-integration-st
2022-04-06 18:23:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-2d42e20c-kafka-clients in namespace opa-integration-st
2022-04-06 18:24:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-30087359-935506946 in namespace opa-integration-st
2022-04-06 18:24:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:24:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorizationSuperUser-FINISHED
2022-04-06 18:24:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:24:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser good-user in namespace opa-integration-st
2022-04-06 18:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.OpaIntegrationST.testOpaAuthorization-FINISHED
2022-04-06 18:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OpaIntegrationST
2022-04-06 18:24:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka opa-cluster in namespace opa-integration-st
2022-04-06 18:24:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 225.452 s - in io.strimzi.systemtest.security.OpaIntegrationST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.SecurityST
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: security-st
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: security-st
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: security-st
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertificates-STARTED
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewCaCertsTriggerByExpiredCertificate-STARTED
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-56 for test case:testCertificates
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-56
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-56
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-56
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:118] Running testCertificates my-cluster-68eb9809
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-57 for test case:testAutoRenewCaCertsTriggerByExpiredCertificate
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-57
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-68eb9809 in namespace namespace-56
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-56
2022-04-06 18:25:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-68eb9809 will have desired state: Ready
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-57
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-57
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:70] Creating secret my-cluster-0543f8c3-cluster-ca-cert
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0543f8c3 in namespace namespace-57
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-06 18:25:20 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0543f8c3 will have desired state: Ready
2022-04-06 18:27:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-68eb9809 is in desired state: Ready
2022-04-06 18:27:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:122] Check Kafka bootstrap certificate
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-bootstrap:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-bootstrap
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:125] OPENSSL OUTPUT: 

CONNECTED(00000003)
---
Certificate chain
 0 s:O = io.strimzi, CN = my-cluster-68eb9809-kafka
   i:O = io.strimzi, CN = cluster-ca v0
-----BEGIN CERTIFICATE-----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-----END CERTIFICATE-----
 1 s:O = io.strimzi, CN = cluster-ca v0
   i:O = io.strimzi, CN = cluster-ca v0
-----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUU5rmJcTZDDf7hqgil1fP5DKHEFcwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjA0MDYxODI1MjBaFw0yMzA0MDYxODI1MjBaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQDSS5/gqu7IPmqOuCvdvEs3NHYulpxBNDl3ANmjHwy2
dYzlGPn049JlhewZyce4ClP72Pp8fMppMyzxjZOTcE1CTWozxyEjMgbdTVlKPxOg
pOAp5imjCPYvZwhZKv8nfKlhJjyE7EzAlO1GTloGqRP0Ny+aMWqJg20S39jsZ+62
MqzEQ2xq7wxdbOYI4gdV9hcap0yGhL+g2bRy6xITWTl52HAIWGxQ6MbDnygjmV8S
7fF+rtYbvOqFqR2i4Ao/CnkLdVyCOOF5yUjyv0gSLN/s2Dj6YPL+DJrGs4mIXcDk
c7u+FITkF6pCFhKTCeYDVd5frd65GfEKKpApeMFle4siR5FAxfR/QbvsdbWqwomh
tDDA/mXicnaiVjR04H5PDGvVL8hIrY5fhAbAUUS7QugQNc6nkEJ8aHCo5oTZdFvQ
7EuJYSJgeepUbDwmLj45EVLNiv7kR2k99Sg/pXDAqoLFqxY7JKT2I6/REBjh++m7
AfZPzpyDAYN8Q/ns7hu1Axf9UC4M9wPTuvBZm5LJ9yhTW2/ZnWbSwIYuuZzTeAc7
4+TszJN7/19pvZWZbM86XLdRXHsn0awbyITp+aypAWqphcACVOkKrsI2D7PB1c9W
c/aCch0ymyek2s6dE/bYs1dsJGPLjkl6lud/PjiM/oLFatsvMbrOIHvMxUPbOaX2
dwIDAQABo0UwQzAdBgNVHQ4EFgQUR4/ftfIiDus79Te6/atlF91jJygwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AM0w5M43g4DEAPcf+B3PYq+8v7RUNmx+AsbJ9V5S1jyGeNZkIYjCLnY742+tzzDb
WricGTTN2R6FINthFhM7cqq1xuXny6wGwJS359njoOQptTVfVNXBVyfv5LAn+vBr
q7X0EQFUUi/CPLbSzW7HWzMxMK7Dxe1xkH/uBzKuu+0p++lxJvafEH07CKzS2diy
hF6ZBIj98R2cSwXkI2J10KKlPkeBJS3iH86+A+Y0eqqss4Fl5Juf+/P7jQjb04za
3bVF+M7YA9vpfbEV5v32OjUdKXhXQwQ4s5ia9P1OGmS3/gHxqHiJ9aRabtf3aalC
eW+s7UIkC1hpO34rtQh0EPrHf1dqO8fkzhtxczZnM4IWvhmRP8FTphaeWjJXbXIp
uAdNeWRqnBTlr+VlS+LeOvP4w+u5qAJx0onkimiMsr1NyWRW7fXU7BASRpIrlTY1
vvQjeDfYBWtk2DZfOD1tFij2zaiflDeUyixCJkr/j61D+NzZzgME9jWp+UFAjJWN
b/pzz03P78Q6L4i1MFOGPkCxNhZDmSdmPTezOqV4fTGHbbVxkhM444zA7+7j54Wb
L5WvJfA+2C6A1waASTkADm+aqHPxz1QsSmwlYgABRgwe3oZoYIIdCmbfvaQC/WHl
jOABpbFKcDLTWnA8kUuotKRVKiT3iVGUyncw4GPYMQDp
-----END CERTIFICATE-----
---
Server certificate
subject=O = io.strimzi, CN = my-cluster-68eb9809-kafka

issuer=O = io.strimzi, CN = cluster-ca v0

---
No client certificate CA names sent
Peer signing digest: SHA256
Peer signature type: RSA-PSS
Server Temp Key: X25519, 253 bits
---
SSL handshake has read 3489 bytes and written 369 bytes
Verification: OK
Verified peername: my-cluster-68eb9809-kafka-bootstrap
---
New, TLSv1.3, Cipher is TLS_AES_256_GCM_SHA384
Server public key is 2048 bit
Secure Renegotiation IS NOT supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
Early data was not sent
Verify return code: 0 (ok)
---



2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:128] Check zookeeper client certificate
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-client:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-client -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-0.key
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:139] Checking certificates for podId 0
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9091
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-0.my-cluster-68eb9809-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-0.my-cluster-68eb9809-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-0.key
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9093
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-0.my-cluster-68eb9809-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-0.my-cluster-68eb9809-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-0.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-0.key
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 2181
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-zookeeper-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-0.my-cluster-68eb9809-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-0.my-cluster-68eb9809-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-0.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-0.key
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 3888
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-zookeeper-0 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-0.my-cluster-68eb9809-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-0.my-cluster-68eb9809-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-0.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-0.key
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:139] Checking certificates for podId 1
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9091
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-1.my-cluster-68eb9809-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-1.my-cluster-68eb9809-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-1.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-1.key
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9093
2022-04-06 18:27:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:27:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClientsCaKeysTriggeredByAnno-STARTED
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-1.my-cluster-68eb9809-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-1.my-cluster-68eb9809-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-1.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-1.key
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 2181
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-zookeeper-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-1.my-cluster-68eb9809-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-1.my-cluster-68eb9809-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-1.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-1.key
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 3888
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-zookeeper-1 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-1.my-cluster-68eb9809-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-1.my-cluster-68eb9809-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-1.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-1.key
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:139] Checking certificates for podId 2
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9091
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-2.my-cluster-68eb9809-kafka-brokers:9091 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-2.my-cluster-68eb9809-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-2.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-2.key
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:141] Check kafka certificate for port 9093
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-kafka-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-kafka-2.my-cluster-68eb9809-kafka-brokers:9093 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-kafka-2.my-cluster-68eb9809-kafka-brokers.namespace-56.svc.cluster.local -cert /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-2.crt -key /opt/kafka/broker-certs/my-cluster-68eb9809-kafka-2.key
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 2181
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-zookeeper-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-2.my-cluster-68eb9809-zookeeper-nodes:2181 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-2.my-cluster-68eb9809-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-2.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-2.key
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:148] Check zookeeper certificate for port 3888
2022-04-06 18:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-56 exec my-cluster-68eb9809-zookeeper-2 -- /bin/bash -c echo -n | openssl s_client -connect my-cluster-68eb9809-zookeeper-2.my-cluster-68eb9809-zookeeper-nodes:3888 -showcerts -CAfile /opt/kafka/cluster-ca-certs/ca.crt -verify_hostname my-cluster-68eb9809-zookeeper-2.my-cluster-68eb9809-zookeeper-nodes.namespace-56.svc.cluster.local -cert /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-2.crt -key /opt/kafka/zookeeper-node-certs/my-cluster-68eb9809-zookeeper-2.key
2022-04-06 18:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 18:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCertificates
2022-04-06 18:27:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-68eb9809 in namespace namespace-56
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-58 for test case:testAutoReplaceClientsCaKeysTriggeredByAnno
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-58
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-58
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-58
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9939db3a in namespace namespace-58
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-06 18:27:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9939db3a will have desired state: Ready
2022-04-06 18:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:27:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-56 for test case:testCertificates
2022-04-06 18:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertificates-FINISHED
2022-04-06 18:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:28:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testClientsCACertRenew-STARTED
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-59 for test case:testClientsCACertRenew
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-59
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-59
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-59
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1f2990d3 in namespace namespace-59
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-59
2022-04-06 18:28:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1f2990d3 will have desired state: Ready
2022-04-06 18:28:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0543f8c3 is in desired state: Ready
2022-04-06 18:28:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1509045005-443690926 in namespace namespace-59
2022-04-06 18:28:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-06 18:28:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1509045005-443690926 will have desired state: Ready
2022-04-06 18:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1509045005-443690926 is in desired state: Ready
2022-04-06 18:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2065837683-1340310002 in namespace namespace-59
2022-04-06 18:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-06 18:28:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2065837683-1340310002 will have desired state: Ready
2022-04-06 18:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2065837683-1340310002 is in desired state: Ready
2022-04-06 18:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0543f8c3-kafka-clients in namespace namespace-59
2022-04-06 18:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-57
2022-04-06 18:28:45 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0543f8c3-kafka-clients will be ready
2022-04-06 18:28:47 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0543f8c3-kafka-clients is ready
2022-04-06 18:28:47 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:28:47 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:660] Checking produced and consumed messages to pod:my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l
2022-04-06 18:28:47 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5ff6e827, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-2065837683-1340310002], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l', podNamespace='namespace-57', bootstrapServer='my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-2065837683-1340310002', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6e911bd2}
2022-04-06 18:28:47 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092:my-topic-2065837683-1340310002 from pod my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l
2022-04-06 18:28:47 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l -n namespace-57 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-2065837683-1340310002
2022-04-06 18:28:49 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:28:49 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:28:49 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@43f1c0e5, messages=[], arguments=[--group-id, my-consumer-group-1463736257, --max-messages, 100, --group-instance-id, instance1583349789, --bootstrap-server, my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-2065837683-1340310002], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l', podNamespace='namespace-57', bootstrapServer='my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-2065837683-1340310002', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1463736257', consumerInstanceId='instance1583349789', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@188270e8}
2022-04-06 18:28:49 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092#my-topic-2065837683-1340310002 from pod my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l
2022-04-06 18:28:49 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l -n namespace-57 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1463736257 --max-messages 100 --group-instance-id instance1583349789 --bootstrap-server my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-2065837683-1340310002
2022-04-06 18:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:131] Waiting for Secret my-cluster-0543f8c3-cluster-ca-cert certificate change
2022-04-06 18:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:138] Certificate in Secret my-cluster-0543f8c3-cluster-ca-cert has changed, was -----BEGIN CERTIFICATE-----
MIIDKjCCAhKgAwIBAgIJAJBt0u1FFTG+MA0GCSqGSIb3DQEBCwUAMCoxEzARBgNV
BAoMCnN0cmltemkuaW8xEzARBgNVBAMMCmNsdXN0ZXItY2EwHhcNMTgxMDIzMTAw
MTExWhcNMTgxMDI0MTAwMTExWjAqMRMwEQYDVQQKDApzdHJpbXppLmlvMRMwEQYD
VQQDDApjbHVzdGVyLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
xpuYrNXYHqw3ajwd12aAeuTlAX4rVwVdPuIex6A4NL8J3d2DV+ngXgNTH//RhiF5
If5KRWSsLei5BUIrwuQutOUNCQwyACmri9+yrx6+tevligiokAUwhHxcDHZpwC3T
+2dzk/BkI++vbSuvjFmBKGQi9gfyoTnStTEQ85KVJUS170hzwDjzaEiJsKpOPx/G
+KTdkAopLucoxr4sxhYeO4mQ2PkT0QL+R8Ohs6LD6v/bqalFP+rS8vibolfxjMNm
lXQCOd8UfXy8OEOaNoNCvhnn/cT/hbEG/ARbV3hHmUh9COV+TSV5dhsbmS5h4MKw
LzP449nGQBmLSkZMu984DQIDAQABo1MwUTAdBgNVHQ4EFgQUOXubcHBZJ7vSzjpi
pfXdFSP3dsEwHwYDVR0jBBgwFoAUOXubcHBZJ7vSzjpipfXdFSP3dsEwDwYDVR0T
AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAFFPGykbzUREDMzh+33i3a8TF
UTQnPMN/SuVbpLfQdpkpLO+aVWjVvJP6qMrI7jRO5zhj4MecAf7YKpe+dyRTTz9B
Dy9BZcujHYjCKdcKBnBFQ7B1xQm9tL1bw+a3ABzSTlhLiBcCxhJEawlWy1Gh18ab
3x/Kqnz0mk/jt5+n9HwlKHuBQVIxRCsfHWwq+WvIfoxM+N//akV8/29hLUf5TlYH
F5CX4G2pA5sSdaDHQ4ekQWuqM6tfvsGLl2KOmEFEgVR4GfaWI4BsUCzlpBNcCGWv
V7klZvBxQPG7MVy0cB8yzTDtUpR0jUFUkZOSp5Pr3yWcwPWEgWkaXYfO+FWETA==
-----END CERTIFICATE-----
, is now -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUQyCQFrWcalbjAXlRvPo9zA5s56QwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjA0MDYxODI1MjFaFw0yMzA0MDYxODI1MjFaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQDJNLbJc5cZtkhAXZbZufRmvty0K/5lcjFUccrfEXGT
Y9x2Ja3saAxWMqfpWCrBH6d6sfVXQVuhKJbcElm3hWMCS23jcmxm/MDxOsfTxfss
xFw2i6pQzYDxTvMgW6o++Ysk3m2S8MGRdMKm3FwmudLg2TE5mKTSs+IZsRBCrFn0
8ZSI/4ht7cDmy8kvOYry3CZ+HpRI/u3LgGEWGLzAf+3vGaCJvDbbmj1ExDrNOOQb
DvH55qWND+2W+wTvecey+t0oGkFhb2g6qyUuYFqIqm/vEO0GbcVKepfDzprqwjJn
AKMi9UsumtxV/2+Wn38870/fxRZJc6N3w25Ctt87/wbbxwwjA7fD+Z4L3R+IHhOq
4SyY2neq0Z+vOUEm1U7WGGJIlIK8yRYZpHPprYFdsiztHhHwbeL5P1kJDzNHvlNH
2J5In7udMQTgpePDpZJHBYKMAeQXSmA3ND+sQIQm01jDQHC5rn8gTTPmz+H86yXH
zpr1b8E4LvOS3i2Vbunmgd4U0N7AAUFrW4k21S+qq6d8iQKeTBn/kFwUTK8LKutj
dyAtmYnDfWOU/dxSSQ0vdhurFjN8a6d/kdS0vXColXpQKJWMxs9ms65zdgf1WoPL
E40DbtCKzEV9YA7KN9R+gT1L9Y8tiLDsb3J+vRDV5S2+MNCxpSKzxeAqQWEbOkrM
rwIDAQABo0UwQzAdBgNVHQ4EFgQUuJbbmyZKtcp2GuxBe3yMgB0O8F4wEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AE6FOfR/XxwV1YHcw0JUkL0N+VnCBiCK8ORATXjuMVLZTul1prcuEsTZJXHVAb8G
4WVSYeaPsDQknAZHQgkvgAj4a8Ma4UifvFjKolCuup8Qp4QD2ve3xC4PuNHZ2tjR
zIg6FbFAJVRdn82CM+LzzCnt4vDbCbYFPdsFs0CqmSF9PWlAkR+oxGzTA8L0hcSa
aBVbXtDh1EVLPqH9GVrYzkG4bhE0lRJ87nmUk8zceIDPSiErjIaPBBJRa+82j5+d
9XmZJI8Ebg5+Te+XDdJlg4AwLjmvTWEkM+h1ZRvNPQLSW/3GC4kHVm5JFjL9RldL
kTVe4ycUbML8cAmqDPnpcBZ2nHMKvgwsBfDLSQ8HZ3dzqJDJYT0WpKjIKeqRTyAQ
Z9xZgoTRQBy15g19rp9vznXd8Gr+73S4eONhp2XCowK7xu+bTQRexrDJ/3D5HeMT
nXqlALLFoWN3p+HK2Yrx2HdsBl2oAh3ngiKyhOGlum1S12yhsHsUErwvElbHqEAL
9UbN0IJgmzAFJewFrApzoZf44U4mp03FnOjsKNSEyqe13VPor8NrR2NbQe1pI3pR
UouSvD6mi5eq6DuiI+7waDrLvU40qT+yVCv3Wofp2lOdKmdYIm/hnNRJyxr6WMVE
GZ7Og03hm5cLOgozinLIcWyxcflugCVjWR9vndD5ngt+
-----END CERTIFICATE-----

2022-04-06 18:28:55 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-06 18:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1f2990d3 is in desired state: Ready
2022-04-06 18:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser strimzi-tls-user-977723212 in namespace namespace-59
2022-04-06 18:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-59
2022-04-06 18:29:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: strimzi-tls-user-977723212 will have desired state: Ready
2022-04-06 18:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: strimzi-tls-user-977723212 is in desired state: Ready
2022-04-06 18:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1596] Change of kafka validity and renewal days - reconciliation should start.
2022-04-06 18:29:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 18:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 18:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 18:29:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 18:29:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 18:29:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 18:29:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 18:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 18:29:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 18:29:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 18:29:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 18:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 18:29:54 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 18:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 18:29:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 18:29:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 18:29:58 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-06 18:29:58 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:672] Checking produced and consumed messages to pod:my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l
2022-04-06 18:29:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@460953f6, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-2065837683-1340310002], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l', podNamespace='namespace-57', bootstrapServer='my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-2065837683-1340310002', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4d877f9e}
2022-04-06 18:29:58 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092:my-topic-2065837683-1340310002 from pod my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l
2022-04-06 18:29:58 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l -n namespace-57 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-2065837683-1340310002
2022-04-06 18:29:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 18:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 18:30:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 18:30:01 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:30:01 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:30:01 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@18e49f5e, messages=[], arguments=[--group-id, my-consumer-group-1463736257, --max-messages, 100, --group-instance-id, instance1932579425, --bootstrap-server, my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092, --topic, my-topic-2065837683-1340310002], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l', podNamespace='namespace-57', bootstrapServer='my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092', topicName='my-topic-2065837683-1340310002', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1463736257', consumerInstanceId='instance1932579425', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6e697682}
2022-04-06 18:30:01 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092#my-topic-2065837683-1340310002 from pod my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l
2022-04-06 18:30:01 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0543f8c3-kafka-clients-579fbf68f9-gsd8l -n namespace-57 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1463736257 --max-messages 100 --group-instance-id instance1932579425 --bootstrap-server my-cluster-0543f8c3-kafka-bootstrap.namespace-57.svc:9092 --topic my-topic-2065837683-1340310002
2022-04-06 18:30:01 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 18:30:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 18:30:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 18:30:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 18:30:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 18:30:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 18:30:07 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:30:07 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:30:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:30:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewCaCertsTriggerByExpiredCertificate
2022-04-06 18:30:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2065837683-1340310002 in namespace namespace-57
2022-04-06 18:30:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 18:30:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 18:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 18:30:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 18:30:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 18:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 18:30:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 18:30:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 18:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 18:30:17 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 18:30:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0543f8c3-kafka-clients in namespace namespace-57
2022-04-06 18:30:18 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 18:30:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 18:30:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 18:30:21 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 18:30:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 18:30:23 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 18:30:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 18:30:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 18:30:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 18:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 18:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 18:30:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 18:30:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 18:30:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 18:30:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 18:30:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-1f2990d3-zookeeper-1=43a0b835-93af-43ab-861b-44e4b5344f16, my-cluster-1f2990d3-zookeeper-0=29d6f8a6-069a-4206-8133-f4b1a89a28cf, my-cluster-1f2990d3-zookeeper-2=96fa4c8e-9dd7-43aa-86a7-0bef029ddd8f} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 18:30:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1f2990d3-kafka rolling update
2022-04-06 18:30:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9939db3a is in desired state: Ready
2022-04-06 18:30:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1039993115-1270099718 in namespace namespace-59
2022-04-06 18:30:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-06 18:30:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1039993115-1270099718 will have desired state: Ready
2022-04-06 18:30:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1039993115-1270099718 is in desired state: Ready
2022-04-06 18:30:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-66991933-1733911885 in namespace namespace-59
2022-04-06 18:30:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-06 18:30:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-66991933-1733911885 will have desired state: Ready
2022-04-06 18:30:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-66991933-1733911885 is in desired state: Ready
2022-04-06 18:30:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9939db3a-kafka-clients in namespace namespace-59
2022-04-06 18:30:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-06 18:30:43 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9939db3a-kafka-clients will be ready
2022-04-06 18:30:45 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9939db3a-kafka-clients is ready
2022-04-06 18:30:45 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:30:45 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:453] Checking produced and consumed messages to pod:my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp
2022-04-06 18:30:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@10efe13e, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-66991933-1733911885], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp', podNamespace='namespace-58', bootstrapServer='my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-66991933-1733911885', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@385d5314}
2022-04-06 18:30:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092:my-topic-66991933-1733911885 from pod my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp
2022-04-06 18:30:45 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp -n namespace-58 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-66991933-1733911885
2022-04-06 18:30:47 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:30:47 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:30:47 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6aed2fbb, messages=[], arguments=[--group-id, my-consumer-group-743032300, --max-messages, 100, --group-instance-id, instance693593558, --bootstrap-server, my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-66991933-1733911885], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp', podNamespace='namespace-58', bootstrapServer='my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-66991933-1733911885', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-743032300', consumerInstanceId='instance693593558', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@750024da}
2022-04-06 18:30:47 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092#my-topic-66991933-1733911885 from pod my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp
2022-04-06 18:30:47 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp -n namespace-58 -- /opt/kafka/consumer.sh --group-id my-consumer-group-743032300 --max-messages 100 --group-instance-id instance693593558 --bootstrap-server my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-66991933-1733911885
2022-04-06 18:30:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1f2990d3-kafka has been successfully rolled
2022-04-06 18:30:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-1f2990d3-kafka to be ready
2022-04-06 18:30:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:30:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:30:53 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:467] Triggering CA cert renewal by adding the annotation
2022-04-06 18:30:53 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:479] Patching secret my-cluster-9939db3a-clients-ca with strimzi.io/force-replace
2022-04-06 18:30:53 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:489] Wait for kafka to rolling restart (1)...
2022-04-06 18:30:53 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-9939db3a-kafka rolling update
2022-04-06 18:30:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1509045005-443690926 in namespace namespace-57
2022-04-06 18:31:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0543f8c3 in namespace namespace-57
2022-04-06 18:31:07 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-57, for cruise control Kafka cluster my-cluster-0543f8c3
2022-04-06 18:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:31:17 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-57 for test case:testAutoRenewCaCertsTriggerByExpiredCertificate
2022-04-06 18:31:22 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:31:22 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithMirrorMaker-STARTED
2022-04-06 18:31:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-1f2990d3-entity-operator rolling update
2022-04-06 18:31:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1f2990d3-entity-operator will be ready
2022-04-06 18:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewCaCertsTriggerByExpiredCertificate-FINISHED
2022-04-06 18:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:32:00 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewClusterCaCertsTriggeredByAnno-STARTED
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-60 for test case:testTlsHostnameVerificationWithMirrorMaker
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-60
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-60
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-60
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5aee314d-source in namespace namespace-60
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-06 18:32:02 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5aee314d-source will have desired state: Ready
2022-04-06 18:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1f2990d3-entity-operator is ready
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-1f2990d3-entity-operator rolling update finished
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1624] Initial ClientsCA cert dates: Wed Apr 06 18:28:25 UTC 2022 --> Tue Apr 26 18:28:25 UTC 2022
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1625] Changed ClientsCA cert dates: Wed Apr 06 18:29:42 UTC 2022 --> Sun Oct 23 18:29:42 UTC 2022
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1626] Initial userCert dates: Wed Apr 06 18:29:42 UTC 2022 --> Tue Apr 26 18:29:42 UTC 2022
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1627] Changed userCert dates: Wed Apr 06 18:31:25 UTC 2022 --> Sun Oct 23 18:31:25 UTC 2022
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClientsCACertRenew
2022-04-06 18:33:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser strimzi-tls-user-977723212 in namespace namespace-59
2022-04-06 18:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-9939db3a-kafka has been successfully rolled
2022-04-06 18:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:510] Wait for kafka to rolling restart (2)...
2022-04-06 18:33:03 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-9939db3a-kafka rolling update
2022-04-06 18:33:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5aee314d-source is in desired state: Ready
2022-04-06 18:33:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5aee314d-target in namespace namespace-60
2022-04-06 18:33:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-06 18:33:12 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5aee314d-target will have desired state: Ready
2022-04-06 18:33:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1f2990d3 in namespace namespace-59
2022-04-06 18:33:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:33:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-59 for test case:testClientsCACertRenew
2022-04-06 18:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testClientsCACertRenew-FINISHED
2022-04-06 18:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:33:50 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithKafkaConnect-STARTED
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-61 for test case:testAutoRenewClusterCaCertsTriggeredByAnno
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-61
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-61
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-61
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-89aed951 in namespace namespace-61
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-06 18:33:55 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-89aed951 will have desired state: Ready
2022-04-06 18:34:54 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-9939db3a-kafka has been successfully rolled
2022-04-06 18:34:54 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-9939db3a-kafka to be ready
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5aee314d-target is in desired state: Ready
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:888] Getting IP of the source bootstrap service for consumer
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:891] Getting IP of the target bootstrap service for producer
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:894] KafkaMirrorMaker without config ssl.endpoint.identification.algorithm will not connect to consumer with address 10.107.168.143:9093
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:895] KafkaMirrorMaker without config ssl.endpoint.identification.algorithm will not connect to producer with address 10.110.241.28:9093
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-5aee314d in namespace namespace-61
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-60
2022-04-06 18:34:57 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:245] Wait until Pod my-cluster-5aee314d-mirror-maker is present
2022-04-06 18:34:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:249] Pod my-cluster-5aee314d-mirror-maker is present
2022-04-06 18:34:58 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:236] Wait until Pod my-cluster-5aee314d-mirror-maker-867b79f974-l97ht is in CrashLoopBackOff state
2022-04-06 18:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [PodUtils:241] Pod my-cluster-5aee314d-mirror-maker-867b79f974-l97ht is in CrashLoopBackOff state
2022-04-06 18:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:930] KafkaMirrorMaker with config ssl.endpoint.identification.algorithm will connect to consumer with address 10.107.168.143:9093
2022-04-06 18:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:931] KafkaMirrorMaker with config ssl.endpoint.identification.algorithm will connect to producer with address 10.110.241.28:9093
2022-04-06 18:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:933] Adding configuration ssl.endpoint.identification.algorithm to the mirror maker...
2022-04-06 18:35:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-5aee314d will have desired state: Ready
2022-04-06 18:35:28 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:525] Checking the certificates have been replaced
2022-04-06 18:35:28 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:536] Checking consumed messages to pod:my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp
2022-04-06 18:35:28 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@32947f1f, messages=[], arguments=[--group-id, my-consumer-group-800615403, --max-messages, 100, --group-instance-id, instance1998779701, --bootstrap-server, my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-66991933-1733911885], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp', podNamespace='namespace-58', bootstrapServer='my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-66991933-1733911885', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-800615403', consumerInstanceId='instance1998779701', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@51d1cf7b}
2022-04-06 18:35:28 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092#my-topic-66991933-1733911885 from pod my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp
2022-04-06 18:35:28 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-9939db3a-kafka-clients-574d446b99-5vvjp -n namespace-58 -- /opt/kafka/consumer.sh --group-id my-consumer-group-800615403 --max-messages 100 --group-instance-id instance1998779701 --bootstrap-server my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-66991933-1733911885
2022-04-06 18:35:35 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:35:35 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:35:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-191942824-747937708 in namespace namespace-61
2022-04-06 18:35:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-06 18:35:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-191942824-747937708 will have desired state: Ready
2022-04-06 18:35:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-191942824-747937708 is in desired state: Ready
2022-04-06 18:35:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9939db3a-kafka-clients-tls in namespace namespace-61
2022-04-06 18:35:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-58
2022-04-06 18:35:36 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9939db3a-kafka-clients-tls will be ready
2022-04-06 18:35:38 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9939db3a-kafka-clients-tls is ready
2022-04-06 18:35:38 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:561] Checking consumed messages to pod:my-cluster-9939db3a-kafka-clients-tls-7fddc49bd4-rv8fh
2022-04-06 18:35:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1be8e615, messages=[], arguments=[--group-id, my-consumer-group-1442972008, --max-messages, 100, --group-instance-id, instance1934039100, --bootstrap-server, my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092, --topic, my-topic-66991933-1733911885], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-9939db3a-kafka-clients-tls-7fddc49bd4-rv8fh', podNamespace='namespace-58', bootstrapServer='my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092', topicName='my-topic-66991933-1733911885', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1442972008', consumerInstanceId='instance1934039100', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@441d43fa}
2022-04-06 18:35:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092#my-topic-66991933-1733911885 from pod my-cluster-9939db3a-kafka-clients-tls-7fddc49bd4-rv8fh
2022-04-06 18:35:38 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-9939db3a-kafka-clients-tls-7fddc49bd4-rv8fh -n namespace-58 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1442972008 --max-messages 100 --group-instance-id instance1934039100 --bootstrap-server my-cluster-9939db3a-kafka-bootstrap.namespace-58.svc:9092 --topic my-topic-66991933-1733911885
2022-04-06 18:35:44 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:35:44 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:35:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:35:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoReplaceClientsCaKeysTriggeredByAnno
2022-04-06 18:35:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9939db3a-kafka-clients in namespace namespace-58
2022-04-06 18:36:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-89aed951 is in desired state: Ready
2022-04-06 18:36:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1798429114-963288485 in namespace namespace-61
2022-04-06 18:36:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-06 18:36:56 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1798429114-963288485 will have desired state: Ready
2022-04-06 18:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1798429114-963288485 is in desired state: Ready
2022-04-06 18:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1010313305-1195879076 in namespace namespace-61
2022-04-06 18:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-06 18:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1010313305-1195879076 will have desired state: Ready
2022-04-06 18:36:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1010313305-1195879076 is in desired state: Ready
2022-04-06 18:36:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-89aed951-kafka-clients in namespace namespace-61
2022-04-06 18:36:58 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-06 18:36:58 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-89aed951-kafka-clients will be ready
2022-04-06 18:37:00 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-89aed951-kafka-clients is ready
2022-04-06 18:37:00 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:37:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:260] Checking produced and consumed messages to pod:my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9
2022-04-06 18:37:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@47a9affe, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092, --topic, my-topic-1010313305-1195879076], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9', podNamespace='namespace-61', bootstrapServer='my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092', topicName='my-topic-1010313305-1195879076', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4ca3ce2c}
2022-04-06 18:37:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092:my-topic-1010313305-1195879076 from pod my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9
2022-04-06 18:37:00 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9 -n namespace-61 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092 --topic my-topic-1010313305-1195879076
2022-04-06 18:37:03 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:37:03 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:37:03 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@211969c9, messages=[], arguments=[--group-id, my-consumer-group-1595565596, --max-messages, 100, --group-instance-id, instance482171948, --bootstrap-server, my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092, --topic, my-topic-1010313305-1195879076], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9', podNamespace='namespace-61', bootstrapServer='my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092', topicName='my-topic-1010313305-1195879076', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1595565596', consumerInstanceId='instance482171948', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@73190b28}
2022-04-06 18:37:03 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092#my-topic-1010313305-1195879076 from pod my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9
2022-04-06 18:37:03 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9 -n namespace-61 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1595565596 --max-messages 100 --group-instance-id instance482171948 --bootstrap-server my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092 --topic my-topic-1010313305-1195879076
2022-04-06 18:37:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9939db3a-kafka-clients-tls in namespace namespace-58
2022-04-06 18:37:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-191942824-747937708 in namespace namespace-58
2022-04-06 18:37:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:37:08 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:37:09 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:274] Triggering CA cert renewal by adding the annotation
2022-04-06 18:37:09 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:286] Patching secret my-cluster-89aed951-cluster-ca-cert with strimzi.io/force-renew
2022-04-06 18:37:09 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:291] Wait for zk to rolling restart ...
2022-04-06 18:37:09 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-89aed951-zookeeper rolling update
2022-04-06 18:37:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1039993115-1270099718 in namespace namespace-58
2022-04-06 18:37:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-66991933-1733911885 in namespace namespace-58
2022-04-06 18:37:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9939db3a in namespace namespace-58
2022-04-06 18:37:34 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-58, for cruise control Kafka cluster my-cluster-9939db3a
2022-04-06 18:37:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:37:44 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-58 for test case:testAutoReplaceClientsCaKeysTriggeredByAnno
2022-04-06 18:38:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClientsCaKeysTriggeredByAnno-FINISHED
2022-04-06 18:38:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:38:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:38:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCaRenewalBreakInMiddle-STARTED
2022-04-06 18:38:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:38:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-62 for test case:testTlsHostnameVerificationWithKafkaConnect
2022-04-06 18:38:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-62
2022-04-06 18:38:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-62
2022-04-06 18:38:21 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-62
2022-04-06 18:38:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7a52fccf in namespace namespace-62
2022-04-06 18:38:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-06 18:38:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7a52fccf will have desired state: Ready
2022-04-06 18:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-89aed951-zookeeper has been successfully rolled
2022-04-06 18:38:44 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-89aed951-zookeeper to be ready
2022-04-06 18:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7a52fccf is in desired state: Ready
2022-04-06 18:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:838] Getting IP of the bootstrap service
2022-04-06 18:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:842] KafkaConnect without config ssl.endpoint.identification.algorithm will not connect to 10.96.117.146:9093
2022-04-06 18:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7a52fccf-kafka-clients in namespace namespace-62
2022-04-06 18:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-06 18:39:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7a52fccf-kafka-clients will be ready
2022-04-06 18:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7a52fccf-kafka-clients is ready
2022-04-06 18:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7a52fccf-scraper in namespace namespace-62
2022-04-06 18:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-06 18:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7a52fccf-scraper will be ready
2022-04-06 18:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7a52fccf-scraper is ready
2022-04-06 18:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-7a52fccf-scraper to be ready
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-7a52fccf-scraper is ready
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-7a52fccf-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-7a52fccf-allow in namespace namespace-62
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-7a52fccf in namespace namespace-62
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-62
2022-04-06 18:39:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:245] Wait until Pod my-cluster-7a52fccf-connect is present
2022-04-06 18:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:249] Pod my-cluster-7a52fccf-connect is present
2022-04-06 18:39:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:236] Wait until Pod my-cluster-7a52fccf-connect-6666b99cc4-kpmnr is in CrashLoopBackOff state
2022-04-06 18:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:295] Wait for kafka to rolling restart ...
2022-04-06 18:39:44 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-89aed951-kafka rolling update
2022-04-06 18:40:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:241] Pod my-cluster-7a52fccf-connect-6666b99cc4-kpmnr is in CrashLoopBackOff state
2022-04-06 18:40:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:872] KafkaConnect with config ssl.endpoint.identification.algorithm will connect to 10.96.117.146:9093
2022-04-06 18:40:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-7a52fccf will have desired state: Ready
2022-04-06 18:41:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-5aee314d is in desired state: Ready
2022-04-06 18:41:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:41:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsHostnameVerificationWithMirrorMaker
2022-04-06 18:41:03 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5aee314d-target in namespace namespace-60
2022-04-06 18:41:13 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-5aee314d in namespace namespace-60
2022-04-06 18:41:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5aee314d-source in namespace namespace-60
2022-04-06 18:41:33 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:41:33 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-60 for test case:testTlsHostnameVerificationWithMirrorMaker
2022-04-06 18:41:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-89aed951-kafka has been successfully rolled
2022-04-06 18:41:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-89aed951-kafka to be ready
2022-04-06 18:42:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithMirrorMaker-FINISHED
2022-04-06 18:42:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:42:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:42:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-STARTED
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-63 for test case:testCaRenewalBreakInMiddle
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-63
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-63
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-63
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3cbec579 in namespace namespace-63
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-06 18:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3cbec579 will have desired state: Ready
2022-04-06 18:42:07 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:299] Wait for EO to rolling restart ...
2022-04-06 18:42:07 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-89aed951-entity-operator rolling update
2022-04-06 18:42:07 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-89aed951-entity-operator will be ready
2022-04-06 18:42:49 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-89aed951-entity-operator is ready
2022-04-06 18:43:00 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-89aed951-entity-operator rolling update finished
2022-04-06 18:43:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:303] Wait for CC and KE to rolling restart ...
2022-04-06 18:43:00 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-89aed951-kafka-exporter rolling update
2022-04-06 18:43:45 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-89aed951-kafka-exporter will be ready
2022-04-06 18:43:45 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-89aed951-kafka-exporter is ready
2022-04-06 18:43:55 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-89aed951-kafka-exporter rolling update finished
2022-04-06 18:43:55 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-89aed951-cruise-control rolling update
2022-04-06 18:43:55 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-89aed951-cruise-control will be ready
2022-04-06 18:43:55 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-89aed951-cruise-control is ready
2022-04-06 18:43:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3cbec579 is in desired state: Ready
2022-04-06 18:43:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2091504663-135770503 in namespace namespace-63
2022-04-06 18:43:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-06 18:43:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2091504663-135770503 will have desired state: Ready
2022-04-06 18:43:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2091504663-135770503 is in desired state: Ready
2022-04-06 18:43:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-252251243-506456672 in namespace namespace-63
2022-04-06 18:43:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-06 18:43:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-252251243-506456672 will have desired state: Ready
2022-04-06 18:43:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-252251243-506456672 is in desired state: Ready
2022-04-06 18:43:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-3cbec579-kafka-clients in namespace namespace-63
2022-04-06 18:43:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-06 18:43:59 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-3cbec579-kafka-clients will be ready
2022-04-06 18:44:01 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-3cbec579-kafka-clients is ready
2022-04-06 18:44:01 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:44:01 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@27f1e088, messages=[], arguments=[USER=my_user_2091504663_135770503, --max-messages, 100, --bootstrap-server, my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-252251243-506456672], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-3cbec579-kafka-clients-78f944764b-wdphd', podNamespace='namespace-63', bootstrapServer='my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-252251243-506456672', maxMessages=100, kafkaUsername='my-user-2091504663-135770503', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@c2d2556}
2022-04-06 18:44:01 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093:my-topic-252251243-506456672 from pod my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:44:01 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-3cbec579-kafka-clients-78f944764b-wdphd -n namespace-63 -- /opt/kafka/producer.sh USER=my_user_2091504663_135770503 --max-messages 100 --bootstrap-server my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-252251243-506456672
2022-04-06 18:44:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:44:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 18:44:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@521ec4f1, messages=[], arguments=[--group-id, my-consumer-group-592178705, USER=my_user_2091504663_135770503, --max-messages, 100, --group-instance-id, instance879601774, --bootstrap-server, my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-252251243-506456672], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-3cbec579-kafka-clients-78f944764b-wdphd', podNamespace='namespace-63', bootstrapServer='my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-252251243-506456672', maxMessages=100, kafkaUsername='my-user-2091504663-135770503', consumerGroupName='my-consumer-group-592178705', consumerInstanceId='instance879601774', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@37fc40cd}
2022-04-06 18:44:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093:my-topic-252251243-506456672 from pod my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:44:04 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-3cbec579-kafka-clients-78f944764b-wdphd -n namespace-63 -- /opt/kafka/consumer.sh --group-id my-consumer-group-592178705 USER=my_user_2091504663_135770503 --max-messages 100 --group-instance-id instance879601774 --bootstrap-server my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-252251243-506456672
2022-04-06 18:44:05 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-89aed951-cruise-control rolling update finished
2022-04-06 18:44:05 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:308] Checking the certificates have been replaced
2022-04-06 18:44:05 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:322] Checking consumed messages to pod:my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9
2022-04-06 18:44:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3c8ec2f, messages=[], arguments=[--group-id, my-consumer-group-123891856, --max-messages, 100, --group-instance-id, instance969922655, --bootstrap-server, my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092, --topic, my-topic-1010313305-1195879076], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9', podNamespace='namespace-61', bootstrapServer='my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092', topicName='my-topic-1010313305-1195879076', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-123891856', consumerInstanceId='instance969922655', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1c3f7559}
2022-04-06 18:44:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092#my-topic-1010313305-1195879076 from pod my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9
2022-04-06 18:44:05 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-89aed951-kafka-clients-5d85f9757-dtnb9 -n namespace-61 -- /opt/kafka/consumer.sh --group-id my-consumer-group-123891856 --max-messages 100 --group-instance-id instance969922655 --bootstrap-server my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9092 --topic my-topic-1010313305-1195879076
2022-04-06 18:44:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:44:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:44:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-89aed951 in namespace namespace-63
2022-04-06 18:44:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-06 18:44:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-89aed951 will have desired state: Ready
2022-04-06 18:44:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:44:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:44:11 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:70] Creating secret my-cluster-3cbec579-cluster-ca-cert
2022-04-06 18:44:11 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1218] No pods of my-cluster-3cbec579-zookeeper are in desired state
2022-04-06 18:44:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-89aed951 is in desired state: Ready
2022-04-06 18:44:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-89aed951-kafka-clients-tls in namespace namespace-61
2022-04-06 18:44:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-61
2022-04-06 18:44:12 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-89aed951-kafka-clients-tls will be ready
2022-04-06 18:44:12 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1218] No pods of my-cluster-3cbec579-zookeeper are in desired state
2022-04-06 18:44:13 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1218] No pods of my-cluster-3cbec579-zookeeper are in desired state
2022-04-06 18:44:14 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-89aed951-kafka-clients-tls is ready
2022-04-06 18:44:14 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:346] Checking consumed messages to pod:my-cluster-89aed951-kafka-clients-tls-5b45ff9f88-tvp2v
2022-04-06 18:44:14 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@36508e56, messages=[], arguments=[--group-id, my-consumer-group-846846368, USER=bob_my_cluster_89aed951, --max-messages, 100, --group-instance-id, instance1041449400, --bootstrap-server, my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9093, --topic, my-topic-1010313305-1195879076], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-89aed951-kafka-clients-tls-5b45ff9f88-tvp2v', podNamespace='namespace-61', bootstrapServer='my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9093', topicName='my-topic-1010313305-1195879076', maxMessages=100, kafkaUsername='bob-my-cluster-89aed951', consumerGroupName='my-consumer-group-846846368', consumerInstanceId='instance1041449400', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@51a2ac86}
2022-04-06 18:44:14 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9093#my-topic-1010313305-1195879076 from pod my-cluster-89aed951-kafka-clients-tls-5b45ff9f88-tvp2v
2022-04-06 18:44:14 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-89aed951-kafka-clients-tls-5b45ff9f88-tvp2v -n namespace-61 -- /opt/kafka/consumer.sh --group-id my-consumer-group-846846368 USER=bob_my_cluster_89aed951 --max-messages 100 --group-instance-id instance1041449400 --bootstrap-server my-cluster-89aed951-kafka-bootstrap.namespace-61.svc:9093 --topic my-topic-1010313305-1195879076
2022-04-06 18:44:14 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1218] No pods of my-cluster-3cbec579-zookeeper are in desired state
2022-04-06 18:44:15 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1221] Pod in 'Pending' state: my-cluster-3cbec579-zookeeper-2
2022-04-06 18:44:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5dbe4049, messages=[], arguments=[--group-id, my-consumer-group-958533272, USER=my_user_2091504663_135770503, --max-messages, 100, --group-instance-id, instance1832668764, --bootstrap-server, my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-252251243-506456672], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-3cbec579-kafka-clients-78f944764b-wdphd', podNamespace='namespace-63', bootstrapServer='my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-252251243-506456672', maxMessages=100, kafkaUsername='my-user-2091504663-135770503', consumerGroupName='my-consumer-group-958533272', consumerInstanceId='instance1832668764', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3c12a0ee}
2022-04-06 18:44:16 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093:my-topic-252251243-506456672 from pod my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:44:16 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-3cbec579-kafka-clients-78f944764b-wdphd -n namespace-63 -- /opt/kafka/consumer.sh --group-id my-consumer-group-958533272 USER=my_user_2091504663_135770503 --max-messages 100 --group-instance-id instance1832668764 --bootstrap-server my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-252251243-506456672
2022-04-06 18:44:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:44:21 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:44:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:44:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewClusterCaCertsTriggeredByAnno
2022-04-06 18:44:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-89aed951-kafka-clients in namespace namespace-61
2022-04-06 18:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:131] Waiting for Secret my-cluster-3cbec579-cluster-ca-cert certificate change
2022-04-06 18:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [SecretUtils:138] Certificate in Secret my-cluster-3cbec579-cluster-ca-cert has changed, was -----BEGIN CERTIFICATE-----
MIIDKjCCAhKgAwIBAgIJAJBt0u1FFTG+MA0GCSqGSIb3DQEBCwUAMCoxEzARBgNV
BAoMCnN0cmltemkuaW8xEzARBgNVBAMMCmNsdXN0ZXItY2EwHhcNMTgxMDIzMTAw
MTExWhcNMTgxMDI0MTAwMTExWjAqMRMwEQYDVQQKDApzdHJpbXppLmlvMRMwEQYD
VQQDDApjbHVzdGVyLWNhMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA
xpuYrNXYHqw3ajwd12aAeuTlAX4rVwVdPuIex6A4NL8J3d2DV+ngXgNTH//RhiF5
If5KRWSsLei5BUIrwuQutOUNCQwyACmri9+yrx6+tevligiokAUwhHxcDHZpwC3T
+2dzk/BkI++vbSuvjFmBKGQi9gfyoTnStTEQ85KVJUS170hzwDjzaEiJsKpOPx/G
+KTdkAopLucoxr4sxhYeO4mQ2PkT0QL+R8Ohs6LD6v/bqalFP+rS8vibolfxjMNm
lXQCOd8UfXy8OEOaNoNCvhnn/cT/hbEG/ARbV3hHmUh9COV+TSV5dhsbmS5h4MKw
LzP449nGQBmLSkZMu984DQIDAQABo1MwUTAdBgNVHQ4EFgQUOXubcHBZJ7vSzjpi
pfXdFSP3dsEwHwYDVR0jBBgwFoAUOXubcHBZJ7vSzjpipfXdFSP3dsEwDwYDVR0T
AQH/BAUwAwEB/zANBgkqhkiG9w0BAQsFAAOCAQEAFFPGykbzUREDMzh+33i3a8TF
UTQnPMN/SuVbpLfQdpkpLO+aVWjVvJP6qMrI7jRO5zhj4MecAf7YKpe+dyRTTz9B
Dy9BZcujHYjCKdcKBnBFQ7B1xQm9tL1bw+a3ABzSTlhLiBcCxhJEawlWy1Gh18ab
3x/Kqnz0mk/jt5+n9HwlKHuBQVIxRCsfHWwq+WvIfoxM+N//akV8/29hLUf5TlYH
F5CX4G2pA5sSdaDHQ4ekQWuqM6tfvsGLl2KOmEFEgVR4GfaWI4BsUCzlpBNcCGWv
V7klZvBxQPG7MVy0cB8yzTDtUpR0jUFUkZOSp5Pr3yWcwPWEgWkaXYfO+FWETA==
-----END CERTIFICATE-----
, is now -----BEGIN CERTIFICATE-----
MIIFLTCCAxWgAwIBAgIUOWPv3Q7OlvlT7TJaiyiD53uCZ7QwDQYJKoZIhvcNAQEN
BQAwLTETMBEGA1UECgwKaW8uc3RyaW16aTEWMBQGA1UEAwwNY2x1c3Rlci1jYSB2
MDAeFw0yMjA0MDYxODQ0MTFaFw0yMjA0MTMxODQ0MTFaMC0xEzARBgNVBAoMCmlv
LnN0cmltemkxFjAUBgNVBAMMDWNsdXN0ZXItY2EgdjAwggIiMA0GCSqGSIb3DQEB
AQUAA4ICDwAwggIKAoICAQDIC3aVLSvkobW8QRXAqqYRPFhpdbwkNWl1h+Jkul6A
jE+x6KT43UNCfcYkPWYtZXILfxd5otzj8k1wNNZyrw8xUPe79iuun37gFE59Qfon
0VcWhkuM16oXBO5P++Y53xKQEKZIn5NYB0OmDq1h5XRZs0+Yi5tvuTYBNYbkGhUo
MpEtAdHoKJzYocag79eySZVWhGmf/v+AN0fcvCZdRFjmdL+i3EiVajjcLCJxqDTv
iCnKVntmwi5ZirEm6giVPY7y6yDqgpUi4gLU/aAnFdpa9v1eShK1RL+3MF/2qkRp
pPItTwhKaOIkDUNJZ64vgt4zr3Cl4klt4GwlHiRp0p1IAsl0XXxJGPFIAsHNK7eR
L1u8GLeR7jBIebcvKYMYK0gwKJcwczVsU4mnhTsHRUij9nJEQmt/n9bITl0xMmUu
br0Fe6IzvZ6nE/T1TlmS0tenhEkOEkOerqbF6fVSlTjo050Jtcxu0NrSYXkPdB7Q
+O25eQ049VsgipJv7bu5ejMhUKr9nCAFhFuo1ZV2QBGdNk2Potw4KMu66tOF+XcP
Xxu+IIS6i4ReeCp94lQVMcXvl5Z63CbSSUM+rr+j1WYLexc2kluCtq2wrydOakfG
YcGTgwH/7mMcFbRpz28JIf6vSoNRNy64Uv2tGT0uLg/sBl1Rxu8ON411C9cEycHk
bwIDAQABo0UwQzAdBgNVHQ4EFgQU7WeyqNCm+lwcqb7+w0yk/8p05ucwEgYDVR0T
AQH/BAgwBgEB/wIBADAOBgNVHQ8BAf8EBAMCAQYwDQYJKoZIhvcNAQENBQADggIB
AGWCww+eTpnjaN1HpizoGyUr1MCRvv1hQpoMER4asnNo0DL3yFCWRltPo7FEr+DR
thL2ptE9Dz1dU8P2CfFggJ7HWQYXKDkso+RHBV30iSNrlFQpg0w+SgIpYNFGFZx4
RH9hTzYYfrcFiY1flCcjvGn2yfDeBkujkOvzD1Fpn29r+B9RT05DxZwjej71hFem
K0K9qd6/QASz60mqjPo7leCz2CU/+zA/9iTBnDd8eXB5ktHPr5OhGd5FvRRSYaP7
O/wUGQsyTPhDRbFQ5brdF7AZsKzgtNInQ5+vJyK/qan6J+biDgxXv87M0BBG0yFy
yiflI+uo4m3Q81DJ86JFJIPMtSrPLpM0ib9Ms3woZSk8qP5KTWnjJ0Nuj2HGo9fA
S67pfhxzux9AVl+6MqS4ymy8l+m7brSwWCrE0XHjwreRwwpfKqrx6iX0yUsviIWI
YMYiQ6WGRi8IK+7m/pgEntc312fw4/1ZN9NJAqAtQXc5AqSOCmStwvwZZCk5Gw/g
hrqIkv6rW5AOLDyYkPelzKtn5hQ+z0udSHo7fiq97XFXWtwdeppyDiqLL58B9zJY
tL3Ww/xN/bvuqXKdXbStD+oTjdW8M9PAexPxr6K4TvgqzkFNkP24TXLb5u8Twmck
M3L5F73qVhsYzC+rp+UgRm2QPQKaJx1tuWB6/jw7SuB7
-----END CERTIFICATE-----

2022-04-06 18:44:22 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-3cbec579-zookeeper rolling update
2022-04-06 18:45:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-89aed951-kafka-clients-tls in namespace namespace-61
2022-04-06 18:45:41 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-89aed951 in namespace namespace-61
2022-04-06 18:45:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-7a52fccf is in desired state: Ready
2022-04-06 18:45:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:45:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTlsHostnameVerificationWithKafkaConnect
2022-04-06 18:45:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7a52fccf-scraper in namespace namespace-62
2022-04-06 18:45:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1798429114-963288485 in namespace namespace-61
2022-04-06 18:46:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1010313305-1195879076 in namespace namespace-61
2022-04-06 18:46:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-89aed951 in namespace namespace-61
2022-04-06 18:46:01 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-61, for cruise control Kafka cluster my-cluster-89aed951
2022-04-06 18:46:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:46:11 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-61 for test case:testAutoRenewClusterCaCertsTriggeredByAnno
2022-04-06 18:46:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-7a52fccf in namespace namespace-62
2022-04-06 18:46:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-7a52fccf-allow in namespace namespace-62
2022-04-06 18:46:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7a52fccf-kafka-clients in namespace namespace-62
2022-04-06 18:46:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewClusterCaCertsTriggeredByAnno-FINISHED
2022-04-06 18:46:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:46:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:46:55 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertRegeneratedAfterInternalCAisDeleted-STARTED
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-64 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-64
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-64
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-64
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b1bafe70 in namespace namespace-64
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-06 18:46:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b1bafe70 will have desired state: Ready
2022-04-06 18:47:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7a52fccf in namespace namespace-62
2022-04-06 18:47:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:47:25 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-62 for test case:testTlsHostnameVerificationWithKafkaConnect
2022-04-06 18:48:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testTlsHostnameVerificationWithKafkaConnect-FINISHED
2022-04-06 18:48:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:48:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:48:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectTlsVersion-STARTED
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-65 for test case:testCertRegeneratedAfterInternalCAisDeleted
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-65
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-65
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-65
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-14228a28 in namespace namespace-65
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-65
2022-04-06 18:48:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-14228a28 will have desired state: Ready
2022-04-06 18:49:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-14228a28 is in desired state: Ready
2022-04-06 18:49:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1175796063-1653183260 in namespace namespace-65
2022-04-06 18:49:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-65
2022-04-06 18:49:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1175796063-1653183260 will have desired state: Ready
2022-04-06 18:49:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1175796063-1653183260 is in desired state: Ready
2022-04-06 18:49:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-681975677-842065741 in namespace namespace-65
2022-04-06 18:49:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-65
2022-04-06 18:49:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-681975677-842065741 will have desired state: Ready
2022-04-06 18:49:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-681975677-842065741 is in desired state: Ready
2022-04-06 18:49:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-14228a28-kafka-clients in namespace namespace-65
2022-04-06 18:49:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-65
2022-04-06 18:49:29 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-14228a28-kafka-clients will be ready
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-14228a28-kafka-clients is ready
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:797] Verifying that secret Secret(apiVersion=v1, data={ca.crt=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, ca.p12=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, ca.password=dFZ6ckRKcDlVTDE2}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations={strimzi.io/ca-cert-generation=0}, clusterName=null, creationTimestamp=2022-04-06T18:48:07Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-14228a28, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-14228a28, strimzi.io/cluster=my-cluster-14228a28, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testCertRegeneratedAfterInternalCAisDeleted}, managedFields=[], name=my-cluster-14228a28-clients-ca-cert, namespace=namespace-65, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-14228a28, uid=02b5f217-8ede-45bb-b2d0-9e3e04d27ff4, additionalProperties={})], resourceVersion=54707, selfLink=/api/v1/namespaces/namespace-65/secrets/my-cluster-14228a28-clients-ca-cert, uid=4464fb57-f874-46df-942d-f613d428e608, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) with name my-cluster-14228a28-clients-ca-cert is present
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:797] Verifying that secret Secret(apiVersion=v1, data={ca.crt=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, ca.p12=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, ca.password=VjFwTGhCZURMazhy}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations={strimzi.io/ca-cert-generation=0}, clusterName=null, creationTimestamp=2022-04-06T18:48:07Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-cluster-14228a28, app.kubernetes.io/managed-by=strimzi-cluster-operator, app.kubernetes.io/name=strimzi, app.kubernetes.io/part-of=strimzi-my-cluster-14228a28, strimzi.io/cluster=my-cluster-14228a28, strimzi.io/kind=Kafka, strimzi.io/name=strimzi, test.case=testCertRegeneratedAfterInternalCAisDeleted}, managedFields=[], name=my-cluster-14228a28-cluster-ca-cert, namespace=namespace-65, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=Kafka, name=my-cluster-14228a28, uid=02b5f217-8ede-45bb-b2d0-9e3e04d27ff4, additionalProperties={})], resourceVersion=54706, selfLink=/api/v1/namespaces/namespace-65/secrets/my-cluster-14228a28-cluster-ca-cert, uid=7edcc8b0-8532-4453-936f-c0fabaf2786c, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) with name my-cluster-14228a28-cluster-ca-cert is present
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:802] Deleting secret my-cluster-14228a28-clients-ca-cert
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:802] Deleting secret my-cluster-14228a28-cluster-ca-cert
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-14228a28-kafka are stable
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 18:49:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 18:49:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 18:49:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 18:49:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 18:49:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 18:49:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 18:49:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 18:49:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 18:49:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 18:49:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 18:49:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 18:49:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 18:49:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 18:49:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 18:49:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 18:49:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 18:49:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 18:49:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 18:49:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 18:49:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 18:49:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 18:49:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 18:49:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 18:49:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 18:49:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 18:49:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 18:49:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 18:49:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 18:49:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 18:49:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 18:49:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 18:49:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 18:49:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 18:49:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 18:49:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 18:49:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 18:49:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 18:49:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 18:49:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 18:49:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 18:49:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 18:49:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 18:49:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 18:49:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 18:49:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 18:49:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 18:49:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 18:49:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 18:49:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 18:49:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 18:49:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 18:49:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 18:49:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 18:49:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 18:49:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 18:49:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 18:49:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 18:49:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 18:49:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 18:49:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 18:49:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 18:49:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 18:49:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 18:49:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 18:49:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 18:49:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 18:49:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 18:49:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 18:49:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 18:49:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 18:49:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 18:49:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 18:49:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 18:49:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 18:49:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 18:49:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 18:49:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 18:49:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 18:49:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 18:49:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 18:49:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 18:49:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 18:49:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 18:49:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 18:49:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 18:49:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 18:49:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 18:49:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 18:49:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 18:49:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 18:49:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 18:49:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 18:49:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 18:49:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b1bafe70 is in desired state: Ready
2022-04-06 18:49:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2033992940-830015114 in namespace namespace-65
2022-04-06 18:49:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-06 18:49:55 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2033992940-830015114 will have desired state: Ready
2022-04-06 18:49:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 18:49:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 18:49:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 18:49:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 18:49:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2033992940-830015114 is in desired state: Ready
2022-04-06 18:49:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-196324947-1141101841 in namespace namespace-65
2022-04-06 18:49:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-06 18:49:56 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-196324947-1141101841 will have desired state: Ready
2022-04-06 18:49:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 18:49:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 18:49:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 18:49:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 18:49:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-196324947-1141101841 is in desired state: Ready
2022-04-06 18:49:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b1bafe70-kafka-clients in namespace namespace-65
2022-04-06 18:49:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-06 18:49:57 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1bafe70-kafka-clients will be ready
2022-04-06 18:49:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 18:49:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 18:49:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 18:49:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 18:49:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 18:49:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 18:49:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 18:49:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 18:49:59 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1bafe70-kafka-clients is ready
2022-04-06 18:49:59 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 18:49:59 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:260] Checking produced and consumed messages to pod:my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg
2022-04-06 18:49:59 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@57296875, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092, --topic, my-topic-196324947-1141101841], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg', podNamespace='namespace-64', bootstrapServer='my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092', topicName='my-topic-196324947-1141101841', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5dec29f6}
2022-04-06 18:49:59 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092:my-topic-196324947-1141101841 from pod my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg
2022-04-06 18:49:59 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg -n namespace-64 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092 --topic my-topic-196324947-1141101841
2022-04-06 18:50:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 18:50:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 18:50:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 18:50:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 18:50:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 18:50:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 18:50:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 18:50:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 18:50:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 18:50:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 18:50:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 18:50:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 18:50:02 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 18:50:02 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 18:50:02 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@55722730, messages=[], arguments=[--group-id, my-consumer-group-1660271667, --max-messages, 100, --group-instance-id, instance1947136613, --bootstrap-server, my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092, --topic, my-topic-196324947-1141101841], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg', podNamespace='namespace-64', bootstrapServer='my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092', topicName='my-topic-196324947-1141101841', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1660271667', consumerInstanceId='instance1947136613', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@696d0072}
2022-04-06 18:50:02 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092#my-topic-196324947-1141101841 from pod my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg
2022-04-06 18:50:02 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg -n namespace-64 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1660271667 --max-messages 100 --group-instance-id instance1947136613 --bootstrap-server my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092 --topic my-topic-196324947-1141101841
2022-04-06 18:50:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 18:50:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 18:50:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 18:50:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 18:50:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 18:50:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 18:50:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 18:50:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 18:50:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 18:50:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 18:50:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 18:50:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 18:50:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 18:50:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 18:50:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 18:50:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 18:50:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 18:50:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 18:50:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 18:50:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:274] Triggering CA cert renewal by adding the annotation
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:286] Patching secret my-cluster-b1bafe70-cluster-ca-cert with strimzi.io/force-renew
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:286] Patching secret my-cluster-b1bafe70-clients-ca-cert with strimzi.io/force-renew
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:291] Wait for zk to rolling restart ...
2022-04-06 18:50:08 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b1bafe70-zookeeper rolling update
2022-04-06 18:50:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 18:50:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 18:50:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 18:50:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 18:50:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 18:50:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 18:50:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 18:50:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 18:50:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 18:50:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 18:50:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 18:50:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 18:50:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 18:50:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 18:50:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 18:50:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 18:50:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 18:50:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 18:50:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 18:50:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 18:50:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 18:50:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 18:50:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 18:50:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 18:50:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 18:50:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 18:50:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 18:50:14 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 18:50:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 18:50:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 18:50:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 18:50:15 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 18:50:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 18:50:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 18:50:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 18:50:16 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 18:50:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 18:50:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 18:50:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 18:50:17 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 18:50:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 18:50:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 18:50:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 18:50:18 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 18:50:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 18:50:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 18:50:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 18:50:19 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 18:50:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 18:50:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 18:50:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 18:50:20 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 18:50:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 18:50:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 18:50:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 18:50:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 18:50:21 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-14228a28-kafka-0 ,my-cluster-14228a28-kafka-1 ,my-cluster-14228a28-kafka-2 ,my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm
2022-04-06 18:50:21 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-14228a28-kafka rolling update
2022-04-06 18:50:48 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-3cbec579-zookeeper has been successfully rolled
2022-04-06 18:50:48 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-3cbec579-zookeeper to be ready
2022-04-06 18:51:16 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-3cbec579-kafka rolling update
2022-04-06 18:51:33 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-b1bafe70-zookeeper has been successfully rolled
2022-04-06 18:51:33 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-b1bafe70-zookeeper to be ready
2022-04-06 18:51:36 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-14228a28-kafka has been successfully rolled
2022-04-06 18:51:36 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-14228a28-kafka to be ready
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-14228a28 will have desired state: Ready
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-14228a28 is in desired state: Ready
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-14228a28 is ready
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-14228a28-clients-ca-cert
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:50] Secret my-cluster-14228a28-clients-ca-cert created
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:46] Waiting for Secret my-cluster-14228a28-cluster-ca-cert
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecretUtils:50] Secret my-cluster-14228a28-cluster-ca-cert created
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:821] Checking consumed messages to pod:my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@638ff8f4, messages=[], arguments=[USER=my_user_1175796063_1653183260, --max-messages, 100, --bootstrap-server, my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093, --topic, my-topic-681975677-842065741], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm', podNamespace='namespace-65', bootstrapServer='my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093', topicName='my-topic-681975677-842065741', maxMessages=100, kafkaUsername='my-user-1175796063-1653183260', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@59abce94}
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093:my-topic-681975677-842065741 from pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm
2022-04-06 18:52:00 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm -n namespace-65 -- /opt/kafka/producer.sh USER=my_user_1175796063_1653183260 --max-messages 100 --bootstrap-server my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093 --topic my-topic-681975677-842065741
2022-04-06 18:52:03 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:295] Wait for kafka to rolling restart ...
2022-04-06 18:52:03 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b1bafe70-kafka rolling update
2022-04-06 18:52:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:52:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 18:52:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@44b5eac7, messages=[], arguments=[--group-id, my-consumer-group-1488603468, USER=my_user_1175796063_1653183260, --max-messages, 100, --group-instance-id, instance698015365, --bootstrap-server, my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093, --topic, my-topic-681975677-842065741], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm', podNamespace='namespace-65', bootstrapServer='my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093', topicName='my-topic-681975677-842065741', maxMessages=100, kafkaUsername='my-user-1175796063-1653183260', consumerGroupName='my-consumer-group-1488603468', consumerInstanceId='instance698015365', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@43ed4340}
2022-04-06 18:52:04 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093:my-topic-681975677-842065741 from pod my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm
2022-04-06 18:52:04 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-14228a28-kafka-clients-6bc74bcd85-hb9jm -n namespace-65 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1488603468 USER=my_user_1175796063_1653183260 --max-messages 100 --group-instance-id instance698015365 --bootstrap-server my-cluster-14228a28-kafka-bootstrap.namespace-65.svc:9093 --topic my-topic-681975677-842065741
2022-04-06 18:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-3cbec579-kafka has been successfully rolled
2022-04-06 18:52:11 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-3cbec579-kafka to be ready
2022-04-06 18:52:12 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:52:12 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:52:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:52:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testCertRegeneratedAfterInternalCAisDeleted
2022-04-06 18:52:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-681975677-842065741 in namespace namespace-65
2022-04-06 18:52:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-14228a28-kafka-clients in namespace namespace-65
2022-04-06 18:52:45 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-3cbec579-entity-operator rolling update
2022-04-06 18:52:45 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-3cbec579-entity-operator will be ready
2022-04-06 18:52:58 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-b1bafe70-kafka has been successfully rolled
2022-04-06 18:52:58 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-b1bafe70-kafka to be ready
2022-04-06 18:53:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1175796063-1653183260 in namespace namespace-65
2022-04-06 18:53:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-14228a28 in namespace namespace-65
2022-04-06 18:53:25 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:299] Wait for EO to rolling restart ...
2022-04-06 18:53:25 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b1bafe70-entity-operator rolling update
2022-04-06 18:53:25 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1bafe70-entity-operator will be ready
2022-04-06 18:53:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:53:32 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-65 for test case:testCertRegeneratedAfterInternalCAisDeleted
2022-04-06 18:53:36 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-3cbec579-entity-operator is ready
2022-04-06 18:53:46 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-3cbec579-entity-operator rolling update finished
2022-04-06 18:53:46 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:1252] Checking produced and consumed messages to pod:my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:53:46 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5e64a42b, messages=[], arguments=[--group-id, my-consumer-group-2033519560, USER=my_user_2091504663_135770503, --max-messages, 100, --group-instance-id, instance383213106, --bootstrap-server, my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-252251243-506456672], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-3cbec579-kafka-clients-78f944764b-wdphd', podNamespace='namespace-63', bootstrapServer='my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-252251243-506456672', maxMessages=100, kafkaUsername='my-user-2091504663-135770503', consumerGroupName='my-consumer-group-2033519560', consumerInstanceId='instance383213106', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1f7ca4a3}
2022-04-06 18:53:46 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093:my-topic-252251243-506456672 from pod my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:53:46 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-3cbec579-kafka-clients-78f944764b-wdphd -n namespace-63 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2033519560 USER=my_user_2091504663_135770503 --max-messages 100 --group-instance-id instance383213106 --bootstrap-server my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-252251243-506456672
2022-04-06 18:53:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:53:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:53:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1161603155-855377589 in namespace namespace-65
2022-04-06 18:53:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-63
2022-04-06 18:53:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1161603155-855377589 will have desired state: Ready
2022-04-06 18:54:06 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1bafe70-entity-operator is ready
2022-04-06 18:54:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertRegeneratedAfterInternalCAisDeleted-FINISHED
2022-04-06 18:54:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:54:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:54:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectCipherSuites-STARTED
2022-04-06 18:54:17 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b1bafe70-entity-operator rolling update finished
2022-04-06 18:54:17 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:303] Wait for CC and KE to rolling restart ...
2022-04-06 18:54:17 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b1bafe70-kafka-exporter rolling update
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-66 for test case:testKafkaAndKafkaConnectTlsVersion
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-66
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-66
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-66
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1287] Deploying Kafka cluster with the support TLSv1.2 TLS
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6576444a in namespace namespace-66
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-06 18:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6576444a will have desired state: Ready
2022-04-06 18:55:17 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1bafe70-kafka-exporter will be ready
2022-04-06 18:55:17 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1bafe70-kafka-exporter is ready
2022-04-06 18:55:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1161603155-855377589 is in desired state: Ready
2022-04-06 18:55:22 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3e7bb222, messages=[], arguments=[USER=my_user_2091504663_135770503, --max-messages, 100, --bootstrap-server, my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-1161603155-855377589], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-3cbec579-kafka-clients-78f944764b-wdphd', podNamespace='namespace-63', bootstrapServer='my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-1161603155-855377589', maxMessages=100, kafkaUsername='my-user-2091504663-135770503', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@37dba081}
2022-04-06 18:55:22 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093:my-topic-1161603155-855377589 from pod my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:55:22 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-3cbec579-kafka-clients-78f944764b-wdphd -n namespace-63 -- /opt/kafka/producer.sh USER=my_user_2091504663_135770503 --max-messages 100 --bootstrap-server my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-1161603155-855377589
2022-04-06 18:55:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 18:55:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 18:55:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@55d3917b, messages=[], arguments=[--group-id, my-consumer-group-2071036781, USER=my_user_2091504663_135770503, --max-messages, 100, --group-instance-id, instance93862476, --bootstrap-server, my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093, --topic, my-topic-1161603155-855377589], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-3cbec579-kafka-clients-78f944764b-wdphd', podNamespace='namespace-63', bootstrapServer='my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093', topicName='my-topic-1161603155-855377589', maxMessages=100, kafkaUsername='my-user-2091504663-135770503', consumerGroupName='my-consumer-group-2071036781', consumerInstanceId='instance93862476', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3fe6e1ae}
2022-04-06 18:55:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093:my-topic-1161603155-855377589 from pod my-cluster-3cbec579-kafka-clients-78f944764b-wdphd
2022-04-06 18:55:26 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-3cbec579-kafka-clients-78f944764b-wdphd -n namespace-63 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2071036781 USER=my_user_2091504663_135770503 --max-messages 100 --group-instance-id instance93862476 --bootstrap-server my-cluster-3cbec579-kafka-bootstrap.namespace-63.svc:9093 --topic my-topic-1161603155-855377589
2022-04-06 18:55:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b1bafe70-kafka-exporter rolling update finished
2022-04-06 18:55:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b1bafe70-cruise-control rolling update
2022-04-06 18:55:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1bafe70-cruise-control will be ready
2022-04-06 18:55:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1bafe70-cruise-control is ready
2022-04-06 18:55:33 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 18:55:33 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 18:55:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:55:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCaRenewalBreakInMiddle
2022-04-06 18:55:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-252251243-506456672 in namespace namespace-63
2022-04-06 18:55:37 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b1bafe70-cruise-control rolling update finished
2022-04-06 18:55:37 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:308] Checking the certificates have been replaced
2022-04-06 18:55:37 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:322] Checking consumed messages to pod:my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg
2022-04-06 18:55:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5f764cf8, messages=[], arguments=[--group-id, my-consumer-group-1757528452, --max-messages, 100, --group-instance-id, instance1452464346, --bootstrap-server, my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092, --topic, my-topic-196324947-1141101841], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg', podNamespace='namespace-64', bootstrapServer='my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092', topicName='my-topic-196324947-1141101841', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1757528452', consumerInstanceId='instance1452464346', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1d0169f6}
2022-04-06 18:55:37 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092#my-topic-196324947-1141101841 from pod my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg
2022-04-06 18:55:37 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1bafe70-kafka-clients-c94f96c94-p98vg -n namespace-64 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1757528452 --max-messages 100 --group-instance-id instance1452464346 --bootstrap-server my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9092 --topic my-topic-196324947-1141101841
2022-04-06 18:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6576444a is in desired state: Ready
2022-04-06 18:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1299] Verifying that Kafka cluster has the accepted configuration:
ssl.enabled.protocols -> TLSv1.2
ssl.protocol -> TLSv1.3
2022-04-06 18:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6576444a-kafka-clients in namespace namespace-66
2022-04-06 18:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-06 18:55:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6576444a-kafka-clients will be ready
2022-04-06 18:55:43 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:55:43 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:55:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-b1bafe70 in namespace namespace-66
2022-04-06 18:55:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-06 18:55:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-b1bafe70 will have desired state: Ready
2022-04-06 18:55:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1161603155-855377589 in namespace namespace-63
2022-04-06 18:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6576444a-kafka-clients is ready
2022-04-06 18:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6576444a-scraper in namespace namespace-66
2022-04-06 18:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-06 18:55:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6576444a-scraper will be ready
2022-04-06 18:55:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-b1bafe70 is in desired state: Ready
2022-04-06 18:55:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b1bafe70-kafka-clients-tls in namespace namespace-64
2022-04-06 18:55:44 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-64
2022-04-06 18:55:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b1bafe70-kafka-clients-tls will be ready
2022-04-06 18:55:45 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b1bafe70-kafka-clients-tls is ready
2022-04-06 18:55:45 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:346] Checking consumed messages to pod:my-cluster-b1bafe70-kafka-clients-tls-865466f569-xbfp7
2022-04-06 18:55:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7e3e4aac, messages=[], arguments=[--group-id, my-consumer-group-516813922, USER=bob_my_cluster_b1bafe70, --max-messages, 100, --group-instance-id, instance1600186376, --bootstrap-server, my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9093, --topic, my-topic-196324947-1141101841], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b1bafe70-kafka-clients-tls-865466f569-xbfp7', podNamespace='namespace-64', bootstrapServer='my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9093', topicName='my-topic-196324947-1141101841', maxMessages=100, kafkaUsername='bob-my-cluster-b1bafe70', consumerGroupName='my-consumer-group-516813922', consumerInstanceId='instance1600186376', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4213b4d8}
2022-04-06 18:55:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9093#my-topic-196324947-1141101841 from pod my-cluster-b1bafe70-kafka-clients-tls-865466f569-xbfp7
2022-04-06 18:55:45 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b1bafe70-kafka-clients-tls-865466f569-xbfp7 -n namespace-64 -- /opt/kafka/consumer.sh --group-id my-consumer-group-516813922 USER=bob_my_cluster_b1bafe70 --max-messages 100 --group-instance-id instance1600186376 --bootstrap-server my-cluster-b1bafe70-kafka-bootstrap.namespace-64.svc:9093 --topic my-topic-196324947-1141101841
2022-04-06 18:55:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6576444a-scraper is ready
2022-04-06 18:55:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-6576444a-scraper to be ready
2022-04-06 18:55:52 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 18:55:52 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 18:55:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 18:55:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewAllCaCertsTriggeredByAnno
2022-04-06 18:55:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b1bafe70-kafka-clients in namespace namespace-64
2022-04-06 18:55:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-3cbec579-kafka-clients in namespace namespace-63
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-6576444a-scraper is ready
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-6576444a-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-6576444a-allow in namespace namespace-66
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-6576444a in namespace namespace-66
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-66
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1322] Verifying that Kafka Connect status is NotReady because of different TLS version
2022-04-06 18:55:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6576444a will have desired state: NotReady
2022-04-06 18:56:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2091504663-135770503 in namespace namespace-63
2022-04-06 18:56:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3cbec579 in namespace namespace-63
2022-04-06 18:57:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:57:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-63 for test case:testCaRenewalBreakInMiddle
2022-04-06 18:57:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b1bafe70-kafka-clients-tls in namespace namespace-64
2022-04-06 18:57:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-b1bafe70 in namespace namespace-64
2022-04-06 18:57:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2033992940-830015114 in namespace namespace-64
2022-04-06 18:57:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-196324947-1141101841 in namespace namespace-64
2022-04-06 18:57:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b1bafe70 in namespace namespace-64
2022-04-06 18:57:42 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-64, for cruise control Kafka cluster my-cluster-b1bafe70
2022-04-06 18:57:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCaRenewalBreakInMiddle-FINISHED
2022-04-06 18:57:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:57:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:57:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testCertRenewalInMaintenanceWindow-STARTED
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-67 for test case:testKafkaAndKafkaConnectCipherSuites
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-67
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-67
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-67
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1362] Deploying Kafka cluster with the support TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 cipher algorithms
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9a2e7c27 in namespace namespace-67
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-06 18:57:51 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a2e7c27 will have desired state: Ready
2022-04-06 18:57:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 18:57:52 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-64 for test case:testAutoRenewAllCaCertsTriggeredByAnno
2022-04-06 18:58:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewAllCaCertsTriggeredByAnno-FINISHED
2022-04-06 18:58:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 18:58:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 18:58:36 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testOwnerReferenceOfCASecrets-STARTED
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-68 for test case:testCertRenewalInMaintenanceWindow
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-68
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-68
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-68
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:698] Maintenance window is: * 3-17 * * * ? *
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-20da23c6 in namespace namespace-68
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-06 18:58:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-20da23c6 will have desired state: Ready
2022-04-06 18:59:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-20da23c6 is in desired state: Ready
2022-04-06 18:59:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-667332777-441709796 in namespace namespace-68
2022-04-06 18:59:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-06 18:59:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-667332777-441709796 will have desired state: Ready
2022-04-06 18:59:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-667332777-441709796 is in desired state: Ready
2022-04-06 18:59:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-996914200-1946080062 in namespace namespace-68
2022-04-06 18:59:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-06 18:59:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-996914200-1946080062 will have desired state: Ready
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-996914200-1946080062 is in desired state: Ready
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-996914200-1946080062 in namespace namespace-68
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-996914200-1946080062 will have desired state: Ready
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-996914200-1946080062 is in desired state: Ready
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-20da23c6-kafka-clients in namespace namespace-68
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-68
2022-04-06 18:59:58 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-20da23c6-kafka-clients will be ready
2022-04-06 19:00:00 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-20da23c6-kafka-clients is ready
2022-04-06 19:00:00 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 19:00:00 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:728] Annotate secret my-cluster-20da23c6-cluster-ca-cert with secret force-renew annotation
2022-04-06 19:00:00 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:735] Wait until maintenance windows starts
2022-04-06 19:00:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a2e7c27 is in desired state: Ready
2022-04-06 19:00:08 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1374] Verifying that Kafka Connect has the accepted configuration:
 ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2022-04-06 19:00:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9a2e7c27-kafka-clients in namespace namespace-68
2022-04-06 19:00:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-06 19:00:08 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9a2e7c27-kafka-clients will be ready
2022-04-06 19:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9a2e7c27-kafka-clients is ready
2022-04-06 19:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9a2e7c27-scraper in namespace namespace-67
2022-04-06 19:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-06 19:00:10 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9a2e7c27-scraper will be ready
2022-04-06 19:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9a2e7c27-scraper is ready
2022-04-06 19:00:12 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-9a2e7c27-scraper to be ready
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-9a2e7c27-scraper is ready
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-9a2e7c27-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-9a2e7c27-allow in namespace namespace-67
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-9a2e7c27 in namespace namespace-68
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-67
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1391] Verifying that Kafka Connect status is NotReady because of different cipher suites complexity of algorithm
2022-04-06 19:00:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9a2e7c27 will have desired state: NotReady
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6576444a is in desired state: NotReady
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1326] Replacing Kafka Connect config to the newest(TLSv1.2) one same as the Kafka broker has.
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1330] Verifying that Kafka Connect has the accepted configuration:
 ssl.enabled.protocols -> TLSv1.2
 ssl.protocol -> TLSv1.3
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.enabled.protocols -> TLSv1.2 change
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.enabled.protocols -> TLSv1.2 change
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.protocol -> TLSv1.3 change
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.protocol -> TLSv1.3 change
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1339] Verifying that Kafka Connect is stable
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-6576444a-connect are stable
2022-04-06 19:00:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:00:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:01:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:01:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:01:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:01:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:01:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:01:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:01:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:01:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:01:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:01:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:01:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:01:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:01:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:01:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:01:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:01:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:01:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:01:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:01:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:01:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:01:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:01:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:01:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:01:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:01:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:01:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:01:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:01:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:01:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:01:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:01:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:01:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:01:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:01:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:01:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-6576444a-connect-66cbf9dfd5-wdtgb is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:01:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-6576444a-connect-66cbf9dfd5-wdtgb
2022-04-06 19:01:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1343] Verifying that Kafka Connect status is Ready because of same TLS version
2022-04-06 19:01:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6576444a will have desired state: Ready
2022-04-06 19:03:00 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:741] Maintenance window starts
2022-04-06 19:03:00 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:745] Wait until rolling update is triggered during maintenance window
2022-04-06 19:03:00 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-20da23c6-kafka rolling update
2022-04-06 19:05:15 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-20da23c6-kafka has been successfully rolled
2022-04-06 19:05:15 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-20da23c6-kafka to be ready
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9a2e7c27 is in desired state: NotReady
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1395] Replacing Kafka Connect config to the cipher suites same as the Kafka broker has.
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1399] Verifying that Kafka Connect has the accepted configuration:
 ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaConnectUtils:101] Waiting for Kafka Connect property ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 change
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [KafkaConnectUtils:109] Kafka Connect property ssl.cipher.suites -> TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 change
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1404] Verifying that Kafka Connect is stable
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-9a2e7c27-connect are stable
2022-04-06 19:05:24 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:05:25 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:05:26 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:05:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:05:28 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:05:29 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:05:30 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:05:31 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:05:32 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:05:33 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:05:34 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:05:35 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:05:36 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:05:37 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:05:38 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:05:39 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:05:40 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:05:41 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:05:42 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:05:43 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:05:44 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-20da23c6 will have desired state: Ready
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-20da23c6 is in desired state: Ready
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-20da23c6 is ready
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:750] Checking consumed messages to pod:my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@61b9a27b, messages=[], arguments=[USER=my_user_667332777_441709796, --max-messages, 100, --bootstrap-server, my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093, --topic, my-topic-996914200-1946080062], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm', podNamespace='namespace-68', bootstrapServer='my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093', topicName='my-topic-996914200-1946080062', maxMessages=100, kafkaUsername='my-user-667332777-441709796', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5defe26e}
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093:my-topic-996914200-1946080062 from pod my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm
2022-04-06 19:05:45 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm -n namespace-68 -- /opt/kafka/producer.sh USER=my_user_667332777_441709796 --max-messages 100 --bootstrap-server my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093 --topic my-topic-996914200-1946080062
2022-04-06 19:05:45 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:05:46 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:05:47 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:05:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 19:05:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 19:05:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6e18b4e7, messages=[], arguments=[--group-id, my-consumer-group-164507637, USER=my_user_667332777_441709796, --max-messages, 100, --group-instance-id, instance1311342872, --bootstrap-server, my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093, --topic, my-topic-996914200-1946080062], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm', podNamespace='namespace-68', bootstrapServer='my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093', topicName='my-topic-996914200-1946080062', maxMessages=100, kafkaUsername='my-user-667332777-441709796', consumerGroupName='my-consumer-group-164507637', consumerInstanceId='instance1311342872', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4cc6d383}
2022-04-06 19:05:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093:my-topic-996914200-1946080062 from pod my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm
2022-04-06 19:05:48 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-20da23c6-kafka-clients-59d667d6c7-4ghdm -n namespace-68 -- /opt/kafka/consumer.sh --group-id my-consumer-group-164507637 USER=my_user_667332777_441709796 --max-messages 100 --group-instance-id instance1311342872 --bootstrap-server my-cluster-20da23c6-kafka-bootstrap.namespace-68.svc:9093 --topic my-topic-996914200-1946080062
2022-04-06 19:05:48 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:05:49 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:05:50 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:05:51 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:05:52 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:05:53 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:05:54 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:05:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 19:05:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 19:05:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:05:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCertRenewalInMaintenanceWindow
2022-04-06 19:05:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-996914200-1946080062 in namespace namespace-68
2022-04-06 19:05:55 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:05:56 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:05:57 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:05:58 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:05:59 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:06:00 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:06:01 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:06:02 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:06:03 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:06:04 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:06:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-20da23c6-kafka-clients in namespace namespace-68
2022-04-06 19:06:05 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:06:06 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:06:07 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:06:08 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:06:09 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:06:10 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:06:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:06:12 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:06:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:322] Pod my-cluster-9a2e7c27-connect-5f7f684489-bdcrt is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:06:13 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-9a2e7c27-connect-5f7f684489-bdcrt
2022-04-06 19:06:13 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:1408] Verifying that Kafka Connect status is Ready because of the same cipher suites complexity of algorithm
2022-04-06 19:06:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9a2e7c27 will have desired state: Ready
2022-04-06 19:06:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-996914200-1946080062 in namespace namespace-68
2022-04-06 19:06:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-667332777-441709796 in namespace namespace-68
2022-04-06 19:07:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-20da23c6 in namespace namespace-68
2022-04-06 19:07:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6576444a is in desired state: Ready
2022-04-06 19:07:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:07:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndKafkaConnectTlsVersion
2022-04-06 19:07:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6576444a-scraper in namespace namespace-66
2022-04-06 19:07:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:07:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-68 for test case:testCertRenewalInMaintenanceWindow
2022-04-06 19:07:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testCertRenewalInMaintenanceWindow-FINISHED
2022-04-06 19:07:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:07:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:07:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoRenewClientsCaCertsTriggeredByAnno-STARTED
2022-04-06 19:07:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-6576444a in namespace namespace-66
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-69 for test case:testOwnerReferenceOfCASecrets
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-69
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-69
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-69
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ec4e3a49 in namespace namespace-69
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-06 19:08:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ec4e3a49 will have desired state: Ready
2022-04-06 19:08:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-6576444a-allow in namespace namespace-66
2022-04-06 19:08:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6576444a-kafka-clients in namespace namespace-66
2022-04-06 19:08:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6576444a in namespace namespace-66
2022-04-06 19:08:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:08:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-66 for test case:testKafkaAndKafkaConnectTlsVersion
2022-04-06 19:09:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ec4e3a49 is in desired state: Ready
2022-04-06 19:09:21 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1432] Listing all cluster CAs for my-cluster-ec4e3a49
2022-04-06 19:09:21 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1436] Deleting Kafka:my-cluster-ec4e3a49
2022-04-06 19:09:21 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUtils:399] Waiting for deletion of Kafka:my-cluster-ec4e3a49
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1440] Checking actual secrets after Kafka deletion
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-ec4e3a49-clients-ca secret is still present
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-ec4e3a49-clients-ca
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-ec4e3a49-clients-ca-cert secret is still present
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-ec4e3a49-clients-ca-cert
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-ec4e3a49-cluster-ca secret is still present
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-ec4e3a49-cluster-ca
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1443] Checking that my-cluster-ec4e3a49-cluster-ca-cert secret is still present
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1446] Deleting secret: my-cluster-ec4e3a49-cluster-ca-cert
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1450] Deploying Kafka with generateSecretOwnerReference set to true
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-second-cluster-my-cluster-ec4e3a49 in namespace namespace-69
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-69
2022-04-06 19:09:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-second-cluster-my-cluster-ec4e3a49 will have desired state: Ready
2022-04-06 19:09:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectTlsVersion-FINISHED
2022-04-06 19:09:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:09:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:09:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testClusterCACertRenew-STARTED
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-70 for test case:testAutoRenewClientsCaCertsTriggeredByAnno
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-70
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-70
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-70
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-eb4b0fe0 in namespace namespace-70
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-06 19:09:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-eb4b0fe0 will have desired state: Ready
2022-04-06 19:11:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9a2e7c27 is in desired state: Ready
2022-04-06 19:11:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:11:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndKafkaConnectCipherSuites
2022-04-06 19:11:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9a2e7c27-scraper in namespace namespace-67
2022-04-06 19:11:43 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-second-cluster-my-cluster-ec4e3a49 is in desired state: Ready
2022-04-06 19:11:43 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1465] Deleting Kafka:my-second-cluster-my-cluster-ec4e3a49
2022-04-06 19:11:43 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaUtils:399] Waiting for deletion of Kafka:my-second-cluster-my-cluster-ec4e3a49
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1469] Checking actual secrets after Kafka deletion
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-ec4e3a49-clients-ca secret is deleted
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-ec4e3a49-clients-ca-cert secret is deleted
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-ec4e3a49-cluster-ca secret is deleted
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:1472] Checking that my-second-cluster-my-cluster-ec4e3a49-cluster-ca-cert secret is deleted
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testOwnerReferenceOfCASecrets
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-second-cluster-my-cluster-ec4e3a49 in namespace namespace-69
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ec4e3a49 in namespace namespace-69
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:11:54 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-69 for test case:testOwnerReferenceOfCASecrets
2022-04-06 19:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-eb4b0fe0 is in desired state: Ready
2022-04-06 19:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2009114348-320200869 in namespace namespace-70
2022-04-06 19:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-06 19:12:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2009114348-320200869 will have desired state: Ready
2022-04-06 19:12:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2009114348-320200869 is in desired state: Ready
2022-04-06 19:12:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-780417872-1811440894 in namespace namespace-70
2022-04-06 19:12:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-06 19:12:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-780417872-1811440894 will have desired state: Ready
2022-04-06 19:12:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-780417872-1811440894 is in desired state: Ready
2022-04-06 19:12:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-eb4b0fe0-kafka-clients in namespace namespace-70
2022-04-06 19:12:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-06 19:12:03 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-eb4b0fe0-kafka-clients will be ready
2022-04-06 19:12:06 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-eb4b0fe0-kafka-clients is ready
2022-04-06 19:12:06 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 19:12:06 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:260] Checking produced and consumed messages to pod:my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj
2022-04-06 19:12:06 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@77ac4a5f, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-780417872-1811440894], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj', podNamespace='namespace-70', bootstrapServer='my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-780417872-1811440894', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66a8015d}
2022-04-06 19:12:06 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092:my-topic-780417872-1811440894 from pod my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj
2022-04-06 19:12:06 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj -n namespace-70 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-780417872-1811440894
2022-04-06 19:12:08 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 19:12:08 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 19:12:08 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@18ef3b8f, messages=[], arguments=[--group-id, my-consumer-group-1376255930, --max-messages, 100, --group-instance-id, instance2132580056, --bootstrap-server, my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-780417872-1811440894], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj', podNamespace='namespace-70', bootstrapServer='my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-780417872-1811440894', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1376255930', consumerInstanceId='instance2132580056', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3eeb77e6}
2022-04-06 19:12:08 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092#my-topic-780417872-1811440894 from pod my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj
2022-04-06 19:12:08 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj -n namespace-70 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1376255930 --max-messages 100 --group-instance-id instance2132580056 --bootstrap-server my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-780417872-1811440894
2022-04-06 19:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:274] Triggering CA cert renewal by adding the annotation
2022-04-06 19:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:286] Patching secret my-cluster-eb4b0fe0-clients-ca-cert with strimzi.io/force-renew
2022-04-06 19:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:295] Wait for kafka to rolling restart ...
2022-04-06 19:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-eb4b0fe0-kafka rolling update
2022-04-06 19:12:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testOwnerReferenceOfCASecrets-FINISHED
2022-04-06 19:12:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:12:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:12:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClusterCaKeysTriggeredByAnno-STARTED
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-71 for test case:testClusterCACertRenew
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-71
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-71
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-71
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-412c64c8 in namespace namespace-71
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-71
2022-04-06 19:12:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-412c64c8 will have desired state: Ready
2022-04-06 19:12:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-9a2e7c27 in namespace namespace-67
2022-04-06 19:12:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-9a2e7c27-allow in namespace namespace-67
2022-04-06 19:12:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9a2e7c27-kafka-clients in namespace namespace-67
2022-04-06 19:13:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9a2e7c27 in namespace namespace-67
2022-04-06 19:13:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:13:33 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-67 for test case:testKafkaAndKafkaConnectCipherSuites
2022-04-06 19:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-412c64c8 is in desired state: Ready
2022-04-06 19:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1512] Change of kafka validity and renewal days - reconciliation should start.
2022-04-06 19:13:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-412c64c8-zookeeper rolling update
2022-04-06 19:13:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-eb4b0fe0-kafka has been successfully rolled
2022-04-06 19:13:44 [ForkJoinPool-3-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-eb4b0fe0-kafka to be ready
2022-04-06 19:14:09 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:308] Checking the certificates have been replaced
2022-04-06 19:14:09 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:322] Checking consumed messages to pod:my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj
2022-04-06 19:14:09 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@60e91984, messages=[], arguments=[--group-id, my-consumer-group-1472021806, --max-messages, 100, --group-instance-id, instance1943956407, --bootstrap-server, my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092, --topic, my-topic-780417872-1811440894], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj', podNamespace='namespace-70', bootstrapServer='my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092', topicName='my-topic-780417872-1811440894', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1472021806', consumerInstanceId='instance1943956407', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3488e176}
2022-04-06 19:14:09 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092#my-topic-780417872-1811440894 from pod my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj
2022-04-06 19:14:09 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb4b0fe0-kafka-clients-78d47b666c-wfwtj -n namespace-70 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1472021806 --max-messages 100 --group-instance-id instance1943956407 --bootstrap-server my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9092 --topic my-topic-780417872-1811440894
2022-04-06 19:14:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testKafkaAndKafkaConnectCipherSuites-FINISHED
2022-04-06 19:14:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:14:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:14:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.SecurityST.testAutoReplaceAllCaKeysTriggeredByAnno-STARTED
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-72 for test case:testAutoReplaceClusterCaKeysTriggeredByAnno
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-72
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-72
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-72
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4a62c54d in namespace namespace-72
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-06 19:14:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4a62c54d will have desired state: Ready
2022-04-06 19:14:22 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:14:22 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:14:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser bob-my-cluster-eb4b0fe0 in namespace namespace-72
2022-04-06 19:14:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-06 19:14:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: bob-my-cluster-eb4b0fe0 will have desired state: Ready
2022-04-06 19:14:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: bob-my-cluster-eb4b0fe0 is in desired state: Ready
2022-04-06 19:14:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-eb4b0fe0-kafka-clients-tls in namespace namespace-70
2022-04-06 19:14:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-70
2022-04-06 19:14:23 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-eb4b0fe0-kafka-clients-tls will be ready
2022-04-06 19:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-eb4b0fe0-kafka-clients-tls is ready
2022-04-06 19:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [SecurityST:346] Checking consumed messages to pod:my-cluster-eb4b0fe0-kafka-clients-tls-6879456766-q5vhl
2022-04-06 19:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@66309e35, messages=[], arguments=[--group-id, my-consumer-group-1177053589, USER=bob_my_cluster_eb4b0fe0, --max-messages, 100, --group-instance-id, instance982523009, --bootstrap-server, my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9093, --topic, my-topic-780417872-1811440894], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-eb4b0fe0-kafka-clients-tls-6879456766-q5vhl', podNamespace='namespace-70', bootstrapServer='my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9093', topicName='my-topic-780417872-1811440894', maxMessages=100, kafkaUsername='bob-my-cluster-eb4b0fe0', consumerGroupName='my-consumer-group-1177053589', consumerInstanceId='instance982523009', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@18ceed20}
2022-04-06 19:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9093#my-topic-780417872-1811440894 from pod my-cluster-eb4b0fe0-kafka-clients-tls-6879456766-q5vhl
2022-04-06 19:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb4b0fe0-kafka-clients-tls-6879456766-q5vhl -n namespace-70 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1177053589 USER=bob_my_cluster_eb4b0fe0 --max-messages 100 --group-instance-id instance982523009 --bootstrap-server my-cluster-eb4b0fe0-kafka-bootstrap.namespace-70.svc:9093 --topic my-topic-780417872-1811440894
2022-04-06 19:14:33 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:14:33 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:14:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:14:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoRenewClientsCaCertsTriggeredByAnno
2022-04-06 19:14:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-eb4b0fe0-kafka-clients in namespace namespace-70
2022-04-06 19:15:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-eb4b0fe0-kafka-clients-tls in namespace namespace-70
2022-04-06 19:15:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser bob-my-cluster-eb4b0fe0 in namespace namespace-70
2022-04-06 19:16:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-412c64c8-zookeeper has been successfully rolled
2022-04-06 19:16:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-412c64c8-zookeeper to be ready
2022-04-06 19:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2009114348-320200869 in namespace namespace-70
2022-04-06 19:16:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-780417872-1811440894 in namespace namespace-70
2022-04-06 19:16:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-eb4b0fe0 in namespace namespace-70
2022-04-06 19:16:24 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-70, for cruise control Kafka cluster my-cluster-eb4b0fe0
2022-04-06 19:16:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:16:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-70 for test case:testAutoRenewClientsCaCertsTriggeredByAnno
2022-04-06 19:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoRenewClientsCaCertsTriggeredByAnno-FINISHED
2022-04-06 19:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:17:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4a62c54d is in desired state: Ready
2022-04-06 19:17:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1795847282-2023667004 in namespace namespace-72
2022-04-06 19:17:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-06 19:17:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1795847282-2023667004 will have desired state: Ready
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-73 for test case:testAutoReplaceAllCaKeysTriggeredByAnno
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-73
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-73
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-73
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:587] Creating a cluster
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b50baeb3 in namespace namespace-73
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-06 19:17:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b50baeb3 will have desired state: Ready
2022-04-06 19:17:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1795847282-2023667004 is in desired state: Ready
2022-04-06 19:17:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-75208733-307100740 in namespace namespace-73
2022-04-06 19:17:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-06 19:17:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-75208733-307100740 will have desired state: Ready
2022-04-06 19:17:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-75208733-307100740 is in desired state: Ready
2022-04-06 19:17:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-4a62c54d-kafka-clients in namespace namespace-73
2022-04-06 19:17:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-06 19:17:23 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-kafka-clients will be ready
2022-04-06 19:17:25 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-kafka-clients is ready
2022-04-06 19:17:25 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 19:17:25 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:453] Checking produced and consumed messages to pod:my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b
2022-04-06 19:17:25 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4afac545, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092, --topic, my-topic-75208733-307100740], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b', podNamespace='namespace-72', bootstrapServer='my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092', topicName='my-topic-75208733-307100740', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@391470b1}
2022-04-06 19:17:25 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092:my-topic-75208733-307100740 from pod my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b
2022-04-06 19:17:25 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b -n namespace-72 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092 --topic my-topic-75208733-307100740
2022-04-06 19:17:28 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 19:17:28 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 19:17:28 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@716ac5b8, messages=[], arguments=[--group-id, my-consumer-group-823753898, --max-messages, 100, --group-instance-id, instance622791274, --bootstrap-server, my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092, --topic, my-topic-75208733-307100740], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b', podNamespace='namespace-72', bootstrapServer='my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092', topicName='my-topic-75208733-307100740', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-823753898', consumerInstanceId='instance622791274', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@482543a5}
2022-04-06 19:17:28 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092#my-topic-75208733-307100740 from pod my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b
2022-04-06 19:17:28 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b -n namespace-72 -- /opt/kafka/consumer.sh --group-id my-consumer-group-823753898 --max-messages 100 --group-instance-id instance622791274 --bootstrap-server my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092 --topic my-topic-75208733-307100740
2022-04-06 19:17:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-412c64c8-kafka rolling update
2022-04-06 19:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:467] Triggering CA cert renewal by adding the annotation
2022-04-06 19:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:479] Patching secret my-cluster-4a62c54d-cluster-ca with strimzi.io/force-replace
2022-04-06 19:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:484] Wait for zk to rolling restart (1)...
2022-04-06 19:17:35 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4a62c54d-zookeeper rolling update
2022-04-06 19:18:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4a62c54d-zookeeper has been successfully rolled
2022-04-06 19:18:55 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:489] Wait for kafka to rolling restart (1)...
2022-04-06 19:18:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4a62c54d-kafka rolling update
2022-04-06 19:19:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-412c64c8-kafka has been successfully rolled
2022-04-06 19:19:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-412c64c8-kafka to be ready
2022-04-06 19:19:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-412c64c8-entity-operator rolling update
2022-04-06 19:19:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b50baeb3 is in desired state: Ready
2022-04-06 19:19:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-2090310259-1267188022 in namespace namespace-73
2022-04-06 19:19:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-06 19:19:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-2090310259-1267188022 will have desired state: Ready
2022-04-06 19:19:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-2090310259-1267188022 is in desired state: Ready
2022-04-06 19:19:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-352443010-816945957 in namespace namespace-73
2022-04-06 19:19:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-06 19:19:32 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-352443010-816945957 will have desired state: Ready
2022-04-06 19:19:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-352443010-816945957 is in desired state: Ready
2022-04-06 19:19:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b50baeb3-kafka-clients in namespace namespace-73
2022-04-06 19:19:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-06 19:19:33 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-kafka-clients will be ready
2022-04-06 19:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-kafka-clients is ready
2022-04-06 19:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 19:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:453] Checking produced and consumed messages to pod:my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf
2022-04-06 19:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@42ca5851, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092, --topic, my-topic-352443010-816945957], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf', podNamespace='namespace-73', bootstrapServer='my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092', topicName='my-topic-352443010-816945957', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b9697e8}
2022-04-06 19:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092:my-topic-352443010-816945957 from pod my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf
2022-04-06 19:19:35 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf -n namespace-73 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092 --topic my-topic-352443010-816945957
2022-04-06 19:19:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-412c64c8-entity-operator will be ready
2022-04-06 19:19:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-06 19:19:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-06 19:19:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@16604834, messages=[], arguments=[--group-id, my-consumer-group-711785963, --max-messages, 100, --group-instance-id, instance1756328188, --bootstrap-server, my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092, --topic, my-topic-352443010-816945957], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf', podNamespace='namespace-73', bootstrapServer='my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092', topicName='my-topic-352443010-816945957', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-711785963', consumerInstanceId='instance1756328188', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4996af39}
2022-04-06 19:19:38 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092#my-topic-352443010-816945957 from pod my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf
2022-04-06 19:19:38 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf -n namespace-73 -- /opt/kafka/consumer.sh --group-id my-consumer-group-711785963 --max-messages 100 --group-instance-id instance1756328188 --bootstrap-server my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092 --topic my-topic-352443010-816945957
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:467] Triggering CA cert renewal by adding the annotation
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:479] Patching secret my-cluster-b50baeb3-cluster-ca with strimzi.io/force-replace
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:479] Patching secret my-cluster-b50baeb3-clients-ca with strimzi.io/force-replace
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:484] Wait for zk to rolling restart (1)...
2022-04-06 19:19:43 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b50baeb3-zookeeper rolling update
2022-04-06 19:20:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-412c64c8-entity-operator is ready
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-412c64c8-entity-operator rolling update finished
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1545] Initial ClusterCA cert dates: Wed Apr 06 19:12:22 UTC 2022 --> Tue Apr 26 19:12:22 UTC 2022
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1546] Changed ClusterCA cert dates: Wed Apr 06 19:13:43 UTC 2022 --> Sun Oct 23 19:13:43 UTC 2022
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1547] KafkaBroker cert creation dates: Wed Apr 06 19:12:53 UTC 2022 --> Tue Apr 26 19:12:53 UTC 2022
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1548] KafkaBroker cert changed dates:  Wed Apr 06 19:17:25 UTC 2022 --> Sun Oct 23 19:17:25 UTC 2022
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1549] Zookeeper cert creation dates: Wed Apr 06 19:12:26 UTC 2022 --> Tue Apr 26 19:12:26 UTC 2022
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SecurityST:1550] Zookeeper cert changed dates:  Wed Apr 06 19:13:44 UTC 2022 --> Sun Oct 23 19:13:44 UTC 2022
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClusterCACertRenew
2022-04-06 19:20:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-412c64c8 in namespace namespace-71
2022-04-06 19:20:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:20:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-71 for test case:testClusterCACertRenew
2022-04-06 19:20:35 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4a62c54d-kafka has been successfully rolled
2022-04-06 19:20:35 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:494] Wait for EO to rolling restart (1)...
2022-04-06 19:20:35 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4a62c54d-entity-operator rolling update
2022-04-06 19:21:05 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-entity-operator will be ready
2022-04-06 19:21:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testClusterCACertRenew-FINISHED
2022-04-06 19:21:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:21:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-b50baeb3-zookeeper has been successfully rolled
2022-04-06 19:21:24 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:489] Wait for kafka to rolling restart (1)...
2022-04-06 19:21:24 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b50baeb3-kafka rolling update
2022-04-06 19:21:51 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-entity-operator is ready
2022-04-06 19:22:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4a62c54d-entity-operator rolling update finished
2022-04-06 19:22:01 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:499] Wait for KafkaExporter and CruiseControl to rolling restart (1)...
2022-04-06 19:22:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4a62c54d-kafka-exporter rolling update
2022-04-06 19:22:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-kafka-exporter will be ready
2022-04-06 19:22:12 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-kafka-exporter is ready
2022-04-06 19:22:22 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4a62c54d-kafka-exporter rolling update finished
2022-04-06 19:22:22 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4a62c54d-cruise-control rolling update
2022-04-06 19:22:27 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-cruise-control will be ready
2022-04-06 19:22:37 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-cruise-control is ready
2022-04-06 19:22:47 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4a62c54d-cruise-control rolling update finished
2022-04-06 19:22:47 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:505] Wait for zk to rolling restart (2)...
2022-04-06 19:22:47 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4a62c54d-zookeeper rolling update
2022-04-06 19:23:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4a62c54d-zookeeper has been successfully rolled
2022-04-06 19:23:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-4a62c54d-zookeeper to be ready
2022-04-06 19:23:59 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:510] Wait for kafka to rolling restart (2)...
2022-04-06 19:23:59 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4a62c54d-kafka rolling update
2022-04-06 19:24:14 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-b50baeb3-kafka has been successfully rolled
2022-04-06 19:24:14 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:494] Wait for EO to rolling restart (1)...
2022-04-06 19:24:14 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b50baeb3-entity-operator rolling update
2022-04-06 19:24:44 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-entity-operator will be ready
2022-04-06 19:25:14 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-entity-operator is ready
2022-04-06 19:25:24 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b50baeb3-entity-operator rolling update finished
2022-04-06 19:25:24 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:499] Wait for KafkaExporter and CruiseControl to rolling restart (1)...
2022-04-06 19:25:24 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b50baeb3-kafka-exporter rolling update
2022-04-06 19:25:24 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-kafka-exporter will be ready
2022-04-06 19:25:34 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4a62c54d-kafka has been successfully rolled
2022-04-06 19:25:34 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-4a62c54d-kafka to be ready
2022-04-06 19:25:58 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-kafka-exporter is ready
2022-04-06 19:26:00 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:515] Wait for EO to rolling restart (2)...
2022-04-06 19:26:00 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4a62c54d-entity-operator rolling update
2022-04-06 19:26:00 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-entity-operator will be ready
2022-04-06 19:26:08 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b50baeb3-kafka-exporter rolling update finished
2022-04-06 19:26:08 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b50baeb3-cruise-control rolling update
2022-04-06 19:26:13 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-cruise-control will be ready
2022-04-06 19:26:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-cruise-control is ready
2022-04-06 19:26:27 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b50baeb3-cruise-control rolling update finished
2022-04-06 19:26:27 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:505] Wait for zk to rolling restart (2)...
2022-04-06 19:26:27 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b50baeb3-zookeeper rolling update
2022-04-06 19:26:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-entity-operator is ready
2022-04-06 19:26:54 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4a62c54d-entity-operator rolling update finished
2022-04-06 19:26:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:520] Wait for KafkaExporter and CruiseControl to rolling restart (2)...
2022-04-06 19:26:54 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4a62c54d-kafka-exporter rolling update
2022-04-06 19:27:27 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-b50baeb3-zookeeper has been successfully rolled
2022-04-06 19:27:27 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-b50baeb3-zookeeper to be ready
2022-04-06 19:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-kafka-exporter will be ready
2022-04-06 19:27:34 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-kafka-exporter is ready
2022-04-06 19:27:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4a62c54d-kafka-exporter rolling update finished
2022-04-06 19:27:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-4a62c54d-cruise-control rolling update
2022-04-06 19:27:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-cruise-control will be ready
2022-04-06 19:27:44 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-cruise-control is ready
2022-04-06 19:27:54 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-4a62c54d-cruise-control rolling update finished
2022-04-06 19:27:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:525] Checking the certificates have been replaced
2022-04-06 19:27:54 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:536] Checking consumed messages to pod:my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b
2022-04-06 19:27:54 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7bc0856c, messages=[], arguments=[--group-id, my-consumer-group-490085066, --max-messages, 100, --group-instance-id, instance227984192, --bootstrap-server, my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092, --topic, my-topic-75208733-307100740], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b', podNamespace='namespace-72', bootstrapServer='my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092', topicName='my-topic-75208733-307100740', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-490085066', consumerInstanceId='instance227984192', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@74460cc}
2022-04-06 19:27:54 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092#my-topic-75208733-307100740 from pod my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b
2022-04-06 19:27:54 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4a62c54d-kafka-clients-5f64d8c957-kcr7b -n namespace-72 -- /opt/kafka/consumer.sh --group-id my-consumer-group-490085066 --max-messages 100 --group-instance-id instance227984192 --bootstrap-server my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092 --topic my-topic-75208733-307100740
2022-04-06 19:27:58 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:510] Wait for kafka to rolling restart (2)...
2022-04-06 19:27:58 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-b50baeb3-kafka rolling update
2022-04-06 19:28:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:28:00 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:28:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-236173793-534350904 in namespace namespace-73
2022-04-06 19:28:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-06 19:28:00 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-236173793-534350904 will have desired state: Ready
2022-04-06 19:28:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-236173793-534350904 is in desired state: Ready
2022-04-06 19:28:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-4a62c54d-kafka-clients-tls in namespace namespace-73
2022-04-06 19:28:01 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-72
2022-04-06 19:28:01 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a62c54d-kafka-clients-tls will be ready
2022-04-06 19:28:03 [ForkJoinPool-3-worker-5] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a62c54d-kafka-clients-tls is ready
2022-04-06 19:28:03 [ForkJoinPool-3-worker-5] [32mINFO [m [SecurityST:561] Checking consumed messages to pod:my-cluster-4a62c54d-kafka-clients-tls-7db7fc8b99-ldrkc
2022-04-06 19:28:03 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@76a3d4f0, messages=[], arguments=[--group-id, my-consumer-group-1422469042, --max-messages, 100, --group-instance-id, instance7822834, --bootstrap-server, my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092, --topic, my-topic-75208733-307100740], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-4a62c54d-kafka-clients-tls-7db7fc8b99-ldrkc', podNamespace='namespace-72', bootstrapServer='my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092', topicName='my-topic-75208733-307100740', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1422469042', consumerInstanceId='instance7822834', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52c58c64}
2022-04-06 19:28:03 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092#my-topic-75208733-307100740 from pod my-cluster-4a62c54d-kafka-clients-tls-7db7fc8b99-ldrkc
2022-04-06 19:28:03 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-4a62c54d-kafka-clients-tls-7db7fc8b99-ldrkc -n namespace-72 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1422469042 --max-messages 100 --group-instance-id instance7822834 --bootstrap-server my-cluster-4a62c54d-kafka-bootstrap.namespace-72.svc:9092 --topic my-topic-75208733-307100740
2022-04-06 19:28:09 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:28:09 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:28:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:28:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoReplaceClusterCaKeysTriggeredByAnno
2022-04-06 19:28:09 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-4a62c54d-kafka-clients in namespace namespace-72
2022-04-06 19:29:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-b50baeb3-kafka has been successfully rolled
2022-04-06 19:29:03 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-b50baeb3-kafka to be ready
2022-04-06 19:29:26 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:515] Wait for EO to rolling restart (2)...
2022-04-06 19:29:26 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b50baeb3-entity-operator rolling update
2022-04-06 19:29:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-4a62c54d-kafka-clients-tls in namespace namespace-72
2022-04-06 19:29:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-236173793-534350904 in namespace namespace-72
2022-04-06 19:29:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1795847282-2023667004 in namespace namespace-72
2022-04-06 19:29:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-75208733-307100740 in namespace namespace-72
2022-04-06 19:29:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4a62c54d in namespace namespace-72
2022-04-06 19:29:49 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-72, for cruise control Kafka cluster my-cluster-4a62c54d
2022-04-06 19:29:56 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-entity-operator will be ready
2022-04-06 19:29:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:29:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-72 for test case:testAutoReplaceClusterCaKeysTriggeredByAnno
2022-04-06 19:30:31 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-entity-operator is ready
2022-04-06 19:30:41 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b50baeb3-entity-operator rolling update finished
2022-04-06 19:30:41 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:520] Wait for KafkaExporter and CruiseControl to rolling restart (2)...
2022-04-06 19:30:41 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b50baeb3-kafka-exporter rolling update
2022-04-06 19:30:43 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceClusterCaKeysTriggeredByAnno-FINISHED
2022-04-06 19:30:43 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:31:36 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-kafka-exporter will be ready
2022-04-06 19:31:36 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-kafka-exporter is ready
2022-04-06 19:31:46 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b50baeb3-kafka-exporter rolling update finished
2022-04-06 19:31:46 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-b50baeb3-cruise-control rolling update
2022-04-06 19:31:46 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-cruise-control will be ready
2022-04-06 19:31:46 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-cruise-control is ready
2022-04-06 19:31:56 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-b50baeb3-cruise-control rolling update finished
2022-04-06 19:31:56 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:525] Checking the certificates have been replaced
2022-04-06 19:31:56 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:536] Checking consumed messages to pod:my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf
2022-04-06 19:31:56 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@75929279, messages=[], arguments=[--group-id, my-consumer-group-1561958965, --max-messages, 100, --group-instance-id, instance441478041, --bootstrap-server, my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092, --topic, my-topic-352443010-816945957], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf', podNamespace='namespace-73', bootstrapServer='my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092', topicName='my-topic-352443010-816945957', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1561958965', consumerInstanceId='instance441478041', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@431fa6db}
2022-04-06 19:31:56 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092#my-topic-352443010-816945957 from pod my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf
2022-04-06 19:31:56 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b50baeb3-kafka-clients-7ff798ff8b-g8xhf -n namespace-73 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1561958965 --max-messages 100 --group-instance-id instance441478041 --bootstrap-server my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092 --topic my-topic-352443010-816945957
2022-04-06 19:32:02 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:32:02 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:32:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-972484506-2050444786 in namespace namespace-73
2022-04-06 19:32:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-06 19:32:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-972484506-2050444786 will have desired state: Ready
2022-04-06 19:32:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-972484506-2050444786 is in desired state: Ready
2022-04-06 19:32:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b50baeb3-kafka-clients-tls in namespace namespace-73
2022-04-06 19:32:03 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-73
2022-04-06 19:32:03 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b50baeb3-kafka-clients-tls will be ready
2022-04-06 19:32:05 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b50baeb3-kafka-clients-tls is ready
2022-04-06 19:32:05 [ForkJoinPool-3-worker-7] [32mINFO [m [SecurityST:561] Checking consumed messages to pod:my-cluster-b50baeb3-kafka-clients-tls-9df6cbbcd-2sk79
2022-04-06 19:32:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@154119f3, messages=[], arguments=[--group-id, my-consumer-group-734597375, --max-messages, 100, --group-instance-id, instance1890012170, --bootstrap-server, my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092, --topic, my-topic-352443010-816945957], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b50baeb3-kafka-clients-tls-9df6cbbcd-2sk79', podNamespace='namespace-73', bootstrapServer='my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092', topicName='my-topic-352443010-816945957', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-734597375', consumerInstanceId='instance1890012170', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2bd64b2c}
2022-04-06 19:32:05 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092#my-topic-352443010-816945957 from pod my-cluster-b50baeb3-kafka-clients-tls-9df6cbbcd-2sk79
2022-04-06 19:32:05 [ForkJoinPool-3-worker-7] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b50baeb3-kafka-clients-tls-9df6cbbcd-2sk79 -n namespace-73 -- /opt/kafka/consumer.sh --group-id my-consumer-group-734597375 --max-messages 100 --group-instance-id instance1890012170 --bootstrap-server my-cluster-b50baeb3-kafka-bootstrap.namespace-73.svc:9092 --topic my-topic-352443010-816945957
2022-04-06 19:32:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-06 19:32:11 [ForkJoinPool-3-worker-7] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-06 19:32:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:32:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testAutoReplaceAllCaKeysTriggeredByAnno
2022-04-06 19:32:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b50baeb3-kafka-clients in namespace namespace-73
2022-04-06 19:32:11 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-2090310259-1267188022 in namespace namespace-73
2022-04-06 19:32:21 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-352443010-816945957 in namespace namespace-73
2022-04-06 19:32:31 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b50baeb3 in namespace namespace-73
2022-04-06 19:32:31 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-73, for cruise control Kafka cluster my-cluster-b50baeb3
2022-04-06 19:32:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b50baeb3-kafka-clients-tls in namespace namespace-73
2022-04-06 19:33:21 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-972484506-2050444786 in namespace namespace-73
2022-04-06 19:33:31 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:33:31 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-73 for test case:testAutoReplaceAllCaKeysTriggeredByAnno
2022-04-06 19:33:38 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.SecurityST.testAutoReplaceAllCaKeysTriggeredByAnno-FINISHED
2022-04-06 19:33:38 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:33:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:33:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context SecurityST is everything deleted.
2022-04-06 19:33:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4,103.378 s - in io.strimzi.systemtest.security.SecurityST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: alternative-reconcile-triggers-st
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: alternative-reconcile-triggers-st
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: alternative-reconcile-triggers-st
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testTriggerRollingUpdateAfterOverrideBootstrap-STARTED
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualRollingUpdateForSinglePod-STARTED
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-74 for test case:testTriggerRollingUpdateAfterOverrideBootstrap
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-74
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-74
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-74
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-75 for test case:testManualRollingUpdateForSinglePod
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-75
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-901ef682 in namespace namespace-74
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-74
2022-04-06 19:33:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-901ef682 will have desired state: Ready
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-75
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-75
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d77fd2f3 in namespace namespace-75
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-75
2022-04-06 19:33:43 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d77fd2f3 will have desired state: Ready
2022-04-06 19:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-901ef682 is in desired state: Ready
2022-04-06 19:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:228] Adding new bootstrap dns: kafka-test.XXXX.azure.XXXX.net to external listeners
2022-04-06 19:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-901ef682-kafka rolling update
2022-04-06 19:36:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d77fd2f3 is in desired state: Ready
2022-04-06 19:36:19 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:290] Trying to roll just single Kafka and single ZK pod
2022-04-06 19:36:19 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d77fd2f3-kafka rolling update
2022-04-06 19:36:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d77fd2f3-kafka has been successfully rolled
2022-04-06 19:36:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-d77fd2f3-kafka to be ready
2022-04-06 19:37:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d77fd2f3 will have desired state: Ready
2022-04-06 19:37:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d77fd2f3 is in desired state: Ready
2022-04-06 19:37:04 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d77fd2f3 is ready
2022-04-06 19:37:04 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d77fd2f3-zookeeper rolling update
2022-04-06 19:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-901ef682-kafka has been successfully rolled
2022-04-06 19:37:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-901ef682-kafka to be ready
2022-04-06 19:37:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d77fd2f3-zookeeper has been successfully rolled
2022-04-06 19:37:34 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-d77fd2f3-zookeeper to be ready
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-901ef682 will have desired state: Ready
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-901ef682 is in desired state: Ready
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-901ef682 is ready
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-901ef682 will have desired state: Ready
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-901ef682 is in desired state: Ready
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:258] Encoding my-cluster-901ef682-kafka-0.crt cert
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:263] Verifying that new DNS is in certificate subject alternative names
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:258] Encoding my-cluster-901ef682-kafka-1.crt cert
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:263] Verifying that new DNS is in certificate subject alternative names
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:258] Encoding my-cluster-901ef682-kafka-2.crt cert
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:263] Verifying that new DNS is in certificate subject alternative names
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTriggerRollingUpdateAfterOverrideBootstrap
2022-04-06 19:37:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-901ef682 in namespace namespace-74
2022-04-06 19:37:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:37:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-74 for test case:testTriggerRollingUpdateAfterOverrideBootstrap
2022-04-06 19:38:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d77fd2f3 will have desired state: Ready
2022-04-06 19:38:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d77fd2f3 is in desired state: Ready
2022-04-06 19:38:05 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d77fd2f3 is ready
2022-04-06 19:38:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:311] Adding anno to all ZK and Kafka pods
2022-04-06 19:38:05 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:320] Checking if the rolling update will be successful for Kafka
2022-04-06 19:38:05 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d77fd2f3-kafka rolling update
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testTriggerRollingUpdateAfterOverrideBootstrap-FINISHED
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testAddingAndRemovingJbodVolumes-STARTED
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-76 for test case:testAddingAndRemovingJbodVolumes
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-76
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-76
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-76
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e97d9715 in namespace namespace-76
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e97d9715 will have desired state: Ready
2022-04-06 19:40:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d77fd2f3-kafka has been successfully rolled
2022-04-06 19:40:16 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-d77fd2f3-kafka to be ready
2022-04-06 19:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d77fd2f3 will have desired state: Ready
2022-04-06 19:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d77fd2f3 is in desired state: Ready
2022-04-06 19:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d77fd2f3 is ready
2022-04-06 19:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [AlternativeReconcileTriggersST:331] Checking if the rolling update will be successful for ZK
2022-04-06 19:40:47 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-d77fd2f3-zookeeper rolling update
2022-04-06 19:41:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e97d9715 is in desired state: Ready
2022-04-06 19:41:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-977136182-1084843984 in namespace namespace-76
2022-04-06 19:41:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:41:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-977136182-1084843984 will have desired state: Ready
2022-04-06 19:41:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-977136182-1084843984 is in desired state: Ready
2022-04-06 19:41:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic continuous-topic in namespace namespace-76
2022-04-06 19:41:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:41:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: continuous-topic will have desired state: Ready
2022-04-06 19:41:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: continuous-topic is in desired state: Ready
2022-04-06 19:41:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 19:41:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace namespace-76
2022-04-06 19:41:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:41:04 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-06 19:41:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-consumer in namespace namespace-76
2022-04-06 19:41:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:41:05 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-consumer will be in active state
2022-04-06 19:41:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-505040425-1145452377 in namespace namespace-76
2022-04-06 19:41:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:41:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-505040425-1145452377 will have desired state: Ready
2022-04-06 19:41:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-505040425-1145452377 is in desired state: Ready
2022-04-06 19:41:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e97d9715-kafka-clients in namespace namespace-76
2022-04-06 19:41:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-76
2022-04-06 19:41:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 19:41:16 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1a2707ad, messages=[], arguments=[USER=my_user_505040425_1145452377, --max-messages, 100, --bootstrap-server, my-cluster-e97d9715-kafka-bootstrap.namespace-76.svc:9093, --topic, my-topic-977136182-1084843984], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e97d9715-kafka-clients-dbc457db9-mtktd', podNamespace='namespace-76', bootstrapServer='my-cluster-e97d9715-kafka-bootstrap.namespace-76.svc:9093', topicName='my-topic-977136182-1084843984', maxMessages=100, kafkaUsername='my-user-505040425-1145452377', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2b9cf85e}
2022-04-06 19:41:16 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-e97d9715-kafka-bootstrap.namespace-76.svc:9093:my-topic-977136182-1084843984 from pod my-cluster-e97d9715-kafka-clients-dbc457db9-mtktd
2022-04-06 19:41:16 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e97d9715-kafka-clients-dbc457db9-mtktd -n namespace-76 -- /opt/kafka/producer.sh USER=my_user_505040425_1145452377 --max-messages 100 --bootstrap-server my-cluster-e97d9715-kafka-bootstrap.namespace-76.svc:9093 --topic my-topic-977136182-1084843984
2022-04-06 19:41:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 19:41:20 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 19:41:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:408] Add JBOD volume to the Kafka cluster my-cluster-e97d9715-kafka
2022-04-06 19:41:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e97d9715-kafka rolling update
2022-04-06 19:42:07 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-d77fd2f3-zookeeper has been successfully rolled
2022-04-06 19:42:07 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-d77fd2f3-zookeeper to be ready
2022-04-06 19:42:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d77fd2f3 will have desired state: Ready
2022-04-06 19:42:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d77fd2f3 is in desired state: Ready
2022-04-06 19:42:35 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-d77fd2f3 is ready
2022-04-06 19:42:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:42:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testManualRollingUpdateForSinglePod
2022-04-06 19:42:35 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d77fd2f3 in namespace namespace-75
2022-04-06 19:42:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e97d9715-kafka has been successfully rolled
2022-04-06 19:42:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-e97d9715-kafka to be ready
2022-04-06 19:42:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:42:45 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-75 for test case:testManualRollingUpdateForSinglePod
2022-04-06 19:43:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e97d9715 will have desired state: Ready
2022-04-06 19:43:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e97d9715 is in desired state: Ready
2022-04-06 19:43:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-e97d9715 is ready
2022-04-06 19:43:08 [ForkJoinPool-3-worker-3] [32mINFO [m [AlternativeReconcileTriggersST:419] Remove JBOD volume to the Kafka cluster my-cluster-e97d9715-kafka
2022-04-06 19:43:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-e97d9715-kafka rolling update
2022-04-06 19:43:10 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:43:10 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualTriggeringRollingUpdate-STARTED
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-77 for test case:testManualTriggeringRollingUpdate
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-77
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-77
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-77
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-eb9e66b6 in namespace namespace-77
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:43:15 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-eb9e66b6 will have desired state: Ready
2022-04-06 19:43:24 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualRollingUpdateForSinglePod-FINISHED
2022-04-06 19:43:24 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:44:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-eb9e66b6 is in desired state: Ready
2022-04-06 19:44:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-354965901-1404862259 in namespace namespace-77
2022-04-06 19:44:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:44:23 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-354965901-1404862259 will have desired state: Ready
2022-04-06 19:44:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-354965901-1404862259 is in desired state: Ready
2022-04-06 19:44:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic continuous-topic in namespace namespace-77
2022-04-06 19:44:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:44:24 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: continuous-topic will have desired state: Ready
2022-04-06 19:44:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: continuous-topic is in desired state: Ready
2022-04-06 19:44:25 [ForkJoinPool-3-worker-5] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 19:44:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace namespace-77
2022-04-06 19:44:25 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:44:25 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-06 19:44:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-consumer in namespace namespace-77
2022-04-06 19:44:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:44:26 [ForkJoinPool-3-worker-5] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-consumer will be in active state
2022-04-06 19:44:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-39197976-358470690 in namespace namespace-77
2022-04-06 19:44:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:44:27 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-39197976-358470690 will have desired state: Ready
2022-04-06 19:44:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-39197976-358470690 is in desired state: Ready
2022-04-06 19:44:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-eb9e66b6-kafka-clients in namespace namespace-77
2022-04-06 19:44:29 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:44:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-e97d9715-kafka has been successfully rolled
2022-04-06 19:44:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-e97d9715-kafka to be ready
2022-04-06 19:44:39 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 19:44:39 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@31e3aadd, messages=[], arguments=[USER=my_user_39197976_358470690, --max-messages, 100, --bootstrap-server, my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-354965901-1404862259], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc', podNamespace='namespace-77', bootstrapServer='my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-354965901-1404862259', maxMessages=100, kafkaUsername='my-user-39197976-358470690', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@590fe158}
2022-04-06 19:44:39 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093:my-topic-354965901-1404862259 from pod my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc
2022-04-06 19:44:39 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc -n namespace-77 -- /opt/kafka/producer.sh USER=my_user_39197976_358470690 --max-messages 100 --bootstrap-server my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-354965901-1404862259
2022-04-06 19:44:42 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 19:44:42 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 19:44:42 [ForkJoinPool-3-worker-5] [32mINFO [m [AlternativeReconcileTriggersST:145] Annotate Kafka StatefulSet my-cluster-eb9e66b6-kafka with manual rolling update annotation
2022-04-06 19:44:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-eb9e66b6-kafka rolling update
2022-04-06 19:44:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e97d9715 will have desired state: Ready
2022-04-06 19:44:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e97d9715 is in desired state: Ready
2022-04-06 19:44:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-e97d9715 is ready
2022-04-06 19:44:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer hello-world-producer and consumer hello-world-consumer finish
2022-04-06 19:46:12 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-eb9e66b6-kafka has been successfully rolled
2022-04-06 19:46:12 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-eb9e66b6-kafka to be ready
2022-04-06 19:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-eb9e66b6 will have desired state: Ready
2022-04-06 19:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-eb9e66b6 is in desired state: Ready
2022-04-06 19:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-eb9e66b6 is ready
2022-04-06 19:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@608e38d3, messages=[], arguments=[--group-id, my-consumer-group-1632223492, USER=my_user_39197976_358470690, --max-messages, 100, --group-instance-id, instance136156363, --bootstrap-server, my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-354965901-1404862259], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc', podNamespace='namespace-77', bootstrapServer='my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-354965901-1404862259', maxMessages=100, kafkaUsername='my-user-39197976-358470690', consumerGroupName='my-consumer-group-1632223492', consumerInstanceId='instance136156363', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7ba39806}
2022-04-06 19:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093:my-topic-354965901-1404862259 from pod my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc
2022-04-06 19:46:41 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc -n namespace-77 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1632223492 USER=my_user_39197976_358470690 --max-messages 100 --group-instance-id instance136156363 --bootstrap-server my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-354965901-1404862259
2022-04-06 19:46:47 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 19:46:47 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 19:46:47 [ForkJoinPool-3-worker-5] [32mINFO [m [AlternativeReconcileTriggersST:166] Annotate Zookeeper StatefulSet my-cluster-eb9e66b6-zookeeper with manual rolling update annotation
2022-04-06 19:46:48 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-eb9e66b6-zookeeper rolling update
2022-04-06 19:48:08 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-eb9e66b6-zookeeper has been successfully rolled
2022-04-06 19:48:08 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-eb9e66b6-zookeeper to be ready
2022-04-06 19:48:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-eb9e66b6 will have desired state: Ready
2022-04-06 19:48:32 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-eb9e66b6 is in desired state: Ready
2022-04-06 19:48:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-eb9e66b6 is ready
2022-04-06 19:48:32 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1f857de0, messages=[], arguments=[--group-id, my-consumer-group-846518817, USER=my_user_39197976_358470690, --max-messages, 100, --group-instance-id, instance1031898254, --bootstrap-server, my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-354965901-1404862259], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc', podNamespace='namespace-77', bootstrapServer='my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-354965901-1404862259', maxMessages=100, kafkaUsername='my-user-39197976-358470690', consumerGroupName='my-consumer-group-846518817', consumerInstanceId='instance1031898254', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6d20a9f}
2022-04-06 19:48:32 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093:my-topic-354965901-1404862259 from pod my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc
2022-04-06 19:48:32 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc -n namespace-77 -- /opt/kafka/consumer.sh --group-id my-consumer-group-846518817 USER=my_user_39197976_358470690 --max-messages 100 --group-instance-id instance1031898254 --bootstrap-server my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-354965901-1404862259
2022-04-06 19:48:39 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 19:48:39 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 19:48:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-25313976-569730033 in namespace namespace-77
2022-04-06 19:48:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-77
2022-04-06 19:48:39 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-25313976-569730033 will have desired state: Ready
2022-04-06 19:48:41 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-25313976-569730033 is in desired state: Ready
2022-04-06 19:48:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@66009dab, messages=[], arguments=[USER=my_user_39197976_358470690, --max-messages, 100, --bootstrap-server, my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-25313976-569730033], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc', podNamespace='namespace-77', bootstrapServer='my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-25313976-569730033', maxMessages=100, kafkaUsername='my-user-39197976-358470690', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7090e7d4}
2022-04-06 19:48:41 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093:my-topic-25313976-569730033 from pod my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc
2022-04-06 19:48:41 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc -n namespace-77 -- /opt/kafka/producer.sh USER=my_user_39197976_358470690 --max-messages 100 --bootstrap-server my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-25313976-569730033
2022-04-06 19:48:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 19:48:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 19:48:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@20ed5810, messages=[], arguments=[--group-id, my-consumer-group-1292429125, USER=my_user_39197976_358470690, --max-messages, 100, --group-instance-id, instance1375803531, --bootstrap-server, my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093, --topic, my-topic-25313976-569730033], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc', podNamespace='namespace-77', bootstrapServer='my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093', topicName='my-topic-25313976-569730033', maxMessages=100, kafkaUsername='my-user-39197976-358470690', consumerGroupName='my-consumer-group-1292429125', consumerInstanceId='instance1375803531', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@525e86f6}
2022-04-06 19:48:45 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093:my-topic-25313976-569730033 from pod my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc
2022-04-06 19:48:45 [ForkJoinPool-3-worker-5] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-eb9e66b6-kafka-clients-7574fdc769-qqrdc -n namespace-77 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1292429125 USER=my_user_39197976_358470690 --max-messages 100 --group-instance-id instance1375803531 --bootstrap-server my-cluster-eb9e66b6-kafka-bootstrap.namespace-77.svc:9093 --topic my-topic-25313976-569730033
2022-04-06 19:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 19:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 19:48:52 [ForkJoinPool-3-worker-5] [32mINFO [m [ClientUtils:61] Waiting till producer hello-world-producer and consumer hello-world-consumer finish
2022-04-06 19:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testAddingAndRemovingJbodVolumes
2022-04-06 19:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-consumer in namespace namespace-76
2022-04-06 19:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace namespace-76
2022-04-06 19:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e97d9715-kafka-clients in namespace namespace-76
2022-04-06 19:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-505040425-1145452377 in namespace namespace-76
2022-04-06 19:50:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-977136182-1084843984 in namespace namespace-76
2022-04-06 19:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic continuous-topic in namespace namespace-76
2022-04-06 19:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e97d9715 in namespace namespace-76
2022-04-06 19:51:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:51:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-76 for test case:testAddingAndRemovingJbodVolumes
2022-04-06 19:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testAddingAndRemovingJbodVolumes-FINISHED
2022-04-06 19:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:53:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:53:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testManualTriggeringRollingUpdate
2022-04-06 19:53:05 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-39197976-358470690 in namespace namespace-77
2022-04-06 19:53:05 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic continuous-topic in namespace namespace-77
2022-04-06 19:53:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-consumer in namespace namespace-77
2022-04-06 19:53:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-25313976-569730033 in namespace namespace-77
2022-04-06 19:53:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace namespace-77
2022-04-06 19:53:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-354965901-1404862259 in namespace namespace-77
2022-04-06 19:53:26 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-eb9e66b6-kafka-clients in namespace namespace-77
2022-04-06 19:53:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-eb9e66b6 in namespace namespace-77
2022-04-06 19:54:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 19:54:06 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-77 for test case:testManualTriggeringRollingUpdate
2022-04-06 19:54:12 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST.testManualTriggeringRollingUpdate-FINISHED
2022-04-06 19:54:12 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 19:54:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 19:54:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context AlternativeReconcileTriggersST is everything deleted.
2022-04-06 19:54:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,234.738 s - in io.strimzi.systemtest.rollingupdate.AlternativeReconcileTriggersST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.RollingUpdateST
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: rolling-update-st
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: rolling-update-st
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: rolling-update-st
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testMetricsChange-STARTED
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4c4a1961 in namespace rolling-update-st
2022-04-06 19:54:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4c4a1961 will have desired state: Ready
2022-04-06 19:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4c4a1961 is in desired state: Ready
2022-04-06 19:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-4c4a1961-kafka-clients in namespace rolling-update-st
2022-04-06 19:56:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:755] Check if metrics are present in pod of Kafka and Zookeeper
2022-04-06 19:56:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:56:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:56:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:56:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.7 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:56:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:56:19 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.8 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:56:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:765] Changing metrics to something else
2022-04-06 19:56:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-4c4a1961-zookeeper are stable
2022-04-06 19:56:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:56:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:56:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:56:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:56:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:56:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:56:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:56:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:56:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:56:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:56:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:56:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:56:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:56:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:56:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:56:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:56:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:56:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:56:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:56:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:56:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:56:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:56:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:56:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:56:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:56:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:56:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:56:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:56:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:56:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:56:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:56:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:56:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:56:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:56:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:56:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:56:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:56:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:56:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:56:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:56:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:56:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:56:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:56:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:56:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:56:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:56:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:56:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:56:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:56:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:56:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:56:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:56:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:56:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:56:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:56:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:56:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:56:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:56:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:56:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:56:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:56:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:56:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:56:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:56:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:56:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:56:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:56:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:56:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:56:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:56:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:56:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:56:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:56:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:56:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:56:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:56:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:56:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:56:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:56:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:56:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:56:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:56:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:56:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:56:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:56:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:56:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:56:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:57:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-4c4a1961-zookeeper-0 ,my-cluster-4c4a1961-zookeeper-1 ,my-cluster-4c4a1961-zookeeper-2
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-4c4a1961-kafka are stable
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 19:57:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:57:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:57:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:57:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:57:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 19:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:57:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 19:57:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:57:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:57:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:57:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:57:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 19:57:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:57:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:57:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:57:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:57:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 19:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:57:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 19:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 19:57:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:57:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:57:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:57:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:57:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 19:57:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:57:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:57:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:57:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:57:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 19:57:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:57:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:57:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:57:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:57:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 19:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:57:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 19:57:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:57:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:57:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:57:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:57:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 19:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:57:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 19:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 19:57:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:57:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:57:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:57:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:57:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 19:57:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:57:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:57:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:57:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:57:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 19:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 19:57:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:57:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:57:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:57:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:57:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 19:57:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:57:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:57:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:57:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:57:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 19:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:57:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 19:57:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:57:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:57:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:57:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:57:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 19:57:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:57:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:57:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:57:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:57:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 19:57:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:57:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:57:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:57:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:57:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 19:57:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:57:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:57:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:57:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:57:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 19:57:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:57:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:57:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:57:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:57:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 19:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 19:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 19:57:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:57:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:57:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:57:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:57:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 19:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:57:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 19:57:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:57:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:57:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:57:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:57:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 19:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:57:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 19:57:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:57:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:57:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:57:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:57:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 19:57:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:57:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:57:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:57:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:57:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 19:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:57:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 19:57:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:57:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:57:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:57:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:57:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 19:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 19:57:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:57:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:57:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:57:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:57:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 19:57:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:57:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:57:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:57:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:57:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 19:57:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:57:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:57:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:57:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:57:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 19:57:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:57:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:57:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:57:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:57:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 19:57:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:57:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:57:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:57:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:57:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 19:57:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 19:57:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 19:57:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 19:57:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 19:57:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 19:57:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 19:57:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 19:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-4c4a1961-kafka-0 ,my-cluster-4c4a1961-kafka-1 ,my-cluster-4c4a1961-kafka-2 ,my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j ,my-cluster-4c4a1961-kafka-exporter-677cdb744d-8rbb4
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:800] Check if Kafka and Zookeeper pods didn't roll
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:804] Check if Kafka and Zookeeper metrics are changed
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:818] Check if metrics are present in pod of Kafka and Zookeeper
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:57:58 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:57:59 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:57:59 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.7 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:57:59 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:58:00 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.8 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 0
2022-04-06 19:58:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:829] Removing metrics from Kafka and Zookeeper and setting them to null
2022-04-06 19:58:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:836] Wait if Kafka and Zookeeper pods will roll
2022-04-06 19:58:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4c4a1961-zookeeper rolling update
2022-04-06 19:59:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4c4a1961-zookeeper has been successfully rolled
2022-04-06 19:59:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-4c4a1961-zookeeper to be ready
2022-04-06 19:59:40 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-4c4a1961-kafka rolling update
2022-04-06 20:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-4c4a1961-kafka has been successfully rolled
2022-04-06 20:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-4c4a1961-kafka to be ready
2022-04-06 20:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4c4a1961 will have desired state: Ready
2022-04-06 20:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4c4a1961 is in desired state: Ready
2022-04-06 20:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-4c4a1961 is ready
2022-04-06 20:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:840] Check if metrics are not existing in pods
2022-04-06 20:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 7
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 7
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 7
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.7 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 7
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.9 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 7
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.8 from Pod my-cluster-4c4a1961-kafka-clients-58d767fb87-xvm2j finished with return code: 7
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMetricsChange
2022-04-06 20:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-4c4a1961-kafka-clients in namespace rolling-update-st
2022-04-06 20:01:18 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4c4a1961 in namespace rolling-update-st
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testMetricsChange-FINISHED
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testClusterOperatorFinishAllRollingUpdates-STARTED
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5618fae9 in namespace rolling-update-st
2022-04-06 20:02:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5618fae9 will have desired state: Ready
2022-04-06 20:03:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5618fae9 is in desired state: Ready
2022-04-06 20:04:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:630] Deleting Cluster Operator pod with labels LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})
2022-04-06 20:04:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:632] Cluster Operator pod deleted
2022-04-06 20:04:03 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-5618fae9-zookeeper rolling update
2022-04-06 20:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-5618fae9-zookeeper has been successfully rolled
2022-04-06 20:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-5618fae9-zookeeper to be ready
2022-04-06 20:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5618fae9 will have desired state: Ready
2022-04-06 20:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5618fae9 is in desired state: Ready
2022-04-06 20:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-5618fae9 is ready
2022-04-06 20:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:639] Deleting Cluster Operator pod with labels LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})
2022-04-06 20:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateST:641] Cluster Operator pod deleted
2022-04-06 20:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-5618fae9-kafka rolling update
2022-04-06 20:06:39 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-5618fae9-kafka has been successfully rolled
2022-04-06 20:06:39 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-5618fae9-kafka to be ready
2022-04-06 20:07:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5618fae9 will have desired state: Ready
2022-04-06 20:07:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5618fae9 is in desired state: Ready
2022-04-06 20:07:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-5618fae9 is ready
2022-04-06 20:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClusterOperatorFinishAllRollingUpdates
2022-04-06 20:07:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5618fae9 in namespace rolling-update-st
2022-04-06 20:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testClusterOperatorFinishAllRollingUpdates-FINISHED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testBrokerConfigurationChangeTriggerRollingUpdate-STARTED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testManualKafkaConfigMapChangeDontTriggerRollingUpdate-STARTED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testZookeeperScaleUpScaleDown-STARTED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testExternalLoggingChangeTriggerRollingUpdate-STARTED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringZookeeperRollingUpdate-STARTED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-STARTED
2022-04-06 20:07:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:07:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringKafkaRollingUpdate-STARTED
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-78 for test case:testZookeeperScaleUpScaleDown
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-78
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-78
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-78
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-959cd718 in namespace namespace-78
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-06 20:07:21 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-959cd718 will have desired state: Ready
2022-04-06 20:08:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-959cd718 is in desired state: Ready
2022-04-06 20:08:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2106654496-927593374 in namespace namespace-78
2022-04-06 20:08:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-06 20:08:35 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2106654496-927593374 will have desired state: Ready
2022-04-06 20:08:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2106654496-927593374 is in desired state: Ready
2022-04-06 20:08:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-875170435-1630047338 in namespace namespace-78
2022-04-06 20:08:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-06 20:08:36 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-875170435-1630047338 will have desired state: Ready
2022-04-06 20:08:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-875170435-1630047338 is in desired state: Ready
2022-04-06 20:08:37 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateST:398] Running zookeeperScaleUpScaleDown with cluster my-cluster-959cd718
2022-04-06 20:08:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-959cd718-kafka-clients in namespace namespace-78
2022-04-06 20:08:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-06 20:08:47 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 20:08:47 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@243aacc9, messages=[], arguments=[USER=my_user_875170435_1630047338, --max-messages, 100, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-2106654496-927593374], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-2106654496-927593374', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@35f456f9}
2022-04-06 20:08:47 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-2106654496-927593374 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:08:47 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/producer.sh USER=my_user_875170435_1630047338 --max-messages 100 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-2106654496-927593374
2022-04-06 20:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateST:426] Scale up Zookeeper to 7
2022-04-06 20:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@65e9d719, messages=[], arguments=[--group-id, my-consumer-group-684421491, USER=my_user_875170435_1630047338, --max-messages, 100, --group-instance-id, instance170691463, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-2106654496-927593374], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-2106654496-927593374', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='my-consumer-group-684421491', consumerInstanceId='instance170691463', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@354f85cd}
2022-04-06 20:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-2106654496-927593374 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:08:51 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/consumer.sh --group-id my-consumer-group-684421491 USER=my_user_875170435_1630047338 --max-messages 100 --group-instance-id instance170691463 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-2106654496-927593374
2022-04-06 20:08:58 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:08:58 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:08:58 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-959cd718-zookeeper to be ready
2022-04-06 20:11:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-959cd718 will have desired state: Ready
2022-04-06 20:11:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-959cd718 is in desired state: Ready
2022-04-06 20:11:43 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-959cd718 is ready
2022-04-06 20:11:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-0 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-1 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-2 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-3 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-4 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-5 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-6 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@197e53ad, messages=[], arguments=[--group-id, my-consumer-group-1867967840, USER=my_user_875170435_1630047338, --max-messages, 100, --group-instance-id, instance2065506447, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-2106654496-927593374], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-2106654496-927593374', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='my-consumer-group-1867967840', consumerInstanceId='instance2065506447', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@26e9494b}
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-2106654496-927593374 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:11:45 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1867967840 USER=my_user_875170435_1630047338 --max-messages 100 --group-instance-id instance2065506447 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-2106654496-927593374
2022-04-06 20:11:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:11:52 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:11:52 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1818129192-727832274 in namespace namespace-78
2022-04-06 20:11:52 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-06 20:11:52 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1818129192-727832274 will have desired state: Ready
2022-04-06 20:11:53 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1818129192-727832274 is in desired state: Ready
2022-04-06 20:11:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@603df778, messages=[], arguments=[USER=my_user_875170435_1630047338, --max-messages, 100, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-1818129192-727832274], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-1818129192-727832274', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2bd43569}
2022-04-06 20:11:53 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-1818129192-727832274 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:11:53 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/producer.sh USER=my_user_875170435_1630047338 --max-messages 100 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-1818129192-727832274
2022-04-06 20:11:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:11:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:11:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@677a803d, messages=[], arguments=[--group-id, my-consumer-group-50127259, USER=my_user_875170435_1630047338, --max-messages, 100, --group-instance-id, instance1644973345, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-1818129192-727832274], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-1818129192-727832274', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='my-consumer-group-50127259', consumerInstanceId='instance1644973345', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@c724925}
2022-04-06 20:11:57 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-1818129192-727832274 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:11:57 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/consumer.sh --group-id my-consumer-group-50127259 USER=my_user_875170435_1630047338 --max-messages 100 --group-instance-id instance1644973345 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-1818129192-727832274
2022-04-06 20:12:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:12:04 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:12:04 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateST:453] Scale down Zookeeper to 3
2022-04-06 20:12:04 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-959cd718-zookeeper to be ready
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-959cd718 will have desired state: Ready
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-959cd718 is in desired state: Ready
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-959cd718 is ready
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-0 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-1 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:12:43 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:12:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-78 exec my-cluster-959cd718-zookeeper-2 -- /bin/bash -c echo mntr | nc localhost 12181
2022-04-06 20:12:44 [ForkJoinPool-3-worker-9] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:12:44 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7ca3742c, messages=[], arguments=[--group-id, my-consumer-group-570801294, USER=my_user_875170435_1630047338, --max-messages, 100, --group-instance-id, instance764298940, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-1818129192-727832274], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-1818129192-727832274', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='my-consumer-group-570801294', consumerInstanceId='instance764298940', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@62862238}
2022-04-06 20:12:44 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-1818129192-727832274 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:12:44 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/consumer.sh --group-id my-consumer-group-570801294 USER=my_user_875170435_1630047338 --max-messages 100 --group-instance-id instance764298940 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-1818129192-727832274
2022-04-06 20:12:50 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:12:50 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:12:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1241563229-1465385597 in namespace namespace-78
2022-04-06 20:12:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-78
2022-04-06 20:12:50 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1241563229-1465385597 will have desired state: Ready
2022-04-06 20:12:51 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1241563229-1465385597 is in desired state: Ready
2022-04-06 20:12:51 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5e033678, messages=[], arguments=[USER=my_user_875170435_1630047338, --max-messages, 100, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-1241563229-1465385597], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-1241563229-1465385597', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4f6b68ed}
2022-04-06 20:12:51 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-1241563229-1465385597 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:12:51 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/producer.sh USER=my_user_875170435_1630047338 --max-messages 100 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-1241563229-1465385597
2022-04-06 20:12:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:12:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:12:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6aa25fe2, messages=[], arguments=[--group-id, my-consumer-group-1069580697, USER=my_user_875170435_1630047338, --max-messages, 100, --group-instance-id, instance376529059, --bootstrap-server, my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093, --topic, my-topic-1241563229-1465385597], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f', podNamespace='namespace-78', bootstrapServer='my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093', topicName='my-topic-1241563229-1465385597', maxMessages=100, kafkaUsername='my-user-875170435-1630047338', consumerGroupName='my-consumer-group-1069580697', consumerInstanceId='instance376529059', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@dc74862}
2022-04-06 20:12:55 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093:my-topic-1241563229-1465385597 from pod my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f
2022-04-06 20:12:55 [ForkJoinPool-3-worker-9] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-959cd718-kafka-clients-885f7dbd9-w9l2f -n namespace-78 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1069580697 USER=my_user_875170435_1630047338 --max-messages 100 --group-instance-id instance376529059 --bootstrap-server my-cluster-959cd718-kafka-bootstrap.namespace-78.svc:9093 --topic my-topic-1241563229-1465385597
2022-04-06 20:13:02 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:13:02 [ForkJoinPool-3-worker-9] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:13:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:13:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testZookeeperScaleUpScaleDown
2022-04-06 20:13:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-959cd718-kafka-clients in namespace namespace-78
2022-04-06 20:13:52 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1241563229-1465385597 in namespace namespace-78
2022-04-06 20:14:02 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1818129192-727832274 in namespace namespace-78
2022-04-06 20:14:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2106654496-927593374 in namespace namespace-78
2022-04-06 20:14:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-875170435-1630047338 in namespace namespace-78
2022-04-06 20:14:12 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-959cd718 in namespace namespace-78
2022-04-06 20:14:22 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:14:22 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-78 for test case:testZookeeperScaleUpScaleDown
2022-04-06 20:15:05 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testZookeeperScaleUpScaleDown-FINISHED
2022-04-06 20:15:05 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-79 for test case:testRecoveryDuringKafkaRollingUpdate
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-79
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-79
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-79
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-80 for test case:testManualKafkaConfigMapChangeDontTriggerRollingUpdate
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-80
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8d4e764c in namespace namespace-79
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-79
2022-04-06 20:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8d4e764c will have desired state: Ready
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-80
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-80
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b352e4fe in namespace namespace-80
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-80
2022-04-06 20:15:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b352e4fe will have desired state: Ready
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b352e4fe is in desired state: Ready
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-b352e4fe are stable
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8d4e764c is in desired state: Ready
2022-04-06 20:17:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1844340034-1502759606 in namespace namespace-80
2022-04-06 20:17:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-79
2022-04-06 20:17:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1844340034-1502759606 will have desired state: Ready
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1844340034-1502759606 is in desired state: Ready
2022-04-06 20:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-254812215-489564274 in namespace namespace-80
2022-04-06 20:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-79
2022-04-06 20:17:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-254812215-489564274 will have desired state: Ready
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:17:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-254812215-489564274 is in desired state: Ready
2022-04-06 20:17:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-8d4e764c-kafka-clients in namespace namespace-80
2022-04-06 20:17:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-79
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:17:23 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 20:17:23 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@21152b13, messages=[], arguments=[USER=my_user_254812215_489564274, --max-messages, 100, --bootstrap-server, my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093, --topic, my-topic-1844340034-1502759606], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v', podNamespace='namespace-79', bootstrapServer='my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093', topicName='my-topic-1844340034-1502759606', maxMessages=100, kafkaUsername='my-user-254812215-489564274', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7c234f88}
2022-04-06 20:17:23 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093:my-topic-1844340034-1502759606 from pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:17:23 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v -n namespace-79 -- /opt/kafka/producer.sh USER=my_user_254812215_489564274 --max-messages 100 --bootstrap-server my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093 --topic my-topic-1844340034-1502759606
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:17:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:17:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:17:27 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateST:203] Update resources for pods
2022-04-06 20:17:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5648a9b1, messages=[], arguments=[--group-id, my-consumer-group-61816765, USER=my_user_254812215_489564274, --max-messages, 100, --group-instance-id, instance908847764, --bootstrap-server, my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093, --topic, my-topic-1844340034-1502759606], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v', podNamespace='namespace-79', bootstrapServer='my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093', topicName='my-topic-1844340034-1502759606', maxMessages=100, kafkaUsername='my-user-254812215-489564274', consumerGroupName='my-consumer-group-61816765', consumerInstanceId='instance908847764', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5e8d5cc0}
2022-04-06 20:17:27 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093:my-topic-1844340034-1502759606 from pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:17:27 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v -n namespace-79 -- /opt/kafka/consumer.sh --group-id my-consumer-group-61816765 USER=my_user_254812215_489564274 --max-messages 100 --group-instance-id instance908847764 --bootstrap-server my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093 --topic my-topic-1844340034-1502759606
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-06 20:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: my-cluster-8d4e764c-kafka will be in pending phase
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:38 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:39 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:40 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:41 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:42 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:17:43 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateST:220] Verifying stability of kafka pods except the one, which is in pending phase
2022-04-06 20:17:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-8d4e764c-kafka are stable
2022-04-06 20:17:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:17:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:17:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:17:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:17:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:17:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:17:45 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:17:45 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:17:45 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-b352e4fe-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-b352e4fe-entity-operator-6bd4bd889b-kjs65 ,my-cluster-b352e4fe-kafka-0 ,my-cluster-b352e4fe-kafka-1 ,my-cluster-b352e4fe-kafka-2 ,my-cluster-b352e4fe-zookeeper-0 ,my-cluster-b352e4fe-zookeeper-1 ,my-cluster-b352e4fe-zookeeper-2
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testManualKafkaConfigMapChangeDontTriggerRollingUpdate
2022-04-06 20:17:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b352e4fe in namespace namespace-80
2022-04-06 20:17:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:17:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:17:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:17:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:17:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:17:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:17:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:17:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:17:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:17:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:17:56 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-80 for test case:testManualKafkaConfigMapChangeDontTriggerRollingUpdate
2022-04-06 20:17:57 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:57 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:57 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:17:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:17:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:17:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:18:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:18:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:18:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:18:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:18:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:18:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:18:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:18:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:18:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:18:02 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testManualKafkaConfigMapChangeDontTriggerRollingUpdate-FINISHED
2022-04-06 20:18:02 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:18:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:18:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:18:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:18:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:18:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:18:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:18:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:18:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:18:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-81 for test case:testBrokerConfigurationChangeTriggerRollingUpdate
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-81
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-81
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-81
2022-04-06 20:18:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:18:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:18:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-82 for test case:testExternalLoggingChangeTriggerRollingUpdate
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-82
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ca9db028 in namespace namespace-81
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-81
2022-04-06 20:18:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ca9db028 will have desired state: Ready
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-82
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-82
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2aabacad in namespace namespace-82
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-82
2022-04-06 20:18:06 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2aabacad will have desired state: Ready
2022-04-06 20:18:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:18:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:18:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:18:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:18:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:18:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:18:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:18:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:18:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:18:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:18:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:18:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:18:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:18:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:18:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:18:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:18:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:18:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:18:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:18:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:18:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:18:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:18:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:18:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:18:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:18:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:18:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:18:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:18:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:18:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:18:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:18:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:18:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:18:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:18:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:18:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:18:20 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:18:20 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:18:20 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:18:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:18:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:18:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:18:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:18:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:18:22 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:18:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:18:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:18:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:18:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:18:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:18:24 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:18:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:18:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:18:25 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:18:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:18:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:18:26 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:18:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:18:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:18:27 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:18:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:18:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:18:28 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:18:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:18:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:18:29 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:18:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:18:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:18:30 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:18:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:18:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:18:31 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:18:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:18:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:18:32 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-8d4e764c-kafka-0 ,my-cluster-8d4e764c-kafka-2 ,my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateST:223] Verifying stability of zookeeper pods
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-8d4e764c-zookeeper are stable
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:18:33 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:18:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:18:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:18:34 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:18:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:18:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:18:35 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:18:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:18:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:18:36 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:18:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:18:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:18:37 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:18:38 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:18:38 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:18:38 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:18:39 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:18:39 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:18:39 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:18:40 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:18:40 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:18:40 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:18:41 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:18:41 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:18:41 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:18:42 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:18:42 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:18:42 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:18:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:18:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:18:43 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:18:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:18:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:18:44 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:18:45 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:18:46 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:18:47 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:18:48 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:18:49 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:18:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:18:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:18:50 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:18:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:18:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:18:54 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:18:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:18:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:18:55 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:18:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:18:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:18:56 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:18:57 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:18:57 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:18:57 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:18:58 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:18:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:18:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:18:59 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:19:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:19:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:19:00 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:19:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:19:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:19:01 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:19:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:19:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:19:02 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:19:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:19:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:19:03 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:19:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:19:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:19:04 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:19:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:19:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:19:05 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:19:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:19:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:19:06 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:19:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:19:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:19:07 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:19:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:19:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:19:08 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:19:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:19:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:19:09 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:19:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:19:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:19:10 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:19:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:19:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:19:11 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:19:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:19:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:19:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:19:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:19:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:19:13 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:19:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:19:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:19:14 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:19:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:19:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:19:15 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:19:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:19:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:19:17 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:19:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:19:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:19:18 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:19:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:19:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:19:19 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:19:20 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:19:20 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:19:20 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:19:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:19:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:19:21 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:322] Pod my-cluster-8d4e764c-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-8d4e764c-zookeeper-0 ,my-cluster-8d4e764c-zookeeper-1 ,my-cluster-8d4e764c-zookeeper-2
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@45205fc1, messages=[], arguments=[--group-id, my-consumer-group-114729870, USER=my_user_254812215_489564274, --max-messages, 100, --group-instance-id, instance794788893, --bootstrap-server, my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093, --topic, my-topic-1844340034-1502759606], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v', podNamespace='namespace-79', bootstrapServer='my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093', topicName='my-topic-1844340034-1502759606', maxMessages=100, kafkaUsername='my-user-254812215-489564274', consumerGroupName='my-consumer-group-114729870', consumerInstanceId='instance794788893', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@56b67187}
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093:my-topic-1844340034-1502759606 from pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v -n namespace-79 -- /opt/kafka/consumer.sh --group-id my-consumer-group-114729870 USER=my_user_254812215_489564274 --max-messages 100 --group-instance-id instance794788893 --bootstrap-server my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093 --topic my-topic-1844340034-1502759606
2022-04-06 20:19:30 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:19:30 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:19:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-06 20:19:30 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-8d4e764c-kafka to be ready
2022-04-06 20:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ca9db028 is in desired state: Ready
2022-04-06 20:20:01 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-ca9db028-kafka rolling update
2022-04-06 20:20:22 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2aabacad is in desired state: Ready
2022-04-06 20:20:22 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2aabacad-zookeeper rolling update
2022-04-06 20:21:26 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-ca9db028-kafka has been successfully rolled
2022-04-06 20:21:26 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-ca9db028-kafka to be ready
2022-04-06 20:21:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2aabacad-zookeeper has been successfully rolled
2022-04-06 20:21:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-2aabacad-zookeeper to be ready
2022-04-06 20:21:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ca9db028 will have desired state: Ready
2022-04-06 20:21:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ca9db028 is in desired state: Ready
2022-04-06 20:21:52 [ForkJoinPool-3-worker-7] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-ca9db028 is ready
2022-04-06 20:21:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:21:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testBrokerConfigurationChangeTriggerRollingUpdate
2022-04-06 20:21:52 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ca9db028 in namespace namespace-81
2022-04-06 20:22:01 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2aabacad-kafka rolling update
2022-04-06 20:22:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:22:02 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-81 for test case:testBrokerConfigurationChangeTriggerRollingUpdate
2022-04-06 20:22:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testBrokerConfigurationChangeTriggerRollingUpdate-FINISHED
2022-04-06 20:22:46 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-83 for test case:testRecoveryDuringZookeeperRollingUpdate
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-84 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-84
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5aa30707 in namespace namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-755284975-1807289505 in namespace namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-06 20:22:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5aa30707 will have desired state: Ready
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-84
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-84
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1be55330 in namespace namespace-84
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-06 20:22:51 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1be55330 will have desired state: Ready
2022-04-06 20:23:11 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2aabacad-kafka has been successfully rolled
2022-04-06 20:23:11 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-2aabacad-kafka to be ready
2022-04-06 20:23:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2aabacad will have desired state: Ready
2022-04-06 20:23:42 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2aabacad is in desired state: Ready
2022-04-06 20:23:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-2aabacad is ready
2022-04-06 20:23:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2aabacad-zookeeper rolling update
2022-04-06 20:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5aa30707 is in desired state: Ready
2022-04-06 20:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-755284975-1807289505 will have desired state: Ready
2022-04-06 20:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-755284975-1807289505 is in desired state: Ready
2022-04-06 20:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1369083742-1984009214 in namespace namespace-83
2022-04-06 20:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-06 20:24:44 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1369083742-1984009214 will have desired state: Ready
2022-04-06 20:24:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1369083742-1984009214 is in desired state: Ready
2022-04-06 20:24:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5aa30707-kafka-clients in namespace namespace-84
2022-04-06 20:24:45 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-06 20:24:55 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 20:24:55 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@532b87b6, messages=[], arguments=[USER=my_user_1369083742_1984009214, --max-messages, 100, --bootstrap-server, my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-755284975-1807289505], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4', podNamespace='namespace-83', bootstrapServer='my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-755284975-1807289505', maxMessages=100, kafkaUsername='my-user-1369083742-1984009214', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1bb3ec45}
2022-04-06 20:24:55 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093:my-topic-755284975-1807289505 from pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4
2022-04-06 20:24:55 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 -n namespace-83 -- /opt/kafka/producer.sh USER=my_user_1369083742_1984009214 --max-messages 100 --bootstrap-server my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-755284975-1807289505
2022-04-06 20:24:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1be55330 is in desired state: Ready
2022-04-06 20:24:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1106433829-226588818 in namespace namespace-84
2022-04-06 20:24:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-06 20:24:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1106433829-226588818 will have desired state: Ready
2022-04-06 20:24:59 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:24:59 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:24:59 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateST:117] Update resources for pods
2022-04-06 20:25:00 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1edf8cfd, messages=[], arguments=[--group-id, my-consumer-group-1452030479, USER=my_user_1369083742_1984009214, --max-messages, 100, --group-instance-id, instance445594750, --bootstrap-server, my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-755284975-1807289505], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4', podNamespace='namespace-83', bootstrapServer='my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-755284975-1807289505', maxMessages=100, kafkaUsername='my-user-1369083742-1984009214', consumerGroupName='my-consumer-group-1452030479', consumerInstanceId='instance445594750', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@11c6af44}
2022-04-06 20:25:00 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093:my-topic-755284975-1807289505 from pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4
2022-04-06 20:25:00 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 -n namespace-83 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1452030479 USER=my_user_1369083742_1984009214 --max-messages 100 --group-instance-id instance445594750 --bootstrap-server my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-755284975-1807289505
2022-04-06 20:25:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1106433829-226588818 is in desired state: Ready
2022-04-06 20:25:00 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:489] Verifying docker image names
2022-04-06 20:25:00 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-04-06 20:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:525] Docker images verified
2022-04-06 20:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:292] Running kafkaScaleUpScaleDown my-cluster-1be55330
2022-04-06 20:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-327963272-1851182979 in namespace namespace-84
2022-04-06 20:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-06 20:25:01 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-327963272-1851182979 will have desired state: Ready
2022-04-06 20:25:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-327963272-1851182979 is in desired state: Ready
2022-04-06 20:25:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-1be55330-kafka-clients in namespace namespace-84
2022-04-06 20:25:02 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-06 20:25:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8d4e764c will have desired state: Ready
2022-04-06 20:25:05 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8d4e764c is in desired state: Ready
2022-04-06 20:25:05 [ForkJoinPool-3-worker-15] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-8d4e764c is ready
2022-04-06 20:25:05 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4c2f21b2, messages=[], arguments=[--group-id, my-consumer-group-1284475345, USER=my_user_254812215_489564274, --max-messages, 100, --group-instance-id, instance1770050900, --bootstrap-server, my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093, --topic, my-topic-1844340034-1502759606], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v', podNamespace='namespace-79', bootstrapServer='my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093', topicName='my-topic-1844340034-1502759606', maxMessages=100, kafkaUsername='my-user-254812215-489564274', consumerGroupName='my-consumer-group-1284475345', consumerInstanceId='instance1770050900', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@b36a942}
2022-04-06 20:25:05 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093:my-topic-1844340034-1502759606 from pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:25:05 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v -n namespace-79 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1284475345 USER=my_user_254812215_489564274 --max-messages 100 --group-instance-id instance1770050900 --bootstrap-server my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093 --topic my-topic-1844340034-1502759606
2022-04-06 20:25:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2aabacad-zookeeper has been successfully rolled
2022-04-06 20:25:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-2aabacad-zookeeper to be ready
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: my-cluster-5aa30707-zookeeper will be in pending phase
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateST:130] Verifying stability of zookeeper pods except the one, which is in pending phase
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-5aa30707-zookeeper are stable
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:25:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:25:08 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:25:08 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:25:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:25:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:25:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:25:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:25:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:25:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:25:12 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-06 20:25:12 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@ac5ed0b, messages=[], arguments=[USER=my_user_1106433829_226588818, --max-messages, 100, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-327963272-1851182979], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-327963272-1851182979', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1559a0f9}
2022-04-06 20:25:12 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-327963272-1851182979 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:25:12 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/producer.sh USER=my_user_1106433829_226588818 --max-messages 100 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-327963272-1851182979
2022-04-06 20:25:12 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:25:12 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:25:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:45] Consumer successfully consumed 100 messages for the 1 time
2022-04-06 20:25:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1305731125-1082516141 in namespace namespace-84
2022-04-06 20:25:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-79
2022-04-06 20:25:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1305731125-1082516141 will have desired state: Ready
2022-04-06 20:25:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:25:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:25:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1305731125-1082516141 is in desired state: Ready
2022-04-06 20:25:13 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7ac53ed3, messages=[], arguments=[USER=my_user_254812215_489564274, --max-messages, 100, --bootstrap-server, my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093, --topic, my-topic-1305731125-1082516141], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v', podNamespace='namespace-79', bootstrapServer='my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093', topicName='my-topic-1305731125-1082516141', maxMessages=100, kafkaUsername='my-user-254812215-489564274', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5ab6b99b}
2022-04-06 20:25:13 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093:my-topic-1305731125-1082516141 from pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:25:13 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v -n namespace-79 -- /opt/kafka/producer.sh USER=my_user_254812215_489564274 --max-messages 100 --bootstrap-server my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093 --topic my-topic-1305731125-1082516141
2022-04-06 20:25:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:25:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:25:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:25:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:25:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:25:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:25:16 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:25:16 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:25:16 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@150c6433, messages=[], arguments=[--group-id, my-consumer-group-1912302371, USER=my_user_1106433829_226588818, --max-messages, 100, --group-instance-id, instance417714378, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-327963272-1851182979], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-327963272-1851182979', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='my-consumer-group-1912302371', consumerInstanceId='instance417714378', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@70e12ea2}
2022-04-06 20:25:16 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-327963272-1851182979 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:25:16 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1912302371 USER=my_user_1106433829_226588818 --max-messages 100 --group-instance-id instance417714378 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-327963272-1851182979
2022-04-06 20:25:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:25:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:25:17 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:25:17 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:25:17 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@71907ff3, messages=[], arguments=[--group-id, my-consumer-group-315148973, USER=my_user_254812215_489564274, --max-messages, 100, --group-instance-id, instance952604483, --bootstrap-server, my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093, --topic, my-topic-1305731125-1082516141], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v', podNamespace='namespace-79', bootstrapServer='my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093', topicName='my-topic-1305731125-1082516141', maxMessages=100, kafkaUsername='my-user-254812215-489564274', consumerGroupName='my-consumer-group-315148973', consumerInstanceId='instance952604483', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4407f470}
2022-04-06 20:25:17 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093:my-topic-1305731125-1082516141 from pod my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v
2022-04-06 20:25:17 [ForkJoinPool-3-worker-15] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-8d4e764c-kafka-clients-5bf86858bc-prr2v -n namespace-79 -- /opt/kafka/consumer.sh --group-id my-consumer-group-315148973 USER=my_user_254812215_489564274 --max-messages 100 --group-instance-id instance952604483 --bootstrap-server my-cluster-8d4e764c-kafka-bootstrap.namespace-79.svc:9093 --topic my-topic-1305731125-1082516141
2022-04-06 20:25:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:25:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:25:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:25:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:25:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:25:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:25:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:25:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:25:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:25:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:25:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:25:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:25:23 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:25:23 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:25:23 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:317] Scale up Kafka to 7
2022-04-06 20:25:23 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1be55330-kafka rolling update
2022-04-06 20:25:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:25:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:25:24 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:25:24 [ForkJoinPool-3-worker-15] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:25:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:25:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryDuringKafkaRollingUpdate
2022-04-06 20:25:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-254812215-489564274 in namespace namespace-79
2022-04-06 20:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:25:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:25:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:25:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:25:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:25:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:25:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:25:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:25:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:25:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:25:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:25:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:25:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:25:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:25:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:25:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:25:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:25:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:25:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:25:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1305731125-1082516141 in namespace namespace-79
2022-04-06 20:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:25:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:25:36 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:25:36 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:25:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:25:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:25:38 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:25:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:25:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:25:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:25:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:25:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:25:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:25:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:25:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:25:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:25:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:25:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:25:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-8d4e764c-kafka-clients in namespace namespace-79
2022-04-06 20:25:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:25:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:25:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:25:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:25:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:25:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:25:48 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:25:48 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:25:49 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:25:49 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:25:50 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:25:50 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:25:51 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:25:51 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:25:52 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:25:52 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:25:53 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:25:53 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:25:54 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:25:54 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:25:55 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:25:55 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:25:56 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:25:56 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-5aa30707-zookeeper-0 ,my-cluster-5aa30707-zookeeper-1
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateST:133] Verifying stability of kafka pods
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-5aa30707-kafka are stable
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:25:57 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 20:25:58 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:25:58 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:25:58 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:25:58 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 20:25:59 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:25:59 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:25:59 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:25:59 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 20:26:00 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:26:00 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:26:00 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:26:00 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 20:26:01 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:26:01 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:26:01 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:26:01 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 20:26:02 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:26:02 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:26:02 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:26:02 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 20:26:03 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:26:03 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:26:03 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:26:03 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 20:26:04 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:26:04 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:26:04 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:26:04 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 20:26:05 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:26:05 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:26:05 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:26:05 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 20:26:06 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:26:06 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:26:06 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:26:06 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 20:26:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:26:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:26:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:26:07 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 20:26:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-2aabacad-kafka rolling update
2022-04-06 20:26:08 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:26:08 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:26:08 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:26:08 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 20:26:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:26:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:26:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:26:09 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 20:26:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:26:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:26:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:26:10 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 20:26:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:26:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:26:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:26:11 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 20:26:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:26:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:26:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:26:12 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 20:26:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:26:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:26:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:26:13 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 20:26:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:26:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:26:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:26:14 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 20:26:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:26:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:26:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:26:15 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 20:26:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:26:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:26:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:26:16 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 20:26:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:26:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:26:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:26:17 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 20:26:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:26:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:26:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:26:18 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 20:26:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:26:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:26:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:26:19 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 20:26:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:26:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:26:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:26:20 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 20:26:21 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:26:21 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:26:21 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:26:21 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 20:26:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:26:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:26:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:26:22 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 20:26:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:26:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:26:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:26:23 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 20:26:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:26:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:26:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:26:24 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 20:26:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1844340034-1502759606 in namespace namespace-79
2022-04-06 20:26:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:26:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:26:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:26:25 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 20:26:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:26:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:26:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:26:26 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 20:26:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:26:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:26:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:26:27 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 20:26:28 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1be55330-kafka has been successfully rolled
2022-04-06 20:26:28 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 7 Pod(s) of my-cluster-1be55330-kafka to be ready
2022-04-06 20:26:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:26:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:26:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:26:28 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 20:26:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:26:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:26:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:26:29 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 20:26:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:26:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:26:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:26:30 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 20:26:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:26:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:26:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:26:31 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 20:26:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:26:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:26:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:26:32 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 20:26:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:26:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:26:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:26:33 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 20:26:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8d4e764c in namespace namespace-79
2022-04-06 20:26:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:26:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:26:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:26:34 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 20:26:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:26:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:26:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:26:35 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 20:26:36 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:26:36 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:26:36 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:26:36 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 20:26:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:26:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:26:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:26:37 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 20:26:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:26:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:26:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:26:39 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 20:26:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:26:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:26:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:26:40 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 20:26:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:26:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:26:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:26:41 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 20:26:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:26:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:26:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:26:42 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 20:26:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:26:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:26:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:26:43 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 20:26:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:26:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:26:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:26:44 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 20:26:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:26:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-79 for test case:testRecoveryDuringKafkaRollingUpdate
2022-04-06 20:26:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:26:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:26:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:26:45 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 20:26:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:26:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:26:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:26:46 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 20:26:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:26:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:26:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:26:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:322] Pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 20:26:47 [ForkJoinPool-3-worker-13] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-5aa30707-kafka-0 ,my-cluster-5aa30707-kafka-1 ,my-cluster-5aa30707-kafka-2 ,my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4
2022-04-06 20:26:47 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-5aa30707-zookeeper to be ready
2022-04-06 20:27:17 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-2aabacad-kafka has been successfully rolled
2022-04-06 20:27:17 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-2aabacad-kafka to be ready
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1be55330 will have desired state: Ready
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1be55330 is in desired state: Ready
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1be55330 is ready
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:327] Kafka scale up to 7 finished
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@17a5ad6f, messages=[], arguments=[--group-id, my-consumer-group-2002611875, USER=my_user_1106433829_226588818, --max-messages, 100, --group-instance-id, instance515641953, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-327963272-1851182979], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-327963272-1851182979', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='my-consumer-group-2002611875', consumerInstanceId='instance515641953', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3ae1dcc3}
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-327963272-1851182979 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:27:25 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2002611875 USER=my_user_1106433829_226588818 --max-messages 100 --group-instance-id instance515641953 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-327963272-1851182979
2022-04-06 20:27:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringKafkaRollingUpdate-FINISHED
2022-04-06 20:27:28 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:27:33 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:27:33 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:27:33 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:339] Scale up Zookeeper to 5
2022-04-06 20:27:33 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 5 Pod(s) of my-cluster-1be55330-zookeeper to be ready
2022-04-06 20:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2aabacad will have desired state: Ready
2022-04-06 20:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2aabacad is in desired state: Ready
2022-04-06 20:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-2aabacad is ready
2022-04-06 20:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testExternalLoggingChangeTriggerRollingUpdate
2022-04-06 20:27:49 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2aabacad in namespace namespace-82
2022-04-06 20:27:59 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:27:59 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-82 for test case:testExternalLoggingChangeTriggerRollingUpdate
2022-04-06 20:28:10 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testExternalLoggingChangeTriggerRollingUpdate-FINISHED
2022-04-06 20:28:10 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1be55330 will have desired state: Ready
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1be55330 is in desired state: Ready
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1be55330 is ready
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:342] Kafka scale up to 5 finished
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@39f256ba, messages=[], arguments=[--group-id, my-consumer-group-130794326, USER=my_user_1106433829_226588818, --max-messages, 100, --group-instance-id, instance1872047886, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-327963272-1851182979], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-327963272-1851182979', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='my-consumer-group-130794326', consumerInstanceId='instance1872047886', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3065d60}
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-327963272-1851182979 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:28:33 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/consumer.sh --group-id my-consumer-group-130794326 USER=my_user_1106433829_226588818 --max-messages 100 --group-instance-id instance1872047886 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-327963272-1851182979
2022-04-06 20:28:40 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:28:40 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:28:40 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:351] Scale down Kafka to 3
2022-04-06 20:28:40 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-1be55330-kafka rolling update
2022-04-06 20:31:00 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-1be55330-kafka has been successfully rolled
2022-04-06 20:31:00 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-1be55330-kafka to be ready
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1be55330 will have desired state: Ready
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1be55330 is in desired state: Ready
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-1be55330 is ready
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateST:356] Kafka scale down to 3 finished
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@52e2c134, messages=[], arguments=[--group-id, my-consumer-group-1650274875, USER=my_user_1106433829_226588818, --max-messages, 100, --group-instance-id, instance1692348470, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-327963272-1851182979], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-327963272-1851182979', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='my-consumer-group-1650274875', consumerInstanceId='instance1692348470', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6bb66a1d}
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-327963272-1851182979 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:31:28 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1650274875 USER=my_user_1106433829_226588818 --max-messages 100 --group-instance-id instance1692348470 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-327963272-1851182979
2022-04-06 20:31:35 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:31:35 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:31:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-522671043-1840391981 in namespace namespace-84
2022-04-06 20:31:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-84
2022-04-06 20:31:35 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-522671043-1840391981 will have desired state: Ready
2022-04-06 20:32:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5aa30707 will have desired state: Ready
2022-04-06 20:32:31 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5aa30707 is in desired state: Ready
2022-04-06 20:32:31 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-5aa30707 is ready
2022-04-06 20:32:31 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@28ce2adf, messages=[], arguments=[--group-id, my-consumer-group-1711337184, USER=my_user_1369083742_1984009214, --max-messages, 100, --group-instance-id, instance1637832682, --bootstrap-server, my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-755284975-1807289505], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4', podNamespace='namespace-83', bootstrapServer='my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-755284975-1807289505', maxMessages=100, kafkaUsername='my-user-1369083742-1984009214', consumerGroupName='my-consumer-group-1711337184', consumerInstanceId='instance1637832682', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@c56089e}
2022-04-06 20:32:31 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093:my-topic-755284975-1807289505 from pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4
2022-04-06 20:32:31 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 -n namespace-83 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1711337184 USER=my_user_1369083742_1984009214 --max-messages 100 --group-instance-id instance1637832682 --bootstrap-server my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-755284975-1807289505
2022-04-06 20:32:39 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:32:39 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:32:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1568680596-308841270 in namespace namespace-84
2022-04-06 20:32:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-83
2022-04-06 20:32:39 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1568680596-308841270 will have desired state: Ready
2022-04-06 20:32:40 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1568680596-308841270 is in desired state: Ready
2022-04-06 20:32:40 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@382b3ffd, messages=[], arguments=[USER=my_user_1369083742_1984009214, --max-messages, 100, --bootstrap-server, my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1568680596-308841270], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4', podNamespace='namespace-83', bootstrapServer='my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1568680596-308841270', maxMessages=100, kafkaUsername='my-user-1369083742-1984009214', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4a94e35f}
2022-04-06 20:32:40 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093:my-topic-1568680596-308841270 from pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4
2022-04-06 20:32:40 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 -n namespace-83 -- /opt/kafka/producer.sh USER=my_user_1369083742_1984009214 --max-messages 100 --bootstrap-server my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1568680596-308841270
2022-04-06 20:32:43 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:32:43 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:32:43 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5d449813, messages=[], arguments=[--group-id, my-consumer-group-707537423, USER=my_user_1369083742_1984009214, --max-messages, 100, --group-instance-id, instance249292973, --bootstrap-server, my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093, --topic, my-topic-1568680596-308841270], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4', podNamespace='namespace-83', bootstrapServer='my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093', topicName='my-topic-1568680596-308841270', maxMessages=100, kafkaUsername='my-user-1369083742-1984009214', consumerGroupName='my-consumer-group-707537423', consumerInstanceId='instance249292973', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@405c29e}
2022-04-06 20:32:43 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093:my-topic-1568680596-308841270 from pod my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4
2022-04-06 20:32:43 [ForkJoinPool-3-worker-13] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5aa30707-kafka-clients-845c9bcdc6-nvrm4 -n namespace-83 -- /opt/kafka/consumer.sh --group-id my-consumer-group-707537423 USER=my_user_1369083742_1984009214 --max-messages 100 --group-instance-id instance249292973 --bootstrap-server my-cluster-5aa30707-kafka-bootstrap.namespace-83.svc:9093 --topic my-topic-1568680596-308841270
2022-04-06 20:32:50 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:32:50 [ForkJoinPool-3-worker-13] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:32:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:32:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryDuringZookeeperRollingUpdate
2022-04-06 20:32:50 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1369083742-1984009214 in namespace namespace-83
2022-04-06 20:33:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1568680596-308841270 in namespace namespace-83
2022-04-06 20:33:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-522671043-1840391981 is in desired state: Ready
2022-04-06 20:33:08 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1c33c8cc, messages=[], arguments=[USER=my_user_1106433829_226588818, --max-messages, 100, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-522671043-1840391981], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-522671043-1840391981', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@581c599a}
2022-04-06 20:33:08 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-522671043-1840391981 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:33:08 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/producer.sh USER=my_user_1106433829_226588818 --max-messages 100 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-522671043-1840391981
2022-04-06 20:33:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5aa30707-kafka-clients in namespace namespace-83
2022-04-06 20:33:11 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-06 20:33:11 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-06 20:33:11 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1d8a6d63, messages=[], arguments=[--group-id, my-consumer-group-2097730521, USER=my_user_1106433829_226588818, --max-messages, 100, --group-instance-id, instance611287841, --bootstrap-server, my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093, --topic, my-topic-522671043-1840391981], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km', podNamespace='namespace-84', bootstrapServer='my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093', topicName='my-topic-522671043-1840391981', maxMessages=100, kafkaUsername='my-user-1106433829-226588818', consumerGroupName='my-consumer-group-2097730521', consumerInstanceId='instance611287841', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@cd1034d}
2022-04-06 20:33:11 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093:my-topic-522671043-1840391981 from pod my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km
2022-04-06 20:33:11 [ForkJoinPool-3-worker-11] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-1be55330-kafka-clients-58f49bb7d4-zx6km -n namespace-84 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2097730521 USER=my_user_1106433829_226588818 --max-messages 100 --group-instance-id instance611287841 --bootstrap-server my-cluster-1be55330-kafka-bootstrap.namespace-84.svc:9093 --topic my-topic-522671043-1840391981
2022-04-06 20:33:18 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-06 20:33:18 [ForkJoinPool-3-worker-11] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-06 20:33:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:33:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaAndZookeeperScaleUpScaleDown
2022-04-06 20:33:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-327963272-1851182979 in namespace namespace-84
2022-04-06 20:33:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-522671043-1840391981 in namespace namespace-84
2022-04-06 20:33:28 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-1be55330-kafka-clients in namespace namespace-84
2022-04-06 20:34:00 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-755284975-1807289505 in namespace namespace-83
2022-04-06 20:34:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1106433829-226588818 in namespace namespace-84
2022-04-06 20:34:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1be55330 in namespace namespace-84
2022-04-06 20:34:10 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5aa30707 in namespace namespace-83
2022-04-06 20:34:18 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:34:18 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-84 for test case:testKafkaAndZookeeperScaleUpScaleDown
2022-04-06 20:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:34:30 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-83 for test case:testRecoveryDuringZookeeperRollingUpdate
2022-04-06 20:35:02 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testKafkaAndZookeeperScaleUpScaleDown-FINISHED
2022-04-06 20:35:02 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:35:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.RollingUpdateST.testRecoveryDuringZookeeperRollingUpdate-FINISHED
2022-04-06 20:35:14 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context RollingUpdateST is everything deleted.
2022-04-06 20:35:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,462.472 s - in io.strimzi.systemtest.rollingupdate.RollingUpdateST
[[1;34mINFO[m] Running io.strimzi.systemtest.log.LoggingChangeST
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: logging-change-st
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: logging-change-st
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: logging-change-st
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels-STARTED
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:618] Checking that original logging config is different from the new one
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:621] Changing logging for cluster-operator
2022-04-06 20:35:20 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:624] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:35:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:31 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:37 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:629] Checking log4j2.properties in CO pod
2022-04-06 20:35:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:633] Checking if CO rolled its pod
2022-04-06 20:35:48 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:48 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:49 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:50 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:50 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:51 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:52 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:52 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:53 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:54 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:54 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:55 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:56 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:56 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 2022-04-06 20:35:26 INFO  ClusterOperator:128 - Triggering periodic reconciliation for namespace *

2022-04-06 20:35:57 [ForkJoinPool-3-worker-3] [33mWARN [m [LoggingChangeST:639] 
2022-04-06 20:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:643] Changing all levels from OFF to INFO/WARN
2022-04-06 20:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:647] Changing logging for cluster-operator
2022-04-06 20:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:650] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:57 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:35:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:35:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:01 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:04 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:04 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:14 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:14 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:15 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:20 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:25 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:32 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:38 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:44 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:655] Checking log4j2.properties in CO pod
2022-04-06 20:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-97bsn -- /bin/bash -c cat /opt/strimzi/custom-config/log4j2.properties
2022-04-06 20:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LoggingChangeST:659] Checking if CO rolled its pod
2022-04-06 20:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testDynamicallySetClusterOperatorLoggingLevels is everything deleted.
2022-04-06 20:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetClusterOperatorLoggingLevels-FINISHED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testMM2LoggingLevelsHierarchy-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLogger-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testJSONFormatLogging-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testNotExistingCMSetsDefaultLogging-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaLoggingLevels-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetEOloggingLevels-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetBridgeLoggingLevels-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetConnectLoggingLevels-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLoggerValue-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaExternalLogging-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:36:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testLoggingHierarchy-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:36:57 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetMM2LoggingLevels-STARTED
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-85 for test case:testNotExistingCMSetsDefaultLogging
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-85
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-85
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-85
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [TestUtils:197] /home/ec2-user/strimzi-kafka-operator/systemtest/../cluster-operator/src/main/resources/kafkaDefaultLoggingProperties
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-86 for test case:testDynamicallySetUnknownKafkaLogger
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-86
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:1320] Deploying Kafka with custom logging
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c020c031 in namespace namespace-85
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-85
2022-04-06 20:36:57 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c020c031 will have desired state: Ready
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-86
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-86
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a04ffa9d in namespace namespace-86
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-86
2022-04-06 20:36:57 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a04ffa9d will have desired state: Ready
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-87 for test case:testDynamicallySetEOloggingLevels
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-87
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-87
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-87
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5d462893 in namespace namespace-87
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-87
2022-04-06 20:37:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5d462893 will have desired state: Ready
2022-04-06 20:38:08 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a04ffa9d is in desired state: Ready
2022-04-06 20:38:08 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-a04ffa9d-kafka rolling update
2022-04-06 20:38:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5d462893 is in desired state: Ready
2022-04-06 20:38:12 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:285] Checking if EO pod contains any log (except configuration)
2022-04-06 20:38:12 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:288] Changing rootLogger level to DEBUG with inline logging
2022-04-06 20:38:12 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:296] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:39:16 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c020c031 is in desired state: Ready
2022-04-06 20:39:16 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:1344] Changing external logging's CM to not existing one
2022-04-06 20:39:16 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 20:39:17 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 20:39:18 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 20:39:19 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 20:39:20 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 20:39:21 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 20:39:22 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 20:39:23 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 20:39:24 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 20:39:25 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:313] Setting external logging OFF
2022-04-06 20:39:25 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:371] Setting log level of TO and UO to OFF - records should not appear in log
2022-04-06 20:39:25 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:378] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:39:26 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 20:39:27 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 20:39:28 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 20:39:29 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 20:39:30 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 20:39:31 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 20:39:32 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 20:39:33 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 20:39:34 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 20:39:35 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 20:39:36 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 20:39:37 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 20:39:38 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 20:39:39 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 20:39:40 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 20:39:41 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 20:39:42 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 20:39:43 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 20:39:43 [ForkJoinPool-3-worker-13] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-a04ffa9d-kafka has been successfully rolled
2022-04-06 20:39:44 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 20:39:45 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 20:39:46 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 20:39:47 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 20:39:48 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 20:39:49 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 20:39:50 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 20:39:51 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 20:39:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:39:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetUnknownKafkaLogger
2022-04-06 20:39:51 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a04ffa9d in namespace namespace-86
2022-04-06 20:39:52 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 20:39:53 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 20:39:54 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 20:39:55 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 20:39:56 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 20:39:57 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 20:39:58 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 20:39:59 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 20:40:00 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 20:40:01 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 20:40:01 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:40:01 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-86 for test case:testDynamicallySetUnknownKafkaLogger
2022-04-06 20:40:02 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 20:40:03 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 20:40:04 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 20:40:05 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 20:40:06 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 20:40:07 [ForkJoinPool-3-worker-5] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-c020c031-kafka-2=92d789c3-a023-4cef-ab80-f5cdf251d4e1, my-cluster-c020c031-kafka-1=63c41902-180d-403a-9892-8daa33ecf24b, my-cluster-c020c031-kafka-0=7fb36820-20bd-4eaa-a3ad-0cbc40a64b26} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 20:40:07 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:1358] Checking that log4j.properties in custom-config isn't empty and configuration is default
2022-04-06 20:40:07 [ForkJoinPool-3-worker-5] [32mINFO [m [LoggingChangeST:1366] Checking if Kafka:my-cluster-c020c031 contains error about non-existing CM
2022-04-06 20:40:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:40:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:348] Delete all resources for testNotExistingCMSetsDefaultLogging
2022-04-06 20:40:07 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c020c031 in namespace namespace-85
2022-04-06 20:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:40:17 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-85 for test case:testNotExistingCMSetsDefaultLogging
2022-04-06 20:40:44 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testNotExistingCMSetsDefaultLogging-FINISHED
2022-04-06 20:40:44 [ForkJoinPool-3-worker-5] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:40:44 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLogger-FINISHED
2022-04-06 20:40:44 [ForkJoinPool-3-worker-13] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-88 for test case:testDynamicallySetKafkaLoggingLevels
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-88
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-88
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-88
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b00fa353 in namespace namespace-88
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-88
2022-04-06 20:40:47 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b00fa353 will have desired state: Ready
2022-04-06 20:40:49 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:396] Setting external logging OFF
2022-04-06 20:40:49 [ForkJoinPool-3-worker-21] [32mINFO [m [LoggingChangeST:432] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetEOloggingLevels
2022-04-06 20:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5d462893 in namespace namespace-87
2022-04-06 20:41:59 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b00fa353 is in desired state: Ready
2022-04-06 20:41:59 [ForkJoinPool-3-worker-17] [32mINFO [m [LoggingChangeST:828] Changing rootLogger level to DEBUG with inline logging
2022-04-06 20:41:59 [ForkJoinPool-3-worker-17] [32mINFO [m [LoggingChangeST:835] Waiting for dynamic change in the kafka pod
2022-04-06 20:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-87 for test case:testDynamicallySetEOloggingLevels
2022-04-06 20:42:06 [ForkJoinPool-3-worker-17] [32mINFO [m [LoggingChangeST:853] Setting external logging INFO
2022-04-06 20:42:06 [ForkJoinPool-3-worker-17] [32mINFO [m [LoggingChangeST:889] Setting log level of kafka INFO
2022-04-06 20:42:06 [ForkJoinPool-3-worker-17] [32mINFO [m [LoggingChangeST:895] Waiting for dynamic change in the kafka pod
2022-04-06 20:42:10 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:42:10 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetKafkaLoggingLevels
2022-04-06 20:42:10 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b00fa353 in namespace namespace-88
2022-04-06 20:42:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetEOloggingLevels-FINISHED
2022-04-06 20:42:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-89 for test case:testDynamicallySetUnknownKafkaLoggerValue
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-89
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-89
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-89
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-3559252e in namespace namespace-89
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-89
2022-04-06 20:42:17 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-3559252e will have desired state: Ready
2022-04-06 20:42:20 [ForkJoinPool-3-worker-17] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:42:20 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-88 for test case:testDynamicallySetKafkaLoggingLevels
2022-04-06 20:43:03 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaLoggingLevels-FINISHED
2022-04-06 20:43:03 [ForkJoinPool-3-worker-17] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-90 for test case:testJSONFormatLogging
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-90
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-90
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-90
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f6988fc9 in namespace namespace-90
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-90
2022-04-06 20:43:07 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f6988fc9 will have desired state: Ready
2022-04-06 20:43:35 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-3559252e is in desired state: Ready
2022-04-06 20:43:35 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 20:43:36 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 20:43:37 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 20:43:38 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 20:43:39 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 20:43:40 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 20:43:41 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 20:43:42 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 20:43:43 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 20:43:44 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 20:43:45 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 20:43:46 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 20:43:47 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 20:43:48 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 20:43:49 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 20:43:50 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 20:43:51 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 20:43:52 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 20:43:53 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 20:43:54 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 20:43:55 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 20:43:56 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 20:43:57 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 20:43:58 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 20:43:59 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 20:44:00 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 20:44:01 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 20:44:02 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 20:44:03 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 20:44:04 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 20:44:05 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 20:44:06 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 20:44:07 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 20:44:08 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 20:44:09 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 20:44:10 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 20:44:11 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 20:44:12 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 20:44:13 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 20:44:14 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 20:44:15 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 20:44:16 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 20:44:17 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 20:44:18 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 20:44:19 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 20:44:20 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 20:44:21 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 20:44:22 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 20:44:23 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 20:44:24 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 20:44:25 [ForkJoinPool-3-worker-19] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-3559252e-kafka-1=0234a4e9-8608-4c02-ab2c-edb7af8b4fef, my-cluster-3559252e-kafka-0=f02d8419-2d91-4030-baf7-57c52a92dd6f, my-cluster-3559252e-kafka-2=c37aa418-5556-47dc-92cd-84beda3031ae} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 20:44:25 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:44:25 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetUnknownKafkaLoggerValue
2022-04-06 20:44:25 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-3559252e in namespace namespace-89
2022-04-06 20:44:26 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f6988fc9 is in desired state: Ready
2022-04-06 20:44:26 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: strimzi-cluster-operator-78689684d4-97bsn
2022-04-06 20:44:26 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-kafka-1
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-kafka-2
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-kafka-0
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-zookeeper-1
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-zookeeper-2
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-zookeeper-0
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-entity-operator-6674679cbb-rp2xl
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [StUtils:278] JSON format logging successfully set for pod: my-cluster-f6988fc9-entity-operator-6674679cbb-rp2xl
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:348] Delete all resources for testJSONFormatLogging
2022-04-06 20:44:27 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f6988fc9 in namespace namespace-90
2022-04-06 20:44:35 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:44:35 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-89 for test case:testDynamicallySetUnknownKafkaLoggerValue
2022-04-06 20:44:37 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:44:37 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-90 for test case:testJSONFormatLogging
2022-04-06 20:45:19 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetUnknownKafkaLoggerValue-FINISHED
2022-04-06 20:45:19 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-91 for test case:testLoggingHierarchy
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-91
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-91
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-91
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-92 for test case:testDynamicallySetMM2LoggingLevels
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-92
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9254c2e0 in namespace namespace-91
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-91
2022-04-06 20:45:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9254c2e0 will have desired state: Ready
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-92
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-92
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5f711dbc-source in namespace namespace-92
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-92
2022-04-06 20:45:22 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5f711dbc-source will have desired state: Ready
2022-04-06 20:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testJSONFormatLogging-FINISHED
2022-04-06 20:45:25 [ForkJoinPool-3-worker-9] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-93 for test case:testDynamicallySetBridgeLoggingLevels
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-27cab217 in namespace namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-27cab217-kafka-clients in namespace namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-27cab217 in namespace namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-93
2022-04-06 20:45:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-27cab217 will have desired state: Ready
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-27cab217 is in desired state: Ready
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-27cab217-kafka-clients will be ready
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-27cab217-kafka-clients is ready
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-27cab217 will have desired state: Ready
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-27cab217 is in desired state: Ready
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [LoggingChangeST:485] Asserting if log is without records
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [LoggingChangeST:488] Changing rootLogger level to DEBUG with inline logging
2022-04-06 20:46:51 [ForkJoinPool-3-worker-25] [32mINFO [m [LoggingChangeST:500] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:47:31 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5f711dbc-source is in desired state: Ready
2022-04-06 20:47:31 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5f711dbc-target in namespace namespace-93
2022-04-06 20:47:31 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-92
2022-04-06 20:47:31 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5f711dbc-target will have desired state: Ready
2022-04-06 20:47:37 [ForkJoinPool-3-worker-25] [32mINFO [m [LoggingChangeST:557] Setting log level of Bridge to OFF - records should not appear in the log
2022-04-06 20:47:37 [ForkJoinPool-3-worker-25] [32mINFO [m [LoggingChangeST:563] Waiting for log4j2.properties will contain desired settings
2022-04-06 20:47:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9254c2e0 is in desired state: Ready
2022-04-06 20:47:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9254c2e0-scraper in namespace namespace-91
2022-04-06 20:47:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-91
2022-04-06 20:47:43 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9254c2e0-scraper will be ready
2022-04-06 20:47:45 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9254c2e0-scraper is ready
2022-04-06 20:47:45 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-9254c2e0-scraper to be ready
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-9254c2e0-scraper is ready
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-9254c2e0-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-9254c2e0-allow in namespace namespace-91
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-91
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-9254c2e0 in namespace namespace-93
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-91
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-9254c2e0 in namespace namespace-93
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-91
2022-04-06 20:47:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-9254c2e0 will have desired state: Ready
2022-04-06 20:48:49 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5f711dbc-target is in desired state: Ready
2022-04-06 20:48:49 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5f711dbc-kafka-clients in namespace namespace-93
2022-04-06 20:48:49 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-92
2022-04-06 20:48:49 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-5f711dbc in namespace namespace-93
2022-04-06 20:48:49 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-92
2022-04-06 20:48:49 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-5f711dbc will have desired state: Ready
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-9254c2e0 is in desired state: Ready
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-9254c2e0 will have desired state: Ready
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-9254c2e0 is in desired state: Ready
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:1398] Changing rootLogger level in KafkaConnector to ERROR with inline logging
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:1404] Waiting for Connect API loggers will contain desired settings
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:1410] Restarting Kafka connector my-cluster-9254c2e0 with class name org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl -X POST http://my-cluster-9254c2e0-connect-api:8083/connectors/my-cluster-9254c2e0/restart
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:168] Wait until KafkaConnector my-cluster-9254c2e0's worker will be in RUNNING state
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl GET http://my-cluster-9254c2e0-connect-api:8083/connectors/my-cluster-9254c2e0/status
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:1417] Checking that logger is same for connector with class name org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:1423] Changing KafkaConnect's root logger to WARN, KafkaConnector: my-cluster-9254c2e0 shouldn't inherit it
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/root
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LoggingChangeST:1437] Checking if KafkaConnector org.apache.kafka.connect.file.FileStreamSourceConnector doesn't inherit logger from KafkaConnect
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:06 [ForkJoinPool-3-worker-1] [33mWARN [m [KafkaConnectorUtils:191] Logger level has changed: {"level":"WARN"}
. Reseting counter from 0 to 0
2022-04-06 20:49:07 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:49:07 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetBridgeLoggingLevels
2022-04-06 20:49:07 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-27cab217-kafka-clients in namespace namespace-93
2022-04-06 20:49:08 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:08 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:08 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 49
2022-04-06 20:49:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:09 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:09 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 48
2022-04-06 20:49:10 [ForkJoinPool-3-worker-9] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-27cab217 in namespace namespace-93
2022-04-06 20:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 47
2022-04-06 20:49:11 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:11 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:11 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 46
2022-04-06 20:49:12 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:12 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:12 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 45
2022-04-06 20:49:14 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-27cab217 in namespace namespace-93
2022-04-06 20:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:14 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 44
2022-04-06 20:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:15 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 43
2022-04-06 20:49:16 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:16 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:16 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 42
2022-04-06 20:49:17 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:17 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:17 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 41
2022-04-06 20:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 40
2022-04-06 20:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 39
2022-04-06 20:49:21 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:21 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:21 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 38
2022-04-06 20:49:22 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:22 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:22 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 37
2022-04-06 20:49:23 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:23 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:23 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 36
2022-04-06 20:49:25 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:25 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:25 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 35
2022-04-06 20:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:26 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 34
2022-04-06 20:49:27 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:27 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 33
2022-04-06 20:49:28 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:28 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:28 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 32
2022-04-06 20:49:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:30 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:30 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 31
2022-04-06 20:49:31 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:31 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:31 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 30
2022-04-06 20:49:32 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:32 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:32 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 29
2022-04-06 20:49:33 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:33 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:33 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 28
2022-04-06 20:49:34 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:34 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:34 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 27
2022-04-06 20:49:36 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:36 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:36 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 26
2022-04-06 20:49:37 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:37 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:37 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 25
2022-04-06 20:49:38 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:38 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:38 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 24
2022-04-06 20:49:39 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:39 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:39 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 23
2022-04-06 20:49:40 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:40 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:40 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 22
2022-04-06 20:49:42 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:42 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:42 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 21
2022-04-06 20:49:43 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:43 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:43 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 20
2022-04-06 20:49:44 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:44 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:44 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 19
2022-04-06 20:49:45 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:45 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:45 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 18
2022-04-06 20:49:47 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:47 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:47 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 17
2022-04-06 20:49:47 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:49:47 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-93 for test case:testDynamicallySetBridgeLoggingLevels
2022-04-06 20:49:48 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:48 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:48 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 16
2022-04-06 20:49:49 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:49 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 15
2022-04-06 20:49:50 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:50 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:50 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 14
2022-04-06 20:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 13
2022-04-06 20:49:52 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-5f711dbc is in desired state: Ready
2022-04-06 20:49:52 [ForkJoinPool-3-worker-23] [32mINFO [m [LoggingChangeST:1123] Changing rootLogger level to DEBUG with inline logging
2022-04-06 20:49:52 [ForkJoinPool-3-worker-23] [32mINFO [m [LoggingChangeST:1132] Waiting for log4j.properties will contain desired settings
2022-04-06 20:49:53 [ForkJoinPool-3-worker-23] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-92 exec my-cluster-5f711dbc-mirrormaker2-6cb6dc6f89-5n6dx -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:49:53 [ForkJoinPool-3-worker-23] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:53 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetBridgeLoggingLevels-FINISHED
2022-04-06 20:49:53 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:49:53 [ForkJoinPool-3-worker-23] [32mINFO [m [LoggingChangeST:1176] Setting log level of MM2 to OFF
2022-04-06 20:49:53 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:53 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:53 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 12
2022-04-06 20:49:53 [ForkJoinPool-3-worker-23] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-92 exec my-cluster-5f711dbc-mirrormaker2-6cb6dc6f89-5n6dx -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:49:53 [ForkJoinPool-3-worker-23] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:54 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:54 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:54 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 11
2022-04-06 20:49:54 [ForkJoinPool-3-worker-23] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-92 exec my-cluster-5f711dbc-mirrormaker2-6cb6dc6f89-5n6dx -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:49:54 [ForkJoinPool-3-worker-23] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:55 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:49:55 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetMM2LoggingLevels
2022-04-06 20:49:55 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5f711dbc-kafka-clients in namespace namespace-92
2022-04-06 20:49:55 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:55 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:55 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 10
2022-04-06 20:49:56 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:56 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5f711dbc-target in namespace namespace-92
2022-04-06 20:49:56 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 9
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-94 for test case:testDynamicallySetKafkaExternalLogging
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-94
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-94
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-94
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-98c67d8b in namespace namespace-94
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-94
2022-04-06 20:49:57 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-98c67d8b will have desired state: Ready
2022-04-06 20:49:58 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-5f711dbc in namespace namespace-92
2022-04-06 20:49:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:58 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 8
2022-04-06 20:49:59 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:49:59 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:49:59 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 7
2022-04-06 20:50:00 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:00 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:00 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 6
2022-04-06 20:50:01 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:01 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:01 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 5
2022-04-06 20:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 4
2022-04-06 20:50:04 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:04 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:04 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 3
2022-04-06 20:50:05 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:05 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:05 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 2
2022-04-06 20:50:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:06 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:06 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 1
2022-04-06 20:50:07 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5f711dbc-source in namespace namespace-92
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-91 exec my-cluster-9254c2e0-scraper-69b7d67645-sqq85 -- curl http://my-cluster-9254c2e0-connect-api:8083/admin/loggers/org.apache.kafka.connect.file.FileStreamSourceConnector
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:189] Logger level is ERROR. Remaining seconds for logger to be stable: 0
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:196] Logger for connector org.apache.kafka.connect.file.FileStreamSourceConnector is stable
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testLoggingHierarchy
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-9254c2e0-allow in namespace namespace-91
2022-04-06 20:50:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-9254c2e0 in namespace namespace-91
2022-04-06 20:50:08 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9254c2e0-scraper in namespace namespace-91
2022-04-06 20:50:17 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9254c2e0 in namespace namespace-91
2022-04-06 20:50:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-9254c2e0 in namespace namespace-91
2022-04-06 20:50:45 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:50:45 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-92 for test case:testDynamicallySetMM2LoggingLevels
2022-04-06 20:50:56 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetMM2LoggingLevels-FINISHED
2022-04-06 20:50:56 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-95 for test case:testMM2LoggingLevelsHierarchy
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-95
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-95
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-95
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0e57fe2f-source in namespace namespace-95
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-06 20:50:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0e57fe2f-source will have desired state: Ready
2022-04-06 20:50:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:50:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-91 for test case:testLoggingHierarchy
2022-04-06 20:51:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testLoggingHierarchy-FINISHED
2022-04-06 20:51:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-96 for test case:testDynamicallySetConnectLoggingLevels
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-96
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-96
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-96
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-446467f6 in namespace namespace-96
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-06 20:51:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-446467f6 will have desired state: Ready
2022-04-06 20:51:42 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-98c67d8b is in desired state: Ready
2022-04-06 20:51:42 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 50
2022-04-06 20:51:43 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 49
2022-04-06 20:51:44 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 48
2022-04-06 20:51:45 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 47
2022-04-06 20:51:46 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 46
2022-04-06 20:51:47 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 45
2022-04-06 20:51:48 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 44
2022-04-06 20:51:49 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 43
2022-04-06 20:51:50 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 42
2022-04-06 20:51:51 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 41
2022-04-06 20:51:52 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 40
2022-04-06 20:51:53 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 39
2022-04-06 20:51:54 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 38
2022-04-06 20:51:55 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 37
2022-04-06 20:51:56 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 36
2022-04-06 20:51:57 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 35
2022-04-06 20:51:58 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 34
2022-04-06 20:51:59 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 33
2022-04-06 20:52:00 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 32
2022-04-06 20:52:01 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 31
2022-04-06 20:52:02 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 30
2022-04-06 20:52:03 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 29
2022-04-06 20:52:04 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 28
2022-04-06 20:52:05 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 27
2022-04-06 20:52:06 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 26
2022-04-06 20:52:07 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 25
2022-04-06 20:52:08 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 24
2022-04-06 20:52:09 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 23
2022-04-06 20:52:10 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 22
2022-04-06 20:52:11 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 21
2022-04-06 20:52:12 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 20
2022-04-06 20:52:13 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 19
2022-04-06 20:52:14 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 18
2022-04-06 20:52:15 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 17
2022-04-06 20:52:16 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 16
2022-04-06 20:52:17 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 15
2022-04-06 20:52:18 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 14
2022-04-06 20:52:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0e57fe2f-source is in desired state: Ready
2022-04-06 20:52:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0e57fe2f-target in namespace namespace-96
2022-04-06 20:52:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-06 20:52:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0e57fe2f-target will have desired state: Ready
2022-04-06 20:52:19 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 13
2022-04-06 20:52:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-446467f6 is in desired state: Ready
2022-04-06 20:52:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-446467f6-scraper in namespace namespace-96
2022-04-06 20:52:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-06 20:52:20 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-446467f6-scraper will be ready
2022-04-06 20:52:20 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 12
2022-04-06 20:52:21 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-446467f6-scraper is ready
2022-04-06 20:52:21 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-446467f6-scraper to be ready
2022-04-06 20:52:21 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 11
2022-04-06 20:52:22 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 10
2022-04-06 20:52:23 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 9
2022-04-06 20:52:24 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 8
2022-04-06 20:52:25 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 7
2022-04-06 20:52:26 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 6
2022-04-06 20:52:27 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 5
2022-04-06 20:52:28 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 4
2022-04-06 20:52:29 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 3
2022-04-06 20:52:30 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 2
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-446467f6-scraper is ready
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-446467f6-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-446467f6-allow in namespace namespace-96
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-446467f6 in namespace namespace-96
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-96
2022-04-06 20:52:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-446467f6 will have desired state: Ready
2022-04-06 20:52:31 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 1
2022-04-06 20:52:33 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:143] {my-cluster-98c67d8b-kafka-0=1b12fc88-b09f-41ad-a120-72f4e1fb8735, my-cluster-98c67d8b-kafka-1=83a0cc08-010a-411f-a309-399d82fcc755, my-cluster-98c67d8b-kafka-2=794c650c-82a9-48d9-8076-32b10ce64a03} pods didn't roll. Remaining seconds for stability: 0
2022-04-06 20:52:36 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-98c67d8b-kafka rolling update
2022-04-06 20:53:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0e57fe2f-target is in desired state: Ready
2022-04-06 20:53:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0e57fe2f-kafka-clients in namespace namespace-96
2022-04-06 20:53:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-06 20:53:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-0e57fe2f in namespace namespace-96
2022-04-06 20:53:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-95
2022-04-06 20:53:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-0e57fe2f will have desired state: Ready
2022-04-06 20:53:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-446467f6 is in desired state: Ready
2022-04-06 20:53:42 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:703] Asserting if log is without records
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:706] Changing rootLogger level to DEBUG with inline logging
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:715] Waiting for log4j.properties will contain desired settings
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-96 exec my-cluster-446467f6-scraper-f4889f96-8pflc -- curl http://my-cluster-446467f6-connect-api:8083/admin/loggers/root
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [LoggingChangeST:760] Setting log level of Connect to OFF
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-96 exec my-cluster-446467f6-scraper-f4889f96-8pflc -- curl http://my-cluster-446467f6-connect-api:8083/admin/loggers/root
2022-04-06 20:53:43 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:11 [ForkJoinPool-3-worker-11] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-98c67d8b-kafka has been successfully rolled
2022-04-06 20:54:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:54:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetConnectLoggingLevels
2022-04-06 20:54:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-446467f6-allow in namespace namespace-96
2022-04-06 20:54:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-446467f6 in namespace namespace-96
2022-04-06 20:54:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:54:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testDynamicallySetKafkaExternalLogging
2022-04-06 20:54:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-98c67d8b in namespace namespace-94
2022-04-06 20:54:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-446467f6-scraper in namespace namespace-96
2022-04-06 20:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:54:24 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-94 for test case:testDynamicallySetKafkaExternalLogging
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-0e57fe2f is in desired state: Ready
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:1253] Waiting for log4j.properties will contain desired settings
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [LoggingChangeST:1258] Changing log levels
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:42 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:43 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:43 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:45 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:45 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:46 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:46 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:47 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:47 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:48 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:48 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-446467f6 in namespace namespace-96
2022-04-06 20:54:50 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:50 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:51 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:51 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:52 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:52 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:53 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:53 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:54 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:56 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:56 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/root
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/org.eclipse.jetty.util.thread.strategy.EatWhatYouKill
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-95 exec my-cluster-0e57fe2f-mirrormaker2-66c795dc78-knpl8 -- curl http://localhost:8083/admin/loggers/org.apache.kafka.connect.runtime.WorkerTask
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testMM2LoggingLevelsHierarchy
2022-04-06 20:54:57 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0e57fe2f-kafka-clients in namespace namespace-95
2022-04-06 20:54:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0e57fe2f-target in namespace namespace-95
2022-04-06 20:55:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:55:03 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-96 for test case:testDynamicallySetConnectLoggingLevels
2022-04-06 20:55:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetKafkaExternalLogging-FINISHED
2022-04-06 20:55:08 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:55:08 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0e57fe2f-source in namespace namespace-95
2022-04-06 20:55:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-0e57fe2f in namespace namespace-95
2022-04-06 20:55:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testDynamicallySetConnectLoggingLevels-FINISHED
2022-04-06 20:55:31 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:55:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 20:55:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-95 for test case:testMM2LoggingLevelsHierarchy
2022-04-06 20:56:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LoggingChangeST.testMM2LoggingLevelsHierarchy-FINISHED
2022-04-06 20:56:05 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 20:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 20:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context LoggingChangeST is everything deleted.
2022-04-06 20:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,249.812 s - in io.strimzi.systemtest.log.LoggingChangeST
[[1;34mINFO[m] Running io.strimzi.systemtest.log.LogSettingST
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: log-setting-st
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: log-setting-st
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: log-setting-st
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka log-setting-cluster-name in namespace log-setting-st
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka gc-set-logging in namespace log-setting-st
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment shared-kafka-clients in namespace log-setting-st
2022-04-06 20:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: log-setting-cluster-name will have desired state: Ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: log-setting-cluster-name is in desired state: Ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: gc-set-logging will have desired state: Ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: gc-set-logging is in desired state: Ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: shared-kafka-clients will be ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: shared-kafka-clients is ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:58:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 20:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testMirrorMaker2LogSetting-STARTED
2022-04-06 20:58:40 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testMirrorMakerLogSetting-STARTED
2022-04-06 20:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:58:40 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 20:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-e9bd6ea3-mirror-maker-2 in namespace log-setting-st
2022-04-06 20:58:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-a7021241-mirror-maker in namespace log-setting-st
2022-04-06 20:58:40 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-a7021241-mirror-maker will have desired state: Ready
2022-04-06 20:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-e9bd6ea3-mirror-maker-2 will have desired state: Ready
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-e9bd6ea3-mirror-maker-2 is in desired state: Ready
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:357] Checking if MirrorMaker2 has log level set properly
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: mirrormaker.root.logger Expected: TRACE
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.mirrormaker.logger.level Expected: TRACE
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 20:59:46 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2 rolling update
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-a7021241-mirror-maker is in desired state: Ready
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:322] Checking if MirrorMaker has log level set properly
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:476] Check log level setting for logger: mirrormaker.root.logger Expected: TRACE
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.mirrormaker.logger.level Expected: TRACE
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-a7021241-mirror-maker-mirror-maker
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-a7021241-mirror-maker-mirror-maker
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 20:59:47 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-a7021241-mirror-maker-mirror-maker rolling update
2022-04-06 21:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2 will be ready
2022-04-06 21:01:01 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2 is ready
2022-04-06 21:01:02 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a7021241-mirror-maker-mirror-maker will be ready
2022-04-06 21:01:02 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a7021241-mirror-maker-mirror-maker is ready
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2 rolling update finished
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2-74d9499c9f7zzxn container my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2 will be ready
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:347] Pod my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2-74d9499c9f7zzxn container my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2 is ready
2022-04-06 21:01:11 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2-74d9499c9f7zzxn with container my-cluster-e9bd6ea3-mirror-maker-2-mirrormaker2
2022-04-06 21:01:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:01:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2LogSetting
2022-04-06 21:01:12 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-e9bd6ea3-mirror-maker-2 in namespace log-setting-st
2022-04-06 21:01:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:01:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testConnectLogSetting-STARTED
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-a7021241-mirror-maker-mirror-maker rolling update finished
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-a7021241-mirror-maker-mirror-maker
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-a7021241-mirror-maker-mirror-maker
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-a7021241-mirror-maker-mirror-maker-655c6b9c97-mrzlz container my-cluster-a7021241-mirror-maker-mirror-maker will be ready
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [PodUtils:347] Pod my-cluster-a7021241-mirror-maker-mirror-maker-655c6b9c97-mrzlz container my-cluster-a7021241-mirror-maker-mirror-maker is ready
2022-04-06 21:01:12 [ForkJoinPool-3-worker-15] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-a7021241-mirror-maker-mirror-maker-655c6b9c97-mrzlz with container my-cluster-a7021241-mirror-maker-mirror-maker
2022-04-06 21:01:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:01:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerLogSetting
2022-04-06 21:01:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-a7021241-mirror-maker in namespace log-setting-st
2022-04-06 21:01:17 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:01:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-872cb00a-connect-scraper in namespace log-setting-st
2022-04-06 21:01:17 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-872cb00a-connect-scraper will be ready
2022-04-06 21:01:18 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-872cb00a-connect-scraper is ready
2022-04-06 21:01:18 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-872cb00a-connect-scraper to be ready
2022-04-06 21:01:22 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:01:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testMirrorMaker2LogSetting-FINISHED
2022-04-06 21:01:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:01:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:01:22 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testBridgeLogSetting-STARTED
2022-04-06 21:01:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:01:23 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testMirrorMakerLogSetting-FINISHED
2022-04-06 21:01:23 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:01:27 [ForkJoinPool-3-worker-7] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:01:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-24864fa0-bridge in namespace log-setting-st
2022-04-06 21:01:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-24864fa0-bridge will have desired state: Ready
2022-04-06 21:01:28 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-872cb00a-connect-scraper is ready
2022-04-06 21:01:28 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-872cb00a-connect-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 21:01:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-872cb00a-connect-allow in namespace log-setting-st
2022-04-06 21:01:28 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 21:01:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-872cb00a-connect in namespace log-setting-st
2022-04-06 21:01:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-872cb00a-connect will have desired state: Ready
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-24864fa0-bridge is in desired state: Ready
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:392] Checking if Bridge has log level set properly
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.subscribe.name Expected: http.openapi.operation.subscribe
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.createConsumer.name Expected: http.openapi.operation.createConsumer
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.createConsumer.level Expected: INFO
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.deleteConsumer.name Expected: http.openapi.operation.deleteConsumer
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.poll.name Expected: http.openapi.operation.poll
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.sendToPartition.level Expected: TRACE
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.unsubscribe.name Expected: http.openapi.operation.unsubscribe
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.unsubscribe.level Expected: DEBUG
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.ready.level Expected: WARN
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.assign.name Expected: http.openapi.operation.assign
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToEnd.name Expected: http.openapi.operation.seekToEnd
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.poll.level Expected: INFO
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.deleteConsumer.level Expected: DEBUG
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.subscribe.level Expected: TRACE
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.openapi.level Expected: TRACE
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.commit.name Expected: http.openapi.operation.commit
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.send.level Expected: ERROR
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seek.name Expected: http.openapi.operation.seek
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.healthy.level Expected: ERROR
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.ready.name Expected: http.openapi.operation.ready
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.logger.bridge.level Expected: ERROR
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seek.level Expected: INFO
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.assign.level Expected: TRACE
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.sendToPartition.name Expected: http.openapi.operation.sendToPartition
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.send.name Expected: http.openapi.operation.send
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.openapi.name Expected: http.openapi.operation.openapi
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.healthy.name Expected: http.openapi.operation.healthy
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.commit.level Expected: DEBUG
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToEnd.level Expected: WARN
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToBeginning.name Expected: http.openapi.operation.seekToBeginning
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:476] Check log level setting for logger: logger.seekToBeginning.level Expected: DEBUG
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-24864fa0-bridge-bridge
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-24864fa0-bridge-bridge
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=true
2022-04-06 21:01:47 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-24864fa0-bridge-bridge rolling update
2022-04-06 21:02:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-24864fa0-bridge-bridge will be ready
2022-04-06 21:02:17 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-24864fa0-bridge-bridge is ready
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-24864fa0-bridge-bridge rolling update finished
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-24864fa0-bridge-bridge
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-24864fa0-bridge-bridge
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-24864fa0-bridge-bridge-68c6bb9db4-qf6tk container my-cluster-24864fa0-bridge-bridge will be ready
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [PodUtils:347] Pod my-cluster-24864fa0-bridge-bridge-68c6bb9db4-qf6tk container my-cluster-24864fa0-bridge-bridge is ready
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-24864fa0-bridge-bridge-68c6bb9db4-qf6tk with container my-cluster-24864fa0-bridge-bridge
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:348] Delete all resources for testBridgeLogSetting
2022-04-06 21:02:27 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-24864fa0-bridge in namespace log-setting-st
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-872cb00a-connect is in desired state: Ready
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:287] Checking if Connect has log level set properly
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.I0Itec.zkclient Expected: ERROR
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:476] Check log level setting for logger: connect.root.logger.level Expected: INFO
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.connect.logger.level Expected: DEBUG
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.reflections Expected: WARN
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-872cb00a-connect-connect
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-872cb00a-connect-connect
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 21:02:36 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-872cb00a-connect-connect rolling update
2022-04-06 21:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testBridgeLogSetting-FINISHED
2022-04-06 21:02:37 [ForkJoinPool-3-worker-7] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:03:56 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-872cb00a-connect-connect will be ready
2022-04-06 21:03:56 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-872cb00a-connect-connect is ready
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-872cb00a-connect-connect rolling update finished
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:498] Checking deployment: my-cluster-872cb00a-connect-connect
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:500] Checking container with name: my-cluster-872cb00a-connect-connect
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:343] Waiting for Pod my-cluster-872cb00a-connect-connect-bb9b58bc5-pfqx7 container my-cluster-872cb00a-connect-connect will be ready
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:347] Pod my-cluster-872cb00a-connect-connect-bb9b58bc5-pfqx7 container my-cluster-872cb00a-connect-connect is ready
2022-04-06 21:04:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogSettingST:453] Checking tini process for pod my-cluster-872cb00a-connect-connect-bb9b58bc5-pfqx7 with container my-cluster-872cb00a-connect-connect
2022-04-06 21:04:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:04:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testConnectLogSetting
2022-04-06 21:04:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-872cb00a-connect-allow in namespace log-setting-st
2022-04-06 21:04:07 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-872cb00a-connect-scraper in namespace log-setting-st
2022-04-06 21:04:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-872cb00a-connect in namespace log-setting-st
2022-04-06 21:04:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:04:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testConnectLogSetting-FINISHED
2022-04-06 21:04:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testCruiseControlLogChange-STARTED
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:409] Check that default/actual root logging level is info
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:414] Check logs in CruiseControl - make sure no DEBUG is found there.
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:422] Wait for change of root logger in log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm.
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:04:47 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:04:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:04:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:03 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:13 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:24 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:29 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:39 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:45 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:50 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:50 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:05:55 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:05:55 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:06:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:06:00 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace log-setting-st exec log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm -- grep -i rootlogger.level /opt/cruise-control/custom-config/log4j2.properties
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:428] Check cruise control logs in pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm and it's container cruise-control .
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testCruiseControlLogChange is everything deleted.
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testCruiseControlLogChange-FINISHED
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.log.LogSettingST.testKafkaLogSetting-STARTED
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1847206819-1866665742 in namespace log-setting-st
2022-04-06 21:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1847206819-1866665742 will have desired state: Ready
2022-04-06 21:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1847206819-1866665742 is in desired state: Ready
2022-04-06 21:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1764633403-858819644 in namespace log-setting-st
2022-04-06 21:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1764633403-858819644 will have desired state: Ready
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1764633403-858819644 is in desired state: Ready
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:217] Checking if Kafka, Zookeeper, TO and UO of cluster:log-setting-cluster-name has log level set properly
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.apache.zookeeper Expected: WARN
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.network.Processor Expected: OFF
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.request.logger Expected: FATAL
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.controller Expected: WARN
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: kafka.root.logger.level Expected: INFO
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.state.change.logger Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.network.RequestChannel$ Expected: ERROR
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.server.KafkaApis Expected: INFO
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka Expected: TRACE
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.apache.kafka Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.authorizer.logger Expected: FATAL
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.kafka.log.LogCleaner Expected: TRACE
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: log4j.logger.org.I0Itec.zkclient.ZkClient Expected: ERROR
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.kafka.logger.level Expected: INFO
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: zookeeper.root.logger Expected: OFF
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.zookeeper.logger.level Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: rootLogger.level Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.operator.logger.level Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: rootLogger.level Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:476] Check log level setting for logger: test.operator.logger.level Expected: DEBUG
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:226] Checking if Kafka, Zookeeper, TO and UO of cluster:log-setting-cluster-name has GC logging enabled in stateful sets/deployments
2022-04-06 21:06:07 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-kafka, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-0, container: kafka
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-1, container: kafka
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-2, container: kafka
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-zookeeper, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-zookeeper-0, container: zookeeper
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=true
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: topic-operator
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=true
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: user-operator
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=true
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:232] Changing JVM options - setting GC logging to false
2022-04-06 21:06:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: log-setting-cluster-name-zookeeper rolling update
2022-04-06 21:06:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: log-setting-cluster-name-zookeeper has been successfully rolled
2022-04-06 21:06:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 1 Pod(s) of log-setting-cluster-name-zookeeper to be ready
2022-04-06 21:06:42 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: log-setting-cluster-name-kafka rolling update
2022-04-06 21:07:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: log-setting-cluster-name-kafka has been successfully rolled
2022-04-06 21:07:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of log-setting-cluster-name-kafka to be ready
2022-04-06 21:08:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment log-setting-cluster-name-entity-operator rolling update
2022-04-06 21:08:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: log-setting-cluster-name-entity-operator will be ready
2022-04-06 21:11:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: log-setting-cluster-name-entity-operator is ready
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment log-setting-cluster-name-entity-operator rolling update finished
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:244] Checking if Kafka, Zookeeper, TO and UO of cluster: log-setting-cluster-name has GC logging disabled in stateful sets/deployments
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-kafka, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-0, container: kafka
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-1, container: kafka
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-2, container: kafka
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-zookeeper, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-zookeeper-0, container: zookeeper
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: topic-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: user-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:250] Checking if Kafka, Zookeeper, TO and UO of cluster: gc-set-logging has GC logging disabled in stateful sets/deployments
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-kafka, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-0, container: kafka
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-1, container: kafka
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-kafka-2, container: kafka
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:512] Checking pods with selector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=log-setting-cluster-name-zookeeper, strimzi.io/cluster=log-setting-cluster-name, strimzi.io/kind=Kafka}, additionalProperties={})
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:516] Checking pod log-setting-cluster-name-zookeeper-0, container: zookeeper
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_KAFKA_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: topic-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:490] Checking deployment: log-setting-cluster-name-entity-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:493] Checking container with name: user-operator
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:529] STRIMZI_GC_LOG_ENABLED=false
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm container cruise-control will be ready
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm container cruise-control is ready
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm with container cruise-control
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm container tls-sidecar will be ready
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm container tls-sidecar is ready
2022-04-06 21:12:01 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-cruise-control-7d5f6cc99d-c2wjm with container tls-sidecar
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz container topic-operator will be ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz container topic-operator is ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz with container topic-operator
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz container user-operator will be ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz container user-operator is ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz with container user-operator
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz container tls-sidecar will be ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz container tls-sidecar is ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-entity-operator-d78c874cf-s7flz with container tls-sidecar
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-0 container kafka will be ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-0 container kafka is ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-0 with container kafka
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-1 container kafka will be ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-1 container kafka is ready
2022-04-06 21:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-1 with container kafka
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-2 container kafka will be ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-2 container kafka is ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-2 with container kafka
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-kafka-exporter-7b59f7cd48-vxlff container log-setting-cluster-name-kafka-exporter will be ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-kafka-exporter-7b59f7cd48-vxlff container log-setting-cluster-name-kafka-exporter is ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-kafka-exporter-7b59f7cd48-vxlff with container log-setting-cluster-name-kafka-exporter
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod log-setting-cluster-name-zookeeper-0 container zookeeper will be ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod log-setting-cluster-name-zookeeper-0 container zookeeper is ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod log-setting-cluster-name-zookeeper-0 with container zookeeper
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq container topic-operator will be ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq container topic-operator is ready
2022-04-06 21:12:03 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq with container topic-operator
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq container user-operator will be ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq container user-operator is ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq with container user-operator
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq container tls-sidecar will be ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq container tls-sidecar is ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-entity-operator-6dccd5cb65-hxkhq with container tls-sidecar
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-kafka-0 container kafka will be ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-kafka-0 container kafka is ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-kafka-0 with container kafka
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:343] Waiting for Pod gc-set-logging-zookeeper-0 container zookeeper will be ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:347] Pod gc-set-logging-zookeeper-0 container zookeeper is ready
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [LogSettingST:453] Checking tini process for pod gc-set-logging-zookeeper-0 with container zookeeper
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaLogSetting
2022-04-06 21:12:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1764633403-858819644 in namespace log-setting-st
2022-04-06 21:12:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1847206819-1866665742 in namespace log-setting-st
2022-04-06 21:12:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:12:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.log.LogSettingST.testKafkaLogSetting-FINISHED
2022-04-06 21:12:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:12:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:12:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for LogSettingST
2022-04-06 21:12:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka gc-set-logging in namespace log-setting-st
2022-04-06 21:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka log-setting-cluster-name in namespace log-setting-st
2022-04-06 21:12:14 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace log-setting-st, for cruise control Kafka cluster log-setting-cluster-name
2022-04-06 21:12:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment shared-kafka-clients in namespace log-setting-st
2022-04-06 21:13:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,030.94 s - in io.strimzi.systemtest.log.LogSettingST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.FeatureGatesIsolatedST
2022-04-06 21:13:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testSwitchingStrimziPodSetFeatureGateOnAndOff-STARTED
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:270] Deploying CO with STS - SPS is disabled
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:13:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:13:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:13:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:13:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:13:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:14:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:14:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:14:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:14:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:14:16 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:14:17 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:14:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_FEATURE_GATES, value=-UseStrimziPodSets, valueFrom=null, additionalProperties={})]
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:14:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:14:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:14:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:14:53 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:281] Deploying Kafka
2022-04-06 21:14:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-384d0917 in namespace infra-namespace
2022-04-06 21:14:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-384d0917 will have desired state: Ready
2022-04-06 21:16:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-384d0917 is in desired state: Ready
2022-04-06 21:16:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1499224822-1674057868 in namespace infra-namespace
2022-04-06 21:16:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1499224822-1674057868 will have desired state: Ready
2022-04-06 21:16:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1499224822-1674057868 is in desired state: Ready
2022-04-06 21:16:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 21:16:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-test-315280675 in namespace infra-namespace
2022-04-06 21:16:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-test-1460401245 in namespace infra-namespace
2022-04-06 21:16:20 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-test-315280675 will be in active state
2022-04-06 21:16:21 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-1460401245 will be in active state
2022-04-06 21:16:21 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:304] Changing FG env variable to enable SPS
2022-04-06 21:16:21 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-06 21:16:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:17:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-06 21:17:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-384d0917-zookeeper rolling update
2022-04-06 21:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-384d0917-zookeeper has been successfully rolled
2022-04-06 21:17:38 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-384d0917-zookeeper to be ready
2022-04-06 21:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-384d0917 will have desired state: Ready
2022-04-06 21:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-384d0917 is in desired state: Ready
2022-04-06 21:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-384d0917 is ready
2022-04-06 21:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-384d0917-kafka rolling update
2022-04-06 21:19:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-384d0917-kafka has been successfully rolled
2022-04-06 21:19:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-384d0917-kafka to be ready
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-384d0917 will have desired state: Ready
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-384d0917 is in desired state: Ready
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-384d0917 is ready
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-384d0917 will have desired state: Ready
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-384d0917 is in desired state: Ready
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:319] Changing FG env variable to disable again SPS
2022-04-06 21:19:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-06 21:19:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:20:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:20:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-06 21:20:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-384d0917-zookeeper rolling update
2022-04-06 21:20:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-384d0917-zookeeper has been successfully rolled
2022-04-06 21:20:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-384d0917-zookeeper to be ready
2022-04-06 21:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-384d0917 will have desired state: Ready
2022-04-06 21:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-384d0917 is in desired state: Ready
2022-04-06 21:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-384d0917 is ready
2022-04-06 21:21:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-384d0917-kafka rolling update
2022-04-06 21:22:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-384d0917-kafka has been successfully rolled
2022-04-06 21:22:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-384d0917-kafka to be ready
2022-04-06 21:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-384d0917 will have desired state: Ready
2022-04-06 21:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-384d0917 is in desired state: Ready
2022-04-06 21:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-384d0917 is ready
2022-04-06 21:22:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer producer-test-315280675 and consumer consumer-test-1460401245 finish
2022-04-06 21:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSwitchingStrimziPodSetFeatureGateOnAndOff
2022-04-06 21:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-test-315280675 in namespace infra-namespace
2022-04-06 21:25:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1499224822-1674057868 in namespace infra-namespace
2022-04-06 21:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-test-1460401245 in namespace infra-namespace
2022-04-06 21:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-384d0917 in namespace infra-namespace
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testSwitchingStrimziPodSetFeatureGateOnAndOff-FINISHED
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testControlPlaneListenerFeatureGate-STARTED
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:25:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:25:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:25:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:25:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:25:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:25:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:26:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:26:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:26:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_FEATURE_GATES, value=-ControlPlaneListener, valueFrom=null, additionalProperties={})]
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:26:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:27:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:27:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:27:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:27:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fa311029 in namespace infra-namespace
2022-04-06 21:27:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa311029 will have desired state: Ready
2022-04-06 21:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa311029 is in desired state: Ready
2022-04-06 21:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:96] Check for presence of ContainerPort 9090/tcp (tcp-ctrlplane) in first Kafka pod.
2022-04-06 21:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:104] Try to send some messages to Kafka over next few minutes.
2022-04-06 21:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-987104133-312366266 in namespace infra-namespace
2022-04-06 21:29:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-987104133-312366266 will have desired state: Ready
2022-04-06 21:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-987104133-312366266 is in desired state: Ready
2022-04-06 21:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 21:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-test-1557165888 in namespace infra-namespace
2022-04-06 21:29:20 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-test-1557165888 will be in active state
2022-04-06 21:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-test-805019672 in namespace infra-namespace
2022-04-06 21:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-805019672 will be in active state
2022-04-06 21:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-805019672 will be in active state
2022-04-06 21:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:127] Delete first found Kafka broker pod.
2022-04-06 21:29:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-fa311029-zookeeper to be ready
2022-04-06 21:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa311029 will have desired state: Ready
2022-04-06 21:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa311029 is in desired state: Ready
2022-04-06 21:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-fa311029 is ready
2022-04-06 21:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:131] Force Rolling Update of Kafka via annotation.
2022-04-06 21:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:139] Wait for next reconciliation to happen.
2022-04-06 21:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-fa311029-zookeeper rolling update
2022-04-06 21:30:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-fa311029-zookeeper has been successfully rolled
2022-04-06 21:30:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-fa311029-zookeeper to be ready
2022-04-06 21:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa311029 will have desired state: Ready
2022-04-06 21:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa311029 is in desired state: Ready
2022-04-06 21:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-fa311029 is ready
2022-04-06 21:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:142] Waiting for clients to finish sending/receiving messages.
2022-04-06 21:31:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-test-1557165888 to finished
2022-04-06 21:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-test-805019672 to finished
2022-04-06 21:32:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:32:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testControlPlaneListenerFeatureGate
2022-04-06 21:32:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-test-1557165888 in namespace infra-namespace
2022-04-06 21:32:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-987104133-312366266 in namespace infra-namespace
2022-04-06 21:32:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-test-805019672 in namespace infra-namespace
2022-04-06 21:32:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fa311029 in namespace infra-namespace
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testControlPlaneListenerFeatureGate-FINISHED
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testStrimziPodSetsFeatureGate-STARTED
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:32:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:32:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:32:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_FEATURE_GATES, value=+UseStrimziPodSets, valueFrom=null, additionalProperties={})]
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:32:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:33:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-25854aa6 in namespace infra-namespace
2022-04-06 21:33:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25854aa6 will have desired state: Ready
2022-04-06 21:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25854aa6 is in desired state: Ready
2022-04-06 21:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:182] Try to send some messages to Kafka over next few minutes.
2022-04-06 21:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-880826922-1399500129 in namespace infra-namespace
2022-04-06 21:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-880826922-1399500129 will have desired state: Ready
2022-04-06 21:36:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-880826922-1399500129 is in desired state: Ready
2022-04-06 21:36:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 21:36:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-test-2017231987 in namespace infra-namespace
2022-04-06 21:36:08 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-test-2017231987 will be in active state
2022-04-06 21:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-test-1148392831 in namespace infra-namespace
2022-04-06 21:36:09 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-1148392831 will be in active state
2022-04-06 21:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-test-1148392831 will be in active state
2022-04-06 21:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:207] Delete first found ZooKeeper pod my-cluster-25854aa6-zookeeper-0
2022-04-06 21:36:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-25854aa6-zookeeper to be ready
2022-04-06 21:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25854aa6 will have desired state: Ready
2022-04-06 21:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25854aa6 is in desired state: Ready
2022-04-06 21:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-25854aa6 is ready
2022-04-06 21:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:213] Delete first found Kafka broker pod my-cluster-25854aa6-kafka-0
2022-04-06 21:36:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-25854aa6-kafka to be ready
2022-04-06 21:37:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25854aa6 will have desired state: Ready
2022-04-06 21:37:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25854aa6 is in desired state: Ready
2022-04-06 21:37:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-25854aa6 is ready
2022-04-06 21:37:41 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:218] Force Rolling Update of ZooKeeper via annotation.
2022-04-06 21:37:41 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:228] Wait for next reconciliation to happen.
2022-04-06 21:37:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-25854aa6-zookeeper rolling update
2022-04-06 21:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-25854aa6-zookeeper has been successfully rolled
2022-04-06 21:39:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-25854aa6-zookeeper to be ready
2022-04-06 21:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25854aa6 will have desired state: Ready
2022-04-06 21:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25854aa6 is in desired state: Ready
2022-04-06 21:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-25854aa6 is ready
2022-04-06 21:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:232] Force Rolling Update of Kafka via annotation.
2022-04-06 21:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:242] Wait for next reconciliation to happen.
2022-04-06 21:39:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-25854aa6-kafka rolling update
2022-04-06 21:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-25854aa6-kafka has been successfully rolled
2022-04-06 21:40:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-25854aa6-kafka to be ready
2022-04-06 21:41:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-25854aa6 will have desired state: Ready
2022-04-06 21:41:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-25854aa6 is in desired state: Ready
2022-04-06 21:41:17 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-25854aa6 is ready
2022-04-06 21:41:17 [ForkJoinPool-3-worker-3] [32mINFO [m [FeatureGatesIsolatedST:245] Waiting for clients to finish sending/receiving messages.
2022-04-06 21:41:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-test-2017231987 to finished
2022-04-06 21:41:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-test-1148392831 to finished
2022-04-06 21:41:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:41:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testStrimziPodSetsFeatureGate
2022-04-06 21:41:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-test-2017231987 in namespace infra-namespace
2022-04-06 21:41:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-880826922-1399500129 in namespace infra-namespace
2022-04-06 21:41:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-test-1148392831 in namespace infra-namespace
2022-04-06 21:41:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-25854aa6 in namespace infra-namespace
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.FeatureGatesIsolatedST.testStrimziPodSetsFeatureGate-FINISHED
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context FeatureGatesIsolatedST is everything deleted.
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,699.265 s - in io.strimzi.systemtest.operators.FeatureGatesIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST
2022-04-06 21:41:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST.testNamespacedRbacScopeDeploysRoles-STARTED
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:42:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:42:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:42:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 21:42:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:42:36 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:42:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:42:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_RBAC_SCOPE, value=NAMESPACE, valueFrom=null, additionalProperties={})]
2022-04-06 21:42:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:42:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator3589234371512195493.yaml in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation5403621277165393330.yaml in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:42:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:43:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:43:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ec664c27 in namespace infra-namespace
2022-04-06 21:43:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ec664c27 will have desired state: Ready
2022-04-06 21:45:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ec664c27 is in desired state: Ready
2022-04-06 21:45:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ec664c27 will have desired state: Ready
2022-04-06 21:45:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ec664c27 is in desired state: Ready
2022-04-06 21:45:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:45:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testNamespacedRbacScopeDeploysRoles
2022-04-06 21:45:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ec664c27 in namespace infra-namespace
2022-04-06 21:45:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:45:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST.testNamespacedRbacScopeDeploysRoles-FINISHED
2022-04-06 21:45:12 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:45:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:45:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context NamespaceRbacScopeOperatorIsolatedST is everything deleted.
2022-04-06 21:45:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 212.284 s - in io.strimzi.systemtest.operators.NamespaceRbacScopeOperatorIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.RecoveryIsolatedST
2022-04-06 21:45:13 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperMetricsConfigDeletion-STARTED
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:45:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:45:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:45:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:45:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 21:45:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:45:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:46:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:46:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:46:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 21:46:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:46:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:46:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 21:46:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 21:46:08 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 21:46:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:46:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:46:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:46:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:47:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:47:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1477721073 in namespace infra-namespace
2022-04-06 21:47:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1477721073 will have desired state: Ready
2022-04-06 21:48:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1477721073 is in desired state: Ready
2022-04-06 21:48:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1477721073 in namespace infra-namespace
2022-04-06 21:48:49 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1477721073 will be ready
2022-04-06 21:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1477721073 is ready
2022-04-06 21:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1477721073 in namespace infra-namespace
2022-04-06 21:48:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1477721073 will have desired state: Ready
2022-04-06 21:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1477721073 is in desired state: Ready
2022-04-06 21:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:191] Running deleteZookeeperMetricsConfig with cluster recovery-cluster-1477721073
2022-04-06 21:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:199] Waiting for creation recovery-cluster-1477721073-zookeeper-config
2022-04-06 21:49:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:30] Waiting for config map recovery-cluster-1477721073-zookeeper-config-fb7166f0-8452-4361-a98b-529c11f93fa0 recovery in namespace infra-namespace
2022-04-06 21:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:33] Config map recovery-cluster-1477721073-zookeeper-config was recovered
2022-04-06 21:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperMetricsConfigDeletion
2022-04-06 21:49:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1477721073 in namespace infra-namespace
2022-04-06 21:49:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1477721073 in namespace infra-namespace
2022-04-06 21:49:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1477721073 in namespace infra-namespace
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperMetricsConfigDeletion-FINISHED
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeServiceDeletion-STARTED
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:50:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:50:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:50:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:50:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:50:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:51:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:51:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:51:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 21:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:51:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:51:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:51:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:51:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:51:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:51:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-555947418 in namespace infra-namespace
2022-04-06 21:51:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-555947418 will have desired state: Ready
2022-04-06 21:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-555947418 is in desired state: Ready
2022-04-06 21:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-555947418 in namespace infra-namespace
2022-04-06 21:54:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-555947418 will be ready
2022-04-06 21:54:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-555947418 is ready
2022-04-06 21:54:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-555947418 in namespace infra-namespace
2022-04-06 21:54:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-555947418 will have desired state: Ready
2022-04-06 21:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-555947418 is in desired state: Ready
2022-04-06 21:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:222] Running deleteKafkaBridgeService with cluster recovery-cluster-555947418
2022-04-06 21:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:227] Waiting for service recovery-cluster-555947418-bridge-service recovery
2022-04-06 21:54:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-555947418-bridge-service-e6659b43-b8f2-4055-8879-ee2aeb6b94a8 in namespace infra-namespace will be recovered
2022-04-06 21:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-555947418-bridge-service in namespace infra-namespace is recovered
2022-04-06 21:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaBridgeServiceDeletion
2022-04-06 21:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-555947418 in namespace infra-namespace
2022-04-06 21:54:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-555947418 in namespace infra-namespace
2022-04-06 21:54:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-555947418 in namespace infra-namespace
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeServiceDeletion-FINISHED
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaHeadlessServiceDeletion-STARTED
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:55:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:55:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:55:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:56:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:56:04 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:56:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 21:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 21:56:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 21:56:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 21:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 21:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-111573973 in namespace infra-namespace
2022-04-06 21:56:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-111573973 will have desired state: Ready
2022-04-06 21:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-111573973 is in desired state: Ready
2022-04-06 21:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-111573973 in namespace infra-namespace
2022-04-06 21:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-111573973 will be ready
2022-04-06 21:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-111573973 is ready
2022-04-06 21:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-111573973 in namespace infra-namespace
2022-04-06 21:58:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-111573973 will have desired state: Ready
2022-04-06 21:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-111573973 is in desired state: Ready
2022-04-06 21:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:143] Running deleteKafkaHeadlessService with cluster recovery-cluster-111573973
2022-04-06 21:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:150] Waiting for creation recovery-cluster-111573973-kafka-brokers
2022-04-06 21:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-111573973-kafka-brokers-ccce2d9a-36e7-4704-949a-b5a8f5f89b2c in namespace infra-namespace will be recovered
2022-04-06 21:59:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-111573973-kafka-brokers in namespace infra-namespace is recovered
2022-04-06 21:59:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:59:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaHeadlessServiceDeletion
2022-04-06 21:59:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-111573973 in namespace infra-namespace
2022-04-06 21:59:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-111573973 in namespace infra-namespace
2022-04-06 21:59:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-111573973 in namespace infra-namespace
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaHeadlessServiceDeletion-FINISHED
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaStatefulSetDeletion-STARTED
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 21:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 21:59:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:00:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:00:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:00:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:00:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:00:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:01:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:01:03 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-2016520011 in namespace infra-namespace
2022-04-06 22:01:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-2016520011 will have desired state: Ready
2022-04-06 22:02:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-2016520011 is in desired state: Ready
2022-04-06 22:02:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-2016520011 in namespace infra-namespace
2022-04-06 22:02:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-2016520011 will be ready
2022-04-06 22:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-2016520011 is ready
2022-04-06 22:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-2016520011 in namespace infra-namespace
2022-04-06 22:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-2016520011 will have desired state: Ready
2022-04-06 22:02:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-2016520011 is in desired state: Ready
2022-04-06 22:02:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-2016520011-kafka will be deleted
2022-04-06 22:02:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2016520011-kafka-0 will be deleted
2022-04-06 22:03:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2016520011-kafka-0 deleted
2022-04-06 22:03:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2016520011-kafka-1 will be deleted
2022-04-06 22:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2016520011-kafka-1 deleted
2022-04-06 22:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-2016520011-kafka-2 will be deleted
2022-04-06 22:03:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-2016520011-kafka-2 deleted
2022-04-06 22:03:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:90] Waiting for recovery recovery-cluster-2016520011-kafka
2022-04-06 22:03:25 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:73] Waiting for StatefulSet recovery-cluster-2016520011-kafka-768340ce-6956-4253-bb97-d424ac9f10d5 recovery in namespace infra-namespace
2022-04-06 22:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:76] StatefulSet recovery-cluster-2016520011-kafka was recovered
2022-04-06 22:03:37 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:39] Waiting for StatefulSet recovery-cluster-2016520011-kafka to be ready
2022-04-06 22:04:02 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:44] Waiting for 3 Pod(s) of StatefulSet recovery-cluster-2016520011-kafka to be ready
2022-04-06 22:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:47] StatefulSet recovery-cluster-2016520011-kafka is ready
2022-04-06 22:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaStatefulSetDeletion
2022-04-06 22:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-2016520011 in namespace infra-namespace
2022-04-06 22:04:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-2016520011 in namespace infra-namespace
2022-04-06 22:04:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-2016520011 in namespace infra-namespace
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaStatefulSetDeletion-FINISHED
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [StrimziPodSetTestCondition:23] According to STRIMZI_FEATURE_GATES env variable with value: , the StatefulSets are used, skipping this StrimziPodSet related test
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromEntityOperatorDeletion-STARTED
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:04:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:05:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:05:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:05:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:05:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:05:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:05:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-120123191 in namespace infra-namespace
2022-04-06 22:06:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-120123191 will have desired state: Ready
2022-04-06 22:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-120123191 is in desired state: Ready
2022-04-06 22:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-120123191 in namespace infra-namespace
2022-04-06 22:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-120123191 will be ready
2022-04-06 22:08:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-120123191 is ready
2022-04-06 22:08:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-120123191 in namespace infra-namespace
2022-04-06 22:08:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-120123191 will have desired state: Ready
2022-04-06 22:09:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-120123191 is in desired state: Ready
2022-04-06 22:09:00 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:64] Running testRecoveryFromEntityOperatorDeletion with cluster recovery-cluster-120123191
2022-04-06 22:09:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-120123191-entity-operator will be deleted
2022-04-06 22:09:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-120123191-entity-operator-544bf9969c-tz6d9 will be deleted
2022-04-06 22:09:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-120123191-entity-operator-544bf9969c-tz6d9 deleted
2022-04-06 22:09:15 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:72] Waiting for recovery recovery-cluster-120123191-entity-operator
2022-04-06 22:09:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:154] Waiting for Deployment recovery-cluster-120123191-entity-operator-63d25bac-002d-46e6-913a-4bef1fe8d274 recovery in namespace infra-namespace
2022-04-06 22:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:157] Deployment recovery-cluster-120123191-entity-operator was recovered
2022-04-06 22:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: recovery-cluster-120123191-entity-operator will be ready
2022-04-06 22:09:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: recovery-cluster-120123191-entity-operator is ready
2022-04-06 22:09:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment recovery-cluster-120123191-entity-operator to be ready
2022-04-06 22:09:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment recovery-cluster-120123191-entity-operator is ready
2022-04-06 22:09:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:09:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromEntityOperatorDeletion
2022-04-06 22:09:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-120123191 in namespace infra-namespace
2022-04-06 22:09:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-120123191 in namespace infra-namespace
2022-04-06 22:10:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-120123191 in namespace infra-namespace
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromEntityOperatorDeletion-FINISHED
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeDeploymentDeletion-STARTED
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:10:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:10:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:10:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:11:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:11:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:11:06 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:11:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:11:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:12:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:12:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-545884934 in namespace infra-namespace
2022-04-06 22:12:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-545884934 will have desired state: Ready
2022-04-06 22:13:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-545884934 is in desired state: Ready
2022-04-06 22:13:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-545884934 in namespace infra-namespace
2022-04-06 22:13:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-545884934 will be ready
2022-04-06 22:13:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-545884934 is ready
2022-04-06 22:13:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-545884934 in namespace infra-namespace
2022-04-06 22:13:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-545884934 will have desired state: Ready
2022-04-06 22:14:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-545884934 is in desired state: Ready
2022-04-06 22:14:25 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:206] Running deleteKafkaBridgeDeployment with cluster recovery-cluster-545884934
2022-04-06 22:14:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-545884934-bridge will be deleted
2022-04-06 22:14:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-545884934-bridge-574b859777-42b5s will be deleted
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-545884934-bridge-574b859777-42b5s deleted
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:215] Waiting for deployment recovery-cluster-545884934-bridge recovery
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:154] Waiting for Deployment recovery-cluster-545884934-bridge-4c390205-0125-401c-83b1-1f5e9d2385bd recovery in namespace infra-namespace
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:157] Deployment recovery-cluster-545884934-bridge was recovered
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaBridgeDeploymentDeletion
2022-04-06 22:14:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-545884934 in namespace infra-namespace
2022-04-06 22:14:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-545884934 in namespace infra-namespace
2022-04-06 22:14:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-545884934 in namespace infra-namespace
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeDeploymentDeletion-FINISHED
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaServiceDeletion-STARTED
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:15:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:15:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:15:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:15:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:15:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:16:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:16:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1704164938 in namespace infra-namespace
2022-04-06 22:16:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1704164938 will have desired state: Ready
2022-04-06 22:18:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1704164938 is in desired state: Ready
2022-04-06 22:18:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1704164938 in namespace infra-namespace
2022-04-06 22:18:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1704164938 will be ready
2022-04-06 22:18:49 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1704164938 is ready
2022-04-06 22:18:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1704164938 in namespace infra-namespace
2022-04-06 22:18:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1704164938 will have desired state: Ready
2022-04-06 22:19:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1704164938 is in desired state: Ready
2022-04-06 22:19:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:115] Running deleteKafkaService with cluster recovery-cluster-1704164938
2022-04-06 22:19:14 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:122] Waiting for creation recovery-cluster-1704164938-kafka-bootstrap
2022-04-06 22:19:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-1704164938-kafka-bootstrap-96d7ee2a-756f-416d-8d36-1210c9dc1f0e in namespace infra-namespace will be recovered
2022-04-06 22:19:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-1704164938-kafka-bootstrap in namespace infra-namespace is recovered
2022-04-06 22:19:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:19:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaServiceDeletion
2022-04-06 22:19:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1704164938 in namespace infra-namespace
2022-04-06 22:19:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1704164938 in namespace infra-namespace
2022-04-06 22:19:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1704164938 in namespace infra-namespace
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaServiceDeletion-FINISHED
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperStatefulSetDeletion-STARTED
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:20:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:20:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:20:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:20:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:20:45 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:20:46 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:21:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:21:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:21:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:21:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1945054772 in namespace infra-namespace
2022-04-06 22:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1945054772 will have desired state: Ready
2022-04-06 22:24:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1945054772 is in desired state: Ready
2022-04-06 22:24:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1945054772 in namespace infra-namespace
2022-04-06 22:24:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1945054772 will be ready
2022-04-06 22:24:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1945054772 is ready
2022-04-06 22:24:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1945054772 in namespace infra-namespace
2022-04-06 22:24:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1945054772 will have desired state: Ready
2022-04-06 22:24:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1945054772 is in desired state: Ready
2022-04-06 22:24:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:121] Waiting when all Pods with prefix recovery-cluster-1945054772-zookeeper will be deleted
2022-04-06 22:24:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-1945054772-zookeeper-0 will be deleted
2022-04-06 22:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-1945054772-zookeeper-0 deleted
2022-04-06 22:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-1945054772-zookeeper-1 will be deleted
2022-04-06 22:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-1945054772-zookeeper-1 deleted
2022-04-06 22:24:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:154] Waiting when Pod recovery-cluster-1945054772-zookeeper-2 will be deleted
2022-04-06 22:24:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:171] Pod recovery-cluster-1945054772-zookeeper-2 deleted
2022-04-06 22:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:107] Waiting for recovery recovery-cluster-1945054772-zookeeper
2022-04-06 22:24:48 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:73] Waiting for StatefulSet recovery-cluster-1945054772-zookeeper-1df3fdb5-c306-402f-bd67-c4ecd09fffc4 recovery in namespace infra-namespace
2022-04-06 22:24:57 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:76] StatefulSet recovery-cluster-1945054772-zookeeper was recovered
2022-04-06 22:24:57 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:39] Waiting for StatefulSet recovery-cluster-1945054772-zookeeper to be ready
2022-04-06 22:25:23 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:44] Waiting for 3 Pod(s) of StatefulSet recovery-cluster-1945054772-zookeeper to be ready
2022-04-06 22:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [StatefulSetUtils:47] StatefulSet recovery-cluster-1945054772-zookeeper is ready
2022-04-06 22:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperStatefulSetDeletion
2022-04-06 22:25:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1945054772 in namespace infra-namespace
2022-04-06 22:25:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1945054772 in namespace infra-namespace
2022-04-06 22:25:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1945054772 in namespace infra-namespace
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperStatefulSetDeletion-FINISHED
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeMetricsConfigDeletion-STARTED
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:26:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:26:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:26:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:26:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:26:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:27:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:27:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:27:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:27:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:27:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:27:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1092145148 in namespace infra-namespace
2022-04-06 22:27:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1092145148 will have desired state: Ready
2022-04-06 22:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1092145148 is in desired state: Ready
2022-04-06 22:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1092145148 in namespace infra-namespace
2022-04-06 22:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1092145148 will be ready
2022-04-06 22:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1092145148 is ready
2022-04-06 22:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1092145148 in namespace infra-namespace
2022-04-06 22:29:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1092145148 will have desired state: Ready
2022-04-06 22:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1092145148 is in desired state: Ready
2022-04-06 22:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:234] Running deleteKafkaBridgeMetricsConfig with cluster recovery-cluster-1092145148
2022-04-06 22:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:239] Waiting for metric config recovery-cluster-1092145148-bridge-config re-creation
2022-04-06 22:30:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:30] Waiting for config map recovery-cluster-1092145148-bridge-config-3354e8e3-2c85-430e-beac-bc33af187d52 recovery in namespace infra-namespace
2022-04-06 22:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:33] Config map recovery-cluster-1092145148-bridge-config was recovered
2022-04-06 22:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaBridgeMetricsConfigDeletion
2022-04-06 22:30:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1092145148 in namespace infra-namespace
2022-04-06 22:30:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1092145148 in namespace infra-namespace
2022-04-06 22:30:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1092145148 in namespace infra-namespace
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaBridgeMetricsConfigDeletion-FINISHED
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperHeadlessServiceDeletion-STARTED
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:31:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:31:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:36 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:31:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:31:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:31:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:31:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:31:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:31:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:31:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:31:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:31:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:31:47 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:31:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:32:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:32:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-331784938 in namespace infra-namespace
2022-04-06 22:32:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-331784938 will have desired state: Ready
2022-04-06 22:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-331784938 is in desired state: Ready
2022-04-06 22:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-331784938 in namespace infra-namespace
2022-04-06 22:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-331784938 will be ready
2022-04-06 22:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-331784938 is ready
2022-04-06 22:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-331784938 in namespace infra-namespace
2022-04-06 22:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-331784938 will have desired state: Ready
2022-04-06 22:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-331784938 is in desired state: Ready
2022-04-06 22:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:157] Running deleteKafkaHeadlessService with cluster recovery-cluster-331784938
2022-04-06 22:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:164] Waiting for creation recovery-cluster-331784938-zookeeper-nodes
2022-04-06 22:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-331784938-zookeeper-nodes-6b3af5bf-3479-4ce8-a71b-6116543d61bc in namespace infra-namespace will be recovered
2022-04-06 22:34:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-331784938-zookeeper-nodes in namespace infra-namespace is recovered
2022-04-06 22:34:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:34:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperHeadlessServiceDeletion
2022-04-06 22:34:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-331784938 in namespace infra-namespace
2022-04-06 22:34:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-331784938 in namespace infra-namespace
2022-04-06 22:34:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-331784938 in namespace infra-namespace
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperHeadlessServiceDeletion-FINISHED
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaMetricsConfigDeletion-STARTED
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:35:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:35:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:35:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:36:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:36:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:36:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:37:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1722279484 in namespace infra-namespace
2022-04-06 22:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1722279484 will have desired state: Ready
2022-04-06 22:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1722279484 is in desired state: Ready
2022-04-06 22:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1722279484 in namespace infra-namespace
2022-04-06 22:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1722279484 will be ready
2022-04-06 22:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1722279484 is ready
2022-04-06 22:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1722279484 in namespace infra-namespace
2022-04-06 22:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1722279484 will have desired state: Ready
2022-04-06 22:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1722279484 is in desired state: Ready
2022-04-06 22:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:171] Running deleteKafkaMetricsConfig with cluster recovery-cluster-1722279484
2022-04-06 22:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:185] Waiting for creation recovery-cluster-1722279484-kafka-config
2022-04-06 22:38:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:30] Waiting for config map recovery-cluster-1722279484-kafka-config-95a3a9ed-28dd-4abe-9a2a-df7eb3686fcc recovery in namespace infra-namespace
2022-04-06 22:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ConfigMapUtils:33] Config map recovery-cluster-1722279484-kafka-config was recovered
2022-04-06 22:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromKafkaMetricsConfigDeletion
2022-04-06 22:39:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1722279484 in namespace infra-namespace
2022-04-06 22:39:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1722279484 in namespace infra-namespace
2022-04-06 22:39:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1722279484 in namespace infra-namespace
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromKafkaMetricsConfigDeletion-FINISHED
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperServiceDeletion-STARTED
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:39:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:39:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:39:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:39:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:39:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:39:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:40:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:40:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:40:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:40:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:40:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:40:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:41:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:41:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-849276728 in namespace infra-namespace
2022-04-06 22:41:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-849276728 will have desired state: Ready
2022-04-06 22:43:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-849276728 is in desired state: Ready
2022-04-06 22:43:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-849276728 in namespace infra-namespace
2022-04-06 22:43:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-849276728 will be ready
2022-04-06 22:43:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-849276728 is ready
2022-04-06 22:43:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-849276728 in namespace infra-namespace
2022-04-06 22:43:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-849276728 will have desired state: Ready
2022-04-06 22:43:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-849276728 is in desired state: Ready
2022-04-06 22:43:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:129] Running deleteKafkaService with cluster recovery-cluster-849276728
2022-04-06 22:43:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RecoveryIsolatedST:136] Waiting for creation recovery-cluster-849276728-zookeeper-client
2022-04-06 22:43:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:58] Waiting when Service recovery-cluster-849276728-zookeeper-client-7f76818c-339b-40a8-b82b-f50212ec59e3 in namespace infra-namespace will be recovered
2022-04-06 22:48:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ServiceUtils:62] recovery-cluster-849276728-zookeeper-client in namespace infra-namespace is recovered
2022-04-06 22:48:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:48:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromZookeeperServiceDeletion
2022-04-06 22:48:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-849276728 in namespace infra-namespace
2022-04-06 22:48:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-849276728 in namespace infra-namespace
2022-04-06 22:48:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-849276728 in namespace infra-namespace
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromZookeeperServiceDeletion-FINISHED
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromImpossibleMemoryRequest-STARTED
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 22:49:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:49:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:49:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:49:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 22:49:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 22:50:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 22:50:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 22:50:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 22:50:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka recovery-cluster-1400754557 in namespace infra-namespace
2022-04-06 22:50:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1400754557 will have desired state: Ready
2022-04-06 22:52:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1400754557 is in desired state: Ready
2022-04-06 22:52:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment kafka-clients-recovery-cluster-1400754557 in namespace infra-namespace
2022-04-06 22:52:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: kafka-clients-recovery-cluster-1400754557 will be ready
2022-04-06 22:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: kafka-clients-recovery-cluster-1400754557 is ready
2022-04-06 22:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge recovery-cluster-1400754557 in namespace infra-namespace
2022-04-06 22:52:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: recovery-cluster-1400754557 will have desired state: Ready
2022-04-06 22:52:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: recovery-cluster-1400754557 is in desired state: Ready
2022-04-06 22:52:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: recovery-cluster-1400754557-kafka will be in pending phase
2022-04-06 22:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:306] Verify that all pods with prefix: recovery-cluster-1400754557-kafka are stable in pending phase
2022-04-06 22:53:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 50
2022-04-06 22:53:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 49
2022-04-06 22:53:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 48
2022-04-06 22:53:16 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 47
2022-04-06 22:53:17 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 46
2022-04-06 22:53:18 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 45
2022-04-06 22:53:19 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 44
2022-04-06 22:53:20 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 43
2022-04-06 22:53:21 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 42
2022-04-06 22:53:22 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 41
2022-04-06 22:53:23 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 40
2022-04-06 22:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 39
2022-04-06 22:53:25 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 38
2022-04-06 22:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 37
2022-04-06 22:53:27 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 36
2022-04-06 22:53:28 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 35
2022-04-06 22:53:29 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 34
2022-04-06 22:53:30 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 33
2022-04-06 22:53:31 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 32
2022-04-06 22:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 31
2022-04-06 22:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 30
2022-04-06 22:53:34 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 29
2022-04-06 22:53:35 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 28
2022-04-06 22:53:36 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 27
2022-04-06 22:53:37 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 26
2022-04-06 22:53:38 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 25
2022-04-06 22:53:39 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 24
2022-04-06 22:53:40 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 23
2022-04-06 22:53:41 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 22
2022-04-06 22:53:42 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 21
2022-04-06 22:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 20
2022-04-06 22:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 19
2022-04-06 22:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 18
2022-04-06 22:53:46 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 17
2022-04-06 22:53:47 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 16
2022-04-06 22:53:48 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 15
2022-04-06 22:53:49 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 14
2022-04-06 22:53:50 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 13
2022-04-06 22:53:51 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 12
2022-04-06 22:53:52 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 11
2022-04-06 22:53:53 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 10
2022-04-06 22:53:54 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 9
2022-04-06 22:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 8
2022-04-06 22:53:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 7
2022-04-06 22:53:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 6
2022-04-06 22:53:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 5
2022-04-06 22:53:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 4
2022-04-06 22:54:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 3
2022-04-06 22:54:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 2
2022-04-06 22:54:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:322] Pod recovery-cluster-1400754557-kafka-0 is in the Pending state. Remaining seconds pod to be stable 1
2022-04-06 22:54:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:335] All pods are stable recovery-cluster-1400754557-kafka-0
2022-04-06 22:54:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of recovery-cluster-1400754557-kafka to be ready
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1400754557 will have desired state: Ready
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1400754557 is in desired state: Ready
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: recovery-cluster-1400754557 is ready
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: recovery-cluster-1400754557 will have desired state: Ready
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: recovery-cluster-1400754557 is in desired state: Ready
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRecoveryFromImpossibleMemoryRequest
2022-04-06 23:00:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment kafka-clients-recovery-cluster-1400754557 in namespace infra-namespace
2022-04-06 23:00:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka recovery-cluster-1400754557 in namespace infra-namespace
2022-04-06 23:00:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge recovery-cluster-1400754557 in namespace infra-namespace
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.RecoveryIsolatedST.testRecoveryFromImpossibleMemoryRequest-FINISHED
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context RecoveryIsolatedST is everything deleted.
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 14, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 4,546.431 s - in io.strimzi.systemtest.operators.RecoveryIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST
2022-04-06 23:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 23:01:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:01:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:01:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:01:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:01:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:01:54 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:02:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:02:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:02:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:02:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 23:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 23:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:04:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:04:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2043299187-274155238 in namespace infra-namespace
2022-04-06 23:04:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2043299187-274155238 will have desired state: Ready
2022-04-06 23:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2043299187-274155238 is in desired state: Ready
2022-04-06 23:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-06 23:04:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-shared-kafka-clients will be ready
2022-04-06 23:04:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-shared-kafka-clients is ready
2022-04-06 23:04:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:04:10 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:04:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatus-STARTED
2022-04-06 23:04:10 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicChangingInSyncReplicasStatus-STARTED
2022-04-06 23:04:10 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:04:10 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:04:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-732039879-1098846008 in namespace infra-namespace
2022-04-06 23:04:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-711b0ca5-mirror-maker in namespace infra-namespace
2022-04-06 23:04:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-732039879-1098846008 will have desired state: Ready
2022-04-06 23:04:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker will have desired state: Ready
2022-04-06 23:04:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-732039879-1098846008 is in desired state: Ready
2022-04-06 23:04:11 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:481] Changing min.insync.replicas to random char
2022-04-06 23:04:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-732039879-1098846008 will have desired state: NotReady
2022-04-06 23:04:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-732039879-1098846008 is in desired state: NotReady
2022-04-06 23:04:12 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:488] Wait 245000 ms for next reconciliation
2022-04-06 23:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker is in desired state: Ready
2022-04-06 23:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker will have desired state: Ready
2022-04-06 23:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker is in desired state: Ready
2022-04-06 23:05:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker will have desired state: NotReady
2022-04-06 23:05:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker is in desired state: NotReady
2022-04-06 23:05:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker will have desired state: Ready
2022-04-06 23:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-711b0ca5-mirror-maker is in desired state: Ready
2022-04-06 23:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMakerStatus
2022-04-06 23:07:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-711b0ca5-mirror-maker in namespace infra-namespace
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatus-FINISHED
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicDecreaseStatus-STARTED
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-445268163-1689205818 in namespace infra-namespace
2022-04-06 23:07:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-445268163-1689205818 will have desired state: Ready
2022-04-06 23:07:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-445268163-1689205818 is in desired state: Ready
2022-04-06 23:07:36 [ForkJoinPool-3-worker-3] [32mINFO [m [CustomResourceStatusIsolatedST:457] Decreasing number of partitions to 1
2022-04-06 23:07:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-445268163-1689205818
2022-04-06 23:07:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-445268163-1689205818 will have desired state: NotReady
2022-04-06 23:07:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-445268163-1689205818 is in desired state: NotReady
2022-04-06 23:07:37 [ForkJoinPool-3-worker-3] [32mINFO [m [CustomResourceStatusIsolatedST:465] Wait 245000 ms for next reconciliation
2022-04-06 23:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicChangingInSyncReplicasStatus
2022-04-06 23:08:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-732039879-1098846008 in namespace infra-namespace
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicChangingInSyncReplicasStatus-FINISHED
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatusNotReady-STARTED
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace infra-namespace
2022-04-06 23:08:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: NotReady
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: NotReady
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:179] Checking status of deployed KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:181] KafkaUser Status: True
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:182] KafkaUser Type: NotReady
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:183] KafkaUser Message: Spec cannot be null
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:184] KafkaUser Reason: InvalidResourceException
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:186] KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: NotReady
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUserUtils:62] Waiting for KafkaUser deletion sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaUserUtils:75] KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef deleted
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaUserStatusNotReady
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser sasl-use-rabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace infra-namespace
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatusNotReady-FINISHED
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatus-STARTED
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-272419539-1026817487 in namespace infra-namespace
2022-04-06 23:08:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-272419539-1026817487 will have desired state: Ready
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-272419539-1026817487 is in desired state: Ready
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:162] Checking status of deployed KafkaUser
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:164] KafkaUser Status: True
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:165] KafkaUser Type: Ready
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:167] KafkaUser is in desired state: Ready
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaUserStatus
2022-04-06 23:08:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-272419539-1026817487 in namespace infra-namespace
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaUserStatus-FINISHED
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatusNotReady-STARTED
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1693723700-872226605 in namespace infra-namespace
2022-04-06 23:08:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1693723700-872226605 will have desired state: NotReady
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1693723700-872226605 is in desired state: NotReady
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-1693723700-872226605 deletion
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicStatusNotReady
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1693723700-872226605 in namespace infra-namespace
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatusNotReady-FINISHED
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaStatusCertificate-STARTED
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [CustomResourceStatusIsolatedST:381] Check if KafkaStatus certificates are the same as secret certificates
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaStatusCertificate is everything deleted.
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaStatusCertificate-FINISHED
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaBridgeStatus-STARTED
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:08:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:09:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:09:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:09:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:09:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-06 23:09:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-06 23:09:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeStatus
2022-04-06 23:10:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaBridgeStatus-FINISHED
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatus-STARTED
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-855366652-2071001333 in namespace infra-namespace
2022-04-06 23:10:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-855366652-2071001333 will have desired state: Ready
2022-04-06 23:10:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-855366652-2071001333 is in desired state: Ready
2022-04-06 23:10:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-855366652-2071001333 will have desired state: Ready
2022-04-06 23:10:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-855366652-2071001333 is in desired state: Ready
2022-04-06 23:10:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:10:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicStatus
2022-04-06 23:10:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-855366652-2071001333 in namespace infra-namespace
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicStatus-FINISHED
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2Status-STARTED
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9a083726 in namespace infra-namespace
2022-04-06 23:11:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a083726 will have desired state: Ready
2022-04-06 23:11:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:11:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicDecreaseStatus
2022-04-06 23:11:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-445268163-1689205818 in namespace infra-namespace
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaTopicDecreaseStatus-FINISHED
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2WrongBootstrap-STARTED
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-763a66c6-mirror-maker-2 in namespace infra-namespace
2022-04-06 23:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-763a66c6-mirror-maker-2 will have desired state: NotReady
2022-04-06 23:12:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a083726 is in desired state: Ready
2022-04-06 23:12:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-9a083726-mirror-maker-2 in namespace infra-namespace
2022-04-06 23:12:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 will have desired state: Ready
2022-04-06 23:12:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-763a66c6-mirror-maker-2 is in desired state: NotReady
2022-04-06 23:12:24 [ForkJoinPool-3-worker-3] [33mWARN [m [DeploymentUtils:213] Deployment my-cluster-763a66c6-mirror-maker-2-mirrormaker2 is not deleted yet! Triggering force delete by cmd client!
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMaker2WrongBootstrap
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-763a66c6-mirror-maker-2 in namespace infra-namespace
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2WrongBootstrap-FINISHED
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectAndConnectorStatus-STARTED
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment custom-resource-status-cluster-name-scraper in namespace infra-namespace
2022-04-06 23:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: custom-resource-status-cluster-name-scraper will be ready
2022-04-06 23:12:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: custom-resource-status-cluster-name-scraper is ready
2022-04-06 23:12:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment custom-resource-status-cluster-name-scraper to be ready
2022-04-06 23:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment custom-resource-status-cluster-name-scraper is ready
2022-04-06 23:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to custom-resource-status-cluster-name-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 23:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy custom-resource-status-cluster-name-allow in namespace infra-namespace
2022-04-06 23:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 23:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:13:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 is in desired state: Ready
2022-04-06 23:13:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 will have desired state: Ready
2022-04-06 23:13:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 is in desired state: Ready
2022-04-06 23:13:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 will have desired state: NotReady
2022-04-06 23:13:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:13:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:13:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:13:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-06 23:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 is in desired state: NotReady
2022-04-06 23:13:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 will have desired state: Ready
2022-04-06 23:14:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-06 23:14:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:15:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-9a083726-mirror-maker-2 is in desired state: Ready
2022-04-06 23:15:48 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-9a083726-mirror-maker-2-mirrormaker2 are stable
2022-04-06 23:15:48 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 50
2022-04-06 23:15:49 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 49
2022-04-06 23:15:50 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 48
2022-04-06 23:15:51 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 47
2022-04-06 23:15:52 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 46
2022-04-06 23:15:53 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 45
2022-04-06 23:15:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:15:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-06 23:15:54 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 44
2022-04-06 23:15:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-06 23:15:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:15:55 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 43
2022-04-06 23:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:15:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: NotReady
2022-04-06 23:15:56 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 42
2022-04-06 23:15:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: NotReady
2022-04-06 23:15:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: custom-resource-status-cluster-name will have desired state: Ready
2022-04-06 23:15:57 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 41
2022-04-06 23:15:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: custom-resource-status-cluster-name is in desired state: Ready
2022-04-06 23:15:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:15:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectAndConnectorStatus
2022-04-06 23:15:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:15:58 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 40
2022-04-06 23:15:59 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 39
2022-04-06 23:16:00 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 38
2022-04-06 23:16:01 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 37
2022-04-06 23:16:02 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 36
2022-04-06 23:16:03 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 35
2022-04-06 23:16:04 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 34
2022-04-06 23:16:05 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 33
2022-04-06 23:16:06 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 32
2022-04-06 23:16:07 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 31
2022-04-06 23:16:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:16:08 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 30
2022-04-06 23:16:09 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 29
2022-04-06 23:16:10 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 28
2022-04-06 23:16:11 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 27
2022-04-06 23:16:12 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 26
2022-04-06 23:16:13 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 25
2022-04-06 23:16:14 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 24
2022-04-06 23:16:15 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 23
2022-04-06 23:16:16 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 22
2022-04-06 23:16:17 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 21
2022-04-06 23:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy custom-resource-status-cluster-name-allow in namespace infra-namespace
2022-04-06 23:16:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment custom-resource-status-cluster-name-scraper in namespace infra-namespace
2022-04-06 23:16:18 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 20
2022-04-06 23:16:19 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 19
2022-04-06 23:16:20 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 18
2022-04-06 23:16:21 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 17
2022-04-06 23:16:22 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 16
2022-04-06 23:16:23 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 15
2022-04-06 23:16:24 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 14
2022-04-06 23:16:25 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 13
2022-04-06 23:16:26 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 12
2022-04-06 23:16:27 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 11
2022-04-06 23:16:28 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 10
2022-04-06 23:16:29 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 9
2022-04-06 23:16:30 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 8
2022-04-06 23:16:31 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 7
2022-04-06 23:16:32 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 6
2022-04-06 23:16:33 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 5
2022-04-06 23:16:34 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 4
2022-04-06 23:16:35 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 3
2022-04-06 23:16:36 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 2
2022-04-06 23:16:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:322] Pod my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574 is in the Running state. Remaining seconds pod to be stable 1
2022-04-06 23:16:37 [ForkJoinPool-3-worker-1] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-9a083726-mirror-maker-2-mirrormaker2-5774d4947fnl574
2022-04-06 23:16:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:16:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMaker2Status
2022-04-06 23:16:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-9a083726-mirror-maker-2 in namespace infra-namespace
2022-04-06 23:16:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9a083726 in namespace infra-namespace
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMaker2Status-FINISHED
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectorWithoutClusterConfig-STARTED
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-8c73bc59 in namespace infra-namespace
2022-04-06 23:16:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-8c73bc59 will have desired state: NotReady
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectAndConnectorStatus-FINISHED
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatusWrongBootstrap-STARTED
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-5d6bc092 in namespace infra-namespace
2022-04-06 23:16:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-5d6bc092 will have desired state: Ready
2022-04-06 23:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-8c73bc59 is in desired state: NotReady
2022-04-06 23:16:58 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectorUtils:98] KafkaConnector: my-cluster-8c73bc59 is not deleted yet, triggering force delete
2022-04-06 23:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectorWithoutClusterConfig
2022-04-06 23:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-8c73bc59 in namespace infra-namespace
2022-04-06 23:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaConnectorWithoutClusterConfig-FINISHED
2022-04-06 23:16:59 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:18:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-5d6bc092 is in desired state: Ready
2022-04-06 23:18:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-5d6bc092 will have desired state: Ready
2022-04-06 23:18:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-5d6bc092 is in desired state: Ready
2022-04-06 23:18:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-5d6bc092 will have desired state: NotReady
2022-04-06 23:18:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-5d6bc092 is in desired state: NotReady
2022-04-06 23:18:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-5d6bc092 will have desired state: Ready
2022-04-06 23:19:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-5d6bc092 is in desired state: Ready
2022-04-06 23:19:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:19:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMakerStatusWrongBootstrap
2022-04-06 23:19:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-5d6bc092 in namespace infra-namespace
2022-04-06 23:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST.testKafkaMirrorMakerStatusWrongBootstrap-FINISHED
2022-04-06 23:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for CustomResourceStatusIsolatedST
2022-04-06 23:19:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2043299187-274155238 in namespace infra-namespace
2022-04-06 23:19:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka custom-resource-status-cluster-name in namespace infra-namespace
2022-04-06 23:19:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-06 23:20:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,171.736 s - in io.strimzi.systemtest.operators.CustomResourceStatusIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST
2022-04-06 23:20:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 23:20:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:20:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:21:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:21:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:21:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:21:16 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:21:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:21:26 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:21:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs-STARTED
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating first-strimzi-cluster-operator in multiple-co-cluster-test namespace
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@40256c9a, clusterOperatorName='first-strimzi-cluster-operator', namespaceInstallTo='multiple-co-cluster-test', namespaceToWatch='multiple-co-cluster-test', bindingsNamespaces=[multiple-co-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=first-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=first-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: multiple-co-cluster-test
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-co-cluster-test
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment first-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:21:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: first-strimzi-cluster-operator will be ready
2022-04-06 23:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: first-strimzi-cluster-operator is ready
2022-04-06 23:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment first-strimzi-cluster-operator to be ready
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment first-strimzi-cluster-operator is ready
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating second-strimzi-cluster-operator in multiple-co-cluster-test namespace
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@40256c9a, clusterOperatorName='second-strimzi-cluster-operator', namespaceInstallTo='multiple-co-cluster-test', namespaceToWatch='multiple-co-cluster-test', bindingsNamespaces=[multiple-co-cluster-test], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=second-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=second-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:254] Environment for ClusterOperator was already prepared! Going to install it now.
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testKafkaCCAndRebalanceWithMultipleCOs from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:22:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-strimzi-cluster-operator will be ready
2022-04-06 23:22:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-strimzi-cluster-operator is ready
2022-04-06 23:22:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment second-strimzi-cluster-operator to be ready
2022-04-06 23:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment second-strimzi-cluster-operator is ready
2022-04-06 23:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:171] Deploying Kafka with {app.kubernetes.io/operator=first-strimzi-cluster-operator} selector of first-strimzi-cluster-operator
2022-04-06 23:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c3f7a150 in namespace multiple-co-cluster-test
2022-04-06 23:23:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c3f7a150 will have desired state: Ready
2022-04-06 23:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c3f7a150 is in desired state: Ready
2022-04-06 23:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:180] Removing CR selector from Kafka and increasing number of replicas to 4, new pod should not appear
2022-04-06 23:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:188] Creating KafkaRebalance when CC doesn't have label for CO, the KR should be ignored
2022-04-06 23:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaRebalance my-cluster-c3f7a150 in namespace multiple-co-cluster-test
2022-04-06 23:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:180] Waiting for cluster stability
2022-04-06 23:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaUtils:208] Kafka cluster is stable after 61 polls.
2022-04-06 23:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:204] Checking if KafkaRebalance is still ignored, after the cluster stability wait
2022-04-06 23:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:209] Adding {app.kubernetes.io/operator=second-strimzi-cluster-operator} selector of second-strimzi-cluster-operator to Kafka
2022-04-06 23:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:212] Waiting for Kafka to scales pods to 4
2022-04-06 23:25:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-c3f7a150-kafka to be ready
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c3f7a150 will have desired state: Ready
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c3f7a150 is in desired state: Ready
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-c3f7a150 is ready
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:75] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): ============================================================================
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:76] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): ProposalReady
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:77] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): ============================================================================
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:90] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): ============================================================================
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:91] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): ProposalReady
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:92] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): ============================================================================
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:94] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): Triggering the rebalance with annotation strimzi.io/rebalance=approve of KafkaRebalance resource
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:63] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): Annotating KafkaRebalance:my-cluster-c3f7a150 with annotation approve
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:98] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): Response from the annotation process kafkarebalance.kafka.strimzi.io/my-cluster-c3f7a150 annotated
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:100] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): Verifying that annotation triggers the Rebalancing state
2022-04-06 23:29:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c3f7a150 will have desired state: Rebalancing
2022-04-06 23:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-c3f7a150 is in desired state: Rebalancing
2022-04-06 23:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRebalanceUtils:104] Reconciliation #8(test) KafkaRebalance(second-co-namespace/my-cluster-c3f7a150): Verifying that KafkaRebalance is in the Ready state
2022-04-06 23:29:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c3f7a150 will have desired state: Ready
2022-04-06 23:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaRebalance: my-cluster-c3f7a150 is in desired state: Ready
2022-04-06 23:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaCCAndRebalanceWithMultipleCOs
2022-04-06 23:29:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:29:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-06 23:29:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment first-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:29:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace multiple-co-cluster-test
2022-04-06 23:29:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment second-strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace multiple-co-cluster-test
2022-04-06 23:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:29:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c3f7a150 in namespace multiple-co-cluster-test
2022-04-06 23:29:59 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace multiple-co-cluster-test, for cruise control Kafka cluster my-cluster-c3f7a150
io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: GET at: https://192.168.49.2:8443/apis/kafka.strimzi.io/v1beta2/namespaces/multiple-co-cluster-test/kafkatopics. Message: Not Found.
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:683)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:662)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:556)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:519)
	at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:503)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.listRequestHelper(BaseOperation.java:133)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.list(BaseOperation.java:415)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.list(BaseOperation.java:404)
	at io.fabric8.kubernetes.client.dsl.base.BaseOperation.list(BaseOperation.java:83)
	at io.strimzi.systemtest.resources.crd.KafkaResource.delete(KafkaResource.java:65)
	at io.strimzi.systemtest.resources.crd.KafkaResource.delete(KafkaResource.java:32)
	at io.strimzi.systemtest.resources.ResourceManager.deleteResource(ResourceManager.java:244)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$createResource$1(ResourceManager.java:217)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$deleteResources$3(ResourceManager.java:360)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.base/java.util.Vector$VectorSpliterator.forEachRemaining(Vector.java:1492)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:290)
	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaRebalance my-cluster-c3f7a150 in namespace multiple-co-cluster-test
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testKafkaCCAndRebalanceWithMultipleCOs-FINISHED
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testMultipleCOsInDifferentNamespaces-STARTED
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding first-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding first-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating first-strimzi-cluster-operator in first-co-namespace namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@4612c0c6, clusterOperatorName='first-strimzi-cluster-operator', namespaceInstallTo='first-co-namespace', namespaceToWatch='*', bindingsNamespaces=[first-co-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=first-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=first-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment first-strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: first-strimzi-cluster-operator will be ready
2022-04-06 23:30:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: first-strimzi-cluster-operator is ready
2022-04-06 23:30:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment first-strimzi-cluster-operator to be ready
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment first-strimzi-cluster-operator is ready
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding second-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding second-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:230] Creating second-strimzi-cluster-operator in second-co-namespace namespace
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.MethodExtensionContext@4612c0c6, clusterOperatorName='second-strimzi-cluster-operator', namespaceInstallTo='second-co-namespace', namespaceToWatch='*', bindingsNamespaces=[second-co-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_RESOURCE_SELECTOR, value=app.kubernetes.io/operator=second-strimzi-cluster-operator, valueFrom=null, additionalProperties={})], extraLabels={app.kubernetes.io/operator=second-strimzi-cluster-operator}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:30:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case testMultipleCOsInDifferentNamespaces from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-strimzi-cluster-operator will be ready
2022-04-06 23:30:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-strimzi-cluster-operator is ready
2022-04-06 23:30:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment second-strimzi-cluster-operator to be ready
2022-04-06 23:30:49 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment second-strimzi-cluster-operator is ready
2022-04-06 23:30:49 [ForkJoinPool-3-worker-3] [33mWARN [m [KubeClusterResource:151] Namespace multiple-co-cluster-test is already created, going to delete it
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: multiple-co-cluster-test
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-co-cluster-test
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: multiple-co-cluster-test
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:100] Deploying Kafka without CR selector
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-5ba5033f in namespace multiple-co-cluster-test
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-5ba5033f will have stable 0 replicas
2022-04-06 23:30:56 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-06 23:30:57 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-06 23:30:58 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-06 23:30:59 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-06 23:31:00 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-06 23:31:01 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-06 23:31:02 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-06 23:31:03 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-06 23:31:04 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-06 23:31:05 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-06 23:31:06 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-06 23:31:07 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-06 23:31:08 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-06 23:31:09 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-06 23:31:10 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-06 23:31:11 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-06 23:31:12 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-06 23:31:13 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-06 23:31:14 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-06 23:31:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-06 23:31:15 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:228] Pod my-cluster-5ba5033f has 0 replicas
2022-04-06 23:31:15 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:110] Adding {app.kubernetes.io/operator=first-strimzi-cluster-operator} selector of first-strimzi-cluster-operator into Kafka CR
2022-04-06 23:31:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-5ba5033f will have desired state: Ready
2022-04-06 23:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-5ba5033f is in desired state: Ready
2022-04-06 23:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1887414120-2105242979 in namespace multiple-co-cluster-test
2022-04-06 23:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-5ba5033f in namespace multiple-co-cluster-test
2022-04-06 23:32:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1887414120-2105242979 will have desired state: Ready
2022-04-06 23:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1887414120-2105242979 is in desired state: Ready
2022-04-06 23:32:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-5ba5033f will have desired state: Ready
2022-04-06 23:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-5ba5033f is in desired state: Ready
2022-04-06 23:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleClusterOperatorsIsolatedST:130] Deploying KafkaConnector with file sink and CR selector - {app.kubernetes.io/operator=second-strimzi-cluster-operator} - different than selector in Kafka
2022-04-06 23:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-5ba5033f in namespace multiple-co-cluster-test
2022-04-06 23:33:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-5ba5033f will have desired state: Ready
2022-04-06 23:33:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-5ba5033f is in desired state: Ready
2022-04-06 23:33:54 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 23:33:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace multiple-co-cluster-test
2022-04-06 23:33:54 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-06 23:33:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:hello-world-producer to finished
2022-04-06 23:34:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-5ba5033f-connect-5bbf7d5bf6-6zmcn
2022-04-06 23:34:03 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-5ba5033f-connect-5bbf7d5bf6-6zmcn
2022-04-06 23:34:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:34:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMultipleCOsInDifferentNamespaces
2022-04-06 23:34:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:34:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:34:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment first-strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding second-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding second-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace first-co-namespace
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:34:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:34:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment second-strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-5ba5033f in namespace multiple-co-cluster-test
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1887414120-2105242979 in namespace multiple-co-cluster-test
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-5ba5033f in namespace multiple-co-cluster-test
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-5ba5033f in namespace multiple-co-cluster-test
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace multiple-co-cluster-test
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-co-namespace
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-co-namespace
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding first-strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding first-strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace first-co-namespace
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST.testMultipleCOsInDifferentNamespaces-FINISHED
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MultipleClusterOperatorsIsolatedST is everything deleted.
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 853.037 s - in io.strimzi.systemtest.operators.MultipleClusterOperatorsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST
2022-04-06 23:34:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorsWhenRackAwarenessIsEnabled-STARTED
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context JUnit Jupiter is everything deleted.
2022-04-06 23:35:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=NAMESPACE, testClassName='null', testMethodName='null'}
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/020-Role-strimzi-cluster-operator-role4932875505339723078.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/021-Role-strimzi-cluster-operator-role6007536178021094330.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/030-Role-strimzi-kafka-broker103177943439750629.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/031-Role-strimzi-entity-operator1115190193215844335.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/033-Role-strimzi-kafka-client6966769522144355876.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator10409061449447270339.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation3113750503198352595.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator17327867411106702093.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation2135829563127815233.yaml in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 23:36:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 23:36:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:99] Deploying Kafka: my-cluster-17e1f5a7, which should not be deployed and error should be present in CR status message
2022-04-06 23:36:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-17e1f5a7 in namespace infra-namespace
2022-04-06 23:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-17e1f5a7-kafka-clients in namespace infra-namespace
2022-04-06 23:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-17e1f5a7-kafka-clients will be ready
2022-04-06 23:37:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-17e1f5a7-kafka-clients is ready
2022-04-06 23:37:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-17e1f5a7-scraper in namespace infra-namespace
2022-04-06 23:37:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-17e1f5a7-scraper will be ready
2022-04-06 23:37:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-17e1f5a7-scraper is ready
2022-04-06 23:37:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-17e1f5a7-scraper to be ready
2022-04-06 23:37:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-17e1f5a7-scraper is ready
2022-04-06 23:37:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-17e1f5a7-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 23:37:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-17e1f5a7-allow in namespace infra-namespace
2022-04-06 23:37:39 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 23:37:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-17e1f5a7 in namespace infra-namespace
2022-04-06 23:37:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:37:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCRBDeletionErrorsWhenRackAwarenessIsEnabled
2022-04-06 23:37:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-17e1f5a7-scraper in namespace infra-namespace
2022-04-06 23:37:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-17e1f5a7-kafka-clients in namespace infra-namespace
2022-04-06 23:38:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-17e1f5a7 in namespace infra-namespace
2022-04-06 23:38:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-17e1f5a7 in namespace infra-namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-17e1f5a7-allow in namespace infra-namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorsWhenRackAwarenessIsEnabled-FINISHED
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorIsIgnoredWhenRackAwarenessIsNotEnabled-STARTED
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:38:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:38:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:39:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:10 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:39:11 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:39:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=NAMESPACE, testClassName='null', testMethodName='null'}
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/020-Role-strimzi-cluster-operator-role17220617158432575058.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRole-strimzi-cluster-operator-role.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/021-Role-strimzi-cluster-operator-role17528317598830838555.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRole-strimzi-kafka-broker.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/030-Role-strimzi-kafka-broker10345817703128898816.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-ClusterRole-strimzi-entity-operator.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/031-Role-strimzi-entity-operator15661235902546123973.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRole-strimzi-kafka-client.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleResource:43] Creating Role from /tmp/033-Role-strimzi-kafka-client9815957088209872446.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator2675005361068558635.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation17764754945565037749.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/020-Role-strimzi-cluster-operator16571469823448149115.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:535] Replaced ClusterRole for Role in /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /tmp/031-Role-strimzi-cluster-operator-entity-operator-delegation5070504291037347162.yaml in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:39:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:39:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:39:58 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 23:40:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 23:40:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:63] Deploying Kafka: my-cluster-91e1cfe2, which should be deployed even the CRBs are not present
2022-04-06 23:40:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-91e1cfe2 in namespace infra-namespace
2022-04-06 23:40:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-91e1cfe2 will have desired state: Ready
2022-04-06 23:41:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-91e1cfe2 is in desired state: Ready
2022-04-06 23:41:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:67] CO log should contain some information about ignoring forbidden access to CRB for Kafka
2022-04-06 23:41:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:71] Deploying KafkaConnect: my-cluster-91e1cfe2 without rack awareness, the CR should be deployed without error
2022-04-06 23:41:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-91e1cfe2 in namespace infra-namespace
2022-04-06 23:41:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-91e1cfe2 will have desired state: Ready
2022-04-06 23:42:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-91e1cfe2 is in desired state: Ready
2022-04-06 23:42:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterOperatorRbacIsolatedST:74] CO log should contain some information about ignoring forbidden access to CRB for KafkaConnect
2022-04-06 23:42:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:42:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCRBDeletionErrorIsIgnoredWhenRackAwarenessIsNotEnabled
2022-04-06 23:42:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-91e1cfe2 in namespace infra-namespace
2022-04-06 23:42:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-91e1cfe2 in namespace infra-namespace
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST.testCRBDeletionErrorIsIgnoredWhenRackAwarenessIsNotEnabled-FINISHED
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ClusterOperatorRbacIsolatedST is everything deleted.
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 484.632 s - in io.strimzi.systemtest.operators.ClusterOperatorRbacIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.bridge.HttpBridgeIsolatedST
2022-04-06 23:42:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:43:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:43:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:43:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:43:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-broker in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-entity-operator in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-kafka-client in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-namespaced in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Role strimzi-cluster-operator-global in namespace infra-namespace
2022-04-06 23:43:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
, extraEnvVars=[EnvVar(name=STRIMZI_CUSTOM_KAFKA_BRIDGE_SERVICE_LABELS, value=app=bar, valueFrom=null, additionalProperties={}), EnvVar(name=STRIMZI_CUSTOM_KAFKA_BRIDGE_SERVICE_ANNOTATIONS, value=bar=app, valueFrom=null, additionalProperties={})]
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:43:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:44:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:44:13 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 23:44:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 23:44:23 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeIsolatedST:434] Deploy Kafka and KafkaBridge before tests
2022-04-06 23:44:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka http-bridge-cluster-name in namespace infra-namespace
2022-04-06 23:44:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: http-bridge-cluster-name will have desired state: Ready
2022-04-06 23:45:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: http-bridge-cluster-name is in desired state: Ready
2022-04-06 23:45:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-06 23:45:30 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-shared-kafka-clients will be ready
2022-04-06 23:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-shared-kafka-clients is ready
2022-04-06 23:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge http-bridge-cluster-name in namespace infra-namespace
2022-04-06 23:45:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-cluster-name will have desired state: Ready
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-cluster-name is in desired state: Ready
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testSendSimpleMessage-STARTED
2022-04-06 23:45:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:45:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testReceiveSimpleMessage-STARTED
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:45:53 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1919129546-1161179765 in namespace infra-namespace
2022-04-06 23:45:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1907131888-262312087 in namespace infra-namespace
2022-04-06 23:45:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1919129546-1161179765 will have desired state: Ready
2022-04-06 23:45:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1907131888-262312087 will have desired state: Ready
2022-04-06 23:45:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1919129546-1161179765 is in desired state: Ready
2022-04-06 23:45:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job producer-1913122693 in namespace infra-namespace
2022-04-06 23:45:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1907131888-262312087 is in desired state: Ready
2022-04-06 23:45:54 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 23:45:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-869419888 in namespace infra-namespace
2022-04-06 23:45:54 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: consumer-869419888 will be in active state
2022-04-06 23:45:54 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: producer-1913122693 will be in active state
2022-04-06 23:45:55 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 23:45:55 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job producer-297311658 in namespace infra-namespace
2022-04-06 23:45:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:producer-1913122693 to finished
2022-04-06 23:45:55 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: producer-297311658 will be in active state
2022-04-06 23:45:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:61] Waiting till producer producer-297311658 and consumer consumer-869419888 finish
2022-04-06 23:46:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:46:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testReceiveSimpleMessage
2022-04-06 23:46:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job consumer-869419888 in namespace infra-namespace
2022-04-06 23:46:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job producer-297311658 in namespace infra-namespace
2022-04-06 23:46:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1907131888-262312087 in namespace infra-namespace
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testReceiveSimpleMessage-FINISHED
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testConfigureDeploymentStrategy-STARTED
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge example-bridge in namespace infra-namespace
2022-04-06 23:46:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: example-bridge will have desired state: Ready
2022-04-06 23:46:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: example-bridge is in desired state: Ready
2022-04-06 23:46:48 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:351] Adding label to KafkaBridge resource, the CR should be recreated
2022-04-06 23:46:48 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: example-bridge-bridge will be ready
2022-04-06 23:46:48 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: example-bridge-bridge is ready
2022-04-06 23:46:48 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment example-bridge-bridge to be ready
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment example-bridge-bridge is ready
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:358] Checking that observed gen. is still on 1 (recreation) and new label is present
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:363] Changing deployment strategy to ROLLING_UPDATE
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: example-bridge will have desired state: Ready
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: example-bridge is in desired state: Ready
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:368] Adding another label to KafkaBridge resource, pods should be rolled
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: example-bridge-bridge will be ready
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: example-bridge-bridge is ready
2022-04-06 23:47:21 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment example-bridge-bridge to be ready
2022-04-06 23:47:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-06 23:47:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job consumer-201826176 in namespace infra-namespace
2022-04-06 23:47:45 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: consumer-201826176 will be in active state
2022-04-06 23:47:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:consumer-201826176 to finished
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:311] Verifying labels on pod type my-bridge
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSendSimpleMessage
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job producer-1913122693 in namespace infra-namespace
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job consumer-201826176 in namespace infra-namespace
2022-04-06 23:47:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1919129546-1161179765 in namespace infra-namespace
2022-04-06 23:48:02 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment example-bridge-bridge is ready
2022-04-06 23:48:02 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:372] Checking that observed gen. higher (rolling update) and label is changed
2022-04-06 23:48:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:48:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigureDeploymentStrategy
2022-04-06 23:48:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge example-bridge in namespace infra-namespace
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testSendSimpleMessage-FINISHED
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomAndUpdatedValues-STARTED
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge custom-bridge in namespace infra-namespace
2022-04-06 23:48:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: custom-bridge will have desired state: Ready
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testConfigureDeploymentStrategy-FINISHED
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeToZero-STARTED
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge scaling-bridge-down in namespace infra-namespace
2022-04-06 23:48:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: scaling-bridge-down will have desired state: Ready
2022-04-06 23:48:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: scaling-bridge-down is in desired state: Ready
2022-04-06 23:48:30 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:285] Scaling KafkaBridge to zero replicas
2022-04-06 23:48:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: http-bridge-cluster-name will have desired state: Ready
2022-04-06 23:48:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: http-bridge-cluster-name is in desired state: Ready
2022-04-06 23:48:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:48:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleBridgeToZero
2022-04-06 23:48:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge scaling-bridge-down in namespace infra-namespace
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: custom-bridge is in desired state: Ready
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeIsolatedST:225] Verify values before update
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix custom-bridge-bridge in pod name
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container custom-bridge-bridge
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name custom-bridge-bridge
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container custom-bridge-bridge
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeIsolatedST:230] Check if actual env variable KAFKA_BRIDGE_PRODUCER_CONFIG has different value than test.value
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeIsolatedST:236] Updating values in Bridge container
2022-04-06 23:48:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment custom-bridge-bridge rolling update
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeToZero-FINISHED
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testDiscoveryAnnotation-STARTED
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testDiscoveryAnnotation is everything deleted.
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testDiscoveryAnnotation-FINISHED
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomBridgeLabelsAreProperlySet-STARTED
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge bridge-my-cluster-4b4655db in namespace infra-namespace
2022-04-06 23:48:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: bridge-my-cluster-4b4655db will have desired state: Ready
2022-04-06 23:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: bridge-my-cluster-4b4655db is in desired state: Ready
2022-04-06 23:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomBridgeLabelsAreProperlySet
2022-04-06 23:49:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge bridge-my-cluster-4b4655db in namespace infra-namespace
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomBridgeLabelsAreProperlySet-FINISHED
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeSubresource-STARTED
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge scaling-bridge-up in namespace infra-namespace
2022-04-06 23:49:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: scaling-bridge-up will have desired state: Ready
2022-04-06 23:49:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: custom-bridge-bridge will be ready
2022-04-06 23:49:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: custom-bridge-bridge is ready
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment custom-bridge-bridge rolling update finished
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [HttpBridgeIsolatedST:253] Verify values after update
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix custom-bridge-bridge in pod name
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container custom-bridge-bridge
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-06 23:49:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge custom-bridge in namespace infra-namespace
2022-04-06 23:49:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: scaling-bridge-up is in desired state: Ready
2022-04-06 23:49:37 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:312] -------> Scaling KafkaBridge subresource <-------
2022-04-06 23:49:37 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:313] Scaling subresource replicas to 4
2022-04-06 23:49:38 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: scaling-bridge-up-bridge will be ready
2022-04-06 23:49:38 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: scaling-bridge-up-bridge is ready
2022-04-06 23:49:38 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment scaling-bridge-up-bridge to be ready
2022-04-06 23:49:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:49:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testCustomAndUpdatedValues-FINISHED
2022-04-06 23:49:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:50:07 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment scaling-bridge-up-bridge is ready
2022-04-06 23:50:07 [ForkJoinPool-3-worker-1] [32mINFO [m [HttpBridgeIsolatedST:317] Check if replicas is set to 4, naming prefix should be same and observed generation higher
2022-04-06 23:50:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:50:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleBridgeSubresource
2022-04-06 23:50:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge scaling-bridge-up in namespace infra-namespace
2022-04-06 23:50:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:50:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeIsolatedST.testScaleBridgeSubresource-FINISHED
2022-04-06 23:50:17 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:50:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:50:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HttpBridgeIsolatedST
2022-04-06 23:50:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-shared-kafka-clients in namespace infra-namespace
2022-04-06 23:50:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka http-bridge-cluster-name in namespace infra-namespace
2022-04-06 23:50:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge http-bridge-cluster-name in namespace infra-namespace
2022-04-06 23:50:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 488.424 s - in io.strimzi.systemtest.bridge.HttpBridgeIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.HelmChartIsolatedST
2022-04-06 23:50:57 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:51:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:32 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:51:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:51:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:51:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:51:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:51:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:52:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmChartIsolatedST:67] Creating resources before the test class
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: kube-system
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace kube-system apply -f -
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Input: apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: tiller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
  - kind: ServiceAccount
    name: tiller
    namespace: kube-system
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:52:08 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmClient:44] Installing helm-chart strimzi-systemtests
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: helm install strimzi-systemtests --set defaultImageRegistry=quay.io,defaultImageRepository=strimzi,fullReconciliationIntervalMs=30000,kafkaBridge.image.tag=latest,resources.limits.memory=512Mi,kafkaBridge.image.repository=strimzi,featureGates=,image.imagePullPolicy=Always,watchAnyNamespace=false,resources.requests.memory=512Mi,operationTimeoutMs=300000,resources.limits.cpu=1000m,logLevelOverride=DEBUG,defaultImageTag=latest,resources.requests.cpu=200m,kafkaBridge.image.registry=quay.io --timeout 120s --debug /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/helm-charts/helm3/strimzi-kafka-operator --namespace infra-namespace --wait
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.HelmChartIsolatedST.testStrimziComponentsViaHelmChart-STARTED
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-eb7adf27-kafka-clients in namespace infra-namespace
2022-04-06 23:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-eb7adf27-kafka-clients will be ready
2022-04-06 23:52:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-eb7adf27-kafka-clients is ready
2022-04-06 23:52:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:52:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-eb7adf27 will have desired state: Ready
2022-04-06 23:53:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-eb7adf27 is in desired state: Ready
2022-04-06 23:53:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-eb7adf27-scraper in namespace infra-namespace
2022-04-06 23:53:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-eb7adf27-scraper will be ready
2022-04-06 23:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-eb7adf27-scraper is ready
2022-04-06 23:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-eb7adf27-scraper to be ready
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-eb7adf27-scraper is ready
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-eb7adf27-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-eb7adf27-allow in namespace infra-namespace
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1025408803-1308713981 in namespace infra-namespace
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1025408803-1308713981 will have desired state: Ready
2022-04-06 23:54:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1025408803-1308713981 is in desired state: Ready
2022-04-06 23:54:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-eb7adf27 will have desired state: Ready
2022-04-06 23:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-eb7adf27 is in desired state: Ready
2022-04-06 23:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-cluster-eb7adf27 will have desired state: Ready
2022-04-06 23:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-cluster-eb7adf27 is in desired state: Ready
2022-04-06 23:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:55:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-eb7adf27 will have desired state: Ready
2022-04-06 23:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-eb7adf27 is in desired state: Ready
2022-04-06 23:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testStrimziComponentsViaHelmChart
2022-04-06 23:55:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:55:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-eb7adf27-scraper in namespace infra-namespace
2022-04-06 23:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1025408803-1308713981 in namespace infra-namespace
2022-04-06 23:55:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:55:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:55:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-eb7adf27 in namespace infra-namespace
2022-04-06 23:55:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-eb7adf27-allow in namespace infra-namespace
2022-04-06 23:55:59 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-eb7adf27-kafka-clients in namespace infra-namespace
2022-04-06 23:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.HelmChartIsolatedST.testStrimziComponentsViaHelmChart-FINISHED
2022-04-06 23:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-06 23:56:39 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmClient:71] Deleting helm-chart:strimzi-systemtests in namespace:infra-namespace
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for HelmChartIsolatedST
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [HelmClient:71] Deleting helm-chart:strimzi-systemtests in namespace:infra-namespace
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:56:40 [ForkJoinPool-3-worker-3] [33mWARN [m [KubeClusterResource:151] Namespace infra-namespace is already created, going to delete it
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:57:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:57:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 23:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 407.116 s - in io.strimzi.systemtest.specific.HelmChartIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.SpecificIsolatedST
2022-04-06 23:57:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:09 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:58:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:19 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:58:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:29 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:58:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:58:39 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:58:40 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:58:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-06 23:59:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-06 23:59:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-06 23:59:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-06 23:59:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-06 23:59:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:59:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-06 23:59:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-06 23:59:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-06 23:59:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:508] 0.21.4
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAware-STARTED
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-2bc643b0 in namespace infra-namespace
2022-04-06 23:59:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-2bc643b0 will have desired state: Ready
2022-04-07 00:01:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-2bc643b0 is in desired state: Ready
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-2bc643b0-kafka-0 -- /bin/bash -c cat /opt/kafka/init/rack.id
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-2bc643b0-kafka-0 -- /bin/bash -c cat /tmp/strimzi.properties | grep broker.rack
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-producer in namespace infra-namespace
2022-04-07 00:01:16 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-producer will be in active state
2022-04-07 00:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job hello-world-consumer in namespace infra-namespace
2022-04-07 00:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: hello-world-consumer will be in active state
2022-04-07 00:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAware
2022-04-07 00:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-producer in namespace infra-namespace
2022-04-07 00:01:18 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-2bc643b0 in namespace infra-namespace
2022-04-07 00:01:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job hello-world-consumer in namespace infra-namespace
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAware-FINISHED
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectWrongDeployment-STARTED
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:01:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:28 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:01:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:01:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:38 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:01:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:01:48 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:01:49 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:01:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=30000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:02:04 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:02:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:02:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-07 00:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:02:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-07 00:02:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b664755f in namespace infra-namespace
2022-04-07 00:02:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b664755f will have desired state: Ready
2022-04-07 00:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b664755f is in desired state: Ready
2022-04-07 00:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b664755f-kafka-clients in namespace infra-namespace
2022-04-07 00:04:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b664755f-kafka-clients will be ready
2022-04-07 00:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b664755f-kafka-clients is ready
2022-04-07 00:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:196] Deploy KafkaConnect with wrong rack-aware topology key: wrong-key
2022-04-07 00:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b664755f-scraper in namespace infra-namespace
2022-04-07 00:04:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b664755f-scraper will be ready
2022-04-07 00:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b664755f-scraper is ready
2022-04-07 00:04:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-b664755f-scraper to be ready
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-b664755f-scraper is ready
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-b664755f-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-b664755f-allow in namespace infra-namespace
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-b664755f in namespace infra-namespace
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-b664755f-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-b664755f-allow in namespace infra-namespace
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 00:04:26 [ForkJoinPool-3-worker-3] [32mINFO [m [PodUtils:277] Wait for at least one pod with prefix: my-cluster-b664755f-connect will be in pending phase
2022-04-07 00:04:27 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:227] Waiting for ClusterOperator to get timeout operation of incorrectly set up KafkaConnect
2022-04-07 00:04:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-b664755f will have desired state: Ready
2022-04-07 00:06:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-b664755f is in desired state: Ready
2022-04-07 00:06:53 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:238] KafkaConnect is ready with changed rack key: 'rack-key'.
2022-04-07 00:06:53 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:239] Verify KafkaConnect rack key update
2022-04-07 00:06:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:156] Send and receive messages through KafkaConnect
2022-04-07 00:06:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-b664755f-connect-797459f767-82wxc -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-b664755f-scraper-c57f67854-zqwzs -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-2043299187-274155238", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-b664755f-connect-api.infra-namespace.svc:8083/connectors
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@595e5847, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-b664755f-kafka-clients-846575b88f-2vqgv', podNamespace='infra-namespace', bootstrapServer='my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@9ec50b}
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092:my-topic-2043299187-274155238 from pod my-cluster-b664755f-kafka-clients-846575b88f-2vqgv
2022-04-07 00:06:54 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b664755f-kafka-clients-846575b88f-2vqgv -n infra-namespace -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-2043299187-274155238
2022-04-07 00:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 00:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 00:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@12a04901, messages=[], arguments=[--group-id, my-consumer-group-1762021198, --max-messages, 100, --group-instance-id, instance204467015, --bootstrap-server, my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-b664755f-kafka-clients-846575b88f-2vqgv', podNamespace='infra-namespace', bootstrapServer='my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1762021198', consumerInstanceId='instance204467015', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@50bebd45}
2022-04-07 00:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092#my-topic-2043299187-274155238 from pod my-cluster-b664755f-kafka-clients-846575b88f-2vqgv
2022-04-07 00:06:57 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-b664755f-kafka-clients-846575b88f-2vqgv -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1762021198 --max-messages 100 --group-instance-id instance204467015 --bootstrap-server my-cluster-b664755f-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-2043299187-274155238
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-b664755f-connect-797459f767-82wxc
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-b664755f-connect-797459f767-82wxc
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:07:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:02 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:07:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:13 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:07:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:07:23 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:07:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:07:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:07:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:223] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@20442d33, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='infra-namespace', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:08:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:08:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:08:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:08:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:08:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment strimzi-cluster-operator rolling update
2022-04-07 00:08:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:08:55 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment strimzi-cluster-operator rolling update finished
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testRackAwareConnectWrongDeployment
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-b664755f-allow in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b664755f-kafka-clients in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b664755f-scraper in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-b664755f-allow in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b664755f in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-b664755f in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testRackAwareConnectWrongDeployment-FINISHED
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.specific.SpecificIsolatedST.testDeployUnsupportedKafka-STARTED
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9e412ed2 in namespace infra-namespace
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SpecificIsolatedST:414] Kafka with version 6.6.6 deployed.
2022-04-07 00:09:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9e412ed2 will have desired state: NotReady
2022-04-07 00:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9e412ed2 is in desired state: NotReady
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployUnsupportedKafka
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9e412ed2 in namespace infra-namespace
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.specific.SpecificIsolatedST.testDeployUnsupportedKafka-FINISHED
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context SpecificIsolatedST is everything deleted.
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 684.682 s - in io.strimzi.systemtest.specific.SpecificIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.specific.DrainCleanerIsolatedST
2022-04-07 00:09:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:09:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:09:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:09:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:09:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:09:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:09:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:09:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:09:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:09:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:09:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:09:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:10:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:10:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:10:04 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:10:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=strimzi-drain-cleaner
namespaceToWatch=strimzi-drain-cleaner
bindingsNamespaces=[strimzi-drain-cleaner]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: strimzi-drain-cleaner
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: strimzi-drain-cleaner
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:10:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:10:20 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:10:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [RequiredMinKubeApiVersionCondition:30] testDrainCleanerWithComponents is @RequiredMinKubeApiVersion with version 1.17, but the running on cluster with 1.16: Ignoring testDrainCleanerWithComponents
2022-04-07 00:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context DrainCleanerIsolatedST is everything deleted.
2022-04-07 00:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 2, Failures: 0, Errors: 0, Skipped: 2, Time elapsed: 117.59 s - in io.strimzi.systemtest.specific.DrainCleanerIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST
2022-04-07 00:11:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace strimzi-drain-cleaner
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:11:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace strimzi-drain-cleaner
2022-04-07 00:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:11:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:12:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-namespace-test
bindingsNamespaces=[infra-namespace, second-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:12:29 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:12:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:12:46 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:12:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:12:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:12:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace second-namespace-test
2022-04-07 00:12:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleNamespaceIsolatedST:59] Deploying Kafka in different namespace than CO when CO watches multiple namespaces
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster in namespace second-namespace-test
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-STARTED
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleNamespaceIsolatedST:45] Deploying TO to watch a different namespace that it is deployed in
2022-04-07 00:14:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace second-namespace-test exec my-cluster-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-07 00:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-666993257-9173283 in namespace infra-namespace
2022-04-07 00:14:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-666993257-9173283 will have desired state: Ready
2022-04-07 00:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-666993257-9173283 is in desired state: Ready
2022-04-07 00:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicOperatorWatchingOtherNamespace
2022-04-07 00:14:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-666993257-9173283 in namespace infra-namespace
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-FINISHED
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-STARTED
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [MultipleNamespaceIsolatedST:69] Deploying KafkaMirrorMaker in different namespace than CO when CO watches multiple namespaces
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-target in namespace second-namespace-test
2022-04-07 00:14:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-target will have desired state: Ready
2022-04-07 00:15:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-target is in desired state: Ready
2022-04-07 00:15:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-07 00:15:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:71] Waiting for creation my-cluster-mirror-maker in namespace second-namespace-test
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployMirrorMakerAcrossMultipleNamespace
2022-04-07 00:16:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-07 00:16:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-target in namespace second-namespace-test
2022-04-07 00:17:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:17:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-FINISHED
2022-04-07 00:17:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:17:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:17:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MultipleNamespaceIsolatedST
2022-04-07 00:17:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace second-namespace-test
2022-04-07 00:17:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 366.222 s - in io.strimzi.systemtest.watcher.MultipleNamespaceIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
2022-04-07 00:17:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:190] Creating resources before the test class
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:17:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:17:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-07 00:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-07 00:17:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:18:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:18:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace, second-namespace-test, third-namespace-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 00:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: third-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: third-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:18:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:18:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:18:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:19:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:19:01 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-04-07 00:19:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster in namespace third-namespace-test
2022-04-07 00:19:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster will have desired state: Ready
2022-04-07 00:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster is in desired state: Ready
2022-04-07 00:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-second in namespace second-namespace-test
2022-04-07 00:20:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-second will have desired state: Ready
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-second is in desired state: Ready
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-STARTED
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:82] Deploying Kafka cluster in different namespace than CO when CO watches all namespaces
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:46] Check if Kafka Cluster my-cluster-second in namespace second-namespace-test
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:51] Kafka condition status: True
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:52] Kafka condition type: Ready
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaInDifferentNsThanClusterOperator is everything deleted.
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testKafkaInDifferentNsThanClusterOperator-FINISHED
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-STARTED
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:66] Deploying TO to watch a different namespace that it is deployed in
2022-04-07 00:21:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-04-07 00:21:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace third-namespace-test exec my-cluster-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-07 00:21:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:21:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2105888785-1213983578 in namespace second-namespace-test
2022-04-07 00:21:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2105888785-1213983578 will have desired state: Ready
2022-04-07 00:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2105888785-1213983578 is in desired state: Ready
2022-04-07 00:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTopicOperatorWatchingOtherNamespace
2022-04-07 00:21:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2105888785-1213983578 in namespace second-namespace-test
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testTopicOperatorWatchingOtherNamespace-FINISHED
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUOWatchingOtherNamespace-STARTED
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:121] Creating user in other namespace than CO and Kafka cluster with UO
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1294180677-270762270 in namespace second-namespace-test
2022-04-07 00:21:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1294180677-270762270 will have desired state: Ready
2022-04-07 00:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1294180677-270762270 is in desired state: Ready
2022-04-07 00:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUOWatchingOtherNamespace
2022-04-07 00:21:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1294180677-270762270 in namespace second-namespace-test
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUOWatchingOtherNamespace-FINISHED
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUserInDifferentNamespace-STARTED
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1294180677-270762270 in namespace second-namespace-test
2022-04-07 00:21:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1294180677-270762270 will have desired state: Ready
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1294180677-270762270 is in desired state: Ready
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:137] KafkaUser condition status: True
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:138] KafkaUser condition type: Ready
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: third-namespace-test
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:148] Copying secret Secret(apiVersion=v1, data={ca.crt=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, user.crt=LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUVJVENDQWdtZ0F3SUJBZ0lVSUt2bzh0TGhUYXh6T0R0L0lqN1U5VUpKZ0pBd0RRWUpLb1pJaHZjTkFRRU4KQlFBd0xURVRNQkVHQTFVRUNnd0thVzh1YzNSeWFXMTZhVEVXTUJRR0ExVUVBd3dOWTJ4cFpXNTBjeTFqWVNCMgpNREFlRncweU1qQTBNRGN3TURJeE5UaGFGdzB5TXpBME1EY3dNREl4TlRoYU1DY3hKVEFqQmdOVkJBTU1IRzE1CkxYVnpaWEl0TVRJNU5ERTRNRFkzTnkweU56QTNOakl5TnpBd2dnRWlNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUIKRHdBd2dnRUtBb0lCQVFDelNvZHZxLzRFWnFZOUlNUFZZMjJDWnRXWWJ2RThya25EOGU0cjE5QmhBN3VwaUl0dwpuN1JEZklsbnVsK05aT0NxYzlnR0w4MVRDOGthM1VnbUJKbk51QS9WRnZuMzNzdk5LZU1wUFZUem94STlZdkJSCkM4U0FDdUlnemJxZzM1ZUhlZGkyVVlwbHFTMm96aFhuWEdhelZub0NPb20rUFg1aTBxSEtIYlBic2FLVlplVXYKYUFyS3VBdm82RDBZQXJBQkIwKzJGL0pqQ0lDd0J2Zk5OOG5XTzNNcS9kOFp1eGNCTmhHdVNhZlI3K1Ard1YrOAptMWdMTXZHblQ4Si9mV1hVejVoOHpNK3ZPRDdUTXVNd0o5L2xBcnNCQ1FjOWhNNkxuQ0szQjV1WnFWY296QzF1Cm05eDBqWWE4NktoOGJ2MGtWMkJHbHJ5eE82b28yYmJiL2EvWkFnTUJBQUdqUHpBOU1CMEdBMVVkRGdRV0JCVHMKbkk1S01WLzJEQkh3OG1wYXRib3Y4T29laXpBTUJnTlZIUk1CQWY4RUFqQUFNQTRHQTFVZER3RUIvd1FFQXdJRgpvREFOQmdrcWhraUc5dzBCQVEwRkFBT0NBZ0VBVkl2ZXR5RUxaYU5VQUVTLzB6ditkTlQyTWNIbmxrWkROL3B0CjhnUFJvTjVobDFkY1ZaUXh4TWQ5dUZQUDhZU1BVcHZpc2traS83QzkzWCsrSDlldW9BVlh2TUpWL1oxQXVaYUQKRFlmaEpMZTBoWEVmbVhwU3hVSXQ4cVhBWHN5VE1yWlduZEdvdE8xVFp2MlU2Z2tpZm92cHRRbDJYakNjbEc3UwpxSUFPUEVIb2E0NHFlMWUvck12TURoYm5RbExvZzFpZWpRQnIzUXg2WnRCVEtxbTg2MzAzU0ZPMWY3TWEvZGRyCmt5S1hLeXR4ZFFKYVBXUWdwamlWbmhRZFhvOHRsSG1NYmR5aTM3K3AxZVMvTlE5Y2RXbm5CY2dEa2h1a2J3MDYKMHByQXc3bjlaZzIyQjJ6c1BPYkF6Nk5OejE1alJxQW9xVkhhNzgwbU5LWW5lOE1jZWR2cXJtbTZUUDZkLy9pUQpQRWF0YmZXc013R2tzRzdSQ2J5UGJmcktXVXhWTHdXeUZHMHZrZGlWZXJSUy9IRWV6ODlqb3k0cklHdVdOaTJICnVuYW1OOHFFYW1EREFnT3h2dDBXazlnaTRDNjkvdS92bkErRHlWWmFybndNZTlOaEVRQVc1azBTM2p1ckc5a3YKY2p1SWZNVTlXNUhpUzhtbThkdDRGUmtzRUhjQUNVOXdJNWFjMFl2WWJaMW4vYjM5MkJzVzk3bm4zY2kwS2F4Nwpsb2owb0RxSThyVisveTBVNkdPV0hjeE1zWlhINjNZOXlSQ1FOdWovYWxwTUM1QkxRc3ZJNktSMGxFQ0dKQ25jCjZyKzNHdjQ5TVpOMWVOdGROVnVIb2NCQzZOelBQSXNlbis5dWI2RUZWWDVwY2szb3RKYlkvNEVUZnhnQzgvbysKVzVPSktmRT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=, user.key=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, user.p12=MIIK8gIBAzCCCrgGCSqGSIb3DQEHAaCCCqkEggqlMIIKoTCCBQ8GCSqGSIb3DQEHBqCCBQAwggT8AgEAMIIE9QYJKoZIhvcNAQcBMBwGCiqGSIb3DQEMAQYwDgQIe68+se9FgTkCAggAgIIEyInyBrw7fKwF/numeGGmzISEvRzuSR4a1mwuAt9JfT0D4qOY2FcZDe7cvAdulHB+kVvOZAd3GPgYR3ZVemr/6T3Vmu9uroJvFm5OsR4XStZE8wLgaOIUUszTbFTnYwTiPPjaweexGs7+EFjljvw5mYR5rRHxrvgPog/+wbX/13DrIEo8eWv4ADlfv37FY+ZNqOKltr7xKCGN8AJoW8lytDtZoa38IJH9gRt1oQSJ4rarRuSvxoJBH6pFYCBh2s69izMDWI/ZsBbiFcGj40RI0IP5WSlPSDhGnZWLMY6c/U2vpsjNI7XP3VIZfoK8CHytKjZEpBlp22OE7vZLWzypRQpZSw1ZvBYBbcW8mycKs+5q2u8X4UIDyoHYyTqba69KVCZKWhmSpPFeRsTdSP+VcBYmb4ovJdD3kASIwsj1sC08Oj3vA7r9vYbgMzfd6njGkFN1jNxqZrrXgRzFQtkqwxGwqkm2DMDCZuzihiqMn1R70XqmGLPCEqGsZ42dsJ/uP0QWY9GjH70SHyQTz8wrCm3ClA9AKtoz/BRJskE370ijt4geUn7jrIiEv2XRSmT0CckSVixKyFG1nzRbkj3tPzdMV/JcBgEG5OyQIBeSnOR4uWlKRVDARqoHKBQzJkGAnFs7fZAgwNaGfmKQF82Pb1CRFr50bux5Vd+AhX4laYrw2Z7DmWwlbxR2MOjFjzHmr0BJLd58BS1GPDv7gN8KfXMlMlkg0YJVAR6Z5ipvHcZCrwM1p0JqbwXVEEGIcturs0gAKxLJg4sPlC9Q4mxcFF/ykicRPh2lRZ0XnBhldp6y7NQjXydMFwo8jA/fFHD2alJ4SDSGb2mzbB5ZvfNGQIn3ygzpwGz4ZY7iFyxKVyPK2xfLZYZWwq4Gq+h33GVqZy7LJSI8DgthXwygmEdUmnVM5SmLm5g/GIAyHd3Qsb1NKDIxnU0xc5JUfVvwoHRFvzl4Lw6wtvlpOSG4BrT8BRJuANK1Z5mN+6V584MY85Nh9KhyubDC+pfwW/NOZZXBkvPEQ+lkvVLB4loR99Er5mgAFUf2lEUYy9KkR6On0IKomItAnI2Y51nxcKb0EsSW0leSLoo8WHeRlRLoac/OVsJSgJvke6UD/ixjaHN+EgiR9sWhAV/agCdP0oG7u1rSP4jnJG7U4iipurw5XqZhdRP+zzT3yJkT03Vs3KU4V8Ph94WO4onYU8jwIl1sTPuYVILFWYHLCkPGyHVQ9eaSjTWYwoBO6c74g9+0lXxJufLpcOJtSnp/2oXn1T4rJ7CyHBMC5713R08abrtEwUzLT47xLtGYMHLWxCN8XZRqIbK4WvpQ/DUYs1rBc8S6BMK7Zl+/mYabj6vXyeZyRQCOl0wobhgLtOKlp0a6ZEGdG5OC3+wRq9zPc7zBcAnioQa8aiT/cczMW78rJva2hwlu8o+HxSFv7gPV/iOdaxIJvyY+8fSqSFQ9I8X5OR5w7k0zFMSb1PQA9X2p5ymXp8IZEY11NNEP6Sg5iCWCQdac+0NBrFys8X9M5APVxEvLT9clUx9iMeiqBVOaZlNiqhFgD4HYRY133lQObpmC8uO8BQINWsslDotxPkqBDUib0vMxXPYvpocfnhobg7lpzXKvofiU7e2SAvPUUTCCBYoGCSqGSIb3DQEHAaCCBXsEggV3MIIFczCCBW8GCyqGSIb3DQEMCgECoIIE7jCCBOowHAYKKoZIhvcNAQwBAzAOBAjcBxhYK1Wd6wICCAAEggTI1siFACGG0N834CVot2vdE86Z/P6mznxHUiJEv8u+ZjSudm6cURtoPME9cq4x7EQDoTHhutFa9P6/vr/xEl+ylVCPW9H3VBjqNBgQkYhwT+Je2DyJgDHByZPoiQev8xIZE52TTMGXDjDa89ulgSt0A60PEzmnStY8GAOl1mw1M7xM5yPdlyyNoHShn29uYOydd0hAVMz6ahPg/pxM1clxqySojR4KLfAIoxQakzGfFXEJ8l0+sfny1YNjvd7F79aeF50HKM6ew824DCTq05kiAKh7wZFH5KfopI3m3P8fG2hurSGwrgdfqY0z9mfmH6C1IytcAY7bCYYWdJovlPL7VulH7/lPlG/JDV9ttBZBm4mf8JBOFDMKcLfDTWvUKq5wgEGNQBa4N2FLY0TjJK6au864j442lmz8YwogIntHH8z6wj/7l7WK6eDzMFvt90BqbOqbAvwRjBU8UmeR+HcAA400ZlURervgdqc4WWP6fgS+zZpzcuH14ABfwtEw7C+c1Vcdn/udqEFo8PkIeosvcs2VUWSrFMFAC+ql40HMeQuVuriJRyGaIweZwn6LmzQ/H03RniyApqBu9jHkWfYk5zHXQXozUTEqHuedctNjo7SszqummGrABY3Wn9tHu0S/nTEwRRckZwg0v/hGUzp7ZfEqQEQ7QO8ICzEA3ONKqFpHVSzJREKoJeRE8sfIGnm9eAfyloDiKViTiGcHOxOt8j7VW9zEGWOReaSlMSFSiHkaC7hM9ThHF08Nv1AWRN+5wrNO7cNfW6tbb4zvVjpy8uLP5RSfl5EV7NEBppg1BNpo9jtjxyWmp6zBlrr4e5zVv6yfeOyEnW+2BvlYE7JAW3phMZLF/BDrw8YdAC/5qkJ8/H9UMfv5cZD5iTyEgM6i8v/DYT8mfQcuj5TQqoPhrP2ALqTTEgCpuOcHE3y71Izn7JqLU+wCumb41MQsUv0wiqRbCUj8BF++a6sTPm62OTzChuHMZQTsYPEdD9Vi070PfV/FVAhdwvobs+Iag9UIVbq8H8OOVBRAacroKNRtl6cRFxkqfEjBOQuPqQNC9Uop+NOEhd2XU23pXRODS7lFiF7HydtBedlr4wFgvHsdb8V/XsspfvPcOZdUGcfESCw6lu29yiMPgi5M6ntm3CGxAfqnTDPWc3ySke+6J+eys92eUdImHHQsI/ryKasvHsKBvOs3bbJ9kJQFcPHdA+zZ69vmvHdPlhk0p96Fx961a6I9hduB2JghmpcVeicZa0Kr49O8udsMJd+c12NZHpciMnJgBv/m8pu6pOr/xWoA+R8iMduVJIRNG8fk7C315kLtTjClR7pECHvZVIeBf+yBPSS+NIE8gRvSyQ1BGqdTjeZRG0GwW8KpDf3w9NWW/2K1OmXIV8NKYJPC/ZEvq6l6SlQrOIdosPTkQjLZc+tBZ+Ue2KM/w1mimj3g1wrPrsmjS+cEaOpNO6bhxoFLR+eu3VQ9TJdPzQLdDilTmWUm6SKOMgNIRsGKjjWcHmIwVMlNCtisAA0xSUryTh00ux7II2vESPyjgykySsrQm7SDTNdyf0c51q7PFlAUm1sKGCeectO68QZ3vsSGwvBfkmV7+bQWtqtVjnah0SlGX5jXFHE1arPplGbBMW4wIwYJKoZIhvcNAQkVMRYEFNS1fkPp36jDVmKZn8iHVrx+CW7TMEcGCSqGSIb3DQEJFDE6HjgAbQB5AC0AdQBzAGUAcgAtADEAMgA5ADQAMQA4ADAANgA3ADcALQAyADcAMAA3ADYAMgAyADcAMDAxMCEwCQYFKw4DAhoFAAQUz865EUqlcXLxAGGbMoSa/OhnAEkECPfnX85SC5VRAgIIAA==, user.password=aDlqQ3BpZ0dCcExs}, immutable=null, kind=Secret, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=2022-04-07T00:21:58Z, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels={app.kubernetes.io/instance=my-user-1294180677-270762270, app.kubernetes.io/managed-by=strimzi-user-operator, app.kubernetes.io/name=strimzi-user-operator, app.kubernetes.io/part-of=strimzi-my-user-1294180677-270762270, strimzi.io/cluster=my-cluster, strimzi.io/kind=KafkaUser, test.case=testUserInDifferentNamespace}, managedFields=[], name=my-user-1294180677-270762270, namespace=second-namespace-test, ownerReferences=[OwnerReference(apiVersion=kafka.strimzi.io/v1beta2, blockOwnerDeletion=false, controller=false, kind=KafkaUser, name=my-user-1294180677-270762270, uid=df5c104d-933f-48a5-ac60-dbe681b6337f, additionalProperties={})], resourceVersion=122840, selfLink=/api/v1/namespaces/second-namespace-test/secrets/my-user-1294180677-270762270, uid=a460a7c3-898d-46be-9e63-e7d445e946ad, additionalProperties={}), stringData=null, type=Opaque, additionalProperties={}) from namespace second-namespace-test to namespace third-namespace-test
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-kafka-clients in namespace third-namespace-test
2022-04-07 00:21:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-kafka-clients will be ready
2022-04-07 00:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-kafka-clients is ready
2022-04-07 00:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 00:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:168] Checking produced and consumed messages to pod:my-cluster-kafka-clients-68d6b58968-25gfb
2022-04-07 00:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1fcb7ebe, messages=[], arguments=[USER=my_user_1294180677_270762270, --max-messages, 100, --bootstrap-server, my-cluster-kafka-bootstrap.third-namespace-test.svc:9093, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-kafka-clients-68d6b58968-25gfb', podNamespace='third-namespace-test', bootstrapServer='my-cluster-kafka-bootstrap.third-namespace-test.svc:9093', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='my-user-1294180677-270762270', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@16930bab}
2022-04-07 00:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 100 messages to my-cluster-kafka-bootstrap.third-namespace-test.svc:9093:my-topic-2043299187-274155238 from pod my-cluster-kafka-clients-68d6b58968-25gfb
2022-04-07 00:22:01 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-kafka-clients-68d6b58968-25gfb -n third-namespace-test -- /opt/kafka/producer.sh USER=my_user_1294180677_270762270 --max-messages 100 --bootstrap-server my-cluster-kafka-bootstrap.third-namespace-test.svc:9093 --topic my-topic-2043299187-274155238
2022-04-07 00:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 00:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 100 messages
2022-04-07 00:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@719265f5, messages=[], arguments=[--group-id, my-consumer-group-1071578758, USER=my_user_1294180677_270762270, --max-messages, 100, --group-instance-id, instance633821167, --bootstrap-server, my-cluster-kafka-bootstrap.third-namespace-test.svc:9093, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-kafka-clients-68d6b58968-25gfb', podNamespace='third-namespace-test', bootstrapServer='my-cluster-kafka-bootstrap.third-namespace-test.svc:9093', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='my-user-1294180677-270762270', consumerGroupName='my-consumer-group-1071578758', consumerInstanceId='instance633821167', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1e0acf71}
2022-04-07 00:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 100 messages from my-cluster-kafka-bootstrap.third-namespace-test.svc:9093:my-topic-2043299187-274155238 from pod my-cluster-kafka-clients-68d6b58968-25gfb
2022-04-07 00:22:05 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-kafka-clients-68d6b58968-25gfb -n third-namespace-test -- /opt/kafka/consumer.sh --group-id my-consumer-group-1071578758 USER=my_user_1294180677_270762270 --max-messages 100 --group-instance-id instance633821167 --bootstrap-server my-cluster-kafka-bootstrap.third-namespace-test.svc:9093 --topic my-topic-2043299187-274155238
2022-04-07 00:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 00:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 100 messages
2022-04-07 00:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUserInDifferentNamespace
2022-04-07 00:22:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-kafka-clients in namespace third-namespace-test
2022-04-07 00:22:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1294180677-270762270 in namespace second-namespace-test
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testUserInDifferentNamespace-FINISHED
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-STARTED
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [AllNamespaceIsolatedST:92] Deploying KafkaMirrorMaker in different namespace than CO when CO watches all namespaces
2022-04-07 00:22:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:22:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-target in namespace second-namespace-test
2022-04-07 00:22:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-target will have desired state: Ready
2022-04-07 00:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-target is in desired state: Ready
2022-04-07 00:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-07 00:23:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractNamespaceST:71] Waiting for creation my-cluster-mirror-maker in namespace second-namespace-test
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster will have desired state: Ready
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster is in desired state: Ready
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployMirrorMakerAcrossMultipleNamespace
2022-04-07 00:25:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster in namespace second-namespace-test
2022-04-07 00:25:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-target in namespace second-namespace-test
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployMirrorMakerAcrossMultipleNamespace-FINISHED
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployKafkaConnectAndKafkaConnectorInOtherNamespaceThanCO-STARTED
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-namespace-test
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bde44067-kafka-clients in namespace second-namespace-test
2022-04-07 00:25:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bde44067-kafka-clients will be ready
2022-04-07 00:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bde44067-kafka-clients is ready
2022-04-07 00:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bde44067kafka-connect-scraper in namespace second-namespace-test
2022-04-07 00:25:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bde44067kafka-connect-scraper will be ready
2022-04-07 00:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bde44067kafka-connect-scraper is ready
2022-04-07 00:25:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-bde44067kafka-connect-scraper to be ready
2022-04-07 00:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-bde44067kafka-connect-scraper is ready
2022-04-07 00:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-bde44067kafka-connect-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 00:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-bde44067kafka-connect-allow in namespace second-namespace-test
2022-04-07 00:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 00:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-bde44067kafka-connect in namespace second-namespace-test
2022-04-07 00:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-bde44067kafka-connect will have desired state: Ready
2022-04-07 00:26:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-bde44067kafka-connect is in desired state: Ready
2022-04-07 00:26:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-bde44067kafka-connect in namespace second-namespace-test
2022-04-07 00:26:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-bde44067kafka-connect will have desired state: Ready
2022-04-07 00:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-bde44067kafka-connect is in desired state: Ready
2022-04-07 00:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-bde44067kafka-connect will have desired state: Ready
2022-04-07 00:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-bde44067kafka-connect is in desired state: Ready
2022-04-07 00:26:39 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 00:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace second-namespace-test exec my-cluster-bde44067kafka-connect-connect-bbc59b9bd-jjj7d -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 00:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 00:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bde44067kafka-connect-kafka-clients in namespace second-namespace-test
2022-04-07 00:26:40 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bde44067kafka-connect-kafka-clients will be ready
2022-04-07 00:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bde44067kafka-connect-kafka-clients is ready
2022-04-07 00:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 00:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@670cc885, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092, --topic, my-topic-2043299187-274155238], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-bde44067-kafka-clients-cb6cf45fd-6gbsk', podNamespace='second-namespace-test', bootstrapServer='my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092', topicName='my-topic-2043299187-274155238', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@35886ce9}
2022-04-07 00:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092:my-topic-2043299187-274155238 from pod my-cluster-bde44067-kafka-clients-cb6cf45fd-6gbsk
2022-04-07 00:26:42 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-bde44067-kafka-clients-cb6cf45fd-6gbsk -n second-namespace-test -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-second-kafka-bootstrap.second-namespace-test.svc:9092 --topic my-topic-2043299187-274155238
2022-04-07 00:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 00:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 00:26:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-bde44067kafka-connect-connect-bbc59b9bd-jjj7d
2022-04-07 00:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-bde44067kafka-connect-connect-bbc59b9bd-jjj7d
2022-04-07 00:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployKafkaConnectAndKafkaConnectorInOtherNamespaceThanCO
2022-04-07 00:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-bde44067kafka-connect in namespace second-namespace-test
2022-04-07 00:26:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bde44067kafka-connect-scraper in namespace second-namespace-test
2022-04-07 00:26:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bde44067kafka-connect-kafka-clients in namespace second-namespace-test
2022-04-07 00:27:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-bde44067kafka-connect-allow in namespace second-namespace-test
2022-04-07 00:27:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bde44067-kafka-clients in namespace second-namespace-test
2022-04-07 00:27:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-bde44067kafka-connect in namespace second-namespace-test
2022-04-07 00:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.watcher.AllNamespaceIsolatedST.testDeployKafkaConnectAndKafkaConnectorInOtherNamespaceThanCO-FINISHED
2022-04-07 00:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for AllNamespaceIsolatedST
2022-04-07 00:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-second in namespace second-namespace-test
2022-04-07 00:28:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster in namespace third-namespace-test
2022-04-07 00:28:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 663.169 s - in io.strimzi.systemtest.watcher.AllNamespaceIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST
2022-04-07 00:28:16 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:28:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-namespace-test
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace third-namespace-test
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace third-namespace-test
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 00:28:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:28:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-namespace-test
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:29:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:29:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:29:33 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:29:59 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 00:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-07 00:30:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to internal realm
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to internal realm
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to internal realm
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:60] Using HTTP endpoints
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:32:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-plain-name will have desired state: Ready
2022-04-07 00:33:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-plain-name is in desired state: Ready
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:33:56 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainProducerConsumer-STARTED
2022-04-07 00:33:56 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainAuthenticationKafkaConnectIsAbleToConnectToKafkaOAuth-STARTED
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:33:56 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:33:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-55c6da2c-kafka-clients in namespace infra-namespace
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-producer-my-cluster-b4adda11 in namespace infra-namespace
2022-04-07 00:33:56 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-producer-my-cluster-b4adda11 will be in active state
2022-04-07 00:33:56 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-55c6da2c-kafka-clients will be ready
2022-04-07 00:33:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-producer-my-cluster-b4adda11 to finished
2022-04-07 00:33:58 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-55c6da2c-kafka-clients is ready
2022-04-07 00:33:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-55c6da2c-scraper in namespace infra-namespace
2022-04-07 00:33:58 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-55c6da2c-scraper will be ready
2022-04-07 00:34:00 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-55c6da2c-scraper is ready
2022-04-07 00:34:00 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-55c6da2c-scraper to be ready
2022-04-07 00:34:06 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-consumer-my-cluster-b4adda11 in namespace infra-namespace
2022-04-07 00:34:06 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-consumer-my-cluster-b4adda11 will be in active state
2022-04-07 00:34:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-consumer-my-cluster-b4adda11 to finished
2022-04-07 00:34:10 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-55c6da2c-scraper is ready
2022-04-07 00:34:10 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-55c6da2c-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 00:34:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-55c6da2c-allow in namespace infra-namespace
2022-04-07 00:34:10 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 00:34:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-55c6da2c in namespace infra-namespace
2022-04-07 00:34:10 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-55c6da2c will have desired state: Ready
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testSaslPlainProducerConsumer
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-consumer-my-cluster-b4adda11 in namespace infra-namespace
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-producer-my-cluster-b4adda11 in namespace infra-namespace
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainProducerConsumer-FINISHED
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerBridge-STARTED
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-393690526-1443785456 in namespace infra-namespace
2022-04-07 00:34:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-393690526-1443785456 will have desired state: Ready
2022-04-07 00:34:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-393690526-1443785456 is in desired state: Ready
2022-04-07 00:34:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-dbe55783 in namespace infra-namespace
2022-04-07 00:34:19 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-dbe55783 will be in active state
2022-04-07 00:34:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-dbe55783 to finished
2022-04-07 00:34:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-dbe55783 in namespace infra-namespace
2022-04-07 00:34:29 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-dbe55783 will be in active state
2022-04-07 00:34:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-dbe55783 to finished
2022-04-07 00:34:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-dbe55783-kafka-clients in namespace infra-namespace
2022-04-07 00:34:41 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-dbe55783-kafka-clients will be ready
2022-04-07 00:34:43 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-dbe55783-kafka-clients is ready
2022-04-07 00:34:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:34:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: oauth-cluster-plain-name will have desired state: Ready
2022-04-07 00:35:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaBridge: oauth-cluster-plain-name is in desired state: Ready
2022-04-07 00:35:02 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:35:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer-my-cluster-dbe55783 in namespace infra-namespace
2022-04-07 00:35:02 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer-my-cluster-dbe55783 will be in active state
2022-04-07 00:35:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:bridge-producer-my-cluster-dbe55783 to finished
2022-04-07 00:35:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-55c6da2c is in desired state: Ready
2022-04-07 00:35:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:35:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testSaslPlainAuthenticationKafkaConnectIsAbleToConnectToKafkaOAuth
2022-04-07 00:35:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-55c6da2c-allow in namespace infra-namespace
2022-04-07 00:35:17 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-55c6da2c in namespace infra-namespace
2022-04-07 00:35:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-55c6da2c-scraper in namespace infra-namespace
2022-04-07 00:36:07 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-55c6da2c-kafka-clients in namespace infra-namespace
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testSaslPlainAuthenticationKafkaConnectIsAbleToConnectToKafkaOAuth-FINISHED
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumer-STARTED
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-988908670-552857901 in namespace infra-namespace
2022-04-07 00:36:47 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-988908670-552857901 will have desired state: Ready
2022-04-07 00:36:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-988908670-552857901 is in desired state: Ready
2022-04-07 00:36:48 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:36:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-f38b51b0 in namespace infra-namespace
2022-04-07 00:36:48 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-f38b51b0 will be in active state
2022-04-07 00:36:49 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-f38b51b0 to finished
2022-04-07 00:36:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:36:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerBridge
2022-04-07 00:36:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-dbe55783-kafka-clients in namespace infra-namespace
2022-04-07 00:36:57 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-f38b51b0 in namespace infra-namespace
2022-04-07 00:36:57 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-f38b51b0 will be in active state
2022-04-07 00:36:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-f38b51b0 to finished
2022-04-07 00:37:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:37:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumer
2022-04-07 00:37:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-f38b51b0 in namespace infra-namespace
2022-04-07 00:37:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-f38b51b0 in namespace infra-namespace
2022-04-07 00:37:09 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-988908670-552857901 in namespace infra-namespace
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumer-FINISHED
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks-STARTED
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthPlainIsolatedST:161] Setting producer and consumer properties
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthPlainIsolatedST:174] Use clients without access token containing audience token
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:37:19 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-8bdd1a3d will be in active state
2022-04-07 00:37:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:oauth-producer-my-cluster-8bdd1a3d to finish with failure.
2022-04-07 00:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer-my-cluster-dbe55783 in namespace infra-namespace
2022-04-07 00:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:37:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-dbe55783 in namespace infra-namespace
2022-04-07 00:37:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-dbe55783 in namespace infra-namespace
2022-04-07 00:37:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-393690526-1443785456 in namespace infra-namespace
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerBridge-FINISHED
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerConnect-STARTED
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1543325891-669895308 in namespace infra-namespace
2022-04-07 00:38:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1543325891-669895308 will have desired state: Ready
2022-04-07 00:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1543325891-669895308 is in desired state: Ready
2022-04-07 00:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-c0d9b208 in namespace infra-namespace
2022-04-07 00:38:04 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-c0d9b208 will be in active state
2022-04-07 00:38:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-c0d9b208 to finished
2022-04-07 00:38:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-c0d9b208 in namespace infra-namespace
2022-04-07 00:38:13 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-c0d9b208 will be in active state
2022-04-07 00:38:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-c0d9b208 to finished
2022-04-07 00:38:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-c0d9b208-kafka-clients in namespace infra-namespace
2022-04-07 00:38:25 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c0d9b208-kafka-clients will be ready
2022-04-07 00:38:27 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c0d9b208-kafka-clients is ready
2022-04-07 00:38:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-c0d9b208-scraper in namespace infra-namespace
2022-04-07 00:38:27 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c0d9b208-scraper will be ready
2022-04-07 00:38:28 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c0d9b208-scraper is ready
2022-04-07 00:38:28 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-c0d9b208-scraper to be ready
2022-04-07 00:38:38 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-c0d9b208-scraper is ready
2022-04-07 00:38:38 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-c0d9b208-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 00:38:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-c0d9b208-allow in namespace infra-namespace
2022-04-07 00:38:38 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 00:38:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-c0d9b208 in namespace infra-namespace
2022-04-07 00:38:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-c0d9b208 will have desired state: Ready
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-c0d9b208 is in desired state: Ready
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-c0d9b208-connect-85f9fdc88d-lmx24 -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-c0d9b208-connect-85f9fdc88d-lmx24 -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-1543325891-669895308", "file": "/tmp/test-file-sink.txt" } }' http://localhost:8083/connectors
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:39:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-c0d9b208-connect-85f9fdc88d-lmx24
2022-04-07 00:39:53 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-c0d9b208-connect-85f9fdc88d-lmx24
2022-04-07 00:39:54 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:39:54 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck-STARTED
2022-04-07 00:39:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:39:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerConnect
2022-04-07 00:39:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-c0d9b208-scraper in namespace infra-namespace
2022-04-07 00:39:59 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:39:59 [ForkJoinPool-3-worker-11] [32mINFO [m [OauthPlainIsolatedST:213] Use clients with clientId not containing 'hello-world' in access token.
2022-04-07 00:39:59 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:39:59 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-producer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:39:59 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-producer-my-cluster-65b33e64 will be in active state
2022-04-07 00:40:00 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:kafka-audience-producer-my-cluster-65b33e64 to finish with failure.
2022-04-07 00:40:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-c0d9b208-kafka-clients in namespace infra-namespace
2022-04-07 00:41:00 [ForkJoinPool-3-worker-15] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testProducerConsumerAudienceTokenChecks$0(OauthPlainIsolatedST.java:176)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks(OauthPlainIsolatedST.java:176)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 00:41:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:100] Client job 'oauth-producer-my-cluster-8bdd1a3d' finished with expected timeout.
2022-04-07 00:41:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:41:00 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-8bdd1a3d will be in active state
2022-04-07 00:41:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:oauth-consumer-my-cluster-8bdd1a3d to finish with failure.
2022-04-07 00:41:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-c0d9b208 in namespace infra-namespace
2022-04-07 00:41:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-c0d9b208-allow in namespace infra-namespace
2022-04-07 00:41:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-c0d9b208 in namespace infra-namespace
2022-04-07 00:41:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-c0d9b208 in namespace infra-namespace
2022-04-07 00:41:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1543325891-669895308 in namespace infra-namespace
2022-04-07 00:41:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:41:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerConnect-FINISHED
2022-04-07 00:41:34 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:43:40 [ForkJoinPool-3-worker-11] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testAccessTokenClaimCheck$2(OauthPlainIsolatedST.java:229)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck(OauthPlainIsolatedST.java:229)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 00:43:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:100] Client job 'kafka-audience-producer-my-cluster-65b33e64' finished with expected timeout.
2022-04-07 00:43:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-consumer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:43:40 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-consumer-my-cluster-65b33e64 will be in active state
2022-04-07 00:43:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:kafka-audience-consumer-my-cluster-65b33e64 to finish with failure.
2022-04-07 00:44:41 [ForkJoinPool-3-worker-15] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testProducerConsumerAudienceTokenChecks$1(OauthPlainIsolatedST.java:178)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks(OauthPlainIsolatedST.java:178)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 00:44:41 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:100] Client job 'oauth-consumer-my-cluster-8bdd1a3d' finished with expected timeout.
2022-04-07 00:44:51 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthPlainIsolatedST:183] Use clients with Access token containing audience token
2022-04-07 00:44:51 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:44:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-producer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:44:51 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-producer-my-cluster-8bdd1a3d will be in active state
2022-04-07 00:44:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-producer-my-cluster-8bdd1a3d to finished
2022-04-07 00:45:00 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job kafka-audience-consumer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:45:00 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: kafka-audience-consumer-my-cluster-8bdd1a3d will be in active state
2022-04-07 00:45:01 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:kafka-audience-consumer-my-cluster-8bdd1a3d to finished
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerAudienceTokenChecks
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-producer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-consumer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-8bdd1a3d in namespace infra-namespace
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerAudienceTokenChecks-FINISHED
2022-04-07 00:45:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:47:21 [ForkJoinPool-3-worker-11] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.lambda$testAccessTokenClaimCheck$3(OauthPlainIsolatedST.java:231)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck(OauthPlainIsolatedST.java:231)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 00:47:21 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:100] Client job 'kafka-audience-consumer-my-cluster-65b33e64' finished with expected timeout.
2022-04-07 00:47:31 [ForkJoinPool-3-worker-11] [32mINFO [m [OauthPlainIsolatedST:236] Use clients with clientId containing 'hello-world' in access token.
2022-04-07 00:47:31 [ForkJoinPool-3-worker-11] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:47:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:47:31 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-65b33e64 will be in active state
2022-04-07 00:47:32 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-65b33e64 to finished
2022-04-07 00:47:40 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:47:40 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-65b33e64 will be in active state
2022-04-07 00:47:41 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-65b33e64 to finished
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for testAccessTokenClaimCheck
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:47:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-consumer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:47:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job kafka-audience-producer-my-cluster-65b33e64 in namespace infra-namespace
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testAccessTokenClaimCheck-FINISHED
2022-04-07 00:47:52 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker-STARTED
2022-04-07 00:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-764466321-1720843644 in namespace infra-namespace
2022-04-07 00:47:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-764466321-1720843644 will have desired state: Ready
2022-04-07 00:47:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-764466321-1720843644 is in desired state: Ready
2022-04-07 00:47:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-bd8fd23e in namespace infra-namespace
2022-04-07 00:47:53 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-bd8fd23e will be in active state
2022-04-07 00:47:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-bd8fd23e to finished
2022-04-07 00:48:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-bd8fd23e in namespace infra-namespace
2022-04-07 00:48:02 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-bd8fd23e will be in active state
2022-04-07 00:48:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-bd8fd23e to finished
2022-04-07 00:48:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-bd8fd23e-target in namespace infra-namespace
2022-04-07 00:48:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-bd8fd23e-target will have desired state: Ready
2022-04-07 00:49:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-bd8fd23e-target is in desired state: Ready
2022-04-07 00:49:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:49:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: oauth-cluster-plain-name will have desired state: Ready
2022-04-07 00:50:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: oauth-cluster-plain-name is in desired state: Ready
2022-04-07 00:50:26 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:440] Deleting the Job
2022-04-07 00:50:26 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:443] Creating new client with new consumer-group and also to point on my-cluster-bd8fd23e-target cluster
2022-04-07 00:50:26 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:50:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer in namespace infra-namespace
2022-04-07 00:50:26 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer will be in active state
2022-04-07 00:50:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer to finished
2022-04-07 00:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMaker
2022-04-07 00:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-bd8fd23e-target in namespace infra-namespace
2022-04-07 00:50:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-bd8fd23e in namespace infra-namespace
2022-04-07 00:50:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-bd8fd23e in namespace infra-namespace
2022-04-07 00:50:38 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-764466321-1720843644 in namespace infra-namespace
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer in namespace infra-namespace
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker-FINISHED
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker2-STARTED
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1521244564-845691685 in namespace infra-namespace
2022-04-07 00:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1521244564-845691685 will have desired state: Ready
2022-04-07 00:50:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1521244564-845691685 is in desired state: Ready
2022-04-07 00:50:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-0ff00ef9 in namespace infra-namespace
2022-04-07 00:50:49 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-0ff00ef9 will be in active state
2022-04-07 00:50:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-0ff00ef9 to finished
2022-04-07 00:50:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-0ff00ef9 in namespace infra-namespace
2022-04-07 00:50:58 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-0ff00ef9 will be in active state
2022-04-07 00:50:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-0ff00ef9 to finished
2022-04-07 00:51:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0ff00ef9-target in namespace infra-namespace
2022-04-07 00:51:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0ff00ef9-target will have desired state: Ready
2022-04-07 00:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0ff00ef9-target is in desired state: Ready
2022-04-07 00:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:52:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: oauth-cluster-plain-name will have desired state: Ready
2022-04-07 00:53:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: oauth-cluster-plain-name is in desired state: Ready
2022-04-07 00:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:590] Deleting the Job oauth-consumer-my-cluster-0ff00ef9
2022-04-07 00:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthPlainIsolatedST:593] Creating new client with new consumer-group and also to point on my-cluster-0ff00ef9-target cluster
2022-04-07 00:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 00:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-0ff00ef9 in namespace infra-namespace
2022-04-07 00:53:33 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-0ff00ef9 will be in active state
2022-04-07 00:53:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-0ff00ef9 to finished
2022-04-07 00:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerMirrorMaker2
2022-04-07 00:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0ff00ef9-target in namespace infra-namespace
2022-04-07 00:53:45 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-0ff00ef9 in namespace infra-namespace
2022-04-07 00:53:45 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-0ff00ef9 in namespace infra-namespace
2022-04-07 00:53:45 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1521244564-845691685 in namespace infra-namespace
2022-04-07 00:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-0ff00ef9 in namespace infra-namespace
2022-04-07 00:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST.testProducerConsumerMirrorMaker2-FINISHED
2022-04-07 00:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 00:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 00:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 00:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthPlainIsolatedST
2022-04-07 00:54:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-plain-name in namespace infra-namespace
2022-04-07 00:54:09 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 00:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthPlainIsolatedST is everything deleted.
2022-04-07 00:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,563.337 s - in io.strimzi.systemtest.security.oauth.OauthPlainIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
2022-04-07 00:54:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:54:44 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:54:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:54:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:54:54 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 00:55:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:04 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:55:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:55:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:24 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 00:55:31 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 00:55:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 00:55:42 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 00:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 00:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 00:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-07 00:55:52 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to kafka-authz realm
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to kafka-authz realm
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to kafka-authz realm
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:55] Using HTTPS endpoints
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-authz-name in namespace infra-namespace
2022-04-07 00:57:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-07 00:58:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-07 00:58:51 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:680] Setting producer and consumer properties
2022-04-07 00:58:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-a-client in namespace infra-namespace
2022-04-07 00:58:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-a-client will have desired state: Ready
2022-04-07 00:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: team-a-client is in desired state: Ready
2022-04-07 00:58:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-b-client in namespace infra-namespace
2022-04-07 00:58:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-b-client will have desired state: Ready
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: team-b-client is in desired state: Ready
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSessionReAuthentication-STARTED
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:443] Verifying that team A producer is able to send messages to the x-topic-example-topic topic -> the topic starting with 'x'
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topic-example-topic in namespace infra-namespace
2022-04-07 00:58:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topic-example-topic will have desired state: Ready
2022-04-07 00:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topic-example-topic is in desired state: Ready
2022-04-07 00:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-example-topic in namespace infra-namespace
2022-04-07 00:58:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-example-topic will have desired state: Ready
2022-04-07 00:58:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-example-topic is in desired state: Ready
2022-04-07 00:58:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5638f96b in namespace infra-namespace
2022-04-07 00:58:56 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5638f96b will be in active state
2022-04-07 00:58:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-5638f96b to finished
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:465] Adding the maxSecondsWithoutReauthentication to Kafka listener with OAuth authentication
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:493] Setting the master realm token's lifespan to 3600s
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X POST -d client_id=admin-cli&client_secret=aGVsbG8td29ybGQtcHJvZHVjZXItc2VjcmV0&grant_type=password&username=admin&password=GEVqpjBs-Ov4eg== https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/master/protocol/openid-connect/token
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X GET https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/master -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfUVZqNllLQm55VE1RbGxMSHRiNDk0OVlCcFJ3UVdBNHVTNk03RGRGU2hrIn0.eyJleHAiOjE2NDkyOTMyMDYsImlhdCI6MTY0OTI5MzE0NiwianRpIjoiMjBhNDM3NTctOTlmOS00MmVjLTgzMmQtOWQ4NjljZWI5NmE0IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiI4MmFlMGU0Ny0wNzg1LTRhMjQtODRiMS0wNjdjMTM3ZDVmMGYiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiOTIzZTMzMDctYjgwMy00NDdlLTliODYtNjg5NTZmZDMwODIyIiwiYWNyIjoiMSIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.Hlbbhp9RKgTPOmw4h1ki-X9z9pQmOYvCc5o0ZX0yizVUMLoSUUv1GsVZ0phtTCyWECOF1ivNxyvQFh1hg6aEPMKkkZPSWqhc4AbVHMi9uTL2mB2t5dg3LLReAmp6m6xqfqjTS7kShrfS2Tw-hy1rYXhonU6I8ivfxadVo7GkJgfNBiETqp844QEHNRG0GRbSqnkEFmGvgO7mWH9dR6ZLmM8osH-X6ktzKd0pc3O4AouOQy2Qh89Co_-AGHxV-QSfLsTNvsA9TGP8w8OYlPYcbLXSOO39vkiqSn9JqoVSLAj1A2azyvbKrBjx5sqziK4-apSw1fcSbLW87__2rb2k_w
2022-04-07 00:59:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X PUT https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/master -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfUVZqNllLQm55VE1RbGxMSHRiNDk0OVlCcFJ3UVdBNHVTNk03RGRGU2hrIn0.eyJleHAiOjE2NDkyOTMyMDYsImlhdCI6MTY0OTI5MzE0NiwianRpIjoiMjBhNDM3NTctOTlmOS00MmVjLTgzMmQtOWQ4NjljZWI5NmE0IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiI4MmFlMGU0Ny0wNzg1LTRhMjQtODRiMS0wNjdjMTM3ZDVmMGYiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiOTIzZTMzMDctYjgwMy00NDdlLTliODYtNjg5NTZmZDMwODIyIiwiYWNyIjoiMSIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.Hlbbhp9RKgTPOmw4h1ki-X9z9pQmOYvCc5o0ZX0yizVUMLoSUUv1GsVZ0phtTCyWECOF1ivNxyvQFh1hg6aEPMKkkZPSWqhc4AbVHMi9uTL2mB2t5dg3LLReAmp6m6xqfqjTS7kShrfS2Tw-hy1rYXhonU6I8ivfxadVo7GkJgfNBiETqp844QEHNRG0GRbSqnkEFmGvgO7mWH9dR6ZLmM8osH-X6ktzKd0pc3O4AouOQy2Qh89Co_-AGHxV-QSfLsTNvsA9TGP8w8OYlPYcbLXSOO39vkiqSn9JqoVSLAj1A2azyvbKrBjx5sqziK4-apSw1fcSbLW87__2rb2k_w -d {"id":"master","realm":"master","displayName":"Keycloak","displayNameHtml":"<div class=\"kc-logo-text\"><span>Keycloak</span></div>","notBefore":0,"revokeRefreshToken":false,"refreshTokenMaxReuse":0,"accessTokenLifespan":"3600","accessTokenLifespanForImplicitFlow":900,"ssoSessionIdleTimeout":1800,"ssoSessionMaxLifespan":36000,"ssoSessionIdleTimeoutRememberMe":0,"ssoSessionMaxLifespanRememberMe":0,"offlineSessionIdleTimeout":2592000,"offlineSessionMaxLifespanEnabled":false,"offlineSessionMaxLifespan":5184000,"clientSessionIdleTimeout":0,"clientSessionMaxLifespan":0,"clientOfflineSessionIdleTimeout":0,"clientOfflineSessionMaxLifespan":0,"accessCodeLifespan":60,"accessCodeLifespanUserAction":300,"accessCodeLifespanLogin":1800,"actionTokenGeneratedByAdminLifespan":43200,"actionTokenGeneratedByUserLifespan":300,"enabled":true,"sslRequired":"external","registrationAllowed":false,"registrationEmailAsUsername":false,"rememberMe":false,"verifyEmail":false,"loginWithEmailAllowed":true,"duplicateEmailsAllowed":false,"resetPasswordAllowed":false,"editUsernameAllowed":false,"bruteForceProtected":false,"permanentLockout":false,"maxFailureWaitSeconds":900,"minimumQuickLoginWaitSeconds":60,"waitIncrementSeconds":60,"quickLoginCheckMilliSeconds":1000,"maxDeltaTimeSeconds":43200,"failureFactor":30,"defaultRoles":["offline_access","uma_authorization"],"requiredCredentials":["password"],"otpPolicyType":"totp","otpPolicyAlgorithm":"HmacSHA1","otpPolicyInitialCounter":0,"otpPolicyDigits":6,"otpPolicyLookAheadWindow":1,"otpPolicyPeriod":30,"otpSupportedApplications":["FreeOTP","Google Authenticator"],"webAuthnPolicyRpEntityName":"keycloak","webAuthnPolicySignatureAlgorithms":["ES256"],"webAuthnPolicyRpId":"","webAuthnPolicyAttestationConveyancePreference":"not specified","webAuthnPolicyAuthenticatorAttachment":"not specified","webAuthnPolicyRequireResidentKey":"not specified","webAuthnPolicyUserVerificationRequirement":"not specified","webAuthnPolicyCreateTimeout":0,"webAuthnPolicyAvoidSameAuthenticatorRegister":false,"webAuthnPolicyAcceptableAaguids":[],"webAuthnPolicyPasswordlessRpEntityName":"keycloak","webAuthnPolicyPasswordlessSignatureAlgorithms":["ES256"],"webAuthnPolicyPasswordlessRpId":"","webAuthnPolicyPasswordlessAttestationConveyancePreference":"not specified","webAuthnPolicyPasswordlessAuthenticatorAttachment":"not specified","webAuthnPolicyPasswordlessRequireResidentKey":"not specified","webAuthnPolicyPasswordlessUserVerificationRequirement":"not specified","webAuthnPolicyPasswordlessCreateTimeout":0,"webAuthnPolicyPasswordlessAvoidSameAuthenticatorRegister":false,"webAuthnPolicyPasswordlessAcceptableAaguids":[],"browserSecurityHeaders":{"contentSecurityPolicyReportOnly":"","xContentTypeOptions":"nosniff","xRobotsTag":"none","xFrameOptions":"SAMEORIGIN","xXSSProtection":"1; mode=block","contentSecurityPolicy":"frame-src 'self'; frame-ancestors 'self'; object-src 'none';","strictTransportSecurity":"max-age=31536000; includeSubDomains"},"smtpServer":{},"eventsEnabled":false,"eventsListeners":["jboss-logging"],"enabledEventTypes":[],"adminEventsEnabled":false,"adminEventsDetailsEnabled":false,"internationalizationEnabled":false,"supportedLocales":[],"browserFlow":"browser","registrationFlow":"registration","directGrantFlow":"direct grant","resetCredentialsFlow":"reset credentials","clientAuthenticationFlow":"clients","dockerAuthenticationFlow":"docker auth","attributes":{},"userManagedAccessAllowed":false} -H Content-Type: application/json
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X POST -d client_id=admin-cli&client_secret=aGVsbG8td29ybGQtcHJvZHVjZXItc2VjcmV0&grant_type=password&username=admin&password=GEVqpjBs-Ov4eg== https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/master/protocol/openid-connect/token
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:508] Getting the kafka-authz kafka client for obtaining the Dev A Team policy for the x topics
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X GET https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfUVZqNllLQm55VE1RbGxMSHRiNDk0OVlCcFJ3UVdBNHVTNk03RGRGU2hrIn0.eyJleHAiOjE2NDkyOTY3NDcsImlhdCI6MTY0OTI5MzE0NywianRpIjoiZjE5OGMzMjgtMjQ1MS00OTExLTgyZjYtNjA2MDQzZTMzM2E1IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiI4MmFlMGU0Ny0wNzg1LTRhMjQtODRiMS0wNjdjMTM3ZDVmMGYiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZDNmMTg2YTMtNWZkZS00NTM5LTk3Y2YtNzdlMzM3NjgyYWUyIiwiYWNyIjoiMSIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.IihtiDh1a_KTjKZohFfW5bRI3ZLLV3l9Uzqi4Vf_Smmdy-viNvRgUxczCbMPUMeFrJ4wyQ-P8z1X_tZwECQ5YcafZa2mYvtw1Qq_mBWppjarDIxcq-jqNWTpfVajVjoNNBljRmaJNsT1wU7SPKeCNSqNzFl_hmBB6ou-jh4jFT0PNLYeI9dhlTcQpFYrKDEi_cHsVDhUVf29UYcV5yJj5zfTZOI2S2-fBg0y-9OIIdlH0gaVKwHsxVTjaszpvH-v0wsQ61w6t3C0-2cgCSaX9VZdQK5Wuh2_nMrQhBtb9s8dakxAmdgR0K5mipN40KAMWWAzHkg5OYjXOyLBU9XDHA
2022-04-07 00:59:07 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X GET https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients/51eeb51c-7e7e-4e59-9d3f-9711df860375/authz/resource-server/policy -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfUVZqNllLQm55VE1RbGxMSHRiNDk0OVlCcFJ3UVdBNHVTNk03RGRGU2hrIn0.eyJleHAiOjE2NDkyOTY3NDcsImlhdCI6MTY0OTI5MzE0NywianRpIjoiZjE5OGMzMjgtMjQ1MS00OTExLTgyZjYtNjA2MDQzZTMzM2E1IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiI4MmFlMGU0Ny0wNzg1LTRhMjQtODRiMS0wNjdjMTM3ZDVmMGYiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZDNmMTg2YTMtNWZkZS00NTM5LTk3Y2YtNzdlMzM3NjgyYWUyIiwiYWNyIjoiMSIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.IihtiDh1a_KTjKZohFfW5bRI3ZLLV3l9Uzqi4Vf_Smmdy-viNvRgUxczCbMPUMeFrJ4wyQ-P8z1X_tZwECQ5YcafZa2mYvtw1Qq_mBWppjarDIxcq-jqNWTpfVajVjoNNBljRmaJNsT1wU7SPKeCNSqNzFl_hmBB6ou-jh4jFT0PNLYeI9dhlTcQpFYrKDEi_cHsVDhUVf29UYcV5yJj5zfTZOI2S2-fBg0y-9OIIdlH0gaVKwHsxVTjaszpvH-v0wsQ61w6t3C0-2cgCSaX9VZdQK5Wuh2_nMrQhBtb9s8dakxAmdgR0K5mipN40KAMWWAzHkg5OYjXOyLBU9XDHA
2022-04-07 00:59:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:08 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:539] Changing the Dev Team A policy for topics starting with x- and checking that job will not be successful
2022-04-07 00:59:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X PUT https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients/51eeb51c-7e7e-4e59-9d3f-9711df860375/authz/resource-server/policy/f9040c42-2743-4b47-9b17-01da6b498f07 -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfUVZqNllLQm55VE1RbGxMSHRiNDk0OVlCcFJ3UVdBNHVTNk03RGRGU2hrIn0.eyJleHAiOjE2NDkyOTY3NDcsImlhdCI6MTY0OTI5MzE0NywianRpIjoiZjE5OGMzMjgtMjQ1MS00OTExLTgyZjYtNjA2MDQzZTMzM2E1IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiI4MmFlMGU0Ny0wNzg1LTRhMjQtODRiMS0wNjdjMTM3ZDVmMGYiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZDNmMTg2YTMtNWZkZS00NTM5LTk3Y2YtNzdlMzM3NjgyYWUyIiwiYWNyIjoiMSIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.IihtiDh1a_KTjKZohFfW5bRI3ZLLV3l9Uzqi4Vf_Smmdy-viNvRgUxczCbMPUMeFrJ4wyQ-P8z1X_tZwECQ5YcafZa2mYvtw1Qq_mBWppjarDIxcq-jqNWTpfVajVjoNNBljRmaJNsT1wU7SPKeCNSqNzFl_hmBB6ou-jh4jFT0PNLYeI9dhlTcQpFYrKDEi_cHsVDhUVf29UYcV5yJj5zfTZOI2S2-fBg0y-9OIIdlH0gaVKwHsxVTjaszpvH-v0wsQ61w6t3C0-2cgCSaX9VZdQK5Wuh2_nMrQhBtb9s8dakxAmdgR0K5mipN40KAMWWAzHkg5OYjXOyLBU9XDHA -d {"id":"f9040c42-2743-4b47-9b17-01da6b498f07","name":"Dev Team A can write to topics that start with x- on any cluster","type":"scope","logic":"POSITIVE","decisionStrategy":"UNANIMOUS","config":{"resources":"[\"Topic:x-*\"]","scopes":"[\"Describe\"]","applyPolicies":"[\"Dev Team A\"]"}} -H Content-Type: application/json
2022-04-07 00:59:08 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 00:59:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-5638f96b to finished
2022-04-07 01:02:48 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestUtils:162] Exception waiting for job finished, null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientSuccess(ClientUtils.java:77)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientSuccess(ClientUtils.java:72)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.lambda$testSessionReAuthentication$2(OauthAuthorizationIsolatedST.java:541)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3082)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSessionReAuthentication(OauthAuthorizationIsolatedST.java:541)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 01:02:48 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:545] Sending messages to topic starting with a- -> the messages should be successfully sent
2022-04-07 01:02:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5638f96b in namespace infra-namespace
2022-04-07 01:02:48 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5638f96b will be in active state
2022-04-07 01:02:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-5638f96b to finished
2022-04-07 01:02:57 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:554] Changing back to the original settings and checking, if the producer will be successful
2022-04-07 01:02:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec strimzi-cluster-operator-78689684d4-pncs5 -- curl -v --insecure -X PUT https://keycloak.infra-namespace.svc.cluster.local:8443/auth/admin/realms/kafka-authz/clients/51eeb51c-7e7e-4e59-9d3f-9711df860375/authz/resource-server/policy/f9040c42-2743-4b47-9b17-01da6b498f07 -H Authorization: Bearer eyJhbGciOiJSUzI1NiIsInR5cCIgOiAiSldUIiwia2lkIiA6ICJfUVZqNllLQm55VE1RbGxMSHRiNDk0OVlCcFJ3UVdBNHVTNk03RGRGU2hrIn0.eyJleHAiOjE2NDkyOTY3NDcsImlhdCI6MTY0OTI5MzE0NywianRpIjoiZjE5OGMzMjgtMjQ1MS00OTExLTgyZjYtNjA2MDQzZTMzM2E1IiwiaXNzIjoiaHR0cHM6Ly9rZXljbG9hay5pbmZyYS1uYW1lc3BhY2Uuc3ZjLmNsdXN0ZXIubG9jYWw6ODQ0My9hdXRoL3JlYWxtcy9tYXN0ZXIiLCJzdWIiOiI4MmFlMGU0Ny0wNzg1LTRhMjQtODRiMS0wNjdjMTM3ZDVmMGYiLCJ0eXAiOiJCZWFyZXIiLCJhenAiOiJhZG1pbi1jbGkiLCJzZXNzaW9uX3N0YXRlIjoiZDNmMTg2YTMtNWZkZS00NTM5LTk3Y2YtNzdlMzM3NjgyYWUyIiwiYWNyIjoiMSIsInNjb3BlIjoiZW1haWwgcHJvZmlsZSIsImVtYWlsX3ZlcmlmaWVkIjpmYWxzZSwicHJlZmVycmVkX3VzZXJuYW1lIjoiYWRtaW4ifQ.IihtiDh1a_KTjKZohFfW5bRI3ZLLV3l9Uzqi4Vf_Smmdy-viNvRgUxczCbMPUMeFrJ4wyQ-P8z1X_tZwECQ5YcafZa2mYvtw1Qq_mBWppjarDIxcq-jqNWTpfVajVjoNNBljRmaJNsT1wU7SPKeCNSqNzFl_hmBB6ou-jh4jFT0PNLYeI9dhlTcQpFYrKDEi_cHsVDhUVf29UYcV5yJj5zfTZOI2S2-fBg0y-9OIIdlH0gaVKwHsxVTjaszpvH-v0wsQ61w6t3C0-2cgCSaX9VZdQK5Wuh2_nMrQhBtb9s8dakxAmdgR0K5mipN40KAMWWAzHkg5OYjXOyLBU9XDHA -d {"id":"f9040c42-2743-4b47-9b17-01da6b498f07","name":"Dev Team A can write to topics that start with x- on any cluster","type":"scope","logic":"POSITIVE","decisionStrategy":"UNANIMOUS","config":{"resources":"[\"Topic:x-*\"]","scopes":"[\"Describe\",\"Write\"]","applyPolicies":"[\"Dev Team A\"]"}} -H Content-Type: application/json
2022-04-07 01:02:58 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:02:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5638f96b in namespace infra-namespace
2022-04-07 01:02:58 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5638f96b will be in active state
2022-04-07 01:02:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-5638f96b to finished
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:568] Changing configuration of Kafka back to it's original form
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSessionReAuthentication
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5638f96b in namespace infra-namespace
2022-04-07 01:04:48 [ForkJoinPool-3-worker-13] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-example-topic in namespace infra-namespace
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5638f96b in namespace infra-namespace
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5638f96b in namespace infra-namespace
2022-04-07 01:04:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topic-example-topic in namespace infra-namespace
2022-04-07 01:04:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSessionReAuthentication-FINISHED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.smokeTestForClients-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSuperUserWithOauthAuthorization-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopicStartingWithXAndTeamBReadFromTopicStartingWithX-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-11] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:04:58 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:04:58 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testKeycloakAuthorizerToDelegateToSimpleAuthorizer-STARTED
2022-04-07 01:04:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-163528163-649133180 in namespace infra-namespace
2022-04-07 01:04:58 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-163528163-649133180 will have desired state: Ready
2022-04-07 01:05:03 [ForkJoinPool-3-worker-25] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:05:03 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topicmy-topic-961500107-661951139 in namespace infra-namespace
2022-04-07 01:05:03 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topicmy-topic-961500107-661951139 will have desired state: Ready
2022-04-07 01:05:16 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topicmy-topic-961500107-661951139 is in desired state: Ready
2022-04-07 01:05:16 [ForkJoinPool-3-worker-25] [32mINFO [m [OauthAuthorizationIsolatedST:347] Verifying that team B is not able write to topic starting with 'x-' because in kafka clusterdoes not have super-users to break authorization rules
2022-04-07 01:05:16 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-268708934-1406075262 in namespace infra-namespace
2022-04-07 01:05:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-163528163-649133180 is in desired state: Ready
2022-04-07 01:05:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-1d8a0be3 in namespace infra-namespace
2022-04-07 01:05:16 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-1d8a0be3 will be in active state
2022-04-07 01:05:16 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-268708934-1406075262 will have desired state: Ready
2022-04-07 01:05:16 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-1d8a0be3 to finished
2022-04-07 01:05:17 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-268708934-1406075262 is in desired state: Ready
2022-04-07 01:05:17 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:05:17 [ForkJoinPool-3-worker-25] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-11c47e4d will be in active state
2022-04-07 01:05:18 [ForkJoinPool-3-worker-25] [32mINFO [m [JobUtils:70] Waiting for job: team-b-client-producer-my-cluster-11c47e4d will be in error state
2022-04-07 01:05:30 [ForkJoinPool-3-worker-25] [32mINFO [m [OauthAuthorizationIsolatedST:370] Verifying that team A is not able read to topic starting with 'x-' because in kafka clusterdoes not have super-users to break authorization rules
2022-04-07 01:05:30 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:05:30 [ForkJoinPool-3-worker-25] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-11c47e4d will be in active state
2022-04-07 01:05:30 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-1d8a0be3 in namespace infra-namespace
2022-04-07 01:05:30 [ForkJoinPool-3-worker-11] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-1d8a0be3 will be in active state
2022-04-07 01:05:31 [ForkJoinPool-3-worker-25] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-consumer-my-cluster-11c47e4d will be in error state
2022-04-07 01:05:31 [ForkJoinPool-3-worker-11] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-consumer-my-cluster-1d8a0be3 to finished
2022-04-07 01:05:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:05:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:348] Delete all resources for smokeTestForClients
2022-04-07 01:05:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-1d8a0be3 in namespace infra-namespace
2022-04-07 01:05:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-1d8a0be3 in namespace infra-namespace
2022-04-07 01:05:37 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-163528163-649133180 in namespace infra-namespace
2022-04-07 01:05:39 [ForkJoinPool-3-worker-25] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: oauth-cluster-authz-name-kafka rolling update
2022-04-07 01:05:47 [ForkJoinPool-3-worker-11] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:05:47 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.smokeTestForClients-FINISHED
2022-04-07 01:05:47 [ForkJoinPool-3-worker-11] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:05:48 [ForkJoinPool-3-worker-23] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:05:48 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topicmy-topic-1580054107-1834944820 in namespace infra-namespace
2022-04-07 01:05:48 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topicmy-topic-1580054107-1834944820 will have desired state: Ready
2022-04-07 01:05:49 [ForkJoinPool-3-worker-25] [32mINFO [m [RollingUpdateUtils:86] Component with name: oauth-cluster-authz-name-kafka has been successfully rolled
2022-04-07 01:05:49 [ForkJoinPool-3-worker-25] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of oauth-cluster-authz-name-kafka to be ready
2022-04-07 01:05:59 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topicmy-topic-1580054107-1834944820 is in desired state: Ready
2022-04-07 01:05:59 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-0f0a11da in namespace infra-namespace
2022-04-07 01:05:59 [ForkJoinPool-3-worker-23] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-0f0a11da will be in active state
2022-04-07 01:06:00 [ForkJoinPool-3-worker-23] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-0f0a11da to finished
2022-04-07 01:06:14 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-consumer-my-cluster-0f0a11da in namespace infra-namespace
2022-04-07 01:06:14 [ForkJoinPool-3-worker-23] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-consumer-my-cluster-0f0a11da will be in active state
2022-04-07 01:06:15 [ForkJoinPool-3-worker-23] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-b-client-consumer-my-cluster-0f0a11da to finished
2022-04-07 01:06:17 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-07 01:06:17 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-07 01:06:17 [ForkJoinPool-3-worker-25] [32mINFO [m [RollingUpdateUtils:132] Kafka: oauth-cluster-authz-name is ready
2022-04-07 01:06:17 [ForkJoinPool-3-worker-25] [32mINFO [m [OauthAuthorizationIsolatedST:404] Verifying that team B is able to write to topic starting with 'x-' and break authorization rule
2022-04-07 01:06:17 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:06:17 [ForkJoinPool-3-worker-25] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-11c47e4d will be in active state
2022-04-07 01:06:19 [ForkJoinPool-3-worker-25] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-b-client-producer-my-cluster-11c47e4d to finished
2022-04-07 01:06:27 [ForkJoinPool-3-worker-25] [32mINFO [m [OauthAuthorizationIsolatedST:409] Verifying that team A is able to write to topic starting with 'x-' and break authorization rule
2022-04-07 01:06:27 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:06:27 [ForkJoinPool-3-worker-25] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-11c47e4d will be in active state
2022-04-07 01:06:27 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:06:27 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAWriteToTopicStartingWithXAndTeamBReadFromTopicStartingWithX
2022-04-07 01:06:27 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-0f0a11da in namespace infra-namespace
2022-04-07 01:06:27 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-consumer-my-cluster-0f0a11da in namespace infra-namespace
2022-04-07 01:06:27 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topicmy-topic-1580054107-1834944820 in namespace infra-namespace
2022-04-07 01:06:28 [ForkJoinPool-3-worker-25] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-consumer-my-cluster-11c47e4d to finished
2022-04-07 01:06:37 [ForkJoinPool-3-worker-23] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:06:37 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopicStartingWithXAndTeamBReadFromTopicStartingWithX-FINISHED
2022-04-07 01:06:37 [ForkJoinPool-3-worker-23] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:06:38 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:06:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-301218165-49662585 in namespace infra-namespace
2022-04-07 01:06:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-301218165-49662585 will have desired state: Ready
2022-04-07 01:06:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-301218165-49662585 is in desired state: Ready
2022-04-07 01:06:39 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:208] Sending 100 messages to broker with topic name a-topic-my-topic-301218165-49662585
2022-04-07 01:06:39 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-e1f35a62 in namespace infra-namespace
2022-04-07 01:06:39 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-e1f35a62 will be in active state
2022-04-07 01:06:40 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:06:40 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:348] Delete all resources for testSuperUserWithOauthAuthorization
2022-04-07 01:06:40 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:06:40 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:06:40 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:06:40 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-268708934-1406075262 in namespace infra-namespace
2022-04-07 01:06:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-e1f35a62 to finished
2022-04-07 01:06:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-e1f35a62 in namespace infra-namespace
2022-04-07 01:06:48 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-e1f35a62 will be in active state
2022-04-07 01:06:49 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-consumer-my-cluster-e1f35a62 will be in error state
2022-04-07 01:06:50 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-11c47e4d in namespace infra-namespace
2022-04-07 01:06:50 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topicmy-topic-961500107-661951139 in namespace infra-namespace
2022-04-07 01:07:00 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:07:00 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testSuperUserWithOauthAuthorization-FINISHED
2022-04-07 01:07:00 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:07:03 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:07:03 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-312948327-1475846973 in namespace infra-namespace
2022-04-07 01:07:03 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-312948327-1475846973 will have desired state: Ready
2022-04-07 01:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-312948327-1475846973 is in desired state: Ready
2022-04-07 01:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [OauthAuthorizationIsolatedST:259] Sending 100 messages to broker with topic name my-topic-2043299187-274155238
2022-04-07 01:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-5b6da817 in namespace infra-namespace
2022-04-07 01:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-5b6da817 will be in active state
2022-04-07 01:07:05 [ForkJoinPool-3-worker-19] [32mINFO [m [JobUtils:70] Waiting for job: team-b-client-producer-my-cluster-5b6da817 will be in error state
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestUtils:162] Exception waiting for job finished, null
io.strimzi.test.WaitException: Timeout after 30000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.JobUtils.waitForJobFailure(JobUtils.java:71)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic(OauthAuthorizationIsolatedST.java:220)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestExecutionWatcher:28] OauthAuthorizationIsolatedST - Exception Timeout after 30000 ms waiting for job finished has been thrown in @Test. Going to collect logs from components.
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [33mWARN [m [LogCollector:247] Searching for logs in all pods failed! Some of the logs will not be stored. Exception messagenull
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 01:07:19 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 01:07:20 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 01:07:20 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 01:07:20 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 01:07:20 [ForkJoinPool-3-worker-1] [32mINFO [m [OauthAbstractST:153] Deleting team-a-client-consumer-my-cluster-e1f35a62 job
2022-04-07 01:07:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:07:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAReadFromTopic
2022-04-07 01:07:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-e1f35a62 in namespace infra-namespace
2022-04-07 01:07:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-e1f35a62 in namespace infra-namespace
2022-04-07 01:07:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-301218165-49662585 in namespace infra-namespace
2022-04-07 01:07:35 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:07:35 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic-FINISHED
2022-04-07 01:07:35 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [1;31mERROR[m [TestUtils:162] Exception waiting for job finished, null
io.strimzi.test.WaitException: Timeout after 30000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.JobUtils.waitForJobFailure(JobUtils.java:71)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic(OauthAuthorizationIsolatedST.java:262)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [1;31mERROR[m [TestExecutionWatcher:28] OauthAuthorizationIsolatedST - Exception Timeout after 30000 ms waiting for job finished has been thrown in @Test. Going to collect logs from components.
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [33mWARN [m [LogCollector:247] Searching for logs in all pods failed! Some of the logs will not be stored. Exception messagenull
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 01:07:35 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 01:07:36 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 01:07:36 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 01:07:36 [ForkJoinPool-3-worker-19] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 01:07:36 [ForkJoinPool-3-worker-19] [32mINFO [m [OauthAbstractST:153] Deleting team-b-client-producer-my-cluster-5b6da817 job
2022-04-07 01:07:38 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:07:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1807018827-1924245105 in namespace infra-namespace
2022-04-07 01:07:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1807018827-1924245105 will have desired state: Ready
2022-04-07 01:07:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1807018827-1924245105 is in desired state: Ready
2022-04-07 01:07:39 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthAuthorizationIsolatedST:146] Sending 100 messages to broker with topic name my-topic-1807018827-1924245105
2022-04-07 01:07:39 [ForkJoinPool-3-worker-15] [32mINFO [m [OauthAuthorizationIsolatedST:147] Producer will not produce messages because authorization topic will failed. Team A can write only to topic starting with 'x-'
2022-04-07 01:07:39 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-2787e778 in namespace infra-namespace
2022-04-07 01:07:39 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-2787e778 will be in active state
2022-04-07 01:07:40 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-producer-my-cluster-2787e778 will be in error state
2022-04-07 01:07:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:07:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamBWriteToTopic
2022-04-07 01:07:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-5b6da817 in namespace infra-namespace
2022-04-07 01:07:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-312948327-1475846973 in namespace infra-namespace
2022-04-07 01:07:51 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:07:51 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic-FINISHED
2022-04-07 01:07:51 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-97 for test case:testKeycloakAuthorizerToDelegateToSimpleAuthorizer
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Secret sso-x509-https-secret in namespace infra-namespace
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Secret team-a-client-secret in namespace infra-namespace
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Secret team-b-client-secret in namespace infra-namespace
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-b6c6b4fa in namespace namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:07:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-b6c6b4fa will have desired state: Ready
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [1;31mERROR[m [TestUtils:162] Exception waiting for job finished, null
io.strimzi.test.WaitException: Timeout after 30000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.JobUtils.waitForJobFailure(JobUtils.java:71)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic(OauthAuthorizationIsolatedST.java:150)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [1;31mERROR[m [TestExecutionWatcher:28] OauthAuthorizationIsolatedST - Exception Timeout after 30000 ms waiting for job finished has been thrown in @Test. Going to collect logs from components.
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [33mWARN [m [LogCollector:247] Searching for logs in all pods failed! Some of the logs will not be stored. Exception messagenull
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 01:08:10 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAWriteToTopic
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-2787e778 in namespace infra-namespace
2022-04-07 01:08:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1807018827-1924245105 in namespace infra-namespace
2022-04-07 01:08:21 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:08:21 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic-FINISHED
2022-04-07 01:08:21 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:09:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-b6c6b4fa is in desired state: Ready
2022-04-07 01:09:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-a-client in namespace namespace-97
2022-04-07 01:09:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:09:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-a-client will have desired state: Ready
2022-04-07 01:09:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaUser: team-a-client is in desired state: Ready
2022-04-07 01:09:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-b-client in namespace namespace-97
2022-04-07 01:09:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:09:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-b-client will have desired state: Ready
2022-04-07 01:09:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaUser: team-b-client is in desired state: Ready
2022-04-07 01:09:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-1852991796-1161964679 in namespace namespace-97
2022-04-07 01:09:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:09:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-1852991796-1161964679 will have desired state: Ready
2022-04-07 01:09:16 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-1852991796-1161964679 is in desired state: Ready
2022-04-07 01:09:16 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-b6c6b4fa in namespace namespace-97
2022-04-07 01:09:16 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:09:16 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-b6c6b4fa will be in active state
2022-04-07 01:09:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-b6c6b4fa to finished
2022-04-07 01:09:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-b6c6b4fa in namespace namespace-97
2022-04-07 01:09:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-97
2022-04-07 01:09:25 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-b6c6b4fa will be in active state
2022-04-07 01:09:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-consumer-my-cluster-b6c6b4fa to finished
2022-04-07 01:09:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:09:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKeycloakAuthorizerToDelegateToSimpleAuthorizer
2022-04-07 01:09:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-b-client in namespace namespace-97
2022-04-07 01:09:33 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Secret team-b-client-secret in namespace namespace-97
2022-04-07 01:09:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-a-client in namespace namespace-97
2022-04-07 01:09:43 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-b6c6b4fa in namespace namespace-97
2022-04-07 01:09:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Secret team-a-client-secret in namespace namespace-97
2022-04-07 01:09:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-b6c6b4fa in namespace namespace-97
2022-04-07 01:09:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-b6c6b4fa in namespace namespace-97
2022-04-07 01:09:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-1852991796-1161964679 in namespace namespace-97
2022-04-07 01:10:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Secret sso-x509-https-secret in namespace namespace-97
2022-04-07 01:10:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:10:13 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-97 for test case:testKeycloakAuthorizerToDelegateToSimpleAuthorizer
2022-04-07 01:10:18 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testKeycloakAuthorizerToDelegateToSimpleAuthorizer-FINISHED
2022-04-07 01:10:18 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:10:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 01:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 01:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthAuthorizationIsolatedST
2022-04-07 01:10:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-a-client in namespace infra-namespace
2022-04-07 01:10:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-authz-name in namespace infra-namespace
2022-04-07 01:10:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 01:10:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-b-client in namespace infra-namespace
2022-04-07 01:10:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:10:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:10:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthAuthorizationIsolatedST is everything deleted.
2022-04-07 01:10:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 10, Failures: 0, Errors: 3, Skipped: 2, Time elapsed: 983.834 s <<< FAILURE! - in io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic(ExtensionContext)  Time elapsed: 156.958 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 30000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.JobUtils.waitForJobFailure(JobUtils.java:71)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic(OauthAuthorizationIsolatedST.java:220)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;31mERROR[m] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic(ExtensionContext)  Time elapsed: 172.867 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 30000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.JobUtils.waitForJobFailure(JobUtils.java:71)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic(OauthAuthorizationIsolatedST.java:262)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;31mERROR[m] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic(ExtensionContext)  Time elapsed: 202.838 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 30000 ms waiting for job finished
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.JobUtils.waitForJobFailure(JobUtils.java:71)
	at io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic(OauthAuthorizationIsolatedST.java:150)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST
2022-04-07 01:10:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:08 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 01:11:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 01:11:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:11:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 01:11:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 01:11:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 01:11:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:11:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 01:11:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:11:45 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 01:12:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 01:12:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 01:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 01:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 01:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-07 01:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to scope-test realm
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to scope-test realm
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to scope-test realm
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:60] Using HTTP endpoints
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-scope-name in namespace infra-namespace
2022-04-07 01:14:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-scope-name will have desired state: Ready
2022-04-07 01:15:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-scope-name is in desired state: Ready
2022-04-07 01:15:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:15:15 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:15:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetCorrectly-STARTED
2022-04-07 01:15:15 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetIncorrectly-STARTED
2022-04-07 01:15:15 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:15:15 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:15:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-ab3d307c-kafka-clients in namespace infra-namespace
2022-04-07 01:15:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-8085d54c-kafka-clients in namespace infra-namespace
2022-04-07 01:15:15 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ab3d307c-kafka-clients will be ready
2022-04-07 01:15:15 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-8085d54c-kafka-clients will be ready
2022-04-07 01:15:18 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ab3d307c-kafka-clients is ready
2022-04-07 01:15:18 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-8085d54c-kafka-clients is ready
2022-04-07 01:15:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-ab3d307c-scraper in namespace infra-namespace
2022-04-07 01:15:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-8085d54c-scraper in namespace infra-namespace
2022-04-07 01:15:18 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ab3d307c-scraper will be ready
2022-04-07 01:15:18 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-8085d54c-scraper will be ready
2022-04-07 01:15:20 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ab3d307c-scraper is ready
2022-04-07 01:15:20 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-ab3d307c-scraper to be ready
2022-04-07 01:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-8085d54c-scraper is ready
2022-04-07 01:15:20 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-8085d54c-scraper to be ready
2022-04-07 01:15:30 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-ab3d307c-scraper is ready
2022-04-07 01:15:30 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-8085d54c-scraper is ready
2022-04-07 01:15:30 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-ab3d307c-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 01:15:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-ab3d307c-allow in namespace infra-namespace
2022-04-07 01:15:30 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-8085d54c-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 01:15:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-8085d54c-allow in namespace infra-namespace
2022-04-07 01:15:30 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 01:15:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-ab3d307c in namespace infra-namespace
2022-04-07 01:15:30 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 01:15:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-8085d54c in namespace infra-namespace
2022-04-07 01:15:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-ab3d307c will have desired state: Ready
2022-04-07 01:15:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:15:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testScopeKafkaConnectSetIncorrectly
2022-04-07 01:15:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-8085d54c-allow in namespace infra-namespace
2022-04-07 01:15:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-8085d54c in namespace infra-namespace
2022-04-07 01:15:54 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-8085d54c-scraper in namespace infra-namespace
2022-04-07 01:16:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-8085d54c-kafka-clients in namespace infra-namespace
2022-04-07 01:16:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-ab3d307c is in desired state: Ready
2022-04-07 01:16:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:16:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testScopeKafkaConnectSetCorrectly
2022-04-07 01:16:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-ab3d307c-allow in namespace infra-namespace
2022-04-07 01:16:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-ab3d307c in namespace infra-namespace
2022-04-07 01:16:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-ab3d307c-scraper in namespace infra-namespace
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetIncorrectly-FINISHED
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetCorrectly-STARTED
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-630507459-1979104775 in namespace infra-namespace
2022-04-07 01:17:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-630507459-1979104775 will have desired state: Ready
2022-04-07 01:17:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-630507459-1979104775 is in desired state: Ready
2022-04-07 01:17:15 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-588772ef in namespace infra-namespace
2022-04-07 01:17:15 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-588772ef will be in active state
2022-04-07 01:17:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-588772ef to finished
2022-04-07 01:17:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:17:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testClientScopeKafkaSetCorrectly
2022-04-07 01:17:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-588772ef in namespace infra-namespace
2022-04-07 01:17:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-630507459-1979104775 in namespace infra-namespace
2022-04-07 01:17:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-ab3d307c-kafka-clients in namespace infra-namespace
2022-04-07 01:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetCorrectly-FINISHED
2022-04-07 01:17:34 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:18:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:18:11 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testScopeKafkaConnectSetCorrectly-FINISHED
2022-04-07 01:18:11 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:18:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:18:11 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetIncorrectly-STARTED
2022-04-07 01:18:11 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:18:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:18:11 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of oauth-cluster-scope-name-kafka to be ready
2022-04-07 01:18:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-scope-name will have desired state: Ready
2022-04-07 01:18:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-scope-name is in desired state: Ready
2022-04-07 01:18:54 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: oauth-cluster-scope-name is ready
2022-04-07 01:18:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1196474785-1584173819 in namespace infra-namespace
2022-04-07 01:18:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1196474785-1584173819 will have desired state: Ready
2022-04-07 01:18:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1196474785-1584173819 is in desired state: Ready
2022-04-07 01:18:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-0e20aafc in namespace infra-namespace
2022-04-07 01:18:55 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-0e20aafc will be in active state
2022-04-07 01:18:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:oauth-producer-my-cluster-0e20aafc to finish with failure.
2022-04-07 01:22:36 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 220000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetIncorrectly(OauthScopeIsolatedST.java:224)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 01:22:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:100] Client job 'oauth-producer-my-cluster-0e20aafc' finished with expected timeout.
2022-04-07 01:22:41 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 1 Pod(s) of oauth-cluster-scope-name-kafka to be ready
2022-04-07 01:23:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-scope-name will have desired state: Ready
2022-04-07 01:23:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-scope-name is in desired state: Ready
2022-04-07 01:23:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: oauth-cluster-scope-name is ready
2022-04-07 01:23:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:23:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testClientScopeKafkaSetIncorrectly
2022-04-07 01:23:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-0e20aafc in namespace infra-namespace
2022-04-07 01:23:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1196474785-1584173819 in namespace infra-namespace
2022-04-07 01:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST.testClientScopeKafkaSetIncorrectly-FINISHED
2022-04-07 01:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:23:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 01:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 01:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthScopeIsolatedST
2022-04-07 01:23:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-scope-name in namespace infra-namespace
2022-04-07 01:23:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 01:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthScopeIsolatedST is everything deleted.
2022-04-07 01:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 788.029 s - in io.strimzi.systemtest.security.oauth.OauthScopeIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST
2022-04-07 01:23:51 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:16 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 01:24:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 01:24:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 01:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 01:24:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:24:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:24:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:24:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 01:24:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:25:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 01:25:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 01:25:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 01:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 01:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 01:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-07 01:25:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to internal realm
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to internal realm
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to internal realm
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakInstance:55] Using HTTPS endpoints
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthTlsIsolatedST:480] Keycloak settings KeycloakInstance{jwksExpireSeconds=500, jwksRefreshSeconds=400, username='admin', password='8Mvjgij_GHbvuQ==', httpsUri='keycloak.infra-namespace.svc.cluster.local:8443', httpUri='keycloak-discovery.infra-namespace.svc.cluster.local:8080', validIssuerUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal', jwksEndpointUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs', oauthTokenEndpointUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token', introspectionEndpointUri='https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token/introspect', userNameClaim='preferred_username', keystorePattern=<tls>\s*<key-stores>\s*<key-store name="kcKeyStore">\s*<credential-reference clear-text=".*"\/>, keystorePasswordPattern=\".*\"}
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-tls-name in namespace infra-namespace
2022-04-07 01:27:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-tls-name will have desired state: Ready
2022-04-07 01:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-tls-name is in desired state: Ready
2022-04-07 01:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser hello-world-producer in namespace infra-namespace
2022-04-07 01:28:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: hello-world-producer will have desired state: Ready
2022-04-07 01:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: hello-world-producer is in desired state: Ready
2022-04-07 01:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:28:44 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumer-STARTED
2022-04-07 01:28:44 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerBridge-STARTED
2022-04-07 01:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:28:44 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-302537156-1141143850 in namespace infra-namespace
2022-04-07 01:28:44 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-534434429-1001708635 in namespace infra-namespace
2022-04-07 01:28:44 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-302537156-1141143850 will have desired state: Ready
2022-04-07 01:28:44 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-534434429-1001708635 will have desired state: Ready
2022-04-07 01:28:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-302537156-1141143850 is in desired state: Ready
2022-04-07 01:28:45 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:28:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-8355569e in namespace infra-namespace
2022-04-07 01:28:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-534434429-1001708635 is in desired state: Ready
2022-04-07 01:28:45 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:28:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-b5a4e16f in namespace infra-namespace
2022-04-07 01:28:45 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-8355569e will be in active state
2022-04-07 01:28:45 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-b5a4e16f will be in active state
2022-04-07 01:28:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-8355569e to finished
2022-04-07 01:28:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-b5a4e16f to finished
2022-04-07 01:28:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-8355569e in namespace infra-namespace
2022-04-07 01:28:55 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-8355569e will be in active state
2022-04-07 01:28:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-b5a4e16f in namespace infra-namespace
2022-04-07 01:28:55 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-b5a4e16f will be in active state
2022-04-07 01:28:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-8355569e to finished
2022-04-07 01:28:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-b5a4e16f to finished
2022-04-07 01:29:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:29:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumer
2022-04-07 01:29:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-8355569e in namespace infra-namespace
2022-04-07 01:29:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-8355569e in namespace infra-namespace
2022-04-07 01:29:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-302537156-1141143850 in namespace infra-namespace
2022-04-07 01:29:08 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-b5a4e16f-kafka-clients in namespace infra-namespace
2022-04-07 01:29:08 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-b5a4e16f-kafka-clients will be ready
2022-04-07 01:29:10 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-b5a4e16f-kafka-clients is ready
2022-04-07 01:29:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge oauth-cluster-tls-name in namespace infra-namespace
2022-04-07 01:29:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: oauth-cluster-tls-name will have desired state: Ready
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumer-FINISHED
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testIntrospectionEndpoint-STARTED
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-358052337-1556106097 in namespace infra-namespace
2022-04-07 01:29:18 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-358052337-1556106097 will have desired state: Ready
2022-04-07 01:29:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-358052337-1556106097 is in desired state: Ready
2022-04-07 01:29:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-tls-name-intro in namespace infra-namespace
2022-04-07 01:29:19 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-tls-name-intro will have desired state: Ready
2022-04-07 01:29:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaBridge: oauth-cluster-tls-name is in desired state: Ready
2022-04-07 01:29:27 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:29:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer-my-cluster-b5a4e16f in namespace infra-namespace
2022-04-07 01:29:27 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer-my-cluster-b5a4e16f will be in active state
2022-04-07 01:29:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:bridge-producer-my-cluster-b5a4e16f to finished
2022-04-07 01:29:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:29:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerBridge
2022-04-07 01:29:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-b5a4e16f-kafka-clients in namespace infra-namespace
2022-04-07 01:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-tls-name-intro is in desired state: Ready
2022-04-07 01:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-5a7e95f7 in namespace infra-namespace
2022-04-07 01:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-5a7e95f7 will be in active state
2022-04-07 01:30:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-5a7e95f7 to finished
2022-04-07 01:30:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer-my-cluster-b5a4e16f in namespace infra-namespace
2022-04-07 01:30:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge oauth-cluster-tls-name in namespace infra-namespace
2022-04-07 01:30:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-5a7e95f7 in namespace infra-namespace
2022-04-07 01:30:30 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-5a7e95f7 will be in active state
2022-04-07 01:30:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-5a7e95f7 to finished
2022-04-07 01:30:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-b5a4e16f in namespace infra-namespace
2022-04-07 01:30:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-b5a4e16f in namespace infra-namespace
2022-04-07 01:30:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-534434429-1001708635 in namespace infra-namespace
2022-04-07 01:30:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:30:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testIntrospectionEndpoint
2022-04-07 01:30:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-5a7e95f7 in namespace infra-namespace
2022-04-07 01:30:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-5a7e95f7 in namespace infra-namespace
2022-04-07 01:30:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-tls-name-intro in namespace infra-namespace
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerBridge-FINISHED
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerConnect-STARTED
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1380068307-549494929 in namespace infra-namespace
2022-04-07 01:30:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1380068307-549494929 will have desired state: Ready
2022-04-07 01:30:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1380068307-549494929 is in desired state: Ready
2022-04-07 01:30:47 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:30:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-bb2f2a41 in namespace infra-namespace
2022-04-07 01:30:47 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-bb2f2a41 will be in active state
2022-04-07 01:30:48 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-358052337-1556106097 in namespace infra-namespace
2022-04-07 01:30:49 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-bb2f2a41 to finished
2022-04-07 01:30:57 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-bb2f2a41 in namespace infra-namespace
2022-04-07 01:30:57 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-bb2f2a41 will be in active state
2022-04-07 01:30:58 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-bb2f2a41 to finished
2022-04-07 01:30:58 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:30:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testIntrospectionEndpoint-FINISHED
2022-04-07 01:30:58 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:31:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment oauth-cluster-tls-name-kafka-clients in namespace infra-namespace
2022-04-07 01:31:09 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: oauth-cluster-tls-name-kafka-clients will be ready
2022-04-07 01:31:11 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: oauth-cluster-tls-name-kafka-clients is ready
2022-04-07 01:31:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-bb2f2a41-scraper in namespace infra-namespace
2022-04-07 01:31:11 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-bb2f2a41-scraper will be ready
2022-04-07 01:31:13 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-bb2f2a41-scraper is ready
2022-04-07 01:31:13 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-bb2f2a41-scraper to be ready
2022-04-07 01:31:23 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-bb2f2a41-scraper is ready
2022-04-07 01:31:23 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-bb2f2a41-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 01:31:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-bb2f2a41-allow in namespace infra-namespace
2022-04-07 01:31:23 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 01:31:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-bb2f2a41 in namespace infra-namespace
2022-04-07 01:31:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-bb2f2a41 will have desired state: Ready
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-bb2f2a41 is in desired state: Ready
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-bb2f2a41-connect-84b698c565-6bhzn -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec oauth-cluster-tls-name-kafka-clients-8cb45f87d-flzhn -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-1380068307-549494929", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-bb2f2a41-connect-api.infra-namespace.svc:8083/connectors
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:32:34 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-bb2f2a41-connect-84b698c565-6bhzn
2022-04-07 01:32:38 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-bb2f2a41-connect-84b698c565-6bhzn
2022-04-07 01:32:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:32:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testProducerConsumerConnect
2022-04-07 01:32:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-bb2f2a41-scraper in namespace infra-namespace
2022-04-07 01:32:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-bb2f2a41 in namespace infra-namespace
2022-04-07 01:32:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-bb2f2a41 in namespace infra-namespace
2022-04-07 01:32:38 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1380068307-549494929 in namespace infra-namespace
2022-04-07 01:33:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment oauth-cluster-tls-name-kafka-clients in namespace infra-namespace
2022-04-07 01:33:58 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-bb2f2a41 in namespace infra-namespace
2022-04-07 01:34:08 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-bb2f2a41-allow in namespace infra-namespace
2022-04-07 01:34:08 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:34:08 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testProducerConsumerConnect-FINISHED
2022-04-07 01:34:08 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:34:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:34:08 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testMirrorMaker-STARTED
2022-04-07 01:34:08 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:34:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-67922920-1180818262 in namespace infra-namespace
2022-04-07 01:34:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-67922920-1180818262 will have desired state: Ready
2022-04-07 01:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-67922920-1180818262 is in desired state: Ready
2022-04-07 01:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-producer-my-cluster-194cf6c7 in namespace infra-namespace
2022-04-07 01:34:10 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-producer-my-cluster-194cf6c7 will be in active state
2022-04-07 01:34:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-producer-my-cluster-194cf6c7 to finished
2022-04-07 01:34:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-194cf6c7 in namespace infra-namespace
2022-04-07 01:34:21 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-194cf6c7 will be in active state
2022-04-07 01:34:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-194cf6c7 to finished
2022-04-07 01:34:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-tls-name-target in namespace infra-namespace
2022-04-07 01:34:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-tls-name-target will have desired state: Ready
2022-04-07 01:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-tls-name-target is in desired state: Ready
2022-04-07 01:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker oauth-cluster-tls-name in namespace infra-namespace
2022-04-07 01:35:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: oauth-cluster-tls-name will have desired state: Ready
2022-04-07 01:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: oauth-cluster-tls-name is in desired state: Ready
2022-04-07 01:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1294180677-270762270 in namespace infra-namespace
2022-04-07 01:36:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1294180677-270762270 will have desired state: Ready
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1294180677-270762270 is in desired state: Ready
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret my-user-1294180677-270762270
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SecretUtils:50] Secret my-user-1294180677-270762270 created
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1294180677-270762270 will have desired state: Ready
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1294180677-270762270 is in desired state: Ready
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [OauthTlsIsolatedST:390] Creating new client with new consumer-group and also to point on oauth-cluster-tls-name-target cluster
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job oauth-consumer-my-cluster-194cf6c7 in namespace infra-namespace
2022-04-07 01:36:46 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: oauth-consumer-my-cluster-194cf6c7 will be in active state
2022-04-07 01:36:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:oauth-consumer-my-cluster-194cf6c7 to finished
2022-04-07 01:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker
2022-04-07 01:36:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker oauth-cluster-tls-name in namespace infra-namespace
2022-04-07 01:36:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job oauth-producer-my-cluster-194cf6c7 in namespace infra-namespace
2022-04-07 01:36:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-194cf6c7 in namespace infra-namespace
2022-04-07 01:36:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-67922920-1180818262 in namespace infra-namespace
2022-04-07 01:36:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job oauth-consumer-my-cluster-194cf6c7 in namespace infra-namespace
2022-04-07 01:36:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1294180677-270762270 in namespace infra-namespace
2022-04-07 01:37:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-tls-name-target in namespace infra-namespace
2022-04-07 01:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST.testMirrorMaker-FINISHED
2022-04-07 01:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 01:37:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 01:37:23 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 01:37:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:37:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthTlsIsolatedST
2022-04-07 01:37:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-tls-name in namespace infra-namespace
2022-04-07 01:37:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 01:37:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser hello-world-producer in namespace infra-namespace
2022-04-07 01:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context OauthTlsIsolatedST is everything deleted.
2022-04-07 01:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 822.258 s - in io.strimzi.systemtest.security.oauth.OauthTlsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST
2022-04-07 01:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 01:37:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 01:37:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 01:37:58 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 01:37:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:37:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 01:37:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 01:37:58 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 01:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:08 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 01:38:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:38:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 01:38:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 01:38:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:38:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 01:38:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:38:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 01:38:34 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 01:38:34 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 01:38:35 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 01:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 01:39:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaRollsWhenTopicIsUnderReplicated-STARTED
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodCrashLooping-STARTED
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-98 for test case:testKafkaRollsWhenTopicIsUnderReplicated
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-98
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-98
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-98
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-99 for test case:testKafkaPodCrashLooping
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-99
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-dfe774ca in namespace namespace-98
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-98
2022-04-07 01:39:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfe774ca will have desired state: Ready
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-99
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-99
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-34ea74ba in namespace namespace-99
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-99
2022-04-07 01:39:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-34ea74ba will have desired state: Ready
2022-04-07 01:41:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-34ea74ba is in desired state: Ready
2022-04-07 01:41:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-34ea74ba will have desired state: NotReady
2022-04-07 01:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfe774ca is in desired state: Ready
2022-04-07 01:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaRollerIsolatedST:105] Running kafkaScaleUpScaleDown my-cluster-dfe774ca
2022-04-07 01:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-dfe774ca-kafka rolling update
2022-04-07 01:43:39 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-dfe774ca-kafka has been successfully rolled
2022-04-07 01:43:39 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-dfe774ca-kafka to be ready
2022-04-07 01:44:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-34ea74ba is in desired state: NotReady
2022-04-07 01:44:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-34ea74ba will have desired state: Ready
2022-04-07 01:44:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfe774ca will have desired state: Ready
2022-04-07 01:44:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfe774ca is in desired state: Ready
2022-04-07 01:44:25 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-dfe774ca is ready
2022-04-07 01:44:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1797738860-1947319713 in namespace namespace-99
2022-04-07 01:44:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-98
2022-04-07 01:44:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1797738860-1947319713 will have desired state: Ready
2022-04-07 01:44:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1797738860-1947319713 is in desired state: Ready
2022-04-07 01:44:26 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaRollerIsolatedST:124] Scaling down to 3
2022-04-07 01:44:26 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-dfe774ca-kafka rolling update
2022-04-07 01:45:46 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-dfe774ca-kafka has been successfully rolled
2022-04-07 01:45:46 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-dfe774ca-kafka to be ready
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfe774ca will have desired state: Ready
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfe774ca is in desired state: Ready
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-dfe774ca is ready
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: my-cluster-dfe774ca are stable
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:46:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-kafka-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-1 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod my-cluster-dfe774ca-zookeeper-2 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:335] All pods are stable my-cluster-dfe774ca-entity-operator-6fc7c8f494-h6wd2 ,my-cluster-dfe774ca-kafka-0 ,my-cluster-dfe774ca-kafka-1 ,my-cluster-dfe774ca-kafka-2 ,my-cluster-dfe774ca-zookeeper-0 ,my-cluster-dfe774ca-zookeeper-1 ,my-cluster-dfe774ca-zookeeper-2
2022-04-07 01:47:02 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-dfe774ca-kafka rolling update
2022-04-07 01:48:28 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-dfe774ca-kafka has been successfully rolled
2022-04-07 01:48:28 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-dfe774ca-kafka to be ready
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-dfe774ca will have desired state: Ready
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-dfe774ca is in desired state: Ready
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-dfe774ca is ready
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:478] Search in strimzi-cluster-operator log for errors in last 567 seconds
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [BaseCmdKubeClient:481] Exception not found
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaRollsWhenTopicIsUnderReplicated
2022-04-07 01:48:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1797738860-1947319713 in namespace namespace-98
2022-04-07 01:49:00 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-dfe774ca in namespace namespace-98
2022-04-07 01:49:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:49:10 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-98 for test case:testKafkaRollsWhenTopicIsUnderReplicated
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaRollsWhenTopicIsUnderReplicated-FINISHED
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPendingDueToRack-STARTED
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-100 for test case:testKafkaPodPendingDueToRack
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-100
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-100
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-100
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c75b465e in namespace namespace-100
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-100
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:209] Wait until Pod my-cluster-c75b465e-kafka will have stable 3 replicas
2022-04-07 01:49:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:39 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-34ea74ba is in desired state: Ready
2022-04-07 01:49:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:49:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodCrashLooping
2022-04-07 01:49:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-34ea74ba in namespace namespace-99
2022-04-07 01:49:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:49:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-99 for test case:testKafkaPodCrashLooping
2022-04-07 01:49:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:49:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:50:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:50:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:50:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:50:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-04-07 01:50:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-04-07 01:50:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-04-07 01:50:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-04-07 01:50:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-04-07 01:50:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-04-07 01:50:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-04-07 01:50:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-04-07 01:50:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-04-07 01:50:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-04-07 01:50:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-04-07 01:50:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-04-07 01:50:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-04-07 01:50:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-04-07 01:50:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-04-07 01:50:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-04-07 01:50:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-04-07 01:50:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-04-07 01:50:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-04-07 01:50:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-04-07 01:50:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-04-07 01:50:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:228] Pod my-cluster-c75b465e-kafka has 3 replicas
2022-04-07 01:50:23 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaRollerIsolatedST:309] Removing requirement for the affinity
2022-04-07 01:50:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c75b465e will have desired state: Ready
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodCrashLooping-FINISHED
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaTopicRFLowerThanMinInSyncReplicas-STARTED
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-101 for test case:testKafkaTopicRFLowerThanMinInSyncReplicas
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-101
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-101
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-101
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9f2da41d in namespace namespace-101
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-101
2022-04-07 01:50:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9f2da41d will have desired state: Ready
2022-04-07 01:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9f2da41d is in desired state: Ready
2022-04-07 01:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-51083555-1877816624 in namespace namespace-101
2022-04-07 01:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-101
2022-04-07 01:51:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-51083555-1877816624 will have desired state: Ready
2022-04-07 01:51:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-51083555-1877816624 is in desired state: Ready
2022-04-07 01:51:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRollerIsolatedST:155] Setting KafkaTopic's min.insync.replicas to be higher than replication factor
2022-04-07 01:51:56 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaRollerIsolatedST:159] Annotate Kafka StatefulSet my-cluster-9f2da41d-kafka with manual rolling update annotation
2022-04-07 01:51:56 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-9f2da41d-kafka rolling update
2022-04-07 01:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-9f2da41d-kafka has been successfully rolled
2022-04-07 01:53:26 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-9f2da41d-kafka to be ready
2022-04-07 01:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9f2da41d will have desired state: Ready
2022-04-07 01:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9f2da41d is in desired state: Ready
2022-04-07 01:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [RollingUpdateUtils:132] Kafka: my-cluster-9f2da41d is ready
2022-04-07 01:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaTopicRFLowerThanMinInSyncReplicas
2022-04-07 01:53:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-51083555-1877816624 in namespace namespace-101
2022-04-07 01:54:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9f2da41d in namespace namespace-101
2022-04-07 01:54:08 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c75b465e is in desired state: Ready
2022-04-07 01:54:08 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaUtils:399] Waiting for deletion of Kafka:my-cluster-c75b465e
2022-04-07 01:54:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 01:54:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodPendingDueToRack
2022-04-07 01:54:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c75b465e in namespace namespace-100
2022-04-07 01:54:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:54:14 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-100 for test case:testKafkaPodPendingDueToRack
2022-04-07 01:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 01:54:15 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-101 for test case:testKafkaTopicRFLowerThanMinInSyncReplicas
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPendingDueToRack-FINISHED
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodImagePullBackOff-STARTED
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-102 for test case:testKafkaPodImagePullBackOff
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-102
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-102
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-102
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-735ba8f7 in namespace namespace-102
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-102
2022-04-07 01:54:57 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-735ba8f7 will have desired state: Ready
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaTopicRFLowerThanMinInSyncReplicas-FINISHED
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPending-STARTED
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-103 for test case:testKafkaPodPending
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-103
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-103
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-103
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-97aa3e5a in namespace namespace-103
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-103
2022-04-07 01:54:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-97aa3e5a will have desired state: Ready
2022-04-07 01:56:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-735ba8f7 is in desired state: Ready
2022-04-07 01:56:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-735ba8f7 will have desired state: NotReady
2022-04-07 01:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-97aa3e5a is in desired state: Ready
2022-04-07 01:57:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-97aa3e5a will have desired state: NotReady
2022-04-07 01:59:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-735ba8f7 is in desired state: NotReady
2022-04-07 01:59:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-735ba8f7 will have desired state: Ready
2022-04-07 01:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-97aa3e5a is in desired state: NotReady
2022-04-07 01:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-97aa3e5a will have desired state: Ready
2022-04-07 02:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-97aa3e5a is in desired state: Ready
2022-04-07 02:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodPending
2022-04-07 02:01:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-97aa3e5a in namespace namespace-103
2022-04-07 02:01:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:01:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-103 for test case:testKafkaPodPending
2022-04-07 02:02:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodPending-FINISHED
2022-04-07 02:02:13 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-735ba8f7 is in desired state: Ready
2022-04-07 02:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaPodImagePullBackOff
2022-04-07 02:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-735ba8f7 in namespace namespace-102
2022-04-07 02:03:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:03:50 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-102 for test case:testKafkaPodImagePullBackOff
2022-04-07 02:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST.testKafkaPodImagePullBackOff-FINISHED
2022-04-07 02:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:04:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:04:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context KafkaRollerIsolatedST is everything deleted.
2022-04-07 02:04:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,604.062 s - in io.strimzi.systemtest.rollingupdate.KafkaRollerIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-07 02:04:17 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:04:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:04:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:04:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:04:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:05:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:05:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:05:19 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 02:05:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 02:05:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-STARTED
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testCustomAndUpdatedValues-STARTED
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-104 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-104
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-104
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-104
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-105 for test case:testCustomAndUpdatedValues
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-105
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a37d6e76 in namespace namespace-104
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:06:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a37d6e76 will have desired state: Ready
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-105
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-105
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6c1329a1 in namespace namespace-105
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-105
2022-04-07 02:06:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6c1329a1 will have desired state: Ready
2022-04-07 02:07:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6c1329a1 is in desired state: Ready
2022-04-07 02:07:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-6c1329a1 in namespace namespace-105
2022-04-07 02:07:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-105
2022-04-07 02:07:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6c1329a1 will have desired state: Ready
2022-04-07 02:07:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a37d6e76 is in desired state: Ready
2022-04-07 02:07:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a37d6e76-scraper in namespace namespace-104
2022-04-07 02:07:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:07:34 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a37d6e76-scraper will be ready
2022-04-07 02:07:35 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a37d6e76-scraper is ready
2022-04-07 02:07:35 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-a37d6e76-scraper to be ready
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-a37d6e76-scraper is ready
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-a37d6e76-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-a37d6e76-allow in namespace namespace-104
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-a37d6e76 in namespace namespace-105
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:07:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-a37d6e76 will have desired state: Ready
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6c1329a1 is in desired state: Ready
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:629] Verify values before update
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-6c1329a1-connect in pod name
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container my-cluster-6c1329a1-connect
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-6c1329a1-connect
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container my-cluster-6c1329a1-connect
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:634] Check if actual env variable KAFKA_CONNECT_CONFIGURATION has different value than test.value
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:640] Updating values in MirrorMaker container
2022-04-07 02:08:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-6c1329a1-connect rolling update
2022-04-07 02:08:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-a37d6e76 is in desired state: Ready
2022-04-07 02:08:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-a37d6e76 in namespace namespace-105
2022-04-07 02:08:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:08:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-a37d6e76 will have desired state: Ready
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-a37d6e76 is in desired state: Ready
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-104 exec my-cluster-a37d6e76-connect-5c8fb4798-5mfdt -- curl -X GET http://localhost:8083/connectors/my-cluster-a37d6e76/status
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-a37d6e76-hello-world-producer in namespace namespace-104
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-a37d6e76-hello-world-consumer in namespace namespace-104
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-104
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-a37d6e76-hello-world-producer will be in active state
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-a37d6e76-hello-world-consumer will be in active state
2022-04-07 02:08:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-a37d6e76-hello-world-producer and consumer my-cluster-a37d6e76-hello-world-consumer finish
2022-04-07 02:08:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6c1329a1-connect will be ready
2022-04-07 02:08:57 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6c1329a1-connect is ready
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-6c1329a1-connect rolling update finished
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:657] Verify values after update
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-6c1329a1-connect in pod name
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container my-cluster-6c1329a1-connect
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-6c1329a1-connect
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container my-cluster-6c1329a1-connect
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-6c1329a1-connect
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container my-cluster-6c1329a1-connect
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-6c1329a1 in namespace namespace-105
2022-04-07 02:09:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6c1329a1 in namespace namespace-105
2022-04-07 02:09:12 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-a37d6e76-connect-5c8fb4798-5mfdt
2022-04-07 02:09:12 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-a37d6e76-connect-5c8fb4798-5mfdt
2022-04-07 02:09:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:09:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testMultiNodeKafkaConnectWithConnectorCreation
2022-04-07 02:09:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-a37d6e76 in namespace namespace-104
2022-04-07 02:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:09:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-105 for test case:testCustomAndUpdatedValues
2022-04-07 02:09:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-a37d6e76 in namespace namespace-104
2022-04-07 02:09:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-a37d6e76-hello-world-consumer in namespace namespace-104
2022-04-07 02:09:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-a37d6e76-hello-world-producer in namespace namespace-104
2022-04-07 02:09:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a37d6e76-scraper in namespace namespace-104
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testCustomAndUpdatedValues-FINISHED
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithPlainAndScramShaAuthentication-STARTED
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-106 for test case:testKafkaConnectWithPlainAndScramShaAuthentication
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-106
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-106
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-106
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a9b1dcfe in namespace namespace-106
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:10:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a9b1dcfe will have desired state: Ready
2022-04-07 02:10:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-a37d6e76-allow in namespace namespace-104
2022-04-07 02:10:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a37d6e76 in namespace namespace-104
2022-04-07 02:10:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:10:32 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-104 for test case:testMultiNodeKafkaConnectWithConnectorCreation
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMultiNodeKafkaConnectWithConnectorCreation-FINISHED
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectScaleUpScaleDown-STARTED
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-107 for test case:testKafkaConnectScaleUpScaleDown
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-107
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-107
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-107
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-717c35cb in namespace namespace-107
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-107
2022-04-07 02:11:00 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-717c35cb will have desired state: Ready
2022-04-07 02:11:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a9b1dcfe is in desired state: Ready
2022-04-07 02:11:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-a9b1dcfe-user in namespace namespace-107
2022-04-07 02:11:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:11:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-a9b1dcfe-user will have desired state: Ready
2022-04-07 02:11:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-a9b1dcfe-user is in desired state: Ready
2022-04-07 02:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1452356780-1690276271 in namespace namespace-107
2022-04-07 02:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1452356780-1690276271 will have desired state: Ready
2022-04-07 02:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1452356780-1690276271 is in desired state: Ready
2022-04-07 02:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-a9b1dcfe-scraper in namespace namespace-106
2022-04-07 02:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:11:22 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a9b1dcfe-scraper will be ready
2022-04-07 02:11:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a9b1dcfe-scraper is ready
2022-04-07 02:11:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-a9b1dcfe-scraper to be ready
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-a9b1dcfe-scraper is ready
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-a9b1dcfe-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-a9b1dcfe-allow in namespace namespace-106
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-a9b1dcfe in namespace namespace-107
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:11:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-a9b1dcfe will have desired state: Ready
2022-04-07 02:12:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-717c35cb is in desired state: Ready
2022-04-07 02:12:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:395] Running kafkaConnectScaleUP namespace-107 in namespace
2022-04-07 02:12:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-717c35cb in namespace namespace-107
2022-04-07 02:12:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-107
2022-04-07 02:12:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-717c35cb will have desired state: Ready
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-a9b1dcfe is in desired state: Ready
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-106 exec my-cluster-a9b1dcfe-connect-86bd8cd9d8-lbxd4 -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:280] Verifying that KafkaConnect pod logs don't contain ERRORs
2022-04-07 02:12:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:283] Creating FileStreamSink connector via pod my-cluster-a9b1dcfe-scraper-5d76d57788-lbkww with topic my-topic-1452356780-1690276271
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-106 exec my-cluster-a9b1dcfe-scraper-5d76d57788-lbkww -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-1452356780-1690276271", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-a9b1dcfe-connect-api.namespace-106.svc:8083/connectors
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-a9b1dcfe-hello-world-producer in namespace namespace-106
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-a9b1dcfe-hello-world-consumer in namespace namespace-106
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-106
2022-04-07 02:12:42 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-a9b1dcfe-hello-world-producer will be in active state
2022-04-07 02:12:43 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-a9b1dcfe-hello-world-consumer will be in active state
2022-04-07 02:12:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-a9b1dcfe-hello-world-producer and consumer my-cluster-a9b1dcfe-hello-world-consumer finish
2022-04-07 02:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-a9b1dcfe-connect-86bd8cd9d8-lbxd4
2022-04-07 02:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-a9b1dcfe-connect-86bd8cd9d8-lbxd4
2022-04-07 02:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectWithPlainAndScramShaAuthentication
2022-04-07 02:12:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-a9b1dcfe in namespace namespace-106
2022-04-07 02:13:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-a9b1dcfe-allow in namespace namespace-106
2022-04-07 02:13:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-a9b1dcfe-hello-world-consumer in namespace namespace-106
2022-04-07 02:13:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-a9b1dcfe-hello-world-producer in namespace namespace-106
2022-04-07 02:13:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1452356780-1690276271 in namespace namespace-106
2022-04-07 02:13:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-717c35cb is in desired state: Ready
2022-04-07 02:13:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:407] Scaling up to 4
2022-04-07 02:13:13 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-717c35cb-connect will be ready
2022-04-07 02:13:13 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-717c35cb-connect is ready
2022-04-07 02:13:13 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-717c35cb-connect to be ready
2022-04-07 02:13:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-a9b1dcfe-scraper in namespace namespace-106
2022-04-07 02:13:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-a9b1dcfe-user in namespace namespace-106
2022-04-07 02:14:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a9b1dcfe in namespace namespace-106
2022-04-07 02:14:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:14:19 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-106 for test case:testKafkaConnectWithPlainAndScramShaAuthentication
2022-04-07 02:14:33 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-717c35cb-connect is ready
2022-04-07 02:14:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:414] Scaling down to 1
2022-04-07 02:14:33 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-717c35cb-connect will be ready
2022-04-07 02:14:33 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-717c35cb-connect is ready
2022-04-07 02:14:33 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-717c35cb-connect to be ready
2022-04-07 02:14:46 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-717c35cb-connect is ready
2022-04-07 02:14:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:14:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectScaleUpScaleDown
2022-04-07 02:14:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-717c35cb in namespace namespace-107
2022-04-07 02:14:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-717c35cb in namespace namespace-107
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithPlainAndScramShaAuthentication-FINISHED
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithConnectorToZero-STARTED
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-108 for test case:testScaleConnectWithConnectorToZero
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-108
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-108
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-108
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c9a779c8 in namespace namespace-108
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-108
2022-04-07 02:15:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c9a779c8 will have desired state: Ready
2022-04-07 02:15:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:15:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-107 for test case:testKafkaConnectScaleUpScaleDown
2022-04-07 02:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:15:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectAndConnectorSubresource-STARTED
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-109 for test case:testScaleConnectAndConnectorSubresource
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-109
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-109
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-109
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1e4092d7 in namespace namespace-109
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-07 02:15:11 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1e4092d7 will have desired state: Ready
2022-04-07 02:15:49 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectScaleUpScaleDown-FINISHED
2022-04-07 02:15:49 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:15:49 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:15:49 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndPausedConnectorWithFileSinkPlugin-STARTED
2022-04-07 02:15:54 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:15:54 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-110 for test case:testKafkaConnectAndPausedConnectorWithFileSinkPlugin
2022-04-07 02:15:54 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-110
2022-04-07 02:15:55 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-110
2022-04-07 02:15:55 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-110
2022-04-07 02:15:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-8d67aba9 in namespace namespace-110
2022-04-07 02:15:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:15:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-8d67aba9 will have desired state: Ready
2022-04-07 02:16:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1e4092d7 is in desired state: Ready
2022-04-07 02:16:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-1e4092d7 in namespace namespace-110
2022-04-07 02:16:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-07 02:16:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-1e4092d7 will have desired state: Ready
2022-04-07 02:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c9a779c8 is in desired state: Ready
2022-04-07 02:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-c9a779c8 in namespace namespace-110
2022-04-07 02:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-108
2022-04-07 02:16:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-c9a779c8 will have desired state: Ready
2022-04-07 02:17:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-8d67aba9 is in desired state: Ready
2022-04-07 02:17:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-945630103-192426356 in namespace namespace-110
2022-04-07 02:17:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:17:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-945630103-192426356 will have desired state: Ready
2022-04-07 02:17:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-945630103-192426356 is in desired state: Ready
2022-04-07 02:17:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-8d67aba9-scraper in namespace namespace-110
2022-04-07 02:17:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:17:12 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-8d67aba9-scraper will be ready
2022-04-07 02:17:14 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-8d67aba9-scraper is ready
2022-04-07 02:17:14 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-8d67aba9-scraper to be ready
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-8d67aba9-scraper is ready
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-8d67aba9-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-8d67aba9-allow in namespace namespace-110
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-8d67aba9 in namespace namespace-110
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:17:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-8d67aba9 will have desired state: Ready
2022-04-07 02:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-1e4092d7 is in desired state: Ready
2022-04-07 02:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-1e4092d7 in namespace namespace-110
2022-04-07 02:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-109
2022-04-07 02:17:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-1e4092d7 will have desired state: Ready
2022-04-07 02:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-1e4092d7 is in desired state: Ready
2022-04-07 02:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:979] -------> Scaling KafkaConnect subresource <-------
2022-04-07 02:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:980] Scaling subresource replicas to 4
2022-04-07 02:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1e4092d7-connect will be ready
2022-04-07 02:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1e4092d7-connect is ready
2022-04-07 02:17:36 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-1e4092d7-connect to be ready
2022-04-07 02:18:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-c9a779c8 is in desired state: Ready
2022-04-07 02:18:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-c9a779c8 in namespace namespace-110
2022-04-07 02:18:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-108
2022-04-07 02:18:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-c9a779c8 will have desired state: Ready
2022-04-07 02:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-c9a779c8 is in desired state: Ready
2022-04-07 02:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:934] Scaling KafkaConnect down to zero
2022-04-07 02:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-c9a779c8 will have desired state: Ready
2022-04-07 02:18:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-c9a779c8 is in desired state: Ready
2022-04-07 02:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectWithConnectorToZero
2022-04-07 02:18:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-c9a779c8 in namespace namespace-108
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-8d67aba9 is in desired state: Ready
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-110 exec my-cluster-8d67aba9-connect-7b6787d5d-lpwxp -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:181] Creating KafkaConnector with 'pause: true'
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-8d67aba9 in namespace namespace-110
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:18:28 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-8d67aba9 will have desired state: Ready
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-8d67aba9 is in desired state: Ready
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-8d67aba9-hello-world-producer in namespace namespace-110
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-8d67aba9-hello-world-consumer in namespace namespace-110
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:18:29 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-8d67aba9-hello-world-producer will be in active state
2022-04-07 02:18:30 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-8d67aba9-hello-world-consumer will be in active state
2022-04-07 02:18:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-8d67aba9-hello-world-producer and consumer my-cluster-8d67aba9-hello-world-consumer finish
2022-04-07 02:18:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-c9a779c8 in namespace namespace-108
2022-04-07 02:18:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c9a779c8 in namespace namespace-108
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-8d67aba9-connect-7b6787d5d-lpwxp
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-8d67aba9-connect-7b6787d5d-lpwxp
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:207] Pausing KafkaConnector: my-cluster-8d67aba9
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-8d67aba9 will have desired state: Ready
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-8d67aba9 is in desired state: Ready
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:213] Clearing FileSink file to check if KafkaConnector will be really paused
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-110 exec my-cluster-8d67aba9-connect-7b6787d5d-lpwxp -- /bin/bash -c truncate -s 0 /tmp/test-file-sink.txt
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-8d67aba9-hello-world-producer in namespace namespace-110
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-8d67aba9-hello-world-consumer in namespace namespace-110
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-110
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-8d67aba9-hello-world-producer will be in active state
2022-04-07 02:18:48 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-8d67aba9-hello-world-consumer will be in active state
2022-04-07 02:18:49 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-8d67aba9-hello-world-producer and consumer my-cluster-8d67aba9-hello-world-consumer finish
2022-04-07 02:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-1e4092d7-connect is ready
2022-04-07 02:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:984] Check if replicas is set to 4, observed generation is higher - for spec and status - naming prefix should be same
2022-04-07 02:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:998] -------> Scaling KafkaConnector subresource <-------
2022-04-07 02:18:51 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:999] Scaling subresource task max to 4
2022-04-07 02:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:1003] Check if taskMax is set to 4
2022-04-07 02:18:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ConnectIsolatedST:1007] Check taskMax on Connect pods API
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-109 exec my-cluster-1e4092d7-connect-74ff8945f6-6tppx -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-1e4092d7
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-109 exec my-cluster-1e4092d7-connect-74ff8945f6-74dxd -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-1e4092d7
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-109 exec my-cluster-1e4092d7-connect-74ff8945f6-x4qfl -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-1e4092d7
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-109 exec my-cluster-1e4092d7-connect-74ff8945f6-zwj44 -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-1e4092d7
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectAndConnectorSubresource
2022-04-07 02:18:53 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-1e4092d7 in namespace namespace-109
2022-04-07 02:18:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:18:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-108 for test case:testScaleConnectWithConnectorToZero
2022-04-07 02:19:03 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-1e4092d7 in namespace namespace-109
2022-04-07 02:19:13 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1e4092d7 in namespace namespace-109
2022-04-07 02:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:19:23 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-109 for test case:testScaleConnectAndConnectorSubresource
2022-04-07 02:19:35 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:219] Because KafkaConnector is paused, no messages should appear to FileSink file
2022-04-07 02:19:35 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-8d67aba9-connect-7b6787d5d-lpwxp
2022-04-07 02:19:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithConnectorToZero-FINISHED
2022-04-07 02:19:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:19:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:19:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero-STARTED
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-111 for test case:testScaleConnectWithoutConnectorToZero
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-111
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-111
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-111
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fa4cd2d9 in namespace namespace-111
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-111
2022-04-07 02:19:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa4cd2d9 will have desired state: Ready
2022-04-07 02:20:07 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectAndConnectorSubresource-FINISHED
2022-04-07 02:20:07 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:20:07 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:20:07 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged-STARTED
2022-04-07 02:20:09 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:20:09 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication-STARTED
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-112 for test case:testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-112
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-112
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-112
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-ec83a646 in namespace namespace-112
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-07 02:20:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-ec83a646 will have desired state: Ready
io.strimzi.test.WaitException: Timeout after 60000 ms waiting for messages in file sink
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaConnectUtils.waitForMessagesInKafkaConnectFileSink(KafkaConnectUtils.java:75)
	at io.strimzi.systemtest.connect.ConnectIsolatedST.lambda$testKafkaConnectAndPausedConnectorWithFileSinkPlugin$1(ConnectIsolatedST.java:220)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:55)
	at org.junit.jupiter.api.AssertThrows.assertThrows(AssertThrows.java:37)
	at org.junit.jupiter.api.Assertions.assertThrows(Assertions.java:3082)
	at io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndPausedConnectorWithFileSinkPlugin(ConnectIsolatedST.java:220)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 02:20:35 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:222] Unpausing KafkaConnector, messages should again appear to FileSink file
2022-04-07 02:20:35 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-8d67aba9 will have desired state: Ready
2022-04-07 02:20:35 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-8d67aba9 is in desired state: Ready
2022-04-07 02:20:35 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-8d67aba9-connect-7b6787d5d-lpwxp
2022-04-07 02:20:36 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-8d67aba9-connect-7b6787d5d-lpwxp
2022-04-07 02:20:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:20:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectAndPausedConnectorWithFileSinkPlugin
2022-04-07 02:20:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-8d67aba9-hello-world-producer in namespace namespace-110
2022-04-07 02:20:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-8d67aba9 in namespace namespace-110
2022-04-07 02:20:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-8d67aba9-hello-world-producer in namespace namespace-110
2022-04-07 02:20:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-8d67aba9-hello-world-consumer in namespace namespace-110
2022-04-07 02:20:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-8d67aba9-hello-world-consumer in namespace namespace-110
2022-04-07 02:20:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-8d67aba9-scraper in namespace namespace-110
2022-04-07 02:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa4cd2d9 is in desired state: Ready
2022-04-07 02:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-fa4cd2d9 in namespace namespace-112
2022-04-07 02:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-111
2022-04-07 02:21:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-fa4cd2d9 will have desired state: Ready
2022-04-07 02:21:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-8d67aba9 in namespace namespace-110
2022-04-07 02:21:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-ec83a646 is in desired state: Ready
2022-04-07 02:21:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1968013148-1621407812 in namespace namespace-112
2022-04-07 02:21:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-07 02:21:29 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1968013148-1621407812 will have desired state: Ready
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1968013148-1621407812 is in desired state: Ready
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1968013148-1621407812 in namespace namespace-112
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1968013148-1621407812 will have desired state: Ready
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1968013148-1621407812 is in desired state: Ready
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-765187061-2082886953 in namespace namespace-112
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-07 02:21:30 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-765187061-2082886953 will have desired state: Ready
2022-04-07 02:21:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-765187061-2082886953 is in desired state: Ready
2022-04-07 02:21:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-ec83a646 in namespace namespace-112
2022-04-07 02:21:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-07 02:21:31 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-ec83a646 will have desired state: Ready
2022-04-07 02:21:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-8d67aba9-allow in namespace namespace-110
2022-04-07 02:21:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-945630103-192426356 in namespace namespace-110
2022-04-07 02:21:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-8d67aba9 in namespace namespace-110
2022-04-07 02:21:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:21:56 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-110 for test case:testKafkaConnectAndPausedConnectorWithFileSinkPlugin
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-fa4cd2d9 is in desired state: Ready
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:891] Scaling KafkaConnect down to zero
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-fa4cd2d9 will have desired state: Ready
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-fa4cd2d9 is in desired state: Ready
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestExecutionWatcher:28] ConnectIsolatedST - Exception 
Expected: is <0>
     but: was <2> has been thrown in @Test. Going to collect logs from components.
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 02:22:14 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 02:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 02:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 02:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 02:22:15 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 02:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 02:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace namespace-111
2022-04-07 02:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace namespace-111
2022-04-07 02:22:16 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-111
2022-04-07 02:22:16 [ForkJoinPool-3-worker-3] [33mWARN [m [LogCollector:298] Unable to collect log from pod: my-cluster-fa4cd2d9-connect-567b958745-hrvdk and container: my-cluster-fa4cd2d9-connect - pod container is not initialized
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace namespace-111
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace namespace-111
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace namespace-111
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace namespace-111
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectWithoutConnectorToZero
2022-04-07 02:22:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-fa4cd2d9 in namespace namespace-111
2022-04-07 02:22:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fa4cd2d9 in namespace namespace-111
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-ec83a646 is in desired state: Ready
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-112 exec my-cluster-ec83a646-connect-d7c49ff6f-c95rm -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1968013148-1621407812 in namespace namespace-112
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-112
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1968013148-1621407812 will have desired state: Ready
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1968013148-1621407812 is in desired state: Ready
2022-04-07 02:22:35 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-ec83a646-connect rolling update
2022-04-07 02:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:22:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-111 for test case:testScaleConnectWithoutConnectorToZero
2022-04-07 02:22:39 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndPausedConnectorWithFileSinkPlugin-FINISHED
2022-04-07 02:22:39 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:22:39 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:22:39 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testConfigureDeploymentStrategy-STARTED
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-113 for test case:testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-113
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-113
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-113
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-75b05b6c in namespace namespace-113
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-75b05b6c will have desired state: Ready
2022-04-07 02:23:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero-FINISHED
2022-04-07 02:23:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:23:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:23:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication-STARTED
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-114 for test case:testConfigureDeploymentStrategy
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-114
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-114
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-114
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-a5b1057c in namespace namespace-114
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-114
2022-04-07 02:23:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-a5b1057c will have desired state: Ready
2022-04-07 02:24:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-75b05b6c is in desired state: Ready
2022-04-07 02:24:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-75b05b6c-user in namespace namespace-114
2022-04-07 02:24:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:24:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-75b05b6c-user will have desired state: Ready
2022-04-07 02:24:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-75b05b6c-user is in desired state: Ready
2022-04-07 02:24:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-687164988-526128473 in namespace namespace-114
2022-04-07 02:24:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:24:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-687164988-526128473 will have desired state: Ready
2022-04-07 02:24:03 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-687164988-526128473 is in desired state: Ready
2022-04-07 02:24:03 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-75b05b6c-scraper in namespace namespace-113
2022-04-07 02:24:03 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:24:03 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-75b05b6c-scraper will be ready
2022-04-07 02:24:05 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-75b05b6c-scraper is ready
2022-04-07 02:24:05 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-75b05b6c-scraper to be ready
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-75b05b6c-scraper is ready
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-75b05b6c-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-75b05b6c-allow in namespace namespace-113
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-75b05b6c in namespace namespace-114
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:24:15 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-75b05b6c will have desired state: Ready
2022-04-07 02:24:15 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-ec83a646-connect will be ready
2022-04-07 02:24:15 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-ec83a646-connect is ready
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-ec83a646-connect rolling update finished
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-112 exec my-cluster-ec83a646-connect-dbd954768-skqhj -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged
2022-04-07 02:24:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-765187061-2082886953 in namespace namespace-112
2022-04-07 02:24:35 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1968013148-1621407812 in namespace namespace-112
2022-04-07 02:24:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-a5b1057c is in desired state: Ready
2022-04-07 02:24:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-a5b1057c in namespace namespace-114
2022-04-07 02:24:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-114
2022-04-07 02:24:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-a5b1057c will have desired state: Ready
2022-04-07 02:24:45 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-ec83a646 in namespace namespace-112
2022-04-07 02:24:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1968013148-1621407812 in namespace namespace-112
2022-04-07 02:24:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1968013148-1621407812 in namespace namespace-112
2022-04-07 02:24:55 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-ec83a646 in namespace namespace-112
2022-04-07 02:25:06 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:25:06 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-112 for test case:testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged
2022-04-07 02:25:25 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-75b05b6c is in desired state: Ready
2022-04-07 02:25:25 [ForkJoinPool-3-worker-19] [32mINFO [m [ConnectIsolatedST:547] Verifying that KafkaConnect pod logs don't contain ERRORs
2022-04-07 02:25:25 [ForkJoinPool-3-worker-19] [32mINFO [m [ConnectIsolatedST:550] Creating FileStreamSink connector via pod my-cluster-75b05b6c-scraper-796b84d584-wrhf4 with topic my-topic-687164988-526128473
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-113 exec my-cluster-75b05b6c-scraper-796b84d584-wrhf4 -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-687164988-526128473", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-75b05b6c-connect-api.namespace-113.svc:8083/connectors
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-75b05b6c-hello-world-producer in namespace namespace-113
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-75b05b6c-hello-world-consumer in namespace namespace-113
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-113
2022-04-07 02:25:26 [ForkJoinPool-3-worker-19] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-75b05b6c-hello-world-producer will be in active state
2022-04-07 02:25:27 [ForkJoinPool-3-worker-19] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-75b05b6c-hello-world-consumer will be in active state
2022-04-07 02:25:27 [ForkJoinPool-3-worker-19] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-75b05b6c-hello-world-producer and consumer my-cluster-75b05b6c-hello-world-consumer finish
2022-04-07 02:25:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectWithScramShaAuthenticationRolledAfterPasswordChanged-FINISHED
2022-04-07 02:25:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:25:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:25:33 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndConnectorFileSinkPlugin-STARTED
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-115 for test case:testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-115
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-115
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-115
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-40bb0c50 in namespace namespace-115
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:25:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-40bb0c50 will have desired state: Ready
2022-04-07 02:25:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-a5b1057c is in desired state: Ready
2022-04-07 02:25:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:1191] Adding label to Connect resource, the CR should be recreated
2022-04-07 02:25:42 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a5b1057c-connect will be ready
2022-04-07 02:25:42 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a5b1057c-connect is ready
2022-04-07 02:25:42 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-a5b1057c-connect to be ready
2022-04-07 02:25:44 [ForkJoinPool-3-worker-19] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-75b05b6c-connect-57689cb4b5-kn58g
2022-04-07 02:25:44 [ForkJoinPool-3-worker-19] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-75b05b6c-connect-57689cb4b5-kn58g
2022-04-07 02:25:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:25:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:348] Delete all resources for testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication
2022-04-07 02:25:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-75b05b6c in namespace namespace-113
2022-04-07 02:25:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-75b05b6c-allow in namespace namespace-113
2022-04-07 02:25:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-75b05b6c-hello-world-consumer in namespace namespace-113
2022-04-07 02:25:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-75b05b6c-hello-world-producer in namespace namespace-113
2022-04-07 02:25:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-687164988-526128473 in namespace namespace-113
2022-04-07 02:25:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-75b05b6c-scraper in namespace namespace-113
2022-04-07 02:26:34 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-75b05b6c-user in namespace namespace-113
2022-04-07 02:26:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-75b05b6c in namespace namespace-113
2022-04-07 02:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-40bb0c50 is in desired state: Ready
2022-04-07 02:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-40bb0c50-user in namespace namespace-115
2022-04-07 02:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:26:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-40bb0c50-user will have desired state: Ready
2022-04-07 02:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-40bb0c50-user is in desired state: Ready
2022-04-07 02:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-161431012-1687398736 in namespace namespace-115
2022-04-07 02:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:26:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-161431012-1687398736 will have desired state: Ready
2022-04-07 02:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-161431012-1687398736 is in desired state: Ready
2022-04-07 02:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-40bb0c50-scraper in namespace namespace-115
2022-04-07 02:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:26:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-40bb0c50-scraper will be ready
2022-04-07 02:26:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-40bb0c50-scraper is ready
2022-04-07 02:26:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-40bb0c50-scraper to be ready
2022-04-07 02:26:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:26:54 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-113 for test case:testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-40bb0c50-scraper is ready
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-40bb0c50-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-40bb0c50-allow in namespace namespace-115
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-40bb0c50 in namespace namespace-115
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:27:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-40bb0c50 will have desired state: Ready
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-a5b1057c-connect is ready
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:1198] Checking that observed gen. is still on 1 (recreation) and new label is present
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:1203] Changing deployment strategy to ROLLING_UPDATE
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-a5b1057c will have desired state: Ready
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-a5b1057c is in desired state: Ready
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:1208] Adding another label to Connect resource, pods should be rolled
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-a5b1057c-connect will be ready
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-a5b1057c-connect is ready
2022-04-07 02:27:09 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-a5b1057c-connect to be ready
2022-04-07 02:27:38 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndScramShaAuthentication-FINISHED
2022-04-07 02:27:38 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:27:38 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:27:38 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testDeployUndeploy-STARTED
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-116 for test case:testKafkaConnectAndConnectorFileSinkPlugin
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-116
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-116
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-116
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0e0d1013 in namespace namespace-116
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-07 02:27:43 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0e0d1013 will have desired state: Ready
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-40bb0c50 is in desired state: Ready
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:63] Waiting until KafkaConnect API is available
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-40bb0c50-connect-867cd6f7b8-8bf5f -- /bin/bash -c curl -I http://localhost:8083/connectors
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:66] KafkaConnect API is available
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:474] Verifying that KafkaConnect pod logs don't contain ERRORs
2022-04-07 02:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectIsolatedST:477] Creating FileStreamSink connector via pod my-cluster-40bb0c50-scraper-6b74c97654-cdpqc with topic my-topic-161431012-1687398736
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-115 exec my-cluster-40bb0c50-scraper-6b74c97654-cdpqc -- /bin/bash -c curl -X POST -H "Content-Type: application/json" --data '{ "name": "sink-test", "config": { "connector.class": "FileStreamSink", "tasks.max": "1", "topics": "my-topic-161431012-1687398736", "file": "/tmp/test-file-sink.txt" } }' http://my-cluster-40bb0c50-connect-api.namespace-115.svc:8083/connectors
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-40bb0c50-hello-world-producer in namespace namespace-115
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-40bb0c50-hello-world-consumer in namespace namespace-115
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-115
2022-04-07 02:28:06 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-40bb0c50-hello-world-producer will be in active state
2022-04-07 02:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-40bb0c50-hello-world-consumer will be in active state
2022-04-07 02:28:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-40bb0c50-hello-world-producer and consumer my-cluster-40bb0c50-hello-world-consumer finish
2022-04-07 02:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:74] Waiting for messages in file sink on my-cluster-40bb0c50-connect-867cd6f7b8-8bf5f
2022-04-07 02:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaConnectUtils:77] Expected messages are in file sink on my-cluster-40bb0c50-connect-867cd6f7b8-8bf5f
2022-04-07 02:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication
2022-04-07 02:28:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-40bb0c50 in namespace namespace-115
2022-04-07 02:28:32 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-a5b1057c-connect is ready
2022-04-07 02:28:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectIsolatedST:1212] Checking that observed gen. higher (rolling update) and label is changed
2022-04-07 02:28:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:28:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigureDeploymentStrategy
2022-04-07 02:28:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-a5b1057c in namespace namespace-114
2022-04-07 02:28:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-a5b1057c in namespace namespace-114
2022-04-07 02:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-40bb0c50-allow in namespace namespace-115
2022-04-07 02:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-40bb0c50-hello-world-consumer in namespace namespace-115
2022-04-07 02:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-40bb0c50-hello-world-producer in namespace namespace-115
2022-04-07 02:28:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-161431012-1687398736 in namespace namespace-115
2022-04-07 02:28:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:28:42 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-114 for test case:testConfigureDeploymentStrategy
2022-04-07 02:28:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-40bb0c50-scraper in namespace namespace-115
2022-04-07 02:28:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:28:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testMountingSecretAndConfigMapAsVolumesAndEnvVars-STARTED
2022-04-07 02:29:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0e0d1013 is in desired state: Ready
2022-04-07 02:29:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0e0d1013-scraper in namespace namespace-116
2022-04-07 02:29:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-07 02:29:02 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0e0d1013-scraper will be ready
2022-04-07 02:29:04 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0e0d1013-scraper is ready
2022-04-07 02:29:04 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-0e0d1013-scraper to be ready
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-0e0d1013-scraper is ready
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-0e0d1013-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-0e0d1013-allow in namespace namespace-116
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-0e0d1013 in namespace namespace-116
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-07 02:29:14 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-0e0d1013 will have desired state: Ready
2022-04-07 02:29:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-40bb0c50-user in namespace namespace-115
2022-04-07 02:29:25 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testConfigureDeploymentStrategy-FINISHED
2022-04-07 02:29:25 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:29:25 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:29:25 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testJvmAndResources-STARTED
2022-04-07 02:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-117 for test case:testMountingSecretAndConfigMapAsVolumesAndEnvVars
2022-04-07 02:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-117
2022-04-07 02:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-117
2022-04-07 02:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-117
2022-04-07 02:29:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c792882c in namespace namespace-117
2022-04-07 02:29:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-07 02:29:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c792882c will have desired state: Ready
2022-04-07 02:29:34 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-40bb0c50 in namespace namespace-115
2022-04-07 02:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:29:44 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-115 for test case:testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication
2022-04-07 02:30:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-0e0d1013 is in desired state: Ready
2022-04-07 02:30:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector license-source in namespace namespace-117
2022-04-07 02:30:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-07 02:30:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: license-source will have desired state: Ready
2022-04-07 02:30:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaConnector: license-source is in desired state: Ready
2022-04-07 02:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-0e0d1013-hello-world-consumer in namespace namespace-116
2022-04-07 02:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-116
2022-04-07 02:30:24 [ForkJoinPool-3-worker-15] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-0e0d1013-hello-world-consumer will be in active state
2022-04-07 02:30:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-0e0d1013-hello-world-consumer to finished
2022-04-07 02:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testSecretsWithKafkaConnectWithTlsAndTlsClientAuthentication-FINISHED
2022-04-07 02:30:28 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-118 for test case:testDeployUndeploy
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-118
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-118
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-118
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f96b47a5 in namespace namespace-118
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-118
2022-04-07 02:30:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f96b47a5 will have desired state: Ready
2022-04-07 02:30:36 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-116 exec my-cluster-0e0d1013-scraper-bcdfc8dbc-mdztv -- /bin/bash -c curl http://my-cluster-0e0d1013-connect-api.namespace-116.svc:8083/connectors/license-source
2022-04-07 02:30:36 [ForkJoinPool-3-worker-15] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:30:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:30:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaConnectAndConnectorFileSinkPlugin
2022-04-07 02:30:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-0e0d1013 in namespace namespace-116
2022-04-07 02:30:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c792882c is in desired state: Ready
2022-04-07 02:30:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-c792882c in namespace namespace-118
2022-04-07 02:30:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-117
2022-04-07 02:30:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-c792882c will have desired state: Ready
2022-04-07 02:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-0e0d1013-hello-world-consumer in namespace namespace-116
2022-04-07 02:30:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector license-source in namespace namespace-116
2022-04-07 02:30:56 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0e0d1013-scraper in namespace namespace-116
2022-04-07 02:31:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-0e0d1013-allow in namespace namespace-116
2022-04-07 02:31:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0e0d1013 in namespace namespace-116
2022-04-07 02:31:46 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:31:46 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-116 for test case:testKafkaConnectAndConnectorFileSinkPlugin
2022-04-07 02:31:50 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f96b47a5 is in desired state: Ready
2022-04-07 02:31:50 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-f96b47a5-scraper in namespace namespace-118
2022-04-07 02:31:50 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-118
2022-04-07 02:31:50 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-f96b47a5-scraper will be ready
2022-04-07 02:31:52 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-f96b47a5-scraper is ready
2022-04-07 02:31:52 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-f96b47a5-scraper to be ready
2022-04-07 02:31:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-c792882c is in desired state: Ready
2022-04-07 02:31:57 [ForkJoinPool-3-worker-1] [32mINFO [m [ConnectIsolatedST:1148] Check if the ENVs contains desired values
2022-04-07 02:31:57 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c printenv MY_CONNECT_SECRET
2022-04-07 02:31:57 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c printenv MY_CONNECT_CONFIG_MAP
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c printenv MY_DOTED_CONNECT_SECRET
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c printenv MY_DOTED_CONNECT_CONFIG_MAP
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ConnectIsolatedST:1154] Check if volumes contains desired values
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c cat external-configuration/connect-config-map/my-key
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c cat external-configuration/connect-secret/my-secret-key
2022-04-07 02:31:58 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c cat external-configuration/connect.config.map/my-key
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-117 exec my-cluster-c792882c-connect-5d77c5dbf8-nh2zl -- /bin/bash -c cat external-configuration/connect.secret/my-secret-key
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testMountingSecretAndConfigMapAsVolumesAndEnvVars
2022-04-07 02:31:59 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-c792882c in namespace namespace-117
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-f96b47a5-scraper is ready
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-f96b47a5-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-f96b47a5-allow in namespace namespace-118
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-118
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-f96b47a5 in namespace namespace-118
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-118
2022-04-07 02:32:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-f96b47a5 will have desired state: Ready
2022-04-07 02:32:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c792882c in namespace namespace-117
2022-04-07 02:32:19 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:32:19 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-117 for test case:testMountingSecretAndConfigMapAsVolumesAndEnvVars
2022-04-07 02:32:30 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testKafkaConnectAndConnectorFileSinkPlugin-FINISHED
2022-04-07 02:32:30 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-119 for test case:testJvmAndResources
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-119
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-119
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-119
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0bcf84df in namespace namespace-119
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-07 02:32:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0bcf84df will have desired state: Ready
2022-04-07 02:32:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testMountingSecretAndConfigMapAsVolumesAndEnvVars-FINISHED
2022-04-07 02:32:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-f96b47a5 is in desired state: Ready
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ConnectIsolatedST:123] Looks like the connect cluster my-cluster deployed OK
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ConnectIsolatedST:140] Verifying docker image names
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ConnectIsolatedST:152] Docker images verified
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:311] Verifying labels on pod type connect
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:315] Verifying labels for pod: my-cluster-f96b47a5-connect-7585f9968b-hnhlf
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:362] Verifying labels for service my-cluster-f96b47a5-connect-api
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:384] Verifying labels for Config maps
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-f96b47a5-connect-config
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-f96b47a5-entity-topic-operator-config
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-f96b47a5-entity-topic-operator-config is not related to current test
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-f96b47a5-entity-user-operator-config
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-f96b47a5-entity-user-operator-config is not related to current test
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-f96b47a5-kafka-config
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-f96b47a5-zookeeper-config
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-f96b47a5-zookeeper-config is not related to current test
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:414] Verifying labels for Service Accounts
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-f96b47a5-connect
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-f96b47a5-entity-operator
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-f96b47a5-kafka
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-f96b47a5-zookeeper
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:348] Delete all resources for testDeployUndeploy
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-f96b47a5-allow in namespace namespace-118
2022-04-07 02:33:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-f96b47a5 in namespace namespace-118
2022-04-07 02:33:16 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-f96b47a5-scraper in namespace namespace-118
2022-04-07 02:33:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0bcf84df is in desired state: Ready
2022-04-07 02:33:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0bcf84df-kafka-clients in namespace namespace-119
2022-04-07 02:33:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-07 02:33:45 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0bcf84df-kafka-clients will be ready
2022-04-07 02:33:47 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0bcf84df-kafka-clients is ready
2022-04-07 02:33:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0bcf84df-scraper in namespace namespace-119
2022-04-07 02:33:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-07 02:33:47 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-0bcf84df-scraper will be ready
2022-04-07 02:33:49 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-0bcf84df-scraper is ready
2022-04-07 02:33:49 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-0bcf84df-scraper to be ready
2022-04-07 02:33:56 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f96b47a5 in namespace namespace-118
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-0bcf84df-scraper is ready
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-0bcf84df-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-0bcf84df-allow in namespace namespace-119
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-0bcf84df in namespace namespace-119
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-119
2022-04-07 02:33:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-0bcf84df will have desired state: Ready
2022-04-07 02:34:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:34:06 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-118 for test case:testDeployUndeploy
2022-04-07 02:34:33 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testDeployUndeploy-FINISHED
2022-04-07 02:34:33 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:35:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-0bcf84df is in desired state: Ready
2022-04-07 02:35:06 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-119 exec my-cluster-0bcf84df-connect-5b975cd786-w7lrm -c my-cluster-0bcf84df-connect -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-07 02:35:06 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:35:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:35:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testJvmAndResources
2022-04-07 02:35:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0bcf84df-scraper in namespace namespace-119
2022-04-07 02:35:06 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0bcf84df-kafka-clients in namespace namespace-119
2022-04-07 02:35:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-0bcf84df in namespace namespace-119
2022-04-07 02:35:46 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0bcf84df in namespace namespace-119
2022-04-07 02:35:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-0bcf84df-allow in namespace namespace-119
2022-04-07 02:35:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:35:56 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-119 for test case:testJvmAndResources
2022-04-07 02:36:40 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testJvmAndResources-FINISHED
2022-04-07 02:36:40 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-04-07 02:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 16, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1,942.592 s <<< FAILURE! - in io.strimzi.systemtest.connect.ConnectIsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero(ExtensionContext)  Time elapsed: 224.015 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: is <0>
     but: was <2>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero(ConnectIsolatedST.java:900)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:396)
	at java.base/java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:721)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.joinConcurrentTasksInReverseOrderToEnableWorkStealing(ForkJoinPoolHierarchicalTestExecutorService.java:162)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:136)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectBuilderIsolatedST
2022-04-07 02:36:40 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:37:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:35 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:37:35 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:37:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:37:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:37:51 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 02:38:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 02:38:02 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 02:38:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 02:38:12 [ForkJoinPool-3-worker-3] [33mWARN [m [ConnectBuilderIsolatedST:546] For running these tests on K8s you have to have internal registry deployed using `minikube start --insecure-registry '10.0.0.0/24'` and `minikube addons enable registry`
2022-04-07 02:38:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka infra-namespace in namespace infra-namespace
2022-04-07 02:38:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: infra-namespace will have desired state: Ready
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: infra-namespace is in desired state: Ready
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:39:29 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildPluginUsingMavenCoordinatesArtifacts-STARTED
2022-04-07 02:39:29 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildWithJarTgzAndZip-STARTED
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:39:29 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:39:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-56521833-1707119970 in namespace infra-namespace
2022-04-07 02:39:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-56521833-1707119970 will have desired state: Ready
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1618554045-2145021914 in namespace infra-namespace
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-7dea3120 in namespace infra-namespace
2022-04-07 02:39:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1618554045-2145021914 will have desired state: Ready
2022-04-07 02:39:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-56521833-1707119970 is in desired state: Ready
2022-04-07 02:39:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-6c091645 in namespace infra-namespace
2022-04-07 02:39:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-6c091645 will have desired state: Ready
2022-04-07 02:39:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1618554045-2145021914 is in desired state: Ready
2022-04-07 02:39:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-7dea3120 will have desired state: Ready
2022-04-07 02:41:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-6c091645 is in desired state: Ready
2022-04-07 02:41:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-6c091645 in namespace infra-namespace
2022-04-07 02:41:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-6c091645 will have desired state: Ready
2022-04-07 02:41:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-6c091645 is in desired state: Ready
2022-04-07 02:41:13 [ForkJoinPool-3-worker-21] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:41:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-6c091645-hello-world-producer in namespace infra-namespace
2022-04-07 02:41:13 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-6c091645-hello-world-producer will be in active state
2022-04-07 02:41:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-6c091645-hello-world-producer to finished
2022-04-07 02:41:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:186] Waiting for message will be in the log
2022-04-07 02:41:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:189] Message Received message with key 'null' and value '"Hello-world - 99"' found in my-cluster-6c091645-connect-6f87cd7789-d5fkm log
2022-04-07 02:41:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:41:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildWithJarTgzAndZip
2022-04-07 02:41:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-6c091645 in namespace infra-namespace
2022-04-07 02:41:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-6c091645-hello-world-producer in namespace infra-namespace
2022-04-07 02:41:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-6c091645 in namespace infra-namespace
2022-04-07 02:41:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-56521833-1707119970 in namespace infra-namespace
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildWithJarTgzAndZip-FINISHED
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildFailsWithWrongChecksumOfArtifact-STARTED
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-742b8db4-scraper in namespace infra-namespace
2022-04-07 02:41:52 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-742b8db4-scraper will be ready
2022-04-07 02:41:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-7dea3120 is in desired state: Ready
2022-04-07 02:41:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector my-cluster-7dea3120-camel-connector in namespace infra-namespace
2022-04-07 02:41:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: my-cluster-7dea3120-camel-connector will have desired state: Ready
2022-04-07 02:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-742b8db4-scraper is ready
2022-04-07 02:41:54 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-742b8db4-scraper to be ready
2022-04-07 02:41:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: my-cluster-7dea3120-camel-connector is in desired state: Ready
2022-04-07 02:41:55 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 02:41:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-7dea3120-hello-world-consumer in namespace infra-namespace
2022-04-07 02:41:55 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-7dea3120-hello-world-consumer will be in active state
2022-04-07 02:41:56 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-7dea3120-hello-world-consumer to finished
2022-04-07 02:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-742b8db4-scraper is ready
2022-04-07 02:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-742b8db4-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-742b8db4-allow in namespace infra-namespace
2022-04-07 02:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-742b8db4 in namespace infra-namespace
2022-04-07 02:42:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-742b8db4 will have desired state: NotReady
2022-04-07 02:42:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-742b8db4 is in desired state: NotReady
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:186] Checking if KafkaConnect status condition contains message about build failure
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:189] Deploying network policies for KafkaConnect
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-742b8db4-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-742b8db4-allow in namespace infra-namespace
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:197] Replacing plugin's checksum with right one
2022-04-07 02:42:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-742b8db4 will have desired state: Ready
2022-04-07 02:42:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:42:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildPluginUsingMavenCoordinatesArtifacts
2022-04-07 02:42:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector my-cluster-7dea3120-camel-connector in namespace infra-namespace
2022-04-07 02:43:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-7dea3120-hello-world-consumer in namespace infra-namespace
2022-04-07 02:43:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-7dea3120 in namespace infra-namespace
2022-04-07 02:43:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1618554045-2145021914 in namespace infra-namespace
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildPluginUsingMavenCoordinatesArtifacts-FINISHED
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [OpenShiftOnlyCondition:25] testPushIntoImageStream is @OpenShiftOnly, but the running cluster is not OpenShift: Ignoring testPushIntoImageStream
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin-STARTED
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-722363998-1623652479 in namespace infra-namespace
2022-04-07 02:43:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-722363998-1623652479 will have desired state: Ready
2022-04-07 02:43:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-722363998-1623652479 is in desired state: Ready
2022-04-07 02:43:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e7c59758-scraper in namespace infra-namespace
2022-04-07 02:43:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e7c59758-scraper will be ready
2022-04-07 02:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e7c59758-scraper is ready
2022-04-07 02:43:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-e7c59758-scraper to be ready
2022-04-07 02:43:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-e7c59758-scraper is ready
2022-04-07 02:43:37 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-e7c59758-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:43:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-e7c59758-allow in namespace infra-namespace
2022-04-07 02:43:37 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:43:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-e7c59758 in namespace infra-namespace
2022-04-07 02:43:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-e7c59758 will have desired state: Ready
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-742b8db4 is in desired state: Ready
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:215] Checking if KafkaConnect API contains EchoSink connector
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-742b8db4-scraper-589cc65f5f-6kqgw -- curl -X GET http://my-cluster-742b8db4-connect-api:8083/connector-plugins
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:220] Checking if KafkaConnect resource contains EchoSink connector in status
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildFailsWithWrongChecksumOfArtifact
2022-04-07 02:44:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-742b8db4 in namespace infra-namespace
2022-04-07 02:44:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-742b8db4-allow in namespace infra-namespace
2022-04-07 02:44:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-742b8db4-allow in namespace infra-namespace
2022-04-07 02:44:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-742b8db4-scraper in namespace infra-namespace
2022-04-07 02:45:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-e7c59758 is in desired state: Ready
2022-04-07 02:45:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectBuilderIsolatedST:370] Creating EchoSink connector
2022-04-07 02:45:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector echo-sink-connector in namespace infra-namespace
2022-04-07 02:45:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: echo-sink-connector will have desired state: Ready
2022-04-07 02:45:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: echo-sink-connector is in desired state: Ready
2022-04-07 02:45:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectBuilderIsolatedST:382] Checking that KafkaConnect API contains EchoSink connector and not Camel-Telegram Connector class name
2022-04-07 02:45:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-e7c59758-scraper-87d7998fd-sfjpq -- curl -X GET http://my-cluster-e7c59758-connect-api:8083/connector-plugins
2022-04-07 02:45:11 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:45:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectBuilderIsolatedST:388] Adding one more connector to the KafkaConnect
2022-04-07 02:45:11 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-e7c59758-connect rolling update
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildFailsWithWrongChecksumOfArtifact-FINISHED
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildOtherPluginTypeWithAndWithoutFileName-STARTED
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1345674396-1019291202 in namespace infra-namespace
2022-04-07 02:45:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1345674396-1019291202 will have desired state: Ready
2022-04-07 02:45:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1345674396-1019291202 is in desired state: Ready
2022-04-07 02:45:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-3654fe3d-scraper in namespace infra-namespace
2022-04-07 02:45:31 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-3654fe3d-scraper will be ready
2022-04-07 02:45:33 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-3654fe3d-scraper is ready
2022-04-07 02:45:33 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-3654fe3d-scraper to be ready
2022-04-07 02:45:43 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-3654fe3d-scraper is ready
2022-04-07 02:45:43 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-3654fe3d-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-04-07 02:45:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy my-cluster-3654fe3d-allow in namespace infra-namespace
2022-04-07 02:45:43 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-04-07 02:45:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-3654fe3d in namespace infra-namespace
2022-04-07 02:45:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-3654fe3d will have desired state: Ready
2022-04-07 02:46:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e7c59758-connect will be ready
2022-04-07 02:46:56 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e7c59758-connect is ready
2022-04-07 02:47:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-e7c59758-connect rolling update finished
2022-04-07 02:47:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectBuilderIsolatedST:399] Creating Camel-HTTP-Sink connector
2022-04-07 02:47:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnector camel-http-connector in namespace infra-namespace
2022-04-07 02:47:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnector: camel-http-connector will have desired state: Ready
2022-04-07 02:47:20 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-3654fe3d is in desired state: Ready
2022-04-07 02:47:20 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:448] Checking that plugin has correct file name: echo-sink-test.jar
2022-04-07 02:47:20 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-3654fe3d-connect-6664c69cfd-9xmkd -- /bin/bash -c ls plugins/plugin-with-other-type/*
2022-04-07 02:47:20 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:47:20 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:461] Removing file name from the plugin, hash should be used
2022-04-07 02:47:20 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-3654fe3d-connect rolling update
2022-04-07 02:49:05 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-3654fe3d-connect will be ready
2022-04-07 02:49:05 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-3654fe3d-connect is ready
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-3654fe3d-connect rolling update finished
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ConnectBuilderIsolatedST:468] Checking that plugin has different name than before
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-3654fe3d-connect-798d57b84f-wfcmq -- /bin/bash -c ls plugins/plugin-with-other-type/*
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testBuildOtherPluginTypeWithAndWithoutFileName
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-3654fe3d-allow in namespace infra-namespace
2022-04-07 02:49:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-3654fe3d in namespace infra-namespace
2022-04-07 02:49:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-3654fe3d-scraper in namespace infra-namespace
2022-04-07 02:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnector: camel-http-connector is in desired state: Ready
2022-04-07 02:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ConnectBuilderIsolatedST:409] Checking if both Connectors were created and Connect contains both plugins
2022-04-07 02:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testUpdateConnectWithAnotherPlugin
2022-04-07 02:49:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-e7c59758 in namespace infra-namespace
2022-04-07 02:49:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector camel-http-connector in namespace infra-namespace
2022-04-07 02:49:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnector echo-sink-connector in namespace infra-namespace
2022-04-07 02:49:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e7c59758-scraper in namespace infra-namespace
2022-04-07 02:50:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1345674396-1019291202 in namespace infra-namespace
2022-04-07 02:50:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:50:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testBuildOtherPluginTypeWithAndWithoutFileName-FINISHED
2022-04-07 02:50:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:50:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-722363998-1623652479 in namespace infra-namespace
2022-04-07 02:50:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy my-cluster-e7c59758-allow in namespace infra-namespace
2022-04-07 02:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectBuilderIsolatedST.testUpdateConnectWithAnotherPlugin-FINISHED
2022-04-07 02:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for ConnectBuilderIsolatedST
2022-04-07 02:50:38 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka infra-namespace in namespace infra-namespace
2022-04-07 02:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 6, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 848.141 s - in io.strimzi.systemtest.connect.ConnectBuilderIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST
2022-04-07 02:50:48 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:51:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 02:51:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 02:51:13 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 02:51:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:51:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 02:51:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:51:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:51:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:51:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:51:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:51:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:51:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:51:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:51:50 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 02:52:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 02:52:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 02:52:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 02:52:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 02:52:17 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST.backupAndRestore-STARTED
2022-04-07 02:52:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 02:52:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-903d1135 in namespace infra-namespace
2022-04-07 02:52:17 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-903d1135 will have desired state: Ready
2022-04-07 02:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-903d1135 is in desired state: Ready
2022-04-07 02:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-903d1135-producer in namespace infra-namespace
2022-04-07 02:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-903d1135-consumer in namespace infra-namespace
2022-04-07 02:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-903d1135-producer will be in active state
2022-04-07 02:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-903d1135-consumer will be in active state
2022-04-07 02:53:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-903d1135-producer and consumer my-cluster-903d1135-consumer finish
2022-04-07 02:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-903d1135-kafka-0 -- /opt/kafka/bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group my-group
2022-04-07 02:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-903d1135-producer in namespace infra-namespace
2022-04-07 02:53:43 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-903d1135-producer will be in active state
2022-04-07 02:53:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-903d1135-producer to finished
2022-04-07 02:53:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ColdBackupScriptIsolatedST:95] Running backup procedure for infra-namespace/my-cluster-903d1135
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/cold-backup/run.sh backup -n infra-namespace -c my-cluster-903d1135 -t /home/ec2-user/strimzi-kafka-operator/systemtest/target/my-cluster-903d1135.tgz -y
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:55:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:55:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:01 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:56:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:56:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:56:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:56:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:32 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:56:32 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:56:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 02:56:48 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 02:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 02:57:14 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 02:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 02:57:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ColdBackupScriptIsolatedST:109] Running restore procedure for infra-namespace/my-cluster-903d1135
2022-04-07 02:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/cold-backup/run.sh restore -n infra-namespace -c my-cluster-903d1135 -s /home/ec2-user/strimzi-kafka-operator/systemtest/target/my-cluster-903d1135.tgz -y
2022-04-07 02:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:58:42 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-903d1135 will have desired state: Ready
2022-04-07 02:59:19 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-903d1135 is in desired state: Ready
2022-04-07 02:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace infra-namespace exec my-cluster-903d1135-kafka-0 -- /opt/kafka/bin/kafka-consumer-groups.sh --describe --bootstrap-server localhost:9092 --group my-group
2022-04-07 02:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 02:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-903d1135-consumer in namespace infra-namespace
2022-04-07 02:59:21 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-903d1135-consumer will be in active state
2022-04-07 02:59:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-903d1135-consumer to finished
2022-04-07 02:59:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-903d1135-consumer in namespace infra-namespace
2022-04-07 02:59:32 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-903d1135-consumer will be in active state
2022-04-07 02:59:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:my-cluster-903d1135-consumer to finished
2022-04-07 02:59:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:59:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for backupAndRestore
2022-04-07 02:59:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-903d1135-producer in namespace infra-namespace
2022-04-07 02:59:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-903d1135-producer in namespace infra-namespace
2022-04-07 02:59:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-903d1135-consumer in namespace infra-namespace
2022-04-07 02:59:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-903d1135-consumer in namespace infra-namespace
2022-04-07 02:59:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-903d1135-consumer in namespace infra-namespace
2022-04-07 02:59:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-903d1135 in namespace infra-namespace
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST.backupAndRestore-FINISHED
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context ColdBackupScriptIsolatedST is everything deleted.
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 545.545 s - in io.strimzi.systemtest.backup.ColdBackupScriptIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-07 02:59:53 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:00:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:29 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 03:00:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:00:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 03:00:39 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 03:00:39 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:00:49 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:00:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:00:49 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:00:49 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 03:00:59 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:00:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 03:01:04 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:01:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 03:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 03:01:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testRestoreOffsetsInConsumerGroup-STARTED
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2ToZero-STARTED
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-120 for test case:testRestoreOffsetsInConsumerGroup
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-121 for test case:testScaleMirrorMaker2ToZero
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-121
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f059a59e-source in namespace namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f059a59e-target in namespace namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:01:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f059a59e-source will have desired state: Ready
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-121
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-121
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-241ff84f-source in namespace namespace-121
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-121
2022-04-07 03:01:27 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-241ff84f-source will have desired state: Ready
2022-04-07 03:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-241ff84f-source is in desired state: Ready
2022-04-07 03:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-241ff84f-target in namespace namespace-121
2022-04-07 03:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-121
2022-04-07 03:02:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-241ff84f-target will have desired state: Ready
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f059a59e-source is in desired state: Ready
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f059a59e-target will have desired state: Ready
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f059a59e-target is in desired state: Ready
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-f059a59e-trg-src in namespace namespace-121
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-f059a59e-src-trg in namespace namespace-121
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic test-sync-offset-1927524476 in namespace namespace-121
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-f059a59e-trg-src will have desired state: Ready
2022-04-07 03:03:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-241ff84f-target is in desired state: Ready
2022-04-07 03:03:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-241ff84f in namespace namespace-121
2022-04-07 03:03:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-121
2022-04-07 03:03:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-241ff84f will have desired state: Ready
2022-04-07 03:04:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-f059a59e-trg-src is in desired state: Ready
2022-04-07 03:04:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-f059a59e-src-trg will have desired state: Ready
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-f059a59e-src-trg is in desired state: Ready
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: test-sync-offset-1927524476 will have desired state: Ready
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: test-sync-offset-1927524476 is in desired state: Ready
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1090] Send & receive 100 messages to/from Source cluster.
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-producer-source-my-consumer-group-88959487 in namespace namespace-120
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-source-my-consumer-group-1043746289 in namespace namespace-120
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-producer-source-my-consumer-group-88959487 will be in active state
2022-04-07 03:04:18 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-source-my-consumer-group-1043746289 will be in active state
2022-04-07 03:04:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-producer-source-my-consumer-group-88959487 to finished
2022-04-07 03:04:27 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-source-my-consumer-group-1043746289 to finished
2022-04-07 03:04:32 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1098] Send 100 messages to Source cluster.
2022-04-07 03:04:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-producer-source-my-consumer-group-88959487 in namespace namespace-120
2022-04-07 03:04:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:04:32 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-producer-source-my-consumer-group-88959487 will be in active state
2022-04-07 03:04:33 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-producer-source-my-consumer-group-88959487 to finished
2022-04-07 03:04:41 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1105] Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it
2022-04-07 03:04:41 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1107] Receive 100 messages from mirrored topic on Target cluster.
2022-04-07 03:04:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-target-my-consumer-group-1097728456 in namespace namespace-120
2022-04-07 03:04:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:04:41 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-target-my-consumer-group-1097728456 will be in active state
2022-04-07 03:04:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-target-my-consumer-group-1097728456 to finished
2022-04-07 03:04:53 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1112] Send 50 messages to Source cluster
2022-04-07 03:04:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-producer-source-my-consumer-group-88959487 in namespace namespace-120
2022-04-07 03:04:53 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:04:53 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-producer-source-my-consumer-group-88959487 will be in active state
2022-04-07 03:04:54 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-producer-source-my-consumer-group-88959487 to finished
2022-04-07 03:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1118] Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it
2022-04-07 03:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1119] Receive 10 msgs from source cluster
2022-04-07 03:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-source-my-consumer-group-1043746289 in namespace namespace-120
2022-04-07 03:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:05:02 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-source-my-consumer-group-1043746289 will be in active state
2022-04-07 03:05:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-source-my-consumer-group-1043746289 to finished
2022-04-07 03:05:13 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-241ff84f is in desired state: Ready
2022-04-07 03:05:13 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:781] Scaling MirrorMaker2 to zero
2022-04-07 03:05:13 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1125] Wait 1 second as 'sync.group.offsets.interval.seconds=1'. As this is insignificant wait, we're skipping it
2022-04-07 03:05:13 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1127] Receive 40 msgs from mirrored topic on Target cluster
2022-04-07 03:05:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-target-my-consumer-group-1097728456 in namespace namespace-120
2022-04-07 03:05:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:05:13 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-target-my-consumer-group-1097728456 will be in active state
2022-04-07 03:05:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:mm2-consumer-target-my-consumer-group-1097728456 to finished
2022-04-07 03:05:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:05:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleMirrorMaker2ToZero
2022-04-07 03:05:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-241ff84f-target in namespace namespace-121
2022-04-07 03:05:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-241ff84f in namespace namespace-121
2022-04-07 03:05:38 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1133] There should be no more messages to read. Try to consume at least 1 message. This client job should fail on timeout.
2022-04-07 03:05:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-target-my-consumer-group-1097728456 in namespace namespace-120
2022-04-07 03:05:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:05:38 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-target-my-consumer-group-1097728456 will be in active state
2022-04-07 03:05:39 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:mm2-consumer-target-my-consumer-group-1097728456 to finish with failure.
2022-04-07 03:05:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-241ff84f-source in namespace namespace-121
2022-04-07 03:05:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:05:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-121 for test case:testScaleMirrorMaker2ToZero
2022-04-07 03:05:56 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:05:56 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2-STARTED
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-122 for test case:testMirrorMaker2
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-122
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-122
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-122
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-06942240-source in namespace namespace-122
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-07 03:06:01 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-06942240-source will have desired state: Ready
2022-04-07 03:06:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2ToZero-FINISHED
2022-04-07 03:06:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:06:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:06:04 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndScramSha512Auth-STARTED
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-123 for test case:testMirrorMaker2TlsAndScramSha512Auth
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-123
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-123
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-123
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-002783fe-source in namespace namespace-123
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:06:09 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-002783fe-source will have desired state: Ready
2022-04-07 03:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-06942240-source is in desired state: Ready
2022-04-07 03:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-06942240-target in namespace namespace-123
2022-04-07 03:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-07 03:07:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-06942240-target will have desired state: Ready
2022-04-07 03:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-002783fe-source is in desired state: Ready
2022-04-07 03:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-002783fe-target in namespace namespace-123
2022-04-07 03:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:07:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-002783fe-target will have desired state: Ready
2022-04-07 03:07:40 [ForkJoinPool-3-worker-21] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 121000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.lambda$testRestoreOffsetsInConsumerGroup$13(MirrorMaker2IsolatedST.java:1137)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testRestoreOffsetsInConsumerGroup(MirrorMaker2IsolatedST.java:1137)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 03:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:100] Client job 'mm2-consumer-target-my-consumer-group-1097728456' finished with expected timeout.
2022-04-07 03:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:1139] As it's Active-Active MM2 mode, there should be no more messages to read from Source cluster topic. This client job should fail on timeout.
2022-04-07 03:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Job mm2-consumer-source-my-consumer-group-1043746289 in namespace namespace-120
2022-04-07 03:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-120
2022-04-07 03:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [JobUtils:81] Waiting for job: mm2-consumer-source-my-consumer-group-1043746289 will be in active state
2022-04-07 03:07:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:89] Waiting for producer/consumer:mm2-consumer-source-my-consumer-group-1043746289 to finish with failure.
2022-04-07 03:08:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-002783fe-target is in desired state: Ready
2022-04-07 03:08:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-1460518697 in namespace namespace-123
2022-04-07 03:08:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:08:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-1460518697 will have desired state: Ready
2022-04-07 03:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-1460518697 is in desired state: Ready
2022-04-07 03:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-002783fe-my-user-source in namespace namespace-123
2022-04-07 03:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:08:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-002783fe-my-user-source will have desired state: Ready
2022-04-07 03:08:21 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-06942240-target is in desired state: Ready
2022-04-07 03:08:21 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-681643216 in namespace namespace-123
2022-04-07 03:08:21 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-07 03:08:21 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-681643216 will have desired state: Ready
2022-04-07 03:08:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-002783fe-my-user-source is in desired state: Ready
2022-04-07 03:08:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-002783fe-my-user-target in namespace namespace-123
2022-04-07 03:08:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:08:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-002783fe-my-user-target will have desired state: Ready
2022-04-07 03:08:22 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-681643216 is in desired state: Ready
2022-04-07 03:08:22 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-06942240-kafka-clients in namespace namespace-122
2022-04-07 03:08:22 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-07 03:08:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-002783fe-my-user-target is in desired state: Ready
2022-04-07 03:08:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-002783fe-kafka-clients in namespace namespace-123
2022-04-07 03:08:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:08:23 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-002783fe-kafka-clients will be ready
2022-04-07 03:08:25 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-002783fe-kafka-clients is ready
2022-04-07 03:08:25 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:08:25 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:536] Sending messages to - topic availability-topic-source-my-topic-133197266-1102044320, cluster my-cluster-002783fe-source and message count of 200
2022-04-07 03:08:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@61292847, messages=[], arguments=[USER=my_cluster_002783fe_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093, --topic, availability-topic-source-my-topic-133197266-1102044320], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093', topicName='availability-topic-source-my-topic-133197266-1102044320', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4e94787f}
2022-04-07 03:08:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093:availability-topic-source-my-topic-133197266-1102044320 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:08:25 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/producer.sh USER=my_cluster_002783fe_my_user_source --max-messages 200 --bootstrap-server my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093 --topic availability-topic-source-my-topic-133197266-1102044320
2022-04-07 03:08:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:08:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:08:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4f82639c, messages=[], arguments=[--group-id, my-consumer-group-1591557830, USER=my_cluster_002783fe_my_user_source, --max-messages, 200, --group-instance-id, instance986297100, --bootstrap-server, my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093, --topic, availability-topic-source-my-topic-133197266-1102044320], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093', topicName='availability-topic-source-my-topic-133197266-1102044320', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-source', consumerGroupName='my-consumer-group-1591557830', consumerInstanceId='instance986297100', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2238a024}
2022-04-07 03:08:29 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093:availability-topic-source-my-topic-133197266-1102044320 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:08:29 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1591557830 USER=my_cluster_002783fe_my_user_source --max-messages 200 --group-instance-id instance986297100 --bootstrap-server my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093 --topic availability-topic-source-my-topic-133197266-1102044320
2022-04-07 03:08:32 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:155] Sending messages to - topic availability-topic-source-my-topic-656528080-1696430345, cluster my-cluster-06942240-source and message count of 100
2022-04-07 03:08:32 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2472638a, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092, --topic, availability-topic-source-my-topic-656528080-1696430345], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092', topicName='availability-topic-source-my-topic-656528080-1696430345', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7f09ebdf}
2022-04-07 03:08:32 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092:availability-topic-source-my-topic-656528080-1696430345 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:08:32 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092 --topic availability-topic-source-my-topic-656528080-1696430345
2022-04-07 03:08:35 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:08:35 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 03:08:35 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@be6d2be, messages=[], arguments=[--group-id, my-consumer-group-841351561, --max-messages, 100, --group-instance-id, instance1437464238, --bootstrap-server, my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092, --topic, availability-topic-source-my-topic-656528080-1696430345], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092', topicName='availability-topic-source-my-topic-656528080-1696430345', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-841351561', consumerInstanceId='instance1437464238', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3b818102}
2022-04-07 03:08:35 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092#availability-topic-source-my-topic-656528080-1696430345 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:08:35 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/consumer.sh --group-id my-consumer-group-841351561 --max-messages 100 --group-instance-id instance1437464238 --bootstrap-server my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092 --topic availability-topic-source-my-topic-656528080-1696430345
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:544] Setting topic to availability-topic-target-my-topic-133197266-1102044320, cluster to my-cluster-002783fe-target and changing user to my-cluster-002783fe-my-user-target
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:553] Sending messages to - topic availability-topic-target-my-topic-133197266-1102044320, cluster my-cluster-002783fe-target and message count of 200
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4d854463, messages=[], arguments=[USER=my_cluster_002783fe_my_user_target, --max-messages, 200, --bootstrap-server, my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093, --topic, availability-topic-target-my-topic-133197266-1102044320], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093', topicName='availability-topic-target-my-topic-133197266-1102044320', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6741113b}
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093:availability-topic-target-my-topic-133197266-1102044320 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:08:36 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/producer.sh USER=my_cluster_002783fe_my_user_target --max-messages 200 --bootstrap-server my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093 --topic availability-topic-target-my-topic-133197266-1102044320
2022-04-07 03:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7619ec33, messages=[], arguments=[--group-id, my-consumer-group-1591557830, USER=my_cluster_002783fe_my_user_target, --max-messages, 200, --group-instance-id, instance874464071, --bootstrap-server, my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093, --topic, availability-topic-target-my-topic-133197266-1102044320], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093', topicName='availability-topic-target-my-topic-133197266-1102044320', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-target', consumerGroupName='my-consumer-group-1591557830', consumerInstanceId='instance874464071', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2adbf477}
2022-04-07 03:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093:availability-topic-target-my-topic-133197266-1102044320 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:08:40 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1591557830 USER=my_cluster_002783fe_my_user_target --max-messages 200 --group-instance-id instance874464071 --bootstrap-server my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093 --topic availability-topic-target-my-topic-133197266-1102044320
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:160] Setting topic to availability-topic-target-my-topic-656528080-1696430345, cluster to my-cluster-06942240-target and changing consumer group
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:168] Sending messages to - topic availability-topic-target-my-topic-656528080-1696430345, cluster my-cluster-06942240-target and message count of 100
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@76a66c27, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092, --topic, availability-topic-target-my-topic-656528080-1696430345], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092', topicName='availability-topic-target-my-topic-656528080-1696430345', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3c3f9e7b}
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092:availability-topic-target-my-topic-656528080-1696430345 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:08:41 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092 --topic availability-topic-target-my-topic-656528080-1696430345
2022-04-07 03:08:44 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:08:44 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 03:08:44 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7c5633cf, messages=[], arguments=[--group-id, my-consumer-group-217244230, --max-messages, 100, --group-instance-id, instance1933962042, --bootstrap-server, my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092, --topic, availability-topic-target-my-topic-656528080-1696430345], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092', topicName='availability-topic-target-my-topic-656528080-1696430345', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-217244230', consumerInstanceId='instance1933962042', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@41986a60}
2022-04-07 03:08:44 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092#availability-topic-target-my-topic-656528080-1696430345 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:08:44 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/consumer.sh --group-id my-consumer-group-217244230 --max-messages 100 --group-instance-id instance1933962042 --bootstrap-server my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092 --topic availability-topic-target-my-topic-656528080-1696430345
2022-04-07 03:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-002783fe in namespace namespace-123
2022-04-07 03:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-123
2022-04-07 03:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-002783fe will have desired state: Ready
2022-04-07 03:08:50 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:08:50 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:08:50 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-06942240 in namespace namespace-123
2022-04-07 03:08:50 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-122
2022-04-07 03:08:50 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-06942240 will have desired state: Ready
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [1;31mERROR[m [TestUtils:162] Exception waiting for Job did not finish within time limit (as expected)., null
io.strimzi.test.WaitException: Timeout after 121000 ms waiting for Job did not finish within time limit (as expected).
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.ClientUtils.waitForClientTimeout(ClientUtils.java:91)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.lambda$testRestoreOffsetsInConsumerGroup$14(MirrorMaker2IsolatedST.java:1143)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:50)
	at org.junit.jupiter.api.AssertDoesNotThrow.assertDoesNotThrow(AssertDoesNotThrow.java:37)
	at org.junit.jupiter.api.Assertions.assertDoesNotThrow(Assertions.java:3135)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testRestoreOffsetsInConsumerGroup(MirrorMaker2IsolatedST.java:1143)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ClientUtils:100] Client job 'mm2-consumer-source-my-consumer-group-1043746289' finished with expected timeout.
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testRestoreOffsetsInConsumerGroup
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-consumer-target-my-consumer-group-1097728456 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-producer-source-my-consumer-group-88959487 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-producer-source-my-consumer-group-88959487 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-consumer-target-my-consumer-group-1097728456 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-consumer-source-my-consumer-group-1043746289 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-consumer-target-my-consumer-group-1097728456 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-consumer-source-my-consumer-group-1043746289 in namespace namespace-120
2022-04-07 03:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic test-sync-offset-1927524476 in namespace namespace-120
2022-04-07 03:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-002783fe is in desired state: Ready
2022-04-07 03:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:597] Setting topic to mirrormaker2-topic-example-1460518697, cluster to my-cluster-002783fe-source and changing user to my-cluster-002783fe-my-user-source
2022-04-07 03:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:606] Sending messages to - topic mirrormaker2-topic-example-1460518697, cluster my-cluster-002783fe-source and message count of 200
2022-04-07 03:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1c38c3b6, messages=[], arguments=[USER=my_cluster_002783fe_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093, --topic, mirrormaker2-topic-example-1460518697], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093', topicName='mirrormaker2-topic-example-1460518697', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@53bf9365}
2022-04-07 03:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093:mirrormaker2-topic-example-1460518697 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/producer.sh USER=my_cluster_002783fe_my_user_source --max-messages 200 --bootstrap-server my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093 --topic mirrormaker2-topic-example-1460518697
2022-04-07 03:09:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:09:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:09:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1af265cf, messages=[], arguments=[--group-id, my-consumer-group-1591557830, USER=my_cluster_002783fe_my_user_source, --max-messages, 200, --group-instance-id, instance1865342021, --bootstrap-server, my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093, --topic, mirrormaker2-topic-example-1460518697], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093', topicName='mirrormaker2-topic-example-1460518697', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-source', consumerGroupName='my-consumer-group-1591557830', consumerInstanceId='instance1865342021', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@35f9fcae}
2022-04-07 03:09:56 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093:mirrormaker2-topic-example-1460518697 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:09:56 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1591557830 USER=my_cluster_002783fe_my_user_source --max-messages 200 --group-instance-id instance1865342021 --bootstrap-server my-cluster-002783fe-source-kafka-bootstrap.namespace-123.svc:9093 --topic mirrormaker2-topic-example-1460518697
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-06942240 is in desired state: Ready
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:183] Looks like the mirrormaker2 cluster my-cluster deployed OK
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:640] Verifying docker image names
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:172] strimzi-cluster-operator
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:653] Docker images verified
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:311] Verifying labels on pod type mirrormaker2
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:315] Verifying labels for pod: my-cluster-06942240-mirrormaker2-8497fc7449-c74xv
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:362] Verifying labels for service my-cluster-06942240-mirrormaker2-api
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:384] Verifying labels for Config maps
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-mirrormaker2-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-source-entity-topic-operator-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-06942240-source-entity-topic-operator-config is not related to current test
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-source-entity-user-operator-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-06942240-source-entity-user-operator-config is not related to current test
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-source-kafka-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-source-zookeeper-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-06942240-source-zookeeper-config is not related to current test
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-target-entity-topic-operator-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-06942240-target-entity-topic-operator-config is not related to current test
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-target-entity-user-operator-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-06942240-target-entity-user-operator-config is not related to current test
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-target-kafka-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-06942240-target-zookeeper-config
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:407] CM my-cluster-06942240-target-zookeeper-config is not related to current test
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:414] Verifying labels for Service Accounts
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-06942240-source-entity-operator
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-06942240-source-kafka
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-06942240-source-zookeeper
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:198] Setting topic to mirrormaker2-topic-example-681643216, cluster to my-cluster-06942240-source and changing consumer group
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:206] Sending messages to - topic mirrormaker2-topic-example-681643216, cluster my-cluster-06942240-source and message count of 100
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@77b4caaa, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092, --topic, mirrormaker2-topic-example-681643216], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092', topicName='mirrormaker2-topic-example-681643216', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@57b81f7b}
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092:mirrormaker2-topic-example-681643216 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:09:58 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092 --topic mirrormaker2-topic-example-681643216
2022-04-07 03:10:01 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:10:01 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 03:10:01 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:210] Consumer in source cluster and topic should receive 100 messages
2022-04-07 03:10:01 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@43cfe27d, messages=[], arguments=[--group-id, my-consumer-group-1289705242, --max-messages, 100, --group-instance-id, instance1094435911, --bootstrap-server, my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092, --topic, mirrormaker2-topic-example-681643216], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092', topicName='mirrormaker2-topic-example-681643216', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1289705242', consumerInstanceId='instance1094435911', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@127f9b02}
2022-04-07 03:10:01 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092#mirrormaker2-topic-example-681643216 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:10:01 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1289705242 --max-messages 100 --group-instance-id instance1094435911 --bootstrap-server my-cluster-06942240-source-kafka-bootstrap.namespace-122.svc:9092 --topic mirrormaker2-topic-example-681643216
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:615] Changing to target - topic my-cluster-002783fe-source.mirrormaker2-topic-example-1460518697, cluster my-cluster-002783fe-target, user my-cluster-002783fe-my-user-target
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:623] Now messages should be mirrored to target topic and cluster
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2d311784, messages=[], arguments=[--group-id, my-consumer-group-1591557830, USER=my_cluster_002783fe_my_user_target, --max-messages, 200, --group-instance-id, instance97949421, --bootstrap-server, my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093, --topic, my-cluster-002783fe-source.mirrormaker2-topic-example-1460518697], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw', podNamespace='namespace-123', bootstrapServer='my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093', topicName='my-cluster-002783fe-source.mirrormaker2-topic-example-1460518697', maxMessages=200, kafkaUsername='my-cluster-002783fe-my-user-target', consumerGroupName='my-consumer-group-1591557830', consumerInstanceId='instance97949421', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@400d3054}
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093:my-cluster-002783fe-source.mirrormaker2-topic-example-1460518697 from pod my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw
2022-04-07 03:10:04 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-002783fe-kafka-clients-799c5b6fd-6qjmw -n namespace-123 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1591557830 USER=my_cluster_002783fe_my_user_target --max-messages 200 --group-instance-id instance97949421 --bootstrap-server my-cluster-002783fe-target-kafka-bootstrap.namespace-123.svc:9093 --topic my-cluster-002783fe-source.mirrormaker2-topic-example-1460518697
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:214] Now setting topic to my-cluster-06942240-source.mirrormaker2-topic-example-681643216 and cluster to my-cluster-06942240-target - the messages should be mirrored
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:222] Consumer in target cluster and topic should receive 100 messages
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2263c4db, messages=[], arguments=[--group-id, my-consumer-group-1879092663, --max-messages, 100, --group-instance-id, instance1748072424, --bootstrap-server, my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092, --topic, my-cluster-06942240-source.mirrormaker2-topic-example-681643216], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092', topicName='my-cluster-06942240-source.mirrormaker2-topic-example-681643216', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1879092663', consumerInstanceId='instance1748072424', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7f6d4f6a}
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092#my-cluster-06942240-source.mirrormaker2-topic-example-681643216 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:10:06 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1879092663 --max-messages 100 --group-instance-id instance1748072424 --bootstrap-server my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092 --topic my-cluster-06942240-source.mirrormaker2-topic-example-681643216
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:628] Messages successfully mirrored
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:78] Waiting for KafkaTopic my-cluster-002783fe-source.mirrormaker2-topic-example-1460518697 creation 
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2TlsAndScramSha512Auth
2022-04-07 03:10:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-002783fe-my-user-target in namespace namespace-123
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:227] Changing topic to my-cluster-06942240-source.availability-topic-source-my-topic-656528080-1696430345
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:233] Check if mm2 mirror automatically created topic
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@70ca23ec, messages=[], arguments=[--group-id, my-consumer-group-1162490694, --max-messages, 100, --group-instance-id, instance688202787, --bootstrap-server, my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092, --topic, my-cluster-06942240-source.availability-topic-source-my-topic-656528080-1696430345], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc', podNamespace='namespace-122', bootstrapServer='my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092', topicName='my-cluster-06942240-source.availability-topic-source-my-topic-656528080-1696430345', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1162490694', consumerInstanceId='instance688202787', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4c2afc1e}
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092#my-cluster-06942240-source.availability-topic-source-my-topic-656528080-1696430345 from pod my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc
2022-04-07 03:10:12 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-06942240-kafka-clients-7bff484fb9-7cvsc -n namespace-122 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1162490694 --max-messages 100 --group-instance-id instance688202787 --bootstrap-server my-cluster-06942240-target-kafka-bootstrap.namespace-122.svc:9092 --topic my-cluster-06942240-source.availability-topic-source-my-topic-656528080-1696430345
2022-04-07 03:10:18 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:10:18 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:10:18 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:236] Mirrored successful
2022-04-07 03:10:18 [ForkJoinPool-3-worker-19] [32mINFO [m [KafkaTopicUtils:124] Waiting for KafkaTopic change my-cluster-06942240-source.mirrormaker2-topic-example-681643216
2022-04-07 03:10:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-002783fe-my-user-source in namespace namespace-123
2022-04-07 03:10:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-002783fe in namespace namespace-123
2022-04-07 03:10:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-002783fe-kafka-clients in namespace namespace-123
2022-04-07 03:10:58 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:10:58 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2
2022-04-07 03:10:58 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-681643216 in namespace namespace-122
2022-04-07 03:11:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-002783fe-target in namespace namespace-123
2022-04-07 03:11:31 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-1460518697 in namespace namespace-123
2022-04-07 03:11:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-002783fe-source in namespace namespace-123
2022-04-07 03:11:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:11:51 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-123 for test case:testMirrorMaker2TlsAndScramSha512Auth
2022-04-07 03:12:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndScramSha512Auth-FINISHED
2022-04-07 03:12:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:12:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:12:18 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2CorrectlyMirrorsHeaders-STARTED
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-124 for test case:testMirrorMaker2CorrectlyMirrorsHeaders
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-124
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-124
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-124
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-13fba170-source in namespace namespace-124
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-07 03:12:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-13fba170-source will have desired state: Ready
io.strimzi.test.WaitException: Timeout after 180000 ms waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:test-sync-offset-1927524476
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.resources.ResourceManager.waitResourceCondition(ResourceManager.java:264)
	at io.strimzi.systemtest.resources.ResourceManager.deleteResource(ResourceManager.java:245)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$createResource$1(ResourceManager.java:217)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$deleteResources$3(ResourceManager.java:360)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.base/java.util.Vector$VectorSpliterator.forEachRemaining(Vector.java:1492)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:290)
	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.helpCC(ForkJoinPool.java:1115)
	at java.base/java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:1687)
	at java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:411)
	at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:159)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:661)
	at io.strimzi.systemtest.resources.ResourceManager.deleteResources(ResourceManager.java:357)
	at io.strimzi.systemtest.AbstractST.afterEachMayOverride(AbstractST.java:538)
	at io.strimzi.systemtest.AbstractST.tearDownTestCase(AbstractST.java:681)
	at jdk.internal.reflect.GeneratedMethodAccessor680.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:126)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptAfterEachMethod(TimeoutExtension.java:108)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeMethodInExtensionContext(ClassBasedTestDescriptor.java:506)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$synthesizeAfterEachMethodAdapter$22(ClassBasedTestDescriptor.java:496)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAfterEachMethods$10(TestMethodTestDescriptor.java:240)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$13(TestMethodTestDescriptor.java:273)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$14(TestMethodTestDescriptor.java:273)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeAllAfterMethodsOrCallbacks(TestMethodTestDescriptor.java:272)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeAfterEachMethods(TestMethodTestDescriptor.java:238)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:139)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
io.strimzi.test.WaitException: Timeout after 180000 ms waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:test-sync-offset-1927524476
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.resources.ResourceManager.waitResourceCondition(ResourceManager.java:264)
	at io.strimzi.systemtest.resources.ResourceManager.deleteResource(ResourceManager.java:245)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$createResource$1(ResourceManager.java:217)
	at io.strimzi.systemtest.resources.ResourceManager.lambda$deleteResources$3(ResourceManager.java:360)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183)
	at java.base/java.util.Vector$VectorSpliterator.forEachRemaining(Vector.java:1492)
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)
	at java.base/java.util.stream.ForEachOps$ForEachTask.compute(ForEachOps.java:290)
	at java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.helpCC(ForkJoinPool.java:1115)
	at java.base/java.util.concurrent.ForkJoinPool.awaitJoin(ForkJoinPool.java:1687)
	at java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:411)
	at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736)
	at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:159)
	at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:173)
	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
	at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:497)
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:661)
	at io.strimzi.systemtest.resources.ResourceManager.deleteResources(ResourceManager.java:357)
	at io.strimzi.systemtest.AbstractST.afterEachMayOverride(AbstractST.java:538)
	at io.strimzi.systemtest.AbstractST.tearDownTestCase(AbstractST.java:681)
	at jdk.internal.reflect.GeneratedMethodAccessor680.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptLifecycleMethod(TimeoutExtension.java:126)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptAfterEachMethod(TimeoutExtension.java:108)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeMethodInExtensionContext(ClassBasedTestDescriptor.java:506)
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$synthesizeAfterEachMethodAdapter$22(ClassBasedTestDescriptor.java:496)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAfterEachMethods$10(TestMethodTestDescriptor.java:240)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$13(TestMethodTestDescriptor.java:273)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeAllAfterMethodsOrCallbacks$14(TestMethodTestDescriptor.java:273)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1541)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeAllAfterMethodsOrCallbacks(TestMethodTestDescriptor.java:272)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeAfterEachMethods(TestMethodTestDescriptor.java:238)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:139)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)
2022-04-07 03:12:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-f059a59e-src-trg in namespace namespace-120
2022-04-07 03:12:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-consumer-source-my-consumer-group-1043746289 in namespace namespace-120
2022-04-07 03:12:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Job mm2-producer-source-my-consumer-group-88959487 in namespace namespace-120
2022-04-07 03:12:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f059a59e-target in namespace namespace-120
2022-04-07 03:13:02 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-f059a59e-trg-src in namespace namespace-120
2022-04-07 03:13:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f059a59e-source in namespace namespace-120
2022-04-07 03:13:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:13:22 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-120 for test case:testRestoreOffsetsInConsumerGroup
2022-04-07 03:13:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-13fba170-source is in desired state: Ready
2022-04-07 03:13:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-13fba170-target in namespace namespace-124
2022-04-07 03:13:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-07 03:13:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-13fba170-target will have desired state: Ready
2022-04-07 03:13:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-06942240 in namespace namespace-122
2022-04-07 03:13:38 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-06942240-kafka-clients in namespace namespace-122
2022-04-07 03:13:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-06942240-target in namespace namespace-122
2022-04-07 03:14:02 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-06942240-source in namespace namespace-122
2022-04-07 03:14:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testRestoreOffsetsInConsumerGroup-FINISHED
2022-04-07 03:14:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:14:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:14:06 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKafkaMirrorMaker2ReflectsConnectorsState-STARTED
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-125 for test case:testKafkaMirrorMaker2ReflectsConnectorsState
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0931d0ba-source in namespace namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0931d0ba-target in namespace namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-07 03:14:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0931d0ba-source will have desired state: Ready
2022-04-07 03:14:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:14:12 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS-STARTED
2022-04-07 03:14:18 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:14:18 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-122 for test case:testMirrorMaker2
2022-04-07 03:14:23 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2-FINISHED
2022-04-07 03:14:23 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:14:23 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:14:23 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testStrimziIdentityReplicationPolicy-STARTED
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-126 for test case:testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-126
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-126
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-126
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-80e7e52c-source in namespace namespace-126
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:14:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-80e7e52c-source will have desired state: Ready
2022-04-07 03:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-13fba170-target is in desired state: Ready
2022-04-07 03:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-cluster-13fba170-source-example-topic in namespace namespace-126
2022-04-07 03:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-07 03:14:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-cluster-13fba170-source-example-topic will have desired state: Ready
2022-04-07 03:14:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-cluster-13fba170-source-example-topic is in desired state: Ready
2022-04-07 03:14:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-13fba170 in namespace namespace-126
2022-04-07 03:14:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-07 03:14:37 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-13fba170 will have desired state: Ready
2022-04-07 03:15:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0931d0ba-source is in desired state: Ready
2022-04-07 03:15:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0931d0ba-target will have desired state: Ready
2022-04-07 03:15:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0931d0ba-target is in desired state: Ready
2022-04-07 03:15:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-0931d0ba in namespace namespace-126
2022-04-07 03:15:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-125
2022-04-07 03:15:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:481] Wait for KafkaMirrorMaker2: my-cluster-0931d0ba will contain desired status message: One or more connectors are in FAILED state
2022-04-07 03:15:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-80e7e52c-source is in desired state: Ready
2022-04-07 03:15:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-13fba170 is in desired state: Ready
2022-04-07 03:15:43 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 03:15:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-13fba170-target-consumer in namespace namespace-126
2022-04-07 03:15:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-07 03:15:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-80e7e52c-target in namespace namespace-126
2022-04-07 03:15:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:15:43 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-13fba170-target-consumer will be in active state
2022-04-07 03:15:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-80e7e52c-target will have desired state: Ready
2022-04-07 03:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 03:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-13fba170-source-producer in namespace namespace-126
2022-04-07 03:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-124
2022-04-07 03:15:44 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-13fba170-source-producer will be in active state
2022-04-07 03:15:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-13fba170-source-producer and consumer my-cluster-13fba170-target-consumer finish
2022-04-07 03:16:44 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:492] KafkaMirrorMaker2: my-cluster-0931d0ba contains desired message in status: One or more connectors are in FAILED state
2022-04-07 03:16:44 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-0931d0ba will have desired state: Ready
2022-04-07 03:16:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-0931d0ba is in desired state: Ready
2022-04-07 03:16:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:16:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaMirrorMaker2ReflectsConnectorsState
2022-04-07 03:16:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0931d0ba-target in namespace namespace-125
2022-04-07 03:16:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-0931d0ba in namespace namespace-125
2022-04-07 03:17:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0931d0ba-source in namespace namespace-125
2022-04-07 03:17:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-80e7e52c-target is in desired state: Ready
2022-04-07 03:17:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-a-494350232 in namespace namespace-126
2022-04-07 03:17:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-a-494350232 will have desired state: Ready
2022-04-07 03:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-a-494350232 is in desired state: Ready
2022-04-07 03:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-b-1810742058 in namespace namespace-126
2022-04-07 03:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-b-1810742058 will have desired state: Ready
2022-04-07 03:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-b-1810742058 is in desired state: Ready
2022-04-07 03:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-80e7e52c-my-user-source in namespace namespace-126
2022-04-07 03:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:12 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-80e7e52c-my-user-source will have desired state: Ready
2022-04-07 03:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-80e7e52c-my-user-source is in desired state: Ready
2022-04-07 03:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-80e7e52c-my-user-target in namespace namespace-126
2022-04-07 03:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:13 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-80e7e52c-my-user-target will have desired state: Ready
2022-04-07 03:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-80e7e52c-my-user-target is in desired state: Ready
2022-04-07 03:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-5235baf4-kafka-clients in namespace namespace-126
2022-04-07 03:17:14 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:17:15 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-125 for test case:testKafkaMirrorMaker2ReflectsConnectorsState
2022-04-07 03:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-598101073-1370746584-test-1 in namespace namespace-126
2022-04-07 03:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-598101073-1370746584-test-1 will have desired state: Ready
2022-04-07 03:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-598101073-1370746584-test-1 is in desired state: Ready
2022-04-07 03:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-598101073-1370746584-test-2 in namespace namespace-126
2022-04-07 03:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-598101073-1370746584-test-2 will have desired state: Ready
2022-04-07 03:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-598101073-1370746584-test-2 is in desired state: Ready
2022-04-07 03:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@58a98f33, messages=[], arguments=[USER=my_cluster_80e7e52c_my_user_target, --max-messages, 200, --bootstrap-server, my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093, --topic, my-topic-598101073-1370746584-test-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc', podNamespace='namespace-126', bootstrapServer='my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093', topicName='my-topic-598101073-1370746584-test-2', maxMessages=200, kafkaUsername='my-cluster-80e7e52c-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2537eda0}
2022-04-07 03:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093:my-topic-598101073-1370746584-test-2 from pod my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc
2022-04-07 03:17:26 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc -n namespace-126 -- /opt/kafka/producer.sh USER=my_cluster_80e7e52c_my_user_target --max-messages 200 --bootstrap-server my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093 --topic my-topic-598101073-1370746584-test-2
2022-04-07 03:17:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:17:26 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2Subresource-STARTED
2022-04-07 03:17:26 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKafkaMirrorMaker2ReflectsConnectorsState-FINISHED
2022-04-07 03:17:26 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:17:26 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:17:26 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testIdentityReplicationPolicy-STARTED
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-127 for test case:testStrimziIdentityReplicationPolicy
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-127
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-127
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-127
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f746d34d-source in namespace namespace-127
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-07 03:17:28 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f746d34d-source will have desired state: Ready
2022-04-07 03:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1e721f65, messages=[], arguments=[--group-id, my-consumer-group-599482361, USER=my_cluster_80e7e52c_my_user_target, --max-messages, 200, --group-instance-id, instance961397027, --bootstrap-server, my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093, --topic, my-topic-598101073-1370746584-test-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc', podNamespace='namespace-126', bootstrapServer='my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093', topicName='my-topic-598101073-1370746584-test-2', maxMessages=200, kafkaUsername='my-cluster-80e7e52c-my-user-target', consumerGroupName='my-consumer-group-599482361', consumerInstanceId='instance961397027', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b2a8be}
2022-04-07 03:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093:my-topic-598101073-1370746584-test-2 from pod my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc
2022-04-07 03:17:30 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc -n namespace-126 -- /opt/kafka/consumer.sh --group-id my-consumer-group-599482361 USER=my_cluster_80e7e52c_my_user_target --max-messages 200 --group-instance-id instance961397027 --bootstrap-server my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093 --topic my-topic-598101073-1370746584-test-2
2022-04-07 03:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:753] Checking log of my-cluster-13fba170-target-consumer job if the headers are correct
2022-04-07 03:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2CorrectlyMirrorsHeaders
2022-04-07 03:17:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-13fba170 in namespace namespace-124
2022-04-07 03:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-80e7e52c in namespace namespace-127
2022-04-07 03:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-126
2022-04-07 03:17:37 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-80e7e52c will have desired state: Ready
2022-04-07 03:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-13fba170-source-producer in namespace namespace-124
2022-04-07 03:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-13fba170-target-consumer in namespace namespace-124
2022-04-07 03:17:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-13fba170-target in namespace namespace-124
2022-04-07 03:17:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-cluster-13fba170-source-example-topic in namespace namespace-124
2022-04-07 03:18:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-13fba170-source in namespace namespace-124
2022-04-07 03:18:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:18:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-124 for test case:testMirrorMaker2CorrectlyMirrorsHeaders
2022-04-07 03:18:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2CorrectlyMirrorsHeaders-FINISHED
2022-04-07 03:18:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:18:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:18:16 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testConfigureDeploymentStrategy-STARTED
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-128 for test case:testScaleMirrorMaker2Subresource
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-128
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-128
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-128
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4a958d07-source in namespace namespace-128
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-128
2022-04-07 03:18:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4a958d07-source will have desired state: Ready
2022-04-07 03:18:31 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f746d34d-source is in desired state: Ready
2022-04-07 03:18:31 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-f746d34d-target in namespace namespace-128
2022-04-07 03:18:31 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-07 03:18:31 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-f746d34d-target will have desired state: Ready
2022-04-07 03:18:44 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-80e7e52c is in desired state: Ready
2022-04-07 03:18:44 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@261a9e80, messages=[], arguments=[USER=my_cluster_80e7e52c_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093, --topic, mirrormaker2-topic-example-a-494350232], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc', podNamespace='namespace-126', bootstrapServer='my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093', topicName='mirrormaker2-topic-example-a-494350232', maxMessages=200, kafkaUsername='my-cluster-80e7e52c-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7807fc5e}
2022-04-07 03:18:44 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093:mirrormaker2-topic-example-a-494350232 from pod my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc
2022-04-07 03:18:44 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc -n namespace-126 -- /opt/kafka/producer.sh USER=my_cluster_80e7e52c_my_user_source --max-messages 200 --bootstrap-server my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093 --topic mirrormaker2-topic-example-a-494350232
2022-04-07 03:18:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:18:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:18:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@159649a0, messages=[], arguments=[--group-id, my-consumer-group-599482361, USER=my_cluster_80e7e52c_my_user_source, --max-messages, 200, --group-instance-id, instance2114992143, --bootstrap-server, my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093, --topic, mirrormaker2-topic-example-a-494350232], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc', podNamespace='namespace-126', bootstrapServer='my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093', topicName='mirrormaker2-topic-example-a-494350232', maxMessages=200, kafkaUsername='my-cluster-80e7e52c-my-user-source', consumerGroupName='my-consumer-group-599482361', consumerInstanceId='instance2114992143', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5ce101b9}
2022-04-07 03:18:48 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093:mirrormaker2-topic-example-a-494350232 from pod my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc
2022-04-07 03:18:48 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc -n namespace-126 -- /opt/kafka/consumer.sh --group-id my-consumer-group-599482361 USER=my_cluster_80e7e52c_my_user_source --max-messages 200 --group-instance-id instance2114992143 --bootstrap-server my-cluster-80e7e52c-source-kafka-bootstrap.namespace-126.svc:9093 --topic mirrormaker2-topic-example-a-494350232
2022-04-07 03:18:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:18:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:18:55 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1551] Consumer in target cluster and topic should receive 200 messages
2022-04-07 03:18:55 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:18:55 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha-STARTED
2022-04-07 03:18:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@20a8c808, messages=[], arguments=[--group-id, my-consumer-group-599482361, USER=my_cluster_80e7e52c_my_user_target, --max-messages, 200, --group-instance-id, instance808935194, --bootstrap-server, my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093, --topic, my-cluster-80e7e52c-source.mirrormaker2-topic-example-a-494350232], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc', podNamespace='namespace-126', bootstrapServer='my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093', topicName='my-cluster-80e7e52c-source.mirrormaker2-topic-example-a-494350232', maxMessages=200, kafkaUsername='my-cluster-80e7e52c-my-user-target', consumerGroupName='my-consumer-group-599482361', consumerInstanceId='instance808935194', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@52b4a240}
2022-04-07 03:18:55 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093:my-cluster-80e7e52c-source.mirrormaker2-topic-example-a-494350232 from pod my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc
2022-04-07 03:18:55 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-5235baf4-kafka-clients-f77cf5b8c-n4hlc -n namespace-126 -- /opt/kafka/consumer.sh --group-id my-consumer-group-599482361 USER=my_cluster_80e7e52c_my_user_target --max-messages 200 --group-instance-id instance808935194 --bootstrap-server my-cluster-80e7e52c-target-kafka-bootstrap.namespace-126.svc:9093 --topic my-cluster-80e7e52c-source.mirrormaker2-topic-example-a-494350232
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1553] Messages successfully mirrored
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [1;31mERROR[m [TestExecutionWatcher:28] MirrorMaker2IsolatedST - Exception null has been thrown in @Test. Going to collect logs from components.
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 03:19:03 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 03:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 03:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 03:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 03:19:05 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 03:19:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 03:19:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:252] Collecting events in Namespace namespace-126
2022-04-07 03:19:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace namespace-126
2022-04-07 03:19:06 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-126
2022-04-07 03:19:10 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace namespace-126
2022-04-07 03:19:10 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace namespace-126
2022-04-07 03:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace namespace-126
2022-04-07 03:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace namespace-126
2022-04-07 03:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 03:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-07 03:19:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-5235baf4-kafka-clients in namespace namespace-126
2022-04-07 03:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4a958d07-source is in desired state: Ready
2022-04-07 03:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-4a958d07-target in namespace namespace-128
2022-04-07 03:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-128
2022-04-07 03:19:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-4a958d07-target will have desired state: Ready
2022-04-07 03:19:40 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-f746d34d-target is in desired state: Ready
2022-04-07 03:19:40 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-cluster-f746d34d in namespace namespace-128
2022-04-07 03:19:40 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-07 03:19:40 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-cluster-f746d34d will have desired state: Ready
2022-04-07 03:19:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaTopic: my-cluster-f746d34d is in desired state: Ready
2022-04-07 03:19:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-f746d34d-kafka-clients in namespace namespace-127
2022-04-07 03:19:41 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-07 03:19:51 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-f746d34d in namespace namespace-128
2022-04-07 03:19:51 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-127
2022-04-07 03:19:51 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-f746d34d will have desired state: Ready
2022-04-07 03:20:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-80e7e52c-my-user-target in namespace namespace-126
2022-04-07 03:20:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-598101073-1370746584-test-2 in namespace namespace-126
2022-04-07 03:20:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-80e7e52c in namespace namespace-126
2022-04-07 03:20:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-598101073-1370746584-test-1 in namespace namespace-126
2022-04-07 03:20:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-4a958d07-target is in desired state: Ready
2022-04-07 03:20:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-4a958d07 in namespace namespace-128
2022-04-07 03:20:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-128
2022-04-07 03:20:27 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-4a958d07 will have desired state: Ready
2022-04-07 03:20:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-a-494350232 in namespace namespace-126
2022-04-07 03:20:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-80e7e52c-my-user-source in namespace namespace-126
2022-04-07 03:20:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-b-1810742058 in namespace namespace-126
2022-04-07 03:21:01 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-80e7e52c-target in namespace namespace-126
2022-04-07 03:21:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-80e7e52c-source in namespace namespace-126
2022-04-07 03:21:13 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-f746d34d is in desired state: Ready
2022-04-07 03:21:13 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:902] Sending and receiving messages via my-cluster-f746d34d-source
2022-04-07 03:21:13 [ForkJoinPool-3-worker-19] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:21:13 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@453ce039, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092, --topic, my-cluster-f746d34d], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk', podNamespace='namespace-127', bootstrapServer='my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092', topicName='my-cluster-f746d34d', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@712c988d}
2022-04-07 03:21:13 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092:my-cluster-f746d34d from pod my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk
2022-04-07 03:21:13 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk -n namespace-127 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092 --topic my-cluster-f746d34d
2022-04-07 03:21:16 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:21:16 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 03:21:16 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6451fe39, messages=[], arguments=[--group-id, my-consumer-group-2021816773, --max-messages, 100, --group-instance-id, instance1667578228, --bootstrap-server, my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092, --topic, my-cluster-f746d34d], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk', podNamespace='namespace-127', bootstrapServer='my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092', topicName='my-cluster-f746d34d', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2021816773', consumerInstanceId='instance1667578228', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@712ce861}
2022-04-07 03:21:16 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092#my-cluster-f746d34d from pod my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk
2022-04-07 03:21:16 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk -n namespace-127 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2021816773 --max-messages 100 --group-instance-id instance1667578228 --bootstrap-server my-cluster-f746d34d-source-kafka-bootstrap.namespace-127.svc:9092 --topic my-cluster-f746d34d
2022-04-07 03:21:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:21:21 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-126 for test case:testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-07 03:21:22 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:21:22 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:21:22 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:917] Changing to my-cluster-f746d34d-target and will try to receive messages
2022-04-07 03:21:22 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@54089743, messages=[], arguments=[--group-id, my-consumer-group-2021816773, --max-messages, 100, --group-instance-id, instance428850170, --bootstrap-server, my-cluster-f746d34d-target-kafka-bootstrap.namespace-127.svc:9092, --topic, my-cluster-f746d34d], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk', podNamespace='namespace-127', bootstrapServer='my-cluster-f746d34d-target-kafka-bootstrap.namespace-127.svc:9092', topicName='my-cluster-f746d34d', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-2021816773', consumerInstanceId='instance428850170', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@46cef7a2}
2022-04-07 03:21:22 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-f746d34d-target-kafka-bootstrap.namespace-127.svc:9092#my-cluster-f746d34d from pod my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk
2022-04-07 03:21:22 [ForkJoinPool-3-worker-19] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-f746d34d-kafka-clients-7bc9b495c4-mwdzk -n namespace-127 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2021816773 --max-messages 100 --group-instance-id instance428850170 --bootstrap-server my-cluster-f746d34d-target-kafka-bootstrap.namespace-127.svc:9092 --topic my-cluster-f746d34d
2022-04-07 03:21:28 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:21:28 [ForkJoinPool-3-worker-19] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:21:28 [ForkJoinPool-3-worker-19] [32mINFO [m [MirrorMaker2IsolatedST:925] Checking if the mirrored topic name is same as the original one
2022-04-07 03:21:31 [ForkJoinPool-3-worker-19] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-127 exec my-cluster-f746d34d-target-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-07 03:21:31 [ForkJoinPool-3-worker-19] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 03:21:33 [ForkJoinPool-3-worker-19] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-127 exec my-cluster-f746d34d-target-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-cluster-f746d34d
2022-04-07 03:21:33 [ForkJoinPool-3-worker-19] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 03:21:33 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:21:33 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:348] Delete all resources for testStrimziIdentityReplicationPolicy
2022-04-07 03:21:33 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-cluster-f746d34d in namespace namespace-127
2022-04-07 03:21:43 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-4a958d07 is in desired state: Ready
2022-04-07 03:21:43 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMaker2IsolatedST:675] -------> Scaling KafkaMirrorMaker2 subresource <-------
2022-04-07 03:21:43 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMaker2IsolatedST:676] Scaling subresource replicas to 4
2022-04-07 03:21:43 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-4a958d07-mirrormaker2 will be ready
2022-04-07 03:21:43 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-4a958d07-mirrormaker2 is ready
2022-04-07 03:21:43 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-4a958d07-mirrormaker2 to be ready
2022-04-07 03:21:43 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-f746d34d in namespace namespace-127
2022-04-07 03:21:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS-FINISHED
2022-04-07 03:21:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:21:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:21:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-STARTED
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-129 for test case:testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-129
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-129
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-129
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7db45797-source in namespace namespace-129
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:21:50 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7db45797-source will have desired state: Ready
2022-04-07 03:21:53 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-f746d34d-kafka-clients in namespace namespace-127
2022-04-07 03:22:33 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f746d34d-target in namespace namespace-127
2022-04-07 03:22:44 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-f746d34d-source in namespace namespace-127
2022-04-07 03:22:54 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:22:54 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-127 for test case:testStrimziIdentityReplicationPolicy
2022-04-07 03:23:02 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-4a958d07-mirrormaker2 is ready
2022-04-07 03:23:02 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMaker2IsolatedST:680] Check if replicas is set to 4, naming prefix should be same and observed generation higher
2022-04-07 03:23:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:23:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleMirrorMaker2Subresource
2022-04-07 03:23:02 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4a958d07-target in namespace namespace-128
2022-04-07 03:23:03 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7db45797-source is in desired state: Ready
2022-04-07 03:23:03 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-7db45797-target in namespace namespace-129
2022-04-07 03:23:03 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:23:03 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-7db45797-target will have desired state: Ready
2022-04-07 03:23:04 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testStrimziIdentityReplicationPolicy-FINISHED
2022-04-07 03:23:04 [ForkJoinPool-3-worker-19] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:23:04 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-4a958d07-source in namespace namespace-128
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-130 for test case:testConfigureDeploymentStrategy
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-130
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-130
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-130
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d64a122a-source in namespace namespace-130
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-130
2022-04-07 03:23:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d64a122a-source will have desired state: Ready
2022-04-07 03:23:12 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-4a958d07 in namespace namespace-128
2022-04-07 03:23:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:23:22 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-128 for test case:testScaleMirrorMaker2Subresource
2022-04-07 03:23:50 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testScaleMirrorMaker2Subresource-FINISHED
2022-04-07 03:23:50 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-131 for test case:testIdentityReplicationPolicy
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-131
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-131
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-131
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0b260d27-source in namespace namespace-131
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-131
2022-04-07 03:23:51 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0b260d27-source will have desired state: Ready
2022-04-07 03:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d64a122a-source is in desired state: Ready
2022-04-07 03:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d64a122a-target in namespace namespace-131
2022-04-07 03:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-130
2022-04-07 03:24:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d64a122a-target will have desired state: Ready
2022-04-07 03:24:23 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-7db45797-target is in desired state: Ready
2022-04-07 03:24:23 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-a-193000320 in namespace namespace-131
2022-04-07 03:24:23 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:24:23 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-b-1753892936 in namespace namespace-131
2022-04-07 03:24:23 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:24:23 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-a-193000320 will have desired state: Ready
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-a-193000320 is in desired state: Ready
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-b-1753892936 will have desired state: Ready
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-b-1753892936 is in desired state: Ready
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7db45797-my-user-source in namespace namespace-131
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-7db45797-my-user-target in namespace namespace-131
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:24:24 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7db45797-my-user-source will have desired state: Ready
2022-04-07 03:24:25 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7db45797-my-user-source is in desired state: Ready
2022-04-07 03:24:25 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-7db45797-my-user-target will have desired state: Ready
2022-04-07 03:24:25 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-7db45797-my-user-target is in desired state: Ready
2022-04-07 03:24:25 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-7db45797-kafka-clients in namespace namespace-129
2022-04-07 03:24:25 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-129
2022-04-07 03:24:25 [ForkJoinPool-3-worker-25] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-7db45797-kafka-clients will be ready
2022-04-07 03:24:27 [ForkJoinPool-3-worker-25] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-7db45797-kafka-clients is ready
2022-04-07 03:24:27 [ForkJoinPool-3-worker-25] [1;31mERROR[m [TestExecutionWatcher:28] MirrorMaker2IsolatedST - Exception Index 0 out of bounds for length 0 has been thrown in @Test. Going to collect logs from components.
2022-04-07 03:24:27 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 03:24:27 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 03:24:27 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 03:24:29 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 03:24:29 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 03:24:29 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 03:24:29 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 03:24:30 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 03:24:30 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:252] Collecting events in Namespace namespace-129
2022-04-07 03:24:30 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace namespace-129
2022-04-07 03:24:30 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-129
2022-04-07 03:24:31 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace namespace-129
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace namespace-129
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace namespace-129
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace namespace-129
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:348] Delete all resources for testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 03:24:32 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7db45797-my-user-source in namespace namespace-129
2022-04-07 03:24:42 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-b-1753892936 in namespace namespace-129
2022-04-07 03:24:52 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-7db45797-kafka-clients in namespace namespace-129
2022-04-07 03:24:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0b260d27-source is in desired state: Ready
2022-04-07 03:24:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-0b260d27-target in namespace namespace-131
2022-04-07 03:24:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-131
2022-04-07 03:24:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-0b260d27-target will have desired state: Ready
2022-04-07 03:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d64a122a-target is in desired state: Ready
2022-04-07 03:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-d64a122a in namespace namespace-131
2022-04-07 03:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-130
2022-04-07 03:25:28 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-d64a122a will have desired state: Ready
2022-04-07 03:25:42 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-7db45797-my-user-target in namespace namespace-129
2022-04-07 03:25:52 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7db45797-target in namespace namespace-129
2022-04-07 03:26:02 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-a-193000320 in namespace namespace-129
2022-04-07 03:26:12 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-7db45797-source in namespace namespace-129
2022-04-07 03:26:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-0b260d27-target is in desired state: Ready
2022-04-07 03:26:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-cluster-0b260d27 in namespace namespace-131
2022-04-07 03:26:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-131
2022-04-07 03:26:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-cluster-0b260d27 will have desired state: Ready
2022-04-07 03:26:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-cluster-0b260d27 is in desired state: Ready
2022-04-07 03:26:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-0b260d27-kafka-clients in namespace namespace-131
2022-04-07 03:26:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-131
2022-04-07 03:26:22 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:26:22 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-129 for test case:testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 03:26:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-0b260d27 in namespace namespace-131
2022-04-07 03:26:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-131
2022-04-07 03:26:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-0b260d27 will have desired state: Ready
2022-04-07 03:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-d64a122a is in desired state: Ready
2022-04-07 03:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:959] Adding label to MirrorMaker2 resource, the CR should be recreated
2022-04-07 03:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d64a122a-mirrormaker2 will be ready
2022-04-07 03:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d64a122a-mirrormaker2 is ready
2022-04-07 03:26:43 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-d64a122a-mirrormaker2 to be ready
2022-04-07 03:26:50 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha-FINISHED
2022-04-07 03:26:50 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-132 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-132
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-132
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-132
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d9d332c2-source in namespace namespace-132
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:26:54 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d9d332c2-source will have desired state: Ready
2022-04-07 03:27:43 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-0b260d27 is in desired state: Ready
2022-04-07 03:27:43 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:833] Sending and receiving messages via my-cluster-0b260d27-source
2022-04-07 03:27:43 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:27:43 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@b4098ba, messages=[], arguments=[--max-messages, 100, --bootstrap-server, my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092, --topic, my-cluster-0b260d27], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt', podNamespace='namespace-131', bootstrapServer='my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092', topicName='my-cluster-0b260d27', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@50a90328}
2022-04-07 03:27:43 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:94] Producing 100 messages to my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092:my-cluster-0b260d27 from pod my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt
2022-04-07 03:27:43 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt -n namespace-131 -- /opt/kafka/producer.sh --max-messages 100 --bootstrap-server my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092 --topic my-cluster-0b260d27
2022-04-07 03:27:46 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:27:46 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:101] Producer produced 100 messages
2022-04-07 03:27:46 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@6dae24ad, messages=[], arguments=[--group-id, my-consumer-group-477670102, --max-messages, 100, --group-instance-id, instance295217780, --bootstrap-server, my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092, --topic, my-cluster-0b260d27], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt', podNamespace='namespace-131', bootstrapServer='my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092', topicName='my-cluster-0b260d27', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-477670102', consumerInstanceId='instance295217780', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@69ff3763}
2022-04-07 03:27:46 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092#my-cluster-0b260d27 from pod my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt
2022-04-07 03:27:46 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt -n namespace-131 -- /opt/kafka/consumer.sh --group-id my-consumer-group-477670102 --max-messages 100 --group-instance-id instance295217780 --bootstrap-server my-cluster-0b260d27-source-kafka-bootstrap.namespace-131.svc:9092 --topic my-cluster-0b260d27
2022-04-07 03:27:52 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:27:52 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:27:52 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:848] Changing to my-cluster-0b260d27-target and will try to receive messages
2022-04-07 03:27:52 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7a0c0931, messages=[], arguments=[--group-id, my-consumer-group-477670102, --max-messages, 100, --group-instance-id, instance866936922, --bootstrap-server, my-cluster-0b260d27-target-kafka-bootstrap.namespace-131.svc:9092, --topic, my-cluster-0b260d27], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt', podNamespace='namespace-131', bootstrapServer='my-cluster-0b260d27-target-kafka-bootstrap.namespace-131.svc:9092', topicName='my-cluster-0b260d27', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-477670102', consumerInstanceId='instance866936922', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7586da73}
2022-04-07 03:27:52 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:157] Consuming 100 messages from my-cluster-0b260d27-target-kafka-bootstrap.namespace-131.svc:9092#my-cluster-0b260d27 from pod my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt
2022-04-07 03:27:52 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-0b260d27-kafka-clients-5c67f9c7bb-zc5lt -n namespace-131 -- /opt/kafka/consumer.sh --group-id my-consumer-group-477670102 --max-messages 100 --group-instance-id instance866936922 --bootstrap-server my-cluster-0b260d27-target-kafka-bootstrap.namespace-131.svc:9092 --topic my-cluster-0b260d27
2022-04-07 03:27:58 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:27:58 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 100 messages
2022-04-07 03:27:58 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMaker2IsolatedST:856] Checking if the mirrored topic name is same as the original one
2022-04-07 03:28:01 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-131 exec my-cluster-0b260d27-target-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-04-07 03:28:01 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 03:28:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d9d332c2-source is in desired state: Ready
2022-04-07 03:28:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-d9d332c2-target in namespace namespace-132
2022-04-07 03:28:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:28:03 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-d9d332c2-target will have desired state: Ready
2022-04-07 03:28:03 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-131 exec my-cluster-0b260d27-target-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-cluster-0b260d27
2022-04-07 03:28:03 [ForkJoinPool-3-worker-21] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 03:28:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:28:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testIdentityReplicationPolicy
2022-04-07 03:28:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-cluster-0b260d27 in namespace namespace-131
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-d64a122a-mirrormaker2 is ready
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:966] Checking that observed gen. is still on 1 (recreation) and new label is present
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:971] Changing deployment strategy to ROLLING_UPDATE
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-d64a122a will have desired state: Ready
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-d64a122a is in desired state: Ready
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:976] Adding another label to MirrorMaker2 resource, pods should be rolled
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-d64a122a-mirrormaker2 will be ready
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-d64a122a-mirrormaker2 is ready
2022-04-07 03:28:05 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-d64a122a-mirrormaker2 to be ready
2022-04-07 03:28:13 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-0b260d27 in namespace namespace-131
2022-04-07 03:28:23 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-0b260d27-kafka-clients in namespace namespace-131
2022-04-07 03:29:03 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0b260d27-target in namespace namespace-131
2022-04-07 03:29:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-d9d332c2-target is in desired state: Ready
2022-04-07 03:29:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-17324008 in namespace namespace-132
2022-04-07 03:29:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:07 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-17324008 will have desired state: Ready
2022-04-07 03:29:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-17324008 is in desired state: Ready
2022-04-07 03:29:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-d9d332c2-my-user-source in namespace namespace-132
2022-04-07 03:29:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-d9d332c2-my-user-source will have desired state: Ready
2022-04-07 03:29:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-d9d332c2-my-user-source is in desired state: Ready
2022-04-07 03:29:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-d9d332c2-my-user-target in namespace namespace-132
2022-04-07 03:29:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:09 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-d9d332c2-my-user-target will have desired state: Ready
2022-04-07 03:29:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-d9d332c2-my-user-target is in desired state: Ready
2022-04-07 03:29:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-d9d332c2-kafka-clients in namespace namespace-132
2022-04-07 03:29:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:14 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-0b260d27-source in namespace namespace-131
2022-04-07 03:29:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-88737283-403981053-test-1 in namespace namespace-132
2022-04-07 03:29:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:21 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-88737283-403981053-test-1 will have desired state: Ready
2022-04-07 03:29:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-88737283-403981053-test-1 is in desired state: Ready
2022-04-07 03:29:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-88737283-403981053-test-2 in namespace namespace-132
2022-04-07 03:29:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:22 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-88737283-403981053-test-2 will have desired state: Ready
2022-04-07 03:29:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-88737283-403981053-test-2 is in desired state: Ready
2022-04-07 03:29:23 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:29:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1330812699-1346052239 in namespace namespace-132
2022-04-07 03:29:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1330812699-1346052239 will have desired state: Ready
2022-04-07 03:29:24 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:29:24 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-131 for test case:testIdentityReplicationPolicy
2022-04-07 03:29:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1330812699-1346052239 is in desired state: Ready
2022-04-07 03:29:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:127] Sending messages to - topic my-topic-88737283-403981053-test-1, cluster my-cluster-d9d332c2-source and message count of 200
2022-04-07 03:29:24 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7445dedf, messages=[], arguments=[USER=my_cluster_d9d332c2_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093, --topic, my-topic-1330812699-1346052239], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093', topicName='my-topic-1330812699-1346052239', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6125e9aa}
2022-04-07 03:29:24 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093:my-topic-1330812699-1346052239 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:29:24 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/producer.sh USER=my_cluster_d9d332c2_my_user_source --max-messages 200 --bootstrap-server my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093 --topic my-topic-1330812699-1346052239
2022-04-07 03:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@656eae1f, messages=[], arguments=[--group-id, my-consumer-group-1966902013, USER=my_cluster_d9d332c2_my_user_source, --max-messages, 200, --group-instance-id, instance1404940845, --bootstrap-server, my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093, --topic, my-topic-1330812699-1346052239], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093', topicName='my-topic-1330812699-1346052239', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-source', consumerGroupName='my-consumer-group-1966902013', consumerInstanceId='instance1404940845', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2b486a65}
2022-04-07 03:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093:my-topic-1330812699-1346052239 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:29:27 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1966902013 USER=my_cluster_d9d332c2_my_user_source --max-messages 200 --group-instance-id instance1404940845 --bootstrap-server my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093 --topic my-topic-1330812699-1346052239
2022-04-07 03:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-d64a122a-mirrormaker2 is ready
2022-04-07 03:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:980] Checking that observed gen. higher (rolling update) and label is changed
2022-04-07 03:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigureDeploymentStrategy
2022-04-07 03:29:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d64a122a-target in namespace namespace-130
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [ClientUtils:133] Sent 200 and received 200
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:328] Setting topic to my-topic-88737283-403981053-test-2, cluster to my-cluster-d9d332c2-target and changing user to my-cluster-d9d332c2-my-user-target
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:337] Sending messages to - topic my-topic-88737283-403981053-test-2, cluster my-cluster-d9d332c2-target and message count of 200
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@225969df, messages=[], arguments=[USER=my_cluster_d9d332c2_my_user_target, --max-messages, 200, --bootstrap-server, my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093, --topic, my-topic-88737283-403981053-test-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093', topicName='my-topic-88737283-403981053-test-2', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3f87274b}
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093:my-topic-88737283-403981053-test-2 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:29:34 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/producer.sh USER=my_cluster_d9d332c2_my_user_target --max-messages 200 --bootstrap-server my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093 --topic my-topic-88737283-403981053-test-2
2022-04-07 03:29:34 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testIdentityReplicationPolicy-FINISHED
2022-04-07 03:29:34 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:29:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d64a122a-source in namespace namespace-130
2022-04-07 03:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5a2465af, messages=[], arguments=[--group-id, my-consumer-group-85033170, USER=my_cluster_d9d332c2_my_user_target, --max-messages, 200, --group-instance-id, instance148025219, --bootstrap-server, my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093, --topic, my-topic-88737283-403981053-test-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093', topicName='my-topic-88737283-403981053-test-2', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-target', consumerGroupName='my-consumer-group-85033170', consumerInstanceId='instance148025219', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4aaa24b3}
2022-04-07 03:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093:my-topic-88737283-403981053-test-2 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:29:38 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/consumer.sh --group-id my-consumer-group-85033170 USER=my_cluster_d9d332c2_my_user_target --max-messages 200 --group-instance-id instance148025219 --bootstrap-server my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093 --topic my-topic-88737283-403981053-test-2
2022-04-07 03:29:43 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-d64a122a in namespace namespace-130
2022-04-07 03:29:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:29:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:29:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-d9d332c2 in namespace namespace-132
2022-04-07 03:29:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-132
2022-04-07 03:29:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-d9d332c2 will have desired state: Ready
2022-04-07 03:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:29:53 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-130 for test case:testConfigureDeploymentStrategy
2022-04-07 03:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testConfigureDeploymentStrategy-FINISHED
2022-04-07 03:30:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:30:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-d9d332c2 is in desired state: Ready
2022-04-07 03:30:52 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:397] Setting topic to mirrormaker2-topic-example-17324008, cluster to my-cluster-d9d332c2-source and changing user to my-cluster-d9d332c2-my-user-source
2022-04-07 03:30:52 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:407] Sending messages to - topic mirrormaker2-topic-example-17324008, cluster my-cluster-d9d332c2-source and message count of 200
2022-04-07 03:30:52 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@570515dd, messages=[], arguments=[USER=my_cluster_d9d332c2_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093, --topic, mirrormaker2-topic-example-17324008], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093', topicName='mirrormaker2-topic-example-17324008', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2ef504b1}
2022-04-07 03:30:52 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093:mirrormaker2-topic-example-17324008 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:30:52 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/producer.sh USER=my_cluster_d9d332c2_my_user_source --max-messages 200 --bootstrap-server my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093 --topic mirrormaker2-topic-example-17324008
2022-04-07 03:30:56 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:30:56 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:30:56 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:411] Receiving messages from - topic mirrormaker2-topic-example-17324008, cluster my-cluster-d9d332c2-source and message count of 200
2022-04-07 03:30:56 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4a88f4de, messages=[], arguments=[--group-id, my-consumer-group-85033170, USER=my_cluster_d9d332c2_my_user_source, --max-messages, 200, --group-instance-id, instance2051784135, --bootstrap-server, my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093, --topic, mirrormaker2-topic-example-17324008], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093', topicName='mirrormaker2-topic-example-17324008', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-source', consumerGroupName='my-consumer-group-85033170', consumerInstanceId='instance2051784135', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@765db2a6}
2022-04-07 03:30:56 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093:mirrormaker2-topic-example-17324008 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:30:56 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/consumer.sh --group-id my-consumer-group-85033170 USER=my_cluster_d9d332c2_my_user_source --max-messages 200 --group-instance-id instance2051784135 --bootstrap-server my-cluster-d9d332c2-source-kafka-bootstrap.namespace-132.svc:9093 --topic mirrormaker2-topic-example-17324008
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:418] Now setting topic to my-cluster-d9d332c2-source.mirrormaker2-topic-example-17324008, cluster to my-cluster-d9d332c2-target and user to my-cluster-d9d332c2-my-user-target - the messages should be mirrored
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:427] Consumer in target cluster and topic should receive 200 messages
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@338aee09, messages=[], arguments=[--group-id, my-consumer-group-85033170, USER=my_cluster_d9d332c2_my_user_target, --max-messages, 200, --group-instance-id, instance196622757, --bootstrap-server, my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093, --topic, my-cluster-d9d332c2-source.mirrormaker2-topic-example-17324008], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96', podNamespace='namespace-132', bootstrapServer='my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093', topicName='my-cluster-d9d332c2-source.mirrormaker2-topic-example-17324008', maxMessages=200, kafkaUsername='my-cluster-d9d332c2-my-user-target', consumerGroupName='my-consumer-group-85033170', consumerInstanceId='instance196622757', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4cdea1b2}
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093:my-cluster-d9d332c2-source.mirrormaker2-topic-example-17324008 from pod my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96
2022-04-07 03:31:03 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-d9d332c2-kafka-clients-7b8b99bfcd-zzc96 -n namespace-132 -- /opt/kafka/consumer.sh --group-id my-consumer-group-85033170 USER=my_cluster_d9d332c2_my_user_target --max-messages 200 --group-instance-id instance196622757 --bootstrap-server my-cluster-d9d332c2-target-kafka-bootstrap.namespace-132.svc:9093 --topic my-cluster-d9d332c2-source.mirrormaker2-topic-example-17324008
2022-04-07 03:31:10 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:31:10 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:31:10 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:432] Messages successfully mirrored
2022-04-07 03:31:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:31:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker2TlsAndTlsClientAuth
2022-04-07 03:31:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-88737283-403981053-test-1 in namespace namespace-132
2022-04-07 03:31:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-17324008 in namespace namespace-132
2022-04-07 03:31:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-d9d332c2-kafka-clients in namespace namespace-132
2022-04-07 03:32:10 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1330812699-1346052239 in namespace namespace-132
2022-04-07 03:32:20 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-d9d332c2 in namespace namespace-132
2022-04-07 03:32:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-88737283-403981053-test-2 in namespace namespace-132
2022-04-07 03:32:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-d9d332c2-my-user-target in namespace namespace-132
2022-04-07 03:32:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-d9d332c2-my-user-source in namespace namespace-132
2022-04-07 03:32:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d9d332c2-target in namespace namespace-132
2022-04-07 03:32:50 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-d9d332c2-source in namespace namespace-132
2022-04-07 03:33:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:33:00 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-132 for test case:testMirrorMaker2TlsAndTlsClientAuth
2022-04-07 03:33:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testMirrorMaker2TlsAndTlsClientAuth-FINISHED
2022-04-07 03:33:05 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-04-07 03:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 13, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 1,991.924 s <<< FAILURE! - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS(ExtensionContext)  Time elapsed: 456.863 s  <<< ERROR!
java.lang.NullPointerException
	at io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils.depSnapshot(DeploymentUtils.java:101)
	at io.strimzi.systemtest.utils.kubeUtils.controllers.DeploymentUtils.depSnapshot(DeploymentUtils.java:96)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS(MirrorMaker2IsolatedST.java:1562)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;31mERROR[m] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha(ExtensionContext)  Time elapsed: 474.395 s  <<< ERROR!
java.lang.IndexOutOfBoundsException: Index 0 out of bounds for length 0
	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
	at java.base/java.util.Objects.checkIndex(Objects.java:372)
	at java.base/java.util.ArrayList.get(ArrayList.java:459)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha(MirrorMaker2IsolatedST.java:1278)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
2022-04-07 03:33:05 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:33:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 03:33:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 03:33:30 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 03:33:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:33:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 03:33:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:33:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:33:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:40 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 03:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 03:33:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:33:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 03:33:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:33:51 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:34:01 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 03:34:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=120000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:06 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:34:07 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 03:34:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 03:34:39 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMaker-STARTED
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-STARTED
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-133 for test case:testMirrorMaker
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-133
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-133
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-133
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-134 for test case:testMirrorMakerTlsAuthenticated
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-134
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-642db8d5-source in namespace namespace-133
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-133
2022-04-07 03:34:49 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-642db8d5-source will have desired state: Ready
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-134
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-134
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6482916d-source in namespace namespace-134
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:34:49 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6482916d-source will have desired state: Ready
2022-04-07 03:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-642db8d5-source is in desired state: Ready
2022-04-07 03:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-642db8d5-target in namespace namespace-134
2022-04-07 03:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-133
2022-04-07 03:36:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-642db8d5-target will have desired state: Ready
2022-04-07 03:36:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6482916d-source is in desired state: Ready
2022-04-07 03:36:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-6482916d-target in namespace namespace-134
2022-04-07 03:36:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:36:11 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-6482916d-target will have desired state: Ready
2022-04-07 03:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-642db8d5-target is in desired state: Ready
2022-04-07 03:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2043299187-274155238-source-1311979463 in namespace namespace-134
2022-04-07 03:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-133
2022-04-07 03:37:14 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2043299187-274155238-source-1311979463 will have desired state: Ready
2022-04-07 03:37:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2043299187-274155238-source-1311979463 is in desired state: Ready
2022-04-07 03:37:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-642db8d5-kafka-clients in namespace namespace-133
2022-04-07 03:37:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-133
2022-04-07 03:37:15 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-642db8d5-kafka-clients will be ready
2022-04-07 03:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-642db8d5-kafka-clients is ready
2022-04-07 03:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@72a402aa, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092, --topic, topic-for-test-broker-1], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092', topicName='topic-for-test-broker-1', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1933cb9a}
2022-04-07 03:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092:topic-for-test-broker-1 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:37:17 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092 --topic topic-for-test-broker-1
2022-04-07 03:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@41b6d558, messages=[], arguments=[--group-id, my-consumer-group-790218660, --max-messages, 200, --group-instance-id, instance1130767275, --bootstrap-server, my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092, --topic, topic-for-test-broker-1], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092', topicName='topic-for-test-broker-1', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-790218660', consumerInstanceId='instance1130767275', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@465ea912}
2022-04-07 03:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092#topic-for-test-broker-1 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:37:19 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/consumer.sh --group-id my-consumer-group-790218660 --max-messages 200 --group-instance-id instance1130767275 --bootstrap-server my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092 --topic topic-for-test-broker-1
2022-04-07 03:37:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-6482916d-target is in desired state: Ready
2022-04-07 03:37:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-2043299187-274155238-source-1578889225 in namespace namespace-134
2022-04-07 03:37:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:24 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-2043299187-274155238-source-1578889225 will have desired state: Ready
2022-04-07 03:37:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-2043299187-274155238-source-1578889225 is in desired state: Ready
2022-04-07 03:37:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-6482916d-my-user-source in namespace namespace-134
2022-04-07 03:37:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:25 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-6482916d-my-user-source will have desired state: Ready
2022-04-07 03:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@33907d32, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092, --topic, topic-for-test-broker-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092', topicName='topic-for-test-broker-2', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@13d52e96}
2022-04-07 03:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092:topic-for-test-broker-2 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:37:25 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092 --topic topic-for-test-broker-2
2022-04-07 03:37:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-6482916d-my-user-source is in desired state: Ready
2022-04-07 03:37:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-6482916d-my-user-target in namespace namespace-134
2022-04-07 03:37:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:26 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-6482916d-my-user-target will have desired state: Ready
2022-04-07 03:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-6482916d-my-user-target is in desired state: Ready
2022-04-07 03:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-6482916d-kafka-clients in namespace namespace-134
2022-04-07 03:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:27 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-6482916d-kafka-clients will be ready
2022-04-07 03:37:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:37:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:37:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@775e4d20, messages=[], arguments=[--group-id, my-consumer-group-691894340, --max-messages, 200, --group-instance-id, instance1565614728, --bootstrap-server, my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092, --topic, topic-for-test-broker-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092', topicName='topic-for-test-broker-2', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-691894340', consumerInstanceId='instance1565614728', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@35546b}
2022-04-07 03:37:28 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092#topic-for-test-broker-2 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:37:28 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/consumer.sh --group-id my-consumer-group-691894340 --max-messages 200 --group-instance-id instance1565614728 --bootstrap-server my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092 --topic topic-for-test-broker-2
2022-04-07 03:37:29 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-6482916d-kafka-clients is ready
2022-04-07 03:37:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1242972876-1332254807-test-1 in namespace namespace-134
2022-04-07 03:37:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:29 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1242972876-1332254807-test-1 will have desired state: Ready
2022-04-07 03:37:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1242972876-1332254807-test-1 is in desired state: Ready
2022-04-07 03:37:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1242972876-1332254807-test-2 in namespace namespace-134
2022-04-07 03:37:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:30 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1242972876-1332254807-test-2 will have desired state: Ready
2022-04-07 03:37:31 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1242972876-1332254807-test-2 is in desired state: Ready
2022-04-07 03:37:31 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:37:31 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3ac2e30a, messages=[], arguments=[USER=my_cluster_6482916d_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-1242972876-1332254807-test-1], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-1242972876-1332254807-test-1', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7d479a62}
2022-04-07 03:37:31 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093:my-topic-1242972876-1332254807-test-1 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:37:31 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/producer.sh USER=my_cluster_6482916d_my_user_source --max-messages 200 --bootstrap-server my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-1242972876-1332254807-test-1
2022-04-07 03:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:37:33 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:37:33 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsScramSha-STARTED
2022-04-07 03:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-642db8d5 in namespace namespace-134
2022-04-07 03:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-133
2022-04-07 03:37:33 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-642db8d5 will have desired state: Ready
2022-04-07 03:37:35 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:37:35 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:37:35 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@717553f3, messages=[], arguments=[--group-id, my-consumer-group-869918072, USER=my_cluster_6482916d_my_user_source, --max-messages, 200, --group-instance-id, instance874255528, --bootstrap-server, my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-1242972876-1332254807-test-1], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-1242972876-1332254807-test-1', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-source', consumerGroupName='my-consumer-group-869918072', consumerInstanceId='instance874255528', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@30205b1f}
2022-04-07 03:37:35 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093:my-topic-1242972876-1332254807-test-1 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:37:35 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/consumer.sh --group-id my-consumer-group-869918072 USER=my_cluster_6482916d_my_user_source --max-messages 200 --group-instance-id instance874255528 --bootstrap-server my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-1242972876-1332254807-test-1
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-135 for test case:testMirrorMakerTlsScramSha
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-135
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-135
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-135
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fa9e9017-source in namespace namespace-135
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:37:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa9e9017-source will have desired state: Ready
2022-04-07 03:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7bc753f8, messages=[], arguments=[USER=my_cluster_6482916d_my_user_target, --max-messages, 200, --bootstrap-server, my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-1242972876-1332254807-test-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-1242972876-1332254807-test-2', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5c023464}
2022-04-07 03:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093:my-topic-1242972876-1332254807-test-2 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:37:42 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/producer.sh USER=my_cluster_6482916d_my_user_target --max-messages 200 --bootstrap-server my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-1242972876-1332254807-test-2
2022-04-07 03:37:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:37:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:37:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@12bc3f35, messages=[], arguments=[--group-id, my-consumer-group-2033264182, USER=my_cluster_6482916d_my_user_target, --max-messages, 200, --group-instance-id, instance578553369, --bootstrap-server, my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-1242972876-1332254807-test-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-1242972876-1332254807-test-2', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-target', consumerGroupName='my-consumer-group-2033264182', consumerInstanceId='instance578553369', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@498d073c}
2022-04-07 03:37:45 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093:my-topic-1242972876-1332254807-test-2 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:37:45 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/consumer.sh --group-id my-consumer-group-2033264182 USER=my_cluster_6482916d_my_user_target --max-messages 200 --group-instance-id instance578553369 --bootstrap-server my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-1242972876-1332254807-test-2
2022-04-07 03:37:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:37:53 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:37:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-6482916d in namespace namespace-135
2022-04-07 03:37:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-134
2022-04-07 03:37:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-6482916d will have desired state: Ready
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-642db8d5 is in desired state: Ready
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:311] Verifying labels on pod type mirror-maker
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:315] Verifying labels for pod: my-cluster-642db8d5-mirror-maker-7988696dc-q8bxs
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:356] Verifying labels for Kafka Connect Services
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:384] Verifying labels for Config maps
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-mirror-maker-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-source-entity-topic-operator-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:407] CM my-cluster-642db8d5-source-entity-topic-operator-config is not related to current test
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-source-entity-user-operator-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:407] CM my-cluster-642db8d5-source-entity-user-operator-config is not related to current test
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-source-kafka-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-source-zookeeper-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:407] CM my-cluster-642db8d5-source-zookeeper-config is not related to current test
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-target-entity-topic-operator-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:407] CM my-cluster-642db8d5-target-entity-topic-operator-config is not related to current test
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-target-entity-user-operator-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:407] CM my-cluster-642db8d5-target-entity-user-operator-config is not related to current test
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-target-kafka-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:388] Verifying labels for CM my-cluster-642db8d5-target-zookeeper-config
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:407] CM my-cluster-642db8d5-target-zookeeper-config is not related to current test
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:414] Verifying labels for Service Accounts
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-642db8d5-source-entity-operator
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-642db8d5-source-kafka
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:427] Verifying labels for service account my-cluster-642db8d5-source-zookeeper
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: kubectl --namespace namespace-133 exec my-cluster-642db8d5-mirror-maker-7988696dc-q8bxs -c my-cluster-642db8d5-mirror-maker -- /bin/bash -c for proc in $(ls -1 /proc/ | grep [0-9]); do if echo "$(ls -lh /proc/$proc/exe 2>/dev/null || true)" | grep -q java; then cat /proc/$proc/cmdline; fi; done
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 03:38:36 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:38:36 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testIncludeList-STARTED
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2edc1f73, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092, --topic, my-topic-2043299187-274155238-source-1311979463], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092', topicName='my-topic-2043299187-274155238-source-1311979463', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3447591e}
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092:my-topic-2043299187-274155238-source-1311979463 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:38:36 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092 --topic my-topic-2043299187-274155238-source-1311979463
2022-04-07 03:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@387c1b58, messages=[], arguments=[--group-id, my-consumer-group-486770023, --max-messages, 200, --group-instance-id, instance1908161812, --bootstrap-server, my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092, --topic, my-topic-2043299187-274155238-source-1311979463], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092', topicName='my-topic-2043299187-274155238-source-1311979463', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-486770023', consumerInstanceId='instance1908161812', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a1bedae}
2022-04-07 03:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092#my-topic-2043299187-274155238-source-1311979463 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:38:38 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/consumer.sh --group-id my-consumer-group-486770023 --max-messages 200 --group-instance-id instance1908161812 --bootstrap-server my-cluster-642db8d5-source-kafka-bootstrap.namespace-133.svc:9092 --topic my-topic-2043299187-274155238-source-1311979463
2022-04-07 03:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@78948cdb, messages=[], arguments=[--group-id, my-consumer-group-1717049682, --max-messages, 200, --group-instance-id, instance1461759906, --bootstrap-server, my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092, --topic, my-topic-2043299187-274155238-source-1311979463], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b', podNamespace='namespace-133', bootstrapServer='my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092', topicName='my-topic-2043299187-274155238-source-1311979463', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-1717049682', consumerInstanceId='instance1461759906', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@367d2ae0}
2022-04-07 03:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092#my-topic-2043299187-274155238-source-1311979463 from pod my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b
2022-04-07 03:38:44 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-642db8d5-kafka-clients-94d8794d7-s6h4b -n namespace-133 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1717049682 --max-messages 200 --group-instance-id instance1461759906 --bootstrap-server my-cluster-642db8d5-target-kafka-bootstrap.namespace-133.svc:9092 --topic my-topic-2043299187-274155238-source-1311979463
2022-04-07 03:38:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa9e9017-source is in desired state: Ready
2022-04-07 03:38:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-fa9e9017-target in namespace namespace-135
2022-04-07 03:38:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:38:47 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-fa9e9017-target will have desired state: Ready
2022-04-07 03:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMaker
2022-04-07 03:38:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2043299187-274155238-source-1311979463 in namespace namespace-133
2022-04-07 03:39:00 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-6482916d is in desired state: Ready
2022-04-07 03:39:00 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@181fcdbf, messages=[], arguments=[USER=my_cluster_6482916d_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-2043299187-274155238-source-1578889225], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-2043299187-274155238-source-1578889225', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1bfffeeb}
2022-04-07 03:39:00 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093:my-topic-2043299187-274155238-source-1578889225 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:39:00 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/producer.sh USER=my_cluster_6482916d_my_user_source --max-messages 200 --bootstrap-server my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-2043299187-274155238-source-1578889225
2022-04-07 03:39:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-642db8d5 in namespace namespace-133
2022-04-07 03:39:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:39:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:39:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7239876f, messages=[], arguments=[--group-id, my-consumer-group-124421437, USER=my_cluster_6482916d_my_user_source, --max-messages, 200, --group-instance-id, instance717933277, --bootstrap-server, my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-2043299187-274155238-source-1578889225], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-2043299187-274155238-source-1578889225', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-source', consumerGroupName='my-consumer-group-124421437', consumerInstanceId='instance717933277', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7cd8de99}
2022-04-07 03:39:04 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093:my-topic-2043299187-274155238-source-1578889225 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:39:04 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/consumer.sh --group-id my-consumer-group-124421437 USER=my_cluster_6482916d_my_user_source --max-messages 200 --group-instance-id instance717933277 --bootstrap-server my-cluster-6482916d-source-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-2043299187-274155238-source-1578889225
2022-04-07 03:39:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-642db8d5-kafka-clients in namespace namespace-133
2022-04-07 03:39:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:39:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:39:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@70c16225, messages=[], arguments=[--group-id, my-consumer-group-236301248, USER=my_cluster_6482916d_my_user_target, --max-messages, 200, --group-instance-id, instance1001209148, --bootstrap-server, my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093, --topic, my-topic-2043299187-274155238-source-1578889225], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn', podNamespace='namespace-134', bootstrapServer='my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093', topicName='my-topic-2043299187-274155238-source-1578889225', maxMessages=200, kafkaUsername='my-cluster-6482916d-my-user-target', consumerGroupName='my-consumer-group-236301248', consumerInstanceId='instance1001209148', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@16552765}
2022-04-07 03:39:11 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093:my-topic-2043299187-274155238-source-1578889225 from pod my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn
2022-04-07 03:39:11 [ForkJoinPool-3-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-6482916d-kafka-clients-5d9ddc89d-ptkzn -n namespace-134 -- /opt/kafka/consumer.sh --group-id my-consumer-group-236301248 USER=my_cluster_6482916d_my_user_target --max-messages 200 --group-instance-id instance1001209148 --bootstrap-server my-cluster-6482916d-target-kafka-bootstrap.namespace-134.svc:9093 --topic my-topic-2043299187-274155238-source-1578889225
2022-04-07 03:39:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:39:18 [ForkJoinPool-3-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:39:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:39:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerTlsAuthenticated
2022-04-07 03:39:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-6482916d-kafka-clients in namespace namespace-134
2022-04-07 03:39:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-642db8d5-target in namespace namespace-133
2022-04-07 03:39:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-6482916d-my-user-target in namespace namespace-134
2022-04-07 03:40:00 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-642db8d5-source in namespace namespace-133
2022-04-07 03:40:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-fa9e9017-target is in desired state: Ready
2022-04-07 03:40:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-406380739-1996585159 in namespace namespace-135
2022-04-07 03:40:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:04 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-406380739-1996585159 will have desired state: Ready
2022-04-07 03:40:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-406380739-1996585159 is in desired state: Ready
2022-04-07 03:40:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-fa9e9017-my-user-source in namespace namespace-135
2022-04-07 03:40:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-fa9e9017-my-user-source will have desired state: Ready
2022-04-07 03:40:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-fa9e9017-my-user-source is in desired state: Ready
2022-04-07 03:40:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-fa9e9017-my-user-target in namespace namespace-135
2022-04-07 03:40:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-fa9e9017-my-user-target will have desired state: Ready
2022-04-07 03:40:07 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-fa9e9017-my-user-target is in desired state: Ready
2022-04-07 03:40:07 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-fa9e9017-kafka-clients in namespace namespace-135
2022-04-07 03:40:07 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:07 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-fa9e9017-kafka-clients will be ready
2022-04-07 03:40:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1242972876-1332254807-test-2 in namespace namespace-134
2022-04-07 03:40:09 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-fa9e9017-kafka-clients is ready
2022-04-07 03:40:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-406380739-1996585159-test-1 in namespace namespace-135
2022-04-07 03:40:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:09 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-406380739-1996585159-test-1 will have desired state: Ready
2022-04-07 03:40:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-406380739-1996585159-test-1 is in desired state: Ready
2022-04-07 03:40:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-406380739-1996585159-test-2 in namespace namespace-135
2022-04-07 03:40:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-406380739-1996585159-test-2 will have desired state: Ready
2022-04-07 03:40:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:40:10 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-133 for test case:testMirrorMaker
2022-04-07 03:40:11 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-406380739-1996585159-test-2 is in desired state: Ready
2022-04-07 03:40:11 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 03:40:11 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@65d8a1fa, messages=[], arguments=[USER=my_cluster_fa9e9017_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159-test-1], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159-test-1', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b402132}
2022-04-07 03:40:11 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159-test-1 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:40:11 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/producer.sh USER=my_cluster_fa9e9017_my_user_source --max-messages 200 --bootstrap-server my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159-test-1
2022-04-07 03:40:15 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:40:15 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:40:15 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4da2d0b6, messages=[], arguments=[--group-id, my-consumer-group-1905153920, USER=my_cluster_fa9e9017_my_user_source, --max-messages, 200, --group-instance-id, instance2075452892, --bootstrap-server, my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159-test-1], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159-test-1', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-source', consumerGroupName='my-consumer-group-1905153920', consumerInstanceId='instance2075452892', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@36bf6a81}
2022-04-07 03:40:15 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159-test-1 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:40:15 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1905153920 USER=my_cluster_fa9e9017_my_user_source --max-messages 200 --group-instance-id instance2075452892 --bootstrap-server my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159-test-1
2022-04-07 03:40:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-2043299187-274155238-source-1578889225 in namespace namespace-134
2022-04-07 03:40:18 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-6482916d in namespace namespace-134
2022-04-07 03:40:23 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:40:23 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:40:23 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@77683fa, messages=[], arguments=[USER=my_cluster_fa9e9017_my_user_target, --max-messages, 200, --bootstrap-server, my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159-test-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159-test-2', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@27813181}
2022-04-07 03:40:23 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159-test-2 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:40:23 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/producer.sh USER=my_cluster_fa9e9017_my_user_target --max-messages 200 --bootstrap-server my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159-test-2
2022-04-07 03:40:26 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-6482916d-my-user-source in namespace namespace-134
2022-04-07 03:40:26 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:40:26 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:40:26 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@112c447b, messages=[], arguments=[--group-id, my-consumer-group-1495194573, USER=my_cluster_fa9e9017_my_user_target, --max-messages, 200, --group-instance-id, instance1983532695, --bootstrap-server, my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159-test-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159-test-2', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-target', consumerGroupName='my-consumer-group-1495194573', consumerInstanceId='instance1983532695', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4ad01af7}
2022-04-07 03:40:26 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159-test-2 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:40:26 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1495194573 USER=my_cluster_fa9e9017_my_user_target --max-messages 200 --group-instance-id instance1983532695 --bootstrap-server my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159-test-2
2022-04-07 03:40:28 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1242972876-1332254807-test-1 in namespace namespace-134
2022-04-07 03:40:34 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:40:34 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:40:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-fa9e9017 in namespace namespace-135
2022-04-07 03:40:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-135
2022-04-07 03:40:34 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-fa9e9017 will have desired state: Ready
2022-04-07 03:40:36 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6482916d-target in namespace namespace-134
2022-04-07 03:40:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMaker-FINISHED
2022-04-07 03:40:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:40:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:40:37 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testCustomAndUpdatedValues-STARTED
2022-04-07 03:40:38 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-6482916d-source in namespace namespace-134
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-136 for test case:testIncludeList
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-136
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-136
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-136
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [MirrorMakerIsolatedST:471] Creating kafka source cluster my-cluster-e9f7cf89-source
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e9f7cf89-source in namespace namespace-136
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-136
2022-04-07 03:40:41 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e9f7cf89-source will have desired state: Ready
2022-04-07 03:40:46 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:40:46 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testConfigureDeploymentStrategy-STARTED
2022-04-07 03:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:40:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-134 for test case:testMirrorMakerTlsAuthenticated
2022-04-07 03:41:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsAuthenticated-FINISHED
2022-04-07 03:41:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:41:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:41:15 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testScaleMirrorMakerToZero-STARTED
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-137 for test case:testConfigureDeploymentStrategy
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-137
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-137
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-137
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-557f5e33-source in namespace namespace-137
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-137
2022-04-07 03:41:16 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-557f5e33-source will have desired state: Ready
2022-04-07 03:41:38 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-fa9e9017 is in desired state: Ready
2022-04-07 03:41:38 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@218c3310, messages=[], arguments=[USER=my_cluster_fa9e9017_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5c70b91}
2022-04-07 03:41:38 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:41:38 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/producer.sh USER=my_cluster_fa9e9017_my_user_source --max-messages 200 --bootstrap-server my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159
2022-04-07 03:41:42 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 03:41:42 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 03:41:42 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@f9f2ca, messages=[], arguments=[--group-id, my-consumer-group-1701885807, USER=my_cluster_fa9e9017_my_user_source, --max-messages, 200, --group-instance-id, instance945687718, --bootstrap-server, my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-source', consumerGroupName='my-consumer-group-1701885807', consumerInstanceId='instance945687718', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@c606427}
2022-04-07 03:41:42 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:41:42 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1701885807 USER=my_cluster_fa9e9017_my_user_source --max-messages 200 --group-instance-id instance945687718 --bootstrap-server my-cluster-fa9e9017-source-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159
2022-04-07 03:41:49 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:41:49 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:41:49 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7d0b5adf, messages=[], arguments=[--group-id, my-consumer-group-1380041877, USER=my_cluster_fa9e9017_my_user_target, --max-messages, 200, --group-instance-id, instance1956619494, --bootstrap-server, my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093, --topic, my-topic-406380739-1996585159], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9', podNamespace='namespace-135', bootstrapServer='my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093', topicName='my-topic-406380739-1996585159', maxMessages=200, kafkaUsername='my-cluster-fa9e9017-my-user-target', consumerGroupName='my-consumer-group-1380041877', consumerInstanceId='instance1956619494', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3ddfeac7}
2022-04-07 03:41:49 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093:my-topic-406380739-1996585159 from pod my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9
2022-04-07 03:41:49 [ForkJoinPool-3-worker-21] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-fa9e9017-kafka-clients-77968c69f9-ccxl9 -n namespace-135 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1380041877 USER=my_cluster_fa9e9017_my_user_target --max-messages 200 --group-instance-id instance1956619494 --bootstrap-server my-cluster-fa9e9017-target-kafka-bootstrap.namespace-135.svc:9093 --topic my-topic-406380739-1996585159
2022-04-07 03:41:55 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e9f7cf89-source is in desired state: Ready
2022-04-07 03:41:55 [ForkJoinPool-3-worker-25] [32mINFO [m [MirrorMakerIsolatedST:473] Creating kafka target cluster my-cluster-e9f7cf89-target
2022-04-07 03:41:55 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-e9f7cf89-target in namespace namespace-137
2022-04-07 03:41:55 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-136
2022-04-07 03:41:55 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-e9f7cf89-target will have desired state: Ready
2022-04-07 03:41:56 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 03:41:56 [ForkJoinPool-3-worker-21] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 03:41:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:41:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testMirrorMakerTlsScramSha
2022-04-07 03:41:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-fa9e9017-kafka-clients in namespace namespace-135
2022-04-07 03:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-557f5e33-source is in desired state: Ready
2022-04-07 03:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-557f5e33-target in namespace namespace-137
2022-04-07 03:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-137
2022-04-07 03:42:23 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-557f5e33-target will have desired state: Ready
2022-04-07 03:42:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-fa9e9017-my-user-target in namespace namespace-135
2022-04-07 03:42:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-406380739-1996585159-test-2 in namespace namespace-135
2022-04-07 03:42:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-fa9e9017 in namespace namespace-135
2022-04-07 03:42:57 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-e9f7cf89-target is in desired state: Ready
2022-04-07 03:42:57 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic included-topic in namespace namespace-137
2022-04-07 03:42:57 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-136
2022-04-07 03:42:57 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: included-topic will have desired state: Ready
2022-04-07 03:42:58 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaTopic: included-topic is in desired state: Ready
2022-04-07 03:42:58 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic not-included-topic in namespace namespace-137
2022-04-07 03:42:58 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-136
2022-04-07 03:42:58 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: not-included-topic will have desired state: Ready
2022-04-07 03:42:59 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaTopic: not-included-topic is in desired state: Ready
2022-04-07 03:42:59 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e9f7cf89-kafka-clients in namespace namespace-136
2022-04-07 03:42:59 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-136
2022-04-07 03:42:59 [ForkJoinPool-3-worker-25] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-e9f7cf89-kafka-clients will be ready
2022-04-07 03:43:01 [ForkJoinPool-3-worker-25] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-e9f7cf89-kafka-clients is ready
2022-04-07 03:43:01 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@66d5cf36, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092, --topic, topic-example-10], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092', topicName='topic-example-10', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6de224d3}
2022-04-07 03:43:01 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092:topic-example-10 from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:43:01 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092 --topic topic-example-10
2022-04-07 03:43:03 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:43:03 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:43:03 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1af22ef0, messages=[], arguments=[--group-id, my-consumer-group-883375864, --max-messages, 200, --group-instance-id, instance1126793828, --bootstrap-server, my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092, --topic, topic-example-10], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092', topicName='topic-example-10', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-883375864', consumerInstanceId='instance1126793828', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66ca5a46}
2022-04-07 03:43:03 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092#topic-example-10 from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:43:03 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/consumer.sh --group-id my-consumer-group-883375864 --max-messages 200 --group-instance-id instance1126793828 --bootstrap-server my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092 --topic topic-example-10
2022-04-07 03:43:06 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-406380739-1996585159-test-1 in namespace namespace-135
2022-04-07 03:43:09 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:43:09 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:43:09 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6da1b68, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092, --topic, topic-example-11], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092', topicName='topic-example-11', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5216eee7}
2022-04-07 03:43:09 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092:topic-example-11 from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:43:09 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 --topic topic-example-11
2022-04-07 03:43:12 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:43:12 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:43:12 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3978449f, messages=[], arguments=[--group-id, my-consumer-group-293223777, --max-messages, 200, --group-instance-id, instance593795404, --bootstrap-server, my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092, --topic, topic-example-11], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092', topicName='topic-example-11', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-293223777', consumerInstanceId='instance593795404', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@32f6699f}
2022-04-07 03:43:12 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092#topic-example-11 from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:43:12 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/consumer.sh --group-id my-consumer-group-293223777 --max-messages 200 --group-instance-id instance593795404 --bootstrap-server my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 --topic topic-example-11
2022-04-07 03:43:16 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-406380739-1996585159 in namespace namespace-135
2022-04-07 03:43:18 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:43:18 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:43:18 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-e9f7cf89 in namespace namespace-136
2022-04-07 03:43:18 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-136
2022-04-07 03:43:18 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-e9f7cf89 will have desired state: Ready
2022-04-07 03:43:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-557f5e33-target is in desired state: Ready
2022-04-07 03:43:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-557f5e33 in namespace namespace-137
2022-04-07 03:43:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-137
2022-04-07 03:43:25 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-557f5e33 will have desired state: Ready
2022-04-07 03:43:26 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-fa9e9017-my-user-source in namespace namespace-135
2022-04-07 03:43:36 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fa9e9017-target in namespace namespace-135
2022-04-07 03:43:46 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-fa9e9017-source in namespace namespace-135
2022-04-07 03:43:56 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:43:56 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-135 for test case:testMirrorMakerTlsScramSha
2022-04-07 03:44:07 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testMirrorMakerTlsScramSha-FINISHED
2022-04-07 03:44:07 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:44:07 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 03:44:07 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testScaleMirrorMakerSubresource-STARTED
2022-04-07 03:44:07 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:44:07 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-138 for test case:testCustomAndUpdatedValues
2022-04-07 03:44:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-138
2022-04-07 03:44:08 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-138
2022-04-07 03:44:08 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-138
2022-04-07 03:44:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c51363ad in namespace namespace-138
2022-04-07 03:44:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-138
2022-04-07 03:44:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c51363ad will have desired state: Ready
2022-04-07 03:44:25 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-e9f7cf89 is in desired state: Ready
2022-04-07 03:44:25 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@aaba53d, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092, --topic, included-topic], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092', topicName='included-topic', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@5c7cf481}
2022-04-07 03:44:25 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092:included-topic from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:44:25 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092 --topic included-topic
2022-04-07 03:44:28 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-557f5e33 is in desired state: Ready
2022-04-07 03:44:28 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMakerIsolatedST:763] Adding label to MirrorMaker resource, the CR should be recreateAndWaitForReadinessd
2022-04-07 03:44:28 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-557f5e33-mirror-maker will be ready
2022-04-07 03:44:28 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-557f5e33-mirror-maker is ready
2022-04-07 03:44:28 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-557f5e33-mirror-maker to be ready
2022-04-07 03:44:28 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:44:28 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:44:28 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@70377e37, messages=[], arguments=[--group-id, my-consumer-group-690034301, --max-messages, 200, --group-instance-id, instance759206190, --bootstrap-server, my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092, --topic, included-topic], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092', topicName='included-topic', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-690034301', consumerInstanceId='instance759206190', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@b909f66}
2022-04-07 03:44:28 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092#included-topic from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:44:28 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/consumer.sh --group-id my-consumer-group-690034301 --max-messages 200 --group-instance-id instance759206190 --bootstrap-server my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092 --topic included-topic
2022-04-07 03:44:34 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:44:34 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:44:34 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1c018256, messages=[], arguments=[--group-id, my-consumer-group-787328456, --max-messages, 200, --group-instance-id, instance1053222861, --bootstrap-server, my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092, --topic, included-topic], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092', topicName='included-topic', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-787328456', consumerInstanceId='instance1053222861', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4b54444c}
2022-04-07 03:44:34 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092#included-topic from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:44:34 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/consumer.sh --group-id my-consumer-group-787328456 --max-messages 200 --group-instance-id instance1053222861 --bootstrap-server my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 --topic included-topic
2022-04-07 03:44:39 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:44:39 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:44:39 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@58685792, messages=[], arguments=[--max-messages, 200, --bootstrap-server, my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092, --topic, not-included-topic], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092', topicName='not-included-topic', maxMessages=200, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@13e5551f}
2022-04-07 03:44:39 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:94] Producing 200 messages to my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092:not-included-topic from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:44:39 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/producer.sh --max-messages 200 --bootstrap-server my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092 --topic not-included-topic
2022-04-07 03:44:42 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 03:44:42 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:101] Producer produced 200 messages
2022-04-07 03:44:42 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@217439ad, messages=[], arguments=[--group-id, my-consumer-group-225642640, --max-messages, 200, --group-instance-id, instance290037948, --bootstrap-server, my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092, --topic, not-included-topic], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092', topicName='not-included-topic', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-225642640', consumerInstanceId='instance290037948', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@31f70ef7}
2022-04-07 03:44:42 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092#not-included-topic from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:44:42 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/consumer.sh --group-id my-consumer-group-225642640 --max-messages 200 --group-instance-id instance290037948 --bootstrap-server my-cluster-e9f7cf89-source-kafka-bootstrap.namespace-136.svc:9092 --topic not-included-topic
2022-04-07 03:44:48 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 03:44:48 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 200 messages
2022-04-07 03:44:48 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2cecd64, messages=[], arguments=[--group-id, my-consumer-group-225642640, --max-messages, 200, --group-instance-id, instance2057791080, --bootstrap-server, my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092, --topic, not-included-topic], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d', podNamespace='namespace-136', bootstrapServer='my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092', topicName='not-included-topic', maxMessages=200, kafkaUsername='null', consumerGroupName='my-consumer-group-225642640', consumerInstanceId='instance2057791080', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1800dd31}
2022-04-07 03:44:48 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:157] Consuming 200 messages from my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092#not-included-topic from pod my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d
2022-04-07 03:44:48 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e9f7cf89-kafka-clients-6d59d7d596-mxk7d -n namespace-136 -- /opt/kafka/consumer.sh --group-id my-consumer-group-225642640 --max-messages 200 --group-instance-id instance2057791080 --bootstrap-server my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 --topic not-included-topic
2022-04-07 03:44:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c51363ad is in desired state: Ready
2022-04-07 03:44:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-c51363ad in namespace namespace-138
2022-04-07 03:44:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-138
2022-04-07 03:44:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-c51363ad will have desired state: Ready
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-c51363ad is in desired state: Ready
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMakerIsolatedST:622] Verify values before update
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-c51363ad-mirror-maker in pod name
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMakerIsolatedST:633] Check if actual env variable KAFKA_MIRRORMAKER_CONFIGURATION_PRODUCER has different value than test.value
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMakerIsolatedST:637] Updating values in MirrorMaker container
2022-04-07 03:45:26 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c51363ad-mirror-maker rolling update
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-557f5e33-mirror-maker is ready
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMakerIsolatedST:770] Checking that observed gen. is still on 1 (recreation) and new label is present
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMakerIsolatedST:775] Changing deployment strategy to ROLLING_UPDATE
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-557f5e33 will have desired state: Ready
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-557f5e33 is in desired state: Ready
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMakerIsolatedST:780] Adding another label to MirrorMaker resource, pods should be rolled
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-557f5e33-mirror-maker will be ready
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-557f5e33-mirror-maker is ready
2022-04-07 03:45:50 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-557f5e33-mirror-maker to be ready
2022-04-07 03:46:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c51363ad-mirror-maker will be ready
2022-04-07 03:46:06 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c51363ad-mirror-maker is ready
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c51363ad-mirror-maker rolling update finished
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [MirrorMakerIsolatedST:654] Verify values after update
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:255] Getting pods by prefix my-cluster-c51363ad-mirror-maker in pod name
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:259] Testing Readiness and Liveness configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:221] Getting pods by prefix in name my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:225] Testing EnvVars configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:190] Getting pods by prefix in name my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:194] Testing configuration for container my-cluster-c51363ad-mirror-maker
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testCustomAndUpdatedValues
2022-04-07 03:46:16 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-c51363ad in namespace namespace-138
2022-04-07 03:46:26 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c51363ad in namespace namespace-138
2022-04-07 03:46:36 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:46:36 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-138 for test case:testCustomAndUpdatedValues
2022-04-07 03:46:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testCustomAndUpdatedValues-FINISHED
2022-04-07 03:46:47 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:199] CLI_KAFKA_VERIFIABLE_CONSUMER RETURN code: 1
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:201] ======STDOUT START=======
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [33mWARN [m [Exec:358] Executor log is too long. Going to strip it and print only first 20000 characters
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:202] /tmp/.properties
Starting Consumer with configuration:

[2022-04-07 03:44:49,759] INFO ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-my-consumer-group-225642640-instance2057791080
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = my-consumer-group-225642640
	group.instance.id = instance2057791080
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 30000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 (ConsumerConfig:376)
[2022-04-07 03:44:49,764] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Initializing the Kafka consumer (KafkaConsumer:695)
[2022-04-07 03:44:49,902] INFO Kafka version: 3.1.0 (AppInfoParser:119)
[2022-04-07 03:44:49,903] INFO Kafka commitId: 37edeed0777bacb3 (AppInfoParser:120)
[2022-04-07 03:44:49,903] INFO Kafka startTimeMs: 1649303089899 (AppInfoParser:121)
[2022-04-07 03:44:49,906] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Kafka consumer initialized (KafkaConsumer:815)
{"timestamp":1649303090058,"name":"startup_complete"}
[2022-04-07 03:44:50,103] INFO [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Subscribed to topic(s): not-included-topic (KafkaConsumer:966)
[2022-04-07 03:44:50,104] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Sending FindCoordinator request to broker my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 (id: -1 rack: null) (ConsumerCoordinator:821)
[2022-04-07 03:44:50,344] DEBUG Resolved host my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc as 10.96.148.227 (ClientUtils:113)
[2022-04-07 03:44:50,345] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Initiating connection to node my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 (id: -1 rack: null) using address my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc/10.96.148.227 (NetworkClient:985)
[2022-04-07 03:44:50,369] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Created socket with SO_RCVBUF = 65536, SO_SNDBUF = 131072, SO_TIMEOUT = 0 to node -1 (Selector:531)
[2022-04-07 03:44:50,370] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Completed connection to node -1. Fetching API versions. (NetworkClient:952)
[2022-04-07 03:44:50,371] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Initiating API versions fetch from node -1. (NetworkClient:966)
[2022-04-07 03:44:50,373] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Sending API_VERSIONS request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=1) and timeout 30000 to node -1: ApiVersionsRequestData(clientSoftwareName='apache-kafka-java', clientSoftwareVersion='3.1.0') (NetworkClient:521)
[2022-04-07 03:44:50,409] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Received API_VERSIONS response from node -1 for request with header RequestHeader(apiKey=API_VERSIONS, apiVersion=3, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=1): ApiVersionsResponseData(errorCode=0, apiKeys=[ApiVersion(apiKey=0, minVersion=0, maxVersion=9), ApiVersion(apiKey=1, minVersion=0, maxVersion=13), ApiVersion(apiKey=2, minVersion=0, maxVersion=7), ApiVersion(apiKey=3, minVersion=0, maxVersion=12), ApiVersion(apiKey=4, minVersion=0, maxVersion=5), ApiVersion(apiKey=5, minVersion=0, maxVersion=3), ApiVersion(apiKey=6, minVersion=0, maxVersion=7), ApiVersion(apiKey=7, minVersion=0, maxVersion=3), ApiVersion(apiKey=8, minVersion=0, maxVersion=8), ApiVersion(apiKey=9, minVersion=0, maxVersion=8), ApiVersion(apiKey=10, minVersion=0, maxVersion=4), ApiVersion(apiKey=11, minVersion=0, maxVersion=7), ApiVersion(apiKey=12, minVersion=0, maxVersion=4), ApiVersion(apiKey=13, minVersion=0, maxVersion=4), ApiVersion(apiKey=14, minVersion=0, maxVersion=5), ApiVersion(apiKey=15, minVersion=0, maxVersion=5), ApiVersion(apiKey=16, minVersion=0, maxVersion=4), ApiVersion(apiKey=17, minVersion=0, maxVersion=1), ApiVersion(apiKey=18, minVersion=0, maxVersion=3), ApiVersion(apiKey=19, minVersion=0, maxVersion=7), ApiVersion(apiKey=20, minVersion=0, maxVersion=6), ApiVersion(apiKey=21, minVersion=0, maxVersion=2), ApiVersion(apiKey=22, minVersion=0, maxVersion=4), ApiVersion(apiKey=23, minVersion=0, maxVersion=4), ApiVersion(apiKey=24, minVersion=0, maxVersion=3), ApiVersion(apiKey=25, minVersion=0, maxVersion=3), ApiVersion(apiKey=26, minVersion=0, maxVersion=3), ApiVersion(apiKey=27, minVersion=0, maxVersion=1), ApiVersion(apiKey=28, minVersion=0, maxVersion=3), ApiVersion(apiKey=29, minVersion=0, maxVersion=2), ApiVersion(apiKey=30, minVersion=0, maxVersion=2), ApiVersion(apiKey=31, minVersion=0, maxVersion=2), ApiVersion(apiKey=32, minVersion=0, maxVersion=4), ApiVersion(apiKey=33, minVersion=0, maxVersion=2), ApiVersion(apiKey=34, minVersion=0, maxVersion=2), ApiVersion(apiKey=35, minVersion=0, maxVersion=2), ApiVersion(apiKey=36, minVersion=0, maxVersion=2), ApiVersion(apiKey=37, minVersion=0, maxVersion=3), ApiVersion(apiKey=38, minVersion=0, maxVersion=2), ApiVersion(apiKey=39, minVersion=0, maxVersion=2), ApiVersion(apiKey=40, minVersion=0, maxVersion=2), ApiVersion(apiKey=41, minVersion=0, maxVersion=2), ApiVersion(apiKey=42, minVersion=0, maxVersion=2), ApiVersion(apiKey=43, minVersion=0, maxVersion=2), ApiVersion(apiKey=44, minVersion=0, maxVersion=1), ApiVersion(apiKey=45, minVersion=0, maxVersion=0), ApiVersion(apiKey=46, minVersion=0, maxVersion=0), ApiVersion(apiKey=47, minVersion=0, maxVersion=0), ApiVersion(apiKey=48, minVersion=0, maxVersion=1), ApiVersion(apiKey=49, minVersion=0, maxVersion=1), ApiVersion(apiKey=50, minVersion=0, maxVersion=0), ApiVersion(apiKey=51, minVersion=0, maxVersion=0), ApiVersion(apiKey=56, minVersion=0, maxVersion=0), ApiVersion(apiKey=57, minVersion=0, maxVersion=0), ApiVersion(apiKey=60, minVersion=0, maxVersion=0), ApiVersion(apiKey=61, minVersion=0, maxVersion=0), ApiVersion(apiKey=65, minVersion=0, maxVersion=0), ApiVersion(apiKey=66, minVersion=0, maxVersion=0), ApiVersion(apiKey=67, minVersion=0, maxVersion=0)], throttleTimeMs=0, supportedFeatures=[], finalizedFeaturesEpoch=0, finalizedFeatures=[]) (NetworkClient:879)
[2022-04-07 03:44:50,476] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Node -1 has finalized features epoch: 0, finalized features: [], supported features: [], API versions: (Produce(0): 0 to 9 [usable: 9], Fetch(1): 0 to 13 [usable: 13], ListOffsets(2): 0 to 7 [usable: 7], Metadata(3): 0 to 12 [usable: 12], LeaderAndIsr(4): 0 to 5 [usable: 5], StopReplica(5): 0 to 3 [usable: 3], UpdateMetadata(6): 0 to 7 [usable: 7], ControlledShutdown(7): 0 to 3 [usable: 3], OffsetCommit(8): 0 to 8 [usable: 8], OffsetFetch(9): 0 to 8 [usable: 8], FindCoordinator(10): 0 to 4 [usable: 4], JoinGroup(11): 0 to 7 [usable: 7], Heartbeat(12): 0 to 4 [usable: 4], LeaveGroup(13): 0 to 4 [usable: 4], SyncGroup(14): 0 to 5 [usable: 5], DescribeGroups(15): 0 to 5 [usable: 5], ListGroups(16): 0 to 4 [usable: 4], SaslHandshake(17): 0 to 1 [usable: 1], ApiVersions(18): 0 to 3 [usable: 3], CreateTopics(19): 0 to 7 [usable: 7], DeleteTopics(20): 0 to 6 [usable: 6], DeleteRecords(21): 0 to 2 [usable: 2], InitProducerId(22): 0 to 4 [usable: 4], OffsetForLeaderEpoch(23): 0 to 4 [usable: 4], AddPartitionsToTxn(24): 0 to 3 [usable: 3], AddOffsetsToTxn(25): 0 to 3 [usable: 3], EndTxn(26): 0 to 3 [usable: 3], WriteTxnMarkers(27): 0 to 1 [usable: 1], TxnOffsetCommit(28): 0 to 3 [usable: 3], DescribeAcls(29): 0 to 2 [usable: 2], CreateAcls(30): 0 to 2 [usable: 2], DeleteAcls(31): 0 to 2 [usable: 2], DescribeConfigs(32): 0 to 4 [usable: 4], AlterConfigs(33): 0 to 2 [usable: 2], AlterReplicaLogDirs(34): 0 to 2 [usable: 2], DescribeLogDirs(35): 0 to 2 [usable: 2], SaslAuthenticate(36): 0 to 2 [usable: 2], CreatePartitions(37): 0 to 3 [usable: 3], CreateDelegationToken(38): 0 to 2 [usable: 2], RenewDelegationToken(39): 0 to 2 [usable: 2], ExpireDelegationToken(40): 0 to 2 [usable: 2], DescribeDelegationToken(41): 0 to 2 [usable: 2], DeleteGroups(42): 0 to 2 [usable: 2], ElectLeaders(43): 0 to 2 [usable: 2], IncrementalAlterConfigs(44): 0 to 1 [usable: 1], AlterPartitionReassignments(45): 0 [usable: 0], ListPartitionReassignments(46): 0 [usable: 0], OffsetDelete(47): 0 [usable: 0], DescribeClientQuotas(48): 0 to 1 [usable: 1], AlterClientQuotas(49): 0 to 1 [usable: 1], DescribeUserScramCredentials(50): 0 [usable: 0], AlterUserScramCredentials(51): 0 [usable: 0], AlterIsr(56): 0 [usable: 0], UpdateFeatures(57): 0 [usable: 0], DescribeCluster(60): 0 [usable: 0], DescribeProducers(61): 0 [usable: 0], DescribeTransactions(65): 0 [usable: 0], ListTransactions(66): 0 [usable: 0], AllocateProducerIds(67): 0 [usable: 0]). (NetworkClient:921)
[2022-04-07 03:44:50,480] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Sending metadata request MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='not-included-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) to node my-cluster-e9f7cf89-target-kafka-bootstrap.namespace-136.svc:9092 (id: -1 rack: null) (NetworkClient:1139)
[2022-04-07 03:44:50,481] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Sending METADATA request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=2) and timeout 30000 to node -1: MetadataRequestData(topics=[MetadataRequestTopic(topicId=AAAAAAAAAAAAAAAAAAAAAA, name='not-included-topic')], allowAutoTopicCreation=true, includeClusterAuthorizedOperations=false, includeTopicAuthorizedOperations=false) (NetworkClient:521)
[2022-04-07 03:44:50,482] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Sending FIND_COORDINATOR request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=0) and timeout 30000 to node -1: FindCoordinatorRequestData(key='', keyType=0, coordinatorKeys=[my-consumer-group-225642640]) (NetworkClient:521)
[2022-04-07 03:44:50,504] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Received METADATA response from node -1 for request with header RequestHeader(apiKey=METADATA, apiVersion=12, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=2): MetadataResponseData(throttleTimeMs=0, brokers=[MetadataResponseBroker(nodeId=0, host='my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc', port=9092, rack=null)], clusterId='cxRgsqKeQA29uS7yuvXzAQ', controllerId=0, topics=[MetadataResponseTopic(errorCode=5, name='not-included-topic', topicId=AAAAAAAAAAAAAAAAAAAAAA, isInternal=false, partitions=[], topicAuthorizedOperations=-2147483648)], clusterAuthorizedOperations=-2147483648) (NetworkClient:879)
[2022-04-07 03:44:50,508] WARN [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Error while fetching metadata with correlation id 2 : {not-included-topic=LEADER_NOT_AVAILABLE} (NetworkClient:1099)
[2022-04-07 03:44:50,508] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Requesting metadata update for topic not-included-topic due to error LEADER_NOT_AVAILABLE (Metadata:363)
[2022-04-07 03:44:50,509] INFO [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Cluster ID: cxRgsqKeQA29uS7yuvXzAQ (Metadata:287)
[2022-04-07 03:44:50,510] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Updated cluster metadata updateVersion 2 to MetadataCache{clusterId='cxRgsqKeQA29uS7yuvXzAQ', nodes={0=my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc:9092 (id: 0 rack: null)}, partitions=[], controller=my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc:9092 (id: 0 rack: null)} (Metadata:291)
[2022-04-07 03:44:50,510] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Received FIND_COORDINATOR response from node -1 for request with header RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=0): FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='my-consumer-group-225642640', nodeId=0, host='my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc', port=9092, errorCode=0, errorMessage='')]) (NetworkClient:879)
[2022-04-07 03:44:50,511] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Received FindCoordinator response ClientResponse(receivedTimeMs=1649303090510, latencyMs=173, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=consumer-my-consumer-group-225642640-instance2057791080, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='my-consumer-group-225642640', nodeId=0, host='my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc', port=9092, errorCode=0, errorMessage='')])) (ConsumerCoordinator:834)
[2022-04-07 03:44:50,512] INFO [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Discovered group coordinator my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc:9092 (id: 2147483647 rack: null) (ConsumerCoordinator:853)
[2022-04-07 03:44:50,517] DEBUG Resolved host my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc as 172.17.0.16 (ClientUtils:113)
[2022-04-07 03:44:50,517] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Initiating connection to node my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc:9092 (id: 2147483647 rack: null) using address my-cluster-e9f7cf89-target-kafka-0.my-cluster-e9f7cf89-target-kafka-brokers.namespace-136.svc/172.17.0.16 (NetworkClient:985)
[2022-04-07 03:44:50,524] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Executing onJoinPrepare with generation -1 and memberId  (ConsumerCoordinator:700)
[2022-04-07 03:44:50,524] DEBUG [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] Heartbeat thread started (ConsumerCoordinator:1367)
[2022-04-07 03:44:50,525] INFO [Consumer instanceId=instance2057791080, clientId=consumer-my-consumer-group-225642640-instance2057791080, groupId=my-consumer-group-225642640] (Re-)joining group (ConsumerCoordinator:535)
[2022-04-07 03:44:50,526] DEBUG [Consumer instanceId=instance2057791080, clientId
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [VerifiableClient:203] ======STDOUT END======
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: false
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 0 messages
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:348] Delete all resources for testIncludeList
2022-04-07 03:46:48 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic not-included-topic in namespace namespace-136
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-139 for test case:testScaleMirrorMakerToZero
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-139
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-139
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-139
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMakerIsolatedST:713] Creating kafka source cluster my-cluster-c510ccad-source
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c510ccad-source in namespace namespace-139
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-139
2022-04-07 03:46:50 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c510ccad-source will have desired state: Ready
2022-04-07 03:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-557f5e33-mirror-maker is ready
2022-04-07 03:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [MirrorMakerIsolatedST:784] Checking that observed gen. higher (rolling update) and label is changed
2022-04-07 03:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:348] Delete all resources for testConfigureDeploymentStrategy
2022-04-07 03:47:22 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-557f5e33-target in namespace namespace-137
2022-04-07 03:47:32 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-557f5e33 in namespace namespace-137
2022-04-07 03:47:42 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-557f5e33-source in namespace namespace-137
2022-04-07 03:47:52 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:47:52 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-137 for test case:testConfigureDeploymentStrategy
2022-04-07 03:47:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c510ccad-source is in desired state: Ready
2022-04-07 03:47:56 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMakerIsolatedST:715] Creating kafka target cluster my-cluster-c510ccad-target
2022-04-07 03:47:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c510ccad-target in namespace namespace-139
2022-04-07 03:47:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-139
2022-04-07 03:47:56 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c510ccad-target will have desired state: Ready
2022-04-07 03:48:02 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e9f7cf89-target in namespace namespace-136
2022-04-07 03:48:12 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic included-topic in namespace namespace-136
2022-04-07 03:48:18 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-e9f7cf89 in namespace namespace-136
2022-04-07 03:48:20 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testConfigureDeploymentStrategy-FINISHED
2022-04-07 03:48:20 [ForkJoinPool-3-worker-15] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:48:20 [ForkJoinPool-3-worker-15] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-e9f7cf89-source in namespace namespace-136
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-140 for test case:testScaleMirrorMakerSubresource
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-140
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-140
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-140
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMakerIsolatedST:674] Creating kafka source cluster my-cluster-1ec9f5b0-source
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1ec9f5b0-source in namespace namespace-140
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-140
2022-04-07 03:48:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1ec9f5b0-source will have desired state: Ready
2022-04-07 03:48:22 [ForkJoinPool-3-worker-19] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e9f7cf89-kafka-clients in namespace namespace-136
2022-04-07 03:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c510ccad-target is in desired state: Ready
2022-04-07 03:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-c510ccad in namespace namespace-140
2022-04-07 03:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-139
2022-04-07 03:49:05 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-c510ccad will have desired state: Ready
2022-04-07 03:49:12 [ForkJoinPool-3-worker-25] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:49:12 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-136 for test case:testIncludeList
2022-04-07 03:49:18 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testIncludeList-FINISHED
2022-04-07 03:49:18 [ForkJoinPool-3-worker-25] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:49:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1ec9f5b0-source is in desired state: Ready
2022-04-07 03:49:25 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMakerIsolatedST:676] Creating kafka target cluster my-cluster-1ec9f5b0-target
2022-04-07 03:49:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1ec9f5b0-target in namespace namespace-140
2022-04-07 03:49:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-140
2022-04-07 03:49:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1ec9f5b0-target will have desired state: Ready
2022-04-07 03:50:17 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-c510ccad is in desired state: Ready
2022-04-07 03:50:17 [ForkJoinPool-3-worker-1] [32mINFO [m [MirrorMakerIsolatedST:725] Scaling MirrorMaker to zero
2022-04-07 03:50:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:50:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleMirrorMakerToZero
2022-04-07 03:50:23 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c510ccad-target in namespace namespace-139
2022-04-07 03:50:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1ec9f5b0-target is in desired state: Ready
2022-04-07 03:50:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker my-cluster-1ec9f5b0 in namespace namespace-140
2022-04-07 03:50:29 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-140
2022-04-07 03:50:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker: my-cluster-1ec9f5b0 will have desired state: Ready
2022-04-07 03:50:33 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-c510ccad in namespace namespace-139
2022-04-07 03:50:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c510ccad-source in namespace namespace-139
2022-04-07 03:50:53 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:50:53 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-139 for test case:testScaleMirrorMakerToZero
2022-04-07 03:51:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testScaleMirrorMakerToZero-FINISHED
2022-04-07 03:51:36 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:51:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker: my-cluster-1ec9f5b0 is in desired state: Ready
2022-04-07 03:51:40 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMakerIsolatedST:685] -------> Scaling KafkaMirrorMaker subresource <-------
2022-04-07 03:51:40 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMakerIsolatedST:686] Scaling subresource replicas to 4
2022-04-07 03:51:40 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-1ec9f5b0-mirror-maker will be ready
2022-04-07 03:51:40 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-1ec9f5b0-mirror-maker is ready
2022-04-07 03:51:40 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:194] Waiting for 4 Pod(s) of Deployment my-cluster-1ec9f5b0-mirror-maker to be ready
2022-04-07 03:52:58 [ForkJoinPool-3-worker-21] [32mINFO [m [DeploymentUtils:197] Deployment my-cluster-1ec9f5b0-mirror-maker is ready
2022-04-07 03:52:58 [ForkJoinPool-3-worker-21] [32mINFO [m [MirrorMakerIsolatedST:690] Check if replicas is set to 4, naming prefix should be same and observed generation higher
2022-04-07 03:52:58 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:52:58 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleMirrorMakerSubresource
2022-04-07 03:52:58 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1ec9f5b0-target in namespace namespace-140
2022-04-07 03:52:58 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1ec9f5b0-source in namespace namespace-140
2022-04-07 03:53:08 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker my-cluster-1ec9f5b0 in namespace namespace-140
2022-04-07 03:53:18 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:53:18 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-140 for test case:testScaleMirrorMakerSubresource
2022-04-07 03:53:45 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST.testScaleMirrorMakerSubresource-FINISHED
2022-04-07 03:53:45 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 03:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context MirrorMakerIsolatedST is everything deleted.
2022-04-07 03:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,239.948 s - in io.strimzi.systemtest.mirrormaker.MirrorMakerIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-04-07 03:53:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:54:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:21 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 03:54:31 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:41 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-metrics-cluster-test
bindingsNamespaces=[infra-namespace, second-metrics-cluster-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-metrics-cluster-test
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 03:54:46 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-metrics-cluster-test
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 03:54:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 03:55:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 03:55:00 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka metrics-cluster-name in namespace infra-namespace
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-04-07 03:55:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: metrics-cluster-name will have desired state: Ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: metrics-cluster-name is in desired state: Ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: second-kafka-cluster will have desired state: Ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: second-kafka-cluster is in desired state: Ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-kafka-clients is ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-metrics-cluster-test-kafka-clients is ready
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-bridge in namespace infra-namespace
2022-04-07 03:58:18 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-bridge will have desired state: Ready
2022-04-07 03:58:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-bridge is in desired state: Ready
2022-04-07 03:58:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-04-07 03:58:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-04-07 03:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: mm2-cluster is in desired state: Ready
2022-04-07 03:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1438331180-1430761917 in namespace infra-namespace
2022-04-07 03:59:57 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1438331180-1430761917 will have desired state: Ready
2022-04-07 03:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1438331180-1430761917 is in desired state: Ready
2022-04-07 03:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-316325671-933600775 in namespace infra-namespace
2022-04-07 03:59:58 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-316325671-933600775 will have desired state: Ready
2022-04-07 03:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-316325671-933600775 is in desired state: Ready
2022-04-07 03:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1280402633-666697540 in namespace infra-namespace
2022-04-07 03:59:59 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1280402633-666697540 will have desired state: Ready
2022-04-07 04:00:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1280402633-666697540 is in desired state: Ready
2022-04-07 04:00:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1870578527-1288256738 in namespace infra-namespace
2022-04-07 04:00:01 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1870578527-1288256738 will have desired state: Ready
2022-04-07 04:00:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1870578527-1288256738 is in desired state: Ready
2022-04-07 04:00:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-492210724-1302819135 in namespace infra-namespace
2022-04-07 04:00:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-492210724-1302819135 will have desired state: Ready
2022-04-07 04:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-492210724-1302819135 is in desired state: Ready
2022-04-07 04:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-04-07 04:00:03 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: metrics-cluster-name is in desired state: Ready
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:72] Apply NetworkPolicy access to cluster-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:90] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to metrics-cluster-name-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to second-kafka-cluster-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to metrics-cluster-name-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to second-kafka-cluster-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-04-07 04:01:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:02:34 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.15 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:36 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.16 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:38 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.17 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:38 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:39 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:40 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:40 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:40 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-STARTED
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-STARTED
2022-04-07 04:02:40 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testZookeeperWatchersCount is everything deleted.
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperWatchersCount-FINISHED
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-STARTED
2022-04-07 04:02:40 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testUserOperatorMetrics is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testUserOperatorMetrics-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testKafkaBrokersCount is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBrokersCount-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testKafkaActiveControllers is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaActiveControllers-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testKafkaTopicUnderReplicatedPartitions is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicUnderReplicatedPartitions-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testZookeeperQuorumSize is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperQuorumSize-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testZookeeperAliveConnections is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testZookeeperAliveConnections-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testKafkaTopicPartitions is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaTopicPartitions-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaConnectIoNetwork is everything deleted.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectIoNetwork-FINISHED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-STARTED
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-producer in namespace infra-namespace
2022-04-07 04:02:41 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: bridge-producer will be in active state
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.23 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testMirrorMaker2Metrics is everything deleted.
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testMirrorMaker2Metrics-FINISHED
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-STARTED
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:02:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:02:42 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job bridge-consumer in namespace infra-namespace
2022-04-07 04:02:42 [ForkJoinPool-3-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: bridge-consumer will be in active state
2022-04-07 04:02:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:02:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:02:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:02:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:02:43 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:422] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-04-07 04:02:44 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:02:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:02:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:02:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:422] Looking for 'strimzi_bridge_kafka_producer_count' in bridge metrics
2022-04-07 04:02:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:02:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:02:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:02:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:430] Looking for 'strimzi_bridge_kafka_consumer_connection_count' in bridge metrics
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.22 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKafkaBridgeMetrics
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job bridge-consumer in namespace infra-namespace
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job bridge-producer in namespace infra-namespace
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaBridgeMetrics-FINISHED
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-STARTED
2022-04-07 04:02:45 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.5 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testClusterOperatorMetrics is everything deleted.
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testClusterOperatorMetrics-FINISHED
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-STARTED
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:02:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:02:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:02:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:452] Verifying that we have more than 0 groups
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testCruiseControlMetrics is everything deleted.
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testCruiseControlMetrics-FINISHED
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-STARTED
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:46 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.19 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: consumer-offsets---84e7a678d08f4bd226872e5cdd4eb527fadc1c6a
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: heartbeats
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: metrics-cluster-name-connect-config
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: metrics-cluster-name-connect-offsets
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: metrics-cluster-name-connect-status
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: mirrormaker2-cluster-configs
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: mirrormaker2-cluster-offsets
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: mirrormaker2-cluster-status
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: my-topic-1280402633-666697540
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: my-topic-1438331180-1430761917
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: my-topic-316325671-933600775
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: second-kafka-cluster.checkpoints.internal
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: strimzi-store-topic---effb8e3e057afce1ecf67c3f5d8e4e3ff177fc55
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: strimzi-topic-operator-kstreams-topic-store-changelog---b75e702040b99be8a9263134de3507fc0cc4017b
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: strimzi.cruisecontrol.metrics
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: strimzi.cruisecontrol.modeltrainingsamples
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:370] KafkaTopic: strimzi.cruisecontrol.partitionmetricsamples
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testTopicOperatorMetrics is everything deleted.
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testTopicOperatorMetrics-FINISHED
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-STARTED
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:02:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:02:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:02:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaConnectRequests is everything deleted.
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectRequests-FINISHED
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-STARTED
2022-04-07 04:02:47 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaConnectResponse is everything deleted.
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaConnectResponse-FINISHED
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-STARTED
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:02:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:02:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:02:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:02:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:610] Metrics collection for pod metrics-cluster-name-kafka-exporter-8454677f49-bjdbf return code - 0
2022-04-07 04:02:48 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment metrics-cluster-name-kafka-exporter rolling update
2022-04-07 04:02:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:02:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:02:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:02:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:02:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:02:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:02:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:02:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:02:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:02:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:02:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:02:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:02:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:02:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:02:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:02:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:02:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:02:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:02:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:02:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:02:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:02:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:02:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:02:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:02:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:02:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:02:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:02:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:02:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:02:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:02:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:02:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:02:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:02:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:02:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:02:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:02:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:02:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:02:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:02:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:02:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:03:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:03:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:03:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:03:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:03:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:03:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:03:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:03:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:03:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:03:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:03:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:03:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:03:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:03:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:03:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:03:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:03:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:03:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:03:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:03:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:03:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:03:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:03:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:03:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:03:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:03:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:03:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:03:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:03:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:03:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:03:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:03:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:03:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:03:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:03:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:03:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:03:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:03:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:03:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:03:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:03:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:03:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:03:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:03:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:03:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:03:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:03:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:03:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:03:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:03:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:03:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:03:22 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:03:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:03:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:03:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:03:23 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:03:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:03:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:03:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:03:24 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:03:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:03:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:03:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:03:25 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:03:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:03:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:03:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:03:26 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:03:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:03:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:03:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:03:27 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:03:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:03:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:03:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:03:28 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:03:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:03:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:03:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:03:29 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:03:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:03:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:03:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:03:30 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 ,second-kafka-cluster-zookeeper-0
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:296] Verify that all pods with prefix: second-kafka-cluster are stable
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:03:31 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 50
2022-04-07 04:03:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:03:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:03:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:03:32 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 49
2022-04-07 04:03:33 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: metrics-cluster-name-kafka-exporter will be ready
2022-04-07 04:03:33 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: metrics-cluster-name-kafka-exporter is ready
2022-04-07 04:03:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:03:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:03:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:03:33 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 48
2022-04-07 04:03:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:03:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:03:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:03:34 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 47
2022-04-07 04:03:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:03:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:03:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:03:35 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 46
2022-04-07 04:03:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:03:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:03:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:03:36 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 45
2022-04-07 04:03:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:03:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:03:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:03:37 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 44
2022-04-07 04:03:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:03:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:03:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:03:38 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 43
2022-04-07 04:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:03:40 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 42
2022-04-07 04:03:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:03:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:03:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:03:41 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 41
2022-04-07 04:03:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:03:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:03:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:03:42 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 40
2022-04-07 04:03:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:03:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:03:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:03:43 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 39
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment metrics-cluster-name-kafka-exporter rolling update finished
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [MetricsIsolatedST:610] Metrics collection for pod metrics-cluster-name-kafka-exporter-7788d69c5d-zk49m return code - 0
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDifferentSetting is everything deleted.
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDifferentSetting-FINISHED
2022-04-07 04:03:43 [ForkJoinPool-3-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:03:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:03:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:03:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:03:44 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 38
2022-04-07 04:03:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:03:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:03:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:03:45 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 37
2022-04-07 04:03:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:03:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:03:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:03:46 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 36
2022-04-07 04:03:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:03:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:03:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:03:47 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 35
2022-04-07 04:03:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:03:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:03:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:03:48 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 34
2022-04-07 04:03:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:03:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:03:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:03:49 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 33
2022-04-07 04:03:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:50 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 32
2022-04-07 04:03:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:51 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 31
2022-04-07 04:03:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:52 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 30
2022-04-07 04:03:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:53 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 29
2022-04-07 04:03:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:54 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 28
2022-04-07 04:03:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:55 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 27
2022-04-07 04:03:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:56 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 26
2022-04-07 04:03:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:57 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 25
2022-04-07 04:03:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:58 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 24
2022-04-07 04:03:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:03:59 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 23
2022-04-07 04:04:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:04:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:04:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:04:00 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 22
2022-04-07 04:04:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:04:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:04:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:04:01 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 21
2022-04-07 04:04:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:04:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:04:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:04:02 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 20
2022-04-07 04:04:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:04:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:04:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:04:03 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 19
2022-04-07 04:04:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:04:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:04:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:04:04 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 18
2022-04-07 04:04:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:04:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:04:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:04:05 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 17
2022-04-07 04:04:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:04:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:04:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:04:06 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 16
2022-04-07 04:04:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:04:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:04:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:04:07 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 15
2022-04-07 04:04:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:04:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:04:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:04:08 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 14
2022-04-07 04:04:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:04:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:04:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:04:09 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 13
2022-04-07 04:04:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:04:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:04:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:04:10 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 12
2022-04-07 04:04:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:04:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:04:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:04:11 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 11
2022-04-07 04:04:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:04:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:04:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:04:12 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 10
2022-04-07 04:04:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:04:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:04:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:04:13 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 9
2022-04-07 04:04:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:04:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:04:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:04:14 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 8
2022-04-07 04:04:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:04:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:04:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:04:15 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 7
2022-04-07 04:04:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:04:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:04:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:04:16 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 6
2022-04-07 04:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:04:17 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 5
2022-04-07 04:04:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:04:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:04:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:04:18 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 4
2022-04-07 04:04:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:04:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:04:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:04:19 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 3
2022-04-07 04:04:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:04:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:04:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:04:20 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 2
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:322] Pod second-kafka-cluster-zookeeper-0 is in the Running state. Remaining seconds pod to be stable 1
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [PodUtils:335] All pods are stable second-kafka-cluster-entity-operator-5f8949dc9c-sxsqs ,second-kafka-cluster-kafka-0 ,second-kafka-cluster-kafka-exporter-6dfb7ccc69-68jk4 ,second-kafka-cluster-zookeeper-0
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:346] In context testKafkaMetricsSettings is everything deleted.
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaMetricsSettings-FINISHED
2022-04-07 04:04:21 [ForkJoinPool-3-worker-21] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:04:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:04:21 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-STARTED
2022-04-07 04:04:21 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:04:21 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: second-metrics-cluster-test
2022-04-07 04:04:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-828055267-519310708 in namespace second-metrics-cluster-test
2022-04-07 04:04:21 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-828055267-519310708 will have desired state: Ready
2022-04-07 04:04:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-828055267-519310708 is in desired state: Ready
2022-04-07 04:04:22 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-5h7bb finished with return code: 0
2022-04-07 04:04:22 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:555] Checking if resource state metric reason message is "none" and KafkaTopic is ready
2022-04-07 04:04:22 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:558] Changing topic name in spec.topicName
2022-04-07 04:04:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-828055267-519310708 will have desired state: NotReady
2022-04-07 04:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-828055267-519310708 is in desired state: NotReady
2022-04-07 04:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-5h7bb finished with return code: 0
2022-04-07 04:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:566] Changing back to it's original name and scaling replicas to be higher number
2022-04-07 04:04:23 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-828055267-519310708
2022-04-07 04:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-5h7bb finished with return code: 0
2022-04-07 04:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:578] Scaling replicas to be higher than before
2022-04-07 04:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaTopicUtils:132] Waiting for KafkaTopic change my-topic-828055267-519310708
2022-04-07 04:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-5h7bb finished with return code: 0
2022-04-07 04:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsIsolatedST:586] Changing KafkaTopic's spec to correct state
2022-04-07 04:04:24 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-828055267-519310708 will have desired state: Ready
2022-04-07 04:04:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-828055267-519310708 is in desired state: Ready
2022-04-07 04:04:25 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod second-metrics-cluster-test-kafka-clients-6c57d6898-5h7bb finished with return code: 0
2022-04-07 04:04:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:04:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:04:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testReconcileStateMetricInTopicOperator
2022-04-07 04:04:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-828055267-519310708 in namespace second-metrics-cluster-test
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testReconcileStateMetricInTopicOperator-FINISHED
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-STARTED
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@6d9edc8c, messages=[], arguments=[--max-messages, 5000, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-316325671-933600775], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='infra-namespace-kafka-clients-748578f786-p92hp', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-316325671-933600775', maxMessages=5000, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7ff55aef}
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 5000 messages to metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092:my-topic-316325671-933600775 from pod infra-namespace-kafka-clients-748578f786-p92hp
2022-04-07 04:04:35 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-p92hp -n infra-namespace -- /opt/kafka/producer.sh --max-messages 5000 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-316325671-933600775
2022-04-07 04:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 04:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 5000 messages
2022-04-07 04:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@567d7a6e, messages=[], arguments=[--group-id, my-consumer-group-1413163735, --max-messages, 5000, --group-instance-id, instance308349469, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-316325671-933600775], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='infra-namespace-kafka-clients-748578f786-p92hp', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-316325671-933600775', maxMessages=5000, kafkaUsername='null', consumerGroupName='my-consumer-group-1413163735', consumerInstanceId='instance308349469', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3e2aa0c2}
2022-04-07 04:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 5000 messages from metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092#my-topic-316325671-933600775 from pod infra-namespace-kafka-clients-748578f786-p92hp
2022-04-07 04:04:38 [ForkJoinPool-3-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-p92hp -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-1413163735 --max-messages 5000 --group-instance-id instance308349469 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-316325671-933600775
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 5000 messages
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.25 from Pod infra-namespace-kafka-clients-748578f786-p92hp finished with return code: 0
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [1;31mERROR[m [TestExecutionWatcher:28] MetricsIsolatedST - Exception 
Expected: a string containing "kafka_consumergroup_current_offset"
     but: was "# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 18
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.17.1"} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 1.904008e+06
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 1.904008e+06
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 1.446551e+06
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 2441
# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
# TYPE go_memstats_gc_cpu_fraction gauge
go_memstats_gc_cpu_fraction 0
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 4.2212e+06
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 1.904008e+06
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 4.399104e+06
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 3.530752e+06
# HELP go_memstats_heap_objects Number of allocated objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 10101
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 4.366336e+06
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 7.929856e+06
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 0
# HELP go_memstats_lookups_total Total number of pointer lookups.
# TYPE go_memstats_lookups_total counter
go_memstats_lookups_total 0
# HELP go_memstats_mallocs_total Total number of mallocs.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 12542
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 9600
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 16384
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 54944
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 65536
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 4.473924e+06
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 1.018209e+06
# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 458752
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 458752
# HELP go_memstats_sys_bytes Number of bytes obtained from system.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 1.5156488e+07
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 6
# HELP kafka_brokers Number of Brokers in the Kafka Cluster.
# TYPE kafka_brokers gauge
kafka_brokers 3
# HELP kafka_exporter_build_info A metric with a constant '1' value labeled by version, revision, branch, and goversion from which kafka_exporter was built.
# TYPE kafka_exporter_build_info gauge
kafka_exporter_build_info{branch="HEAD",goversion="go1.17.1",revision="0d5d4ac4ba63948748cc2c53b35ed95c310cd6f2",version="1.4.2"} 1
# HELP kafka_topic_partition_current_offset Current Offset of a Broker at Topic/Partition
# TYPE kafka_topic_partition_current_offset gauge
kafka_topic_partition_current_offset{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partition_in_sync_replica Number of In-Sync Replicas for this Topic/Partition
# TYPE kafka_topic_partition_in_sync_replica gauge
kafka_topic_partition_in_sync_replica{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="1",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="2",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="4",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="5",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_leader Leader Broker ID of this Topic/Partition
# TYPE kafka_topic_partition_leader gauge
kafka_topic_partition_leader{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_leader{partition="1",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_leader{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_leader{partition="4",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_leader{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_leader_is_preferred 1 if Topic/Partition is using the Preferred Broker
# TYPE kafka_topic_partition_leader_is_preferred gauge
kafka_topic_partition_leader_is_preferred{partition="0",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="1",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="2",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="3",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="4",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="5",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="6",topic="my-topic-1438331180-1430761917"} 1
# HELP kafka_topic_partition_oldest_offset Oldest Offset of a Broker at Topic/Partition
# TYPE kafka_topic_partition_oldest_offset gauge
kafka_topic_partition_oldest_offset{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partition_replicas Number of Replicas for this Topic/Partition
# TYPE kafka_topic_partition_replicas gauge
kafka_topic_partition_replicas{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="1",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="2",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="4",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="5",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_under_replicated_partition 1 if Topic/Partition is under Replicated
# TYPE kafka_topic_partition_under_replicated_partition gauge
kafka_topic_partition_under_replicated_partition{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partitions Number of partitions for this Topic
# TYPE kafka_topic_partitions gauge
kafka_topic_partitions{topic="my-topic-1438331180-1430761917"} 7
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 0.02
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 10
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 1.355776e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.64930417096e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 7.33478912e+08
# HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes.
# TYPE process_virtual_memory_max_bytes gauge
process_virtual_memory_max_bytes 1.8446744073709552e+19
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 0
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0
" has been thrown in @Test. Going to collect logs from components.
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [33mWARN [m [LogCollector:247] Searching for logs in all pods failed! Some of the logs will not be stored. Exception messagenull
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 04:04:44 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDataAfterExchange is everything deleted.
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-FINISHED
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MetricsIsolatedST
2022-04-07 04:04:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-492210724-1302819135 in namespace infra-namespace
2022-04-07 04:04:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-bridge in namespace infra-namespace
2022-04-07 04:04:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-04-07 04:04:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-04-07 04:05:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1280402633-666697540 in namespace infra-namespace
2022-04-07 04:05:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1438331180-1430761917 in namespace infra-namespace
2022-04-07 04:05:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1870578527-1288256738 in namespace infra-namespace
2022-04-07 04:05:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-316325671-933600775 in namespace infra-namespace
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-04-07 04:05:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka metrics-cluster-name in namespace infra-namespace
2022-04-07 04:05:25 [ForkJoinPool-3-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace infra-namespace, for cruise control Kafka cluster metrics-cluster-name
2022-04-07 04:05:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-04-07 04:06:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-04-07 04:06:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 20, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 779.712 s <<< FAILURE! - in io.strimzi.systemtest.metrics.MetricsIsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange(ExtensionContext)  Time elapsed: 9.529 s  <<< FAILURE!
java.lang.AssertionError: 

Expected: a string containing "kafka_consumergroup_current_offset"
     but: was "# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 18
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.17.1"} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 1.904008e+06
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 1.904008e+06
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 1.446551e+06
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 2441
# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
# TYPE go_memstats_gc_cpu_fraction gauge
go_memstats_gc_cpu_fraction 0
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 4.2212e+06
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 1.904008e+06
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 4.399104e+06
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 3.530752e+06
# HELP go_memstats_heap_objects Number of allocated objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 10101
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 4.366336e+06
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 7.929856e+06
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 0
# HELP go_memstats_lookups_total Total number of pointer lookups.
# TYPE go_memstats_lookups_total counter
go_memstats_lookups_total 0
# HELP go_memstats_mallocs_total Total number of mallocs.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 12542
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 9600
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 16384
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 54944
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 65536
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 4.473924e+06
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 1.018209e+06
# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 458752
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 458752
# HELP go_memstats_sys_bytes Number of bytes obtained from system.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 1.5156488e+07
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 6
# HELP kafka_brokers Number of Brokers in the Kafka Cluster.
# TYPE kafka_brokers gauge
kafka_brokers 3
# HELP kafka_exporter_build_info A metric with a constant '1' value labeled by version, revision, branch, and goversion from which kafka_exporter was built.
# TYPE kafka_exporter_build_info gauge
kafka_exporter_build_info{branch="HEAD",goversion="go1.17.1",revision="0d5d4ac4ba63948748cc2c53b35ed95c310cd6f2",version="1.4.2"} 1
# HELP kafka_topic_partition_current_offset Current Offset of a Broker at Topic/Partition
# TYPE kafka_topic_partition_current_offset gauge
kafka_topic_partition_current_offset{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partition_in_sync_replica Number of In-Sync Replicas for this Topic/Partition
# TYPE kafka_topic_partition_in_sync_replica gauge
kafka_topic_partition_in_sync_replica{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="1",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="2",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="4",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="5",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_leader Leader Broker ID of this Topic/Partition
# TYPE kafka_topic_partition_leader gauge
kafka_topic_partition_leader{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_leader{partition="1",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_leader{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_leader{partition="4",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_leader{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_leader_is_preferred 1 if Topic/Partition is using the Preferred Broker
# TYPE kafka_topic_partition_leader_is_preferred gauge
kafka_topic_partition_leader_is_preferred{partition="0",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="1",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="2",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="3",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="4",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="5",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="6",topic="my-topic-1438331180-1430761917"} 1
# HELP kafka_topic_partition_oldest_offset Oldest Offset of a Broker at Topic/Partition
# TYPE kafka_topic_partition_oldest_offset gauge
kafka_topic_partition_oldest_offset{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partition_replicas Number of Replicas for this Topic/Partition
# TYPE kafka_topic_partition_replicas gauge
kafka_topic_partition_replicas{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="1",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="2",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="4",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="5",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_under_replicated_partition 1 if Topic/Partition is under Replicated
# TYPE kafka_topic_partition_under_replicated_partition gauge
kafka_topic_partition_under_replicated_partition{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partitions Number of partitions for this Topic
# TYPE kafka_topic_partitions gauge
kafka_topic_partitions{topic="my-topic-1438331180-1430761917"} 7
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 0.02
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 10
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 1.355776e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.64930417096e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 7.33478912e+08
# HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes.
# TYPE process_virtual_memory_max_bytes gauge
process_virtual_memory_max_bytes 1.8446744073709552e+19
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 0
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0
"
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
	at io.strimzi.systemtest.metrics.MetricsIsolatedST.lambda$testKafkaExporterDataAfterExchange$9(MetricsIsolatedST.java:245)
	at java.base/java.util.HashMap.forEach(HashMap.java:1337)
	at io.strimzi.systemtest.metrics.MetricsIsolatedST.lambda$testKafkaExporterDataAfterExchange$10(MetricsIsolatedST.java:243)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:142)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange(MetricsIsolatedST.java:240)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.JmxIsolatedST
2022-04-07 04:06:45 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:07:10 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:07:10 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:20 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:07:20 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:20 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:07:30 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:07:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-04-07 04:07:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-04-07 04:07:30 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:07:40 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:07:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:07:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:07:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:07:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:07:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:07:50 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:07:51 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:08:12 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 04:08:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 04:08:37 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 04:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 04:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context JmxIsolatedST is everything deleted.
2022-04-07 04:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;33mWARNING[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 1, Time elapsed: 121.579 s - in io.strimzi.systemtest.metrics.JmxIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.dump.LogDumpScriptIsolatedST
2022-04-07 04:08:47 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:12 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:09:12 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:22 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:22 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:09:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:09:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:09:32 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:09:32 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:42 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:09:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:09:52 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:10:02 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@57d5c5af
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:07 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:10:08 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 04:10:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 04:10:44 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 04:10:54 [ForkJoinPool-3-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 04:10:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:10:54 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.dump.LogDumpScriptIsolatedST.dumpPartitions-STARTED
2022-04-07 04:10:54 [ForkJoinPool-3-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:10:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-1842042a in namespace infra-namespace
2022-04-07 04:10:54 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-1842042a will have desired state: Ready
2022-04-07 04:12:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-1842042a is in desired state: Ready
2022-04-07 04:12:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-1842042a-hello-world-producer in namespace infra-namespace
2022-04-07 04:12:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job my-cluster-1842042a-hello-world-consumer in namespace infra-namespace
2022-04-07 04:12:11 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-1842042a-hello-world-producer will be in active state
2022-04-07 04:12:11 [ForkJoinPool-3-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: my-cluster-1842042a-hello-world-consumer will be in active state
2022-04-07 04:12:11 [ForkJoinPool-3-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer my-cluster-1842042a-hello-world-producer and consumer my-cluster-1842042a-hello-world-consumer finish
2022-04-07 04:12:27 [ForkJoinPool-3-worker-3] [32mINFO [m [LogDumpScriptIsolatedST:78] Print partition segments from cluster infra-namespace/my-cluster-1842042a
2022-04-07 04:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/log-dump/run.sh partition --namespace infra-namespace --cluster my-cluster-1842042a --topic my-topic-1107116890-543077788 --partition 0 --dry-run
2022-04-07 04:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 04:12:28 [ForkJoinPool-3-worker-3] [32mINFO [m [LogDumpScriptIsolatedST:87] Dump topic partition from cluster infra-namespace/my-cluster-1842042a
2022-04-07 04:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/log-dump/run.sh partition --namespace infra-namespace --cluster my-cluster-1842042a --topic my-topic-1107116890-543077788 --partition 0 --out-path /home/ec2-user/strimzi-kafka-operator/systemtest/target/my-cluster-1842042a
2022-04-07 04:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 04:12:30 [ForkJoinPool-3-worker-3] [32mINFO [m [LogDumpScriptIsolatedST:99] Dump consumer offsets partition from cluster infra-namespace/my-cluster-1842042a
2022-04-07 04:12:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Command: /home/ec2-user/strimzi-kafka-operator/systemtest/../tools/log-dump/run.sh cg_offsets --namespace infra-namespace --cluster my-cluster-1842042a --group-id my-group --out-path /home/ec2-user/strimzi-kafka-operator/systemtest/target/my-cluster-1842042a
2022-04-07 04:12:35 [ForkJoinPool-3-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 04:12:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:12:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for dumpPartitions
2022-04-07 04:12:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-1842042a-hello-world-producer in namespace infra-namespace
2022-04-07 04:12:35 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-1842042a in namespace infra-namespace
2022-04-07 04:12:35 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job my-cluster-1842042a-hello-world-consumer in namespace infra-namespace
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.dump.LogDumpScriptIsolatedST.dumpPartitions-FINISHED
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:346] In context LogDumpScriptIsolatedST is everything deleted.
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 238.213 s - in io.strimzi.systemtest.dump.LogDumpScriptIsolatedST
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 04:12:45 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:12:45 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:12:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:12:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:12:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:12:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:12:55 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:12:55 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:13:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:13:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:13:05 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:05 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:15 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:13:15 [ForkJoinPool-3-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:13:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:25 [ForkJoinPool-3-worker-21] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:25 [ForkJoinPool-3-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-04-07 04:13:31 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-07 04:13:31 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:346] In context JUnit Jupiter is everything deleted.
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@c813e65
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:13:56 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 04:14:22 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 04:14:22 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha-STARTED
2022-04-07 04:14:32 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:14:32 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS-STARTED
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-141 for test case:testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-141
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-141
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-141
2022-04-07 04:14:32 [ForkJoinPool-4-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:14:32 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-142 for test case:testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-07 04:14:32 [ForkJoinPool-4-worker-1] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-142
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9a077507-source in namespace namespace-141
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:14:32 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a077507-source will have desired state: Ready
2022-04-07 04:14:33 [ForkJoinPool-4-worker-1] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-142
2022-04-07 04:14:33 [ForkJoinPool-4-worker-1] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-142
2022-04-07 04:14:33 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c3100c55-source in namespace namespace-142
2022-04-07 04:14:33 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:14:33 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c3100c55-source will have desired state: Ready
2022-04-07 04:15:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a077507-source is in desired state: Ready
2022-04-07 04:15:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-9a077507-target in namespace namespace-142
2022-04-07 04:15:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:15:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-9a077507-target will have desired state: Ready
2022-04-07 04:15:59 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c3100c55-source is in desired state: Ready
2022-04-07 04:15:59 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-c3100c55-target in namespace namespace-142
2022-04-07 04:15:59 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:15:59 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-c3100c55-target will have desired state: Ready
2022-04-07 04:16:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-9a077507-target is in desired state: Ready
2022-04-07 04:16:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-a-1809186724 in namespace namespace-142
2022-04-07 04:16:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:16:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-b-756184246 in namespace namespace-142
2022-04-07 04:16:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:16:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-a-1809186724 will have desired state: Ready
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-a-1809186724 is in desired state: Ready
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-b-756184246 will have desired state: Ready
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-b-756184246 is in desired state: Ready
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-9a077507-my-user-source in namespace namespace-142
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-9a077507-my-user-target in namespace namespace-142
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:16:58 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-9a077507-my-user-source will have desired state: Ready
2022-04-07 04:16:59 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-9a077507-my-user-source is in desired state: Ready
2022-04-07 04:16:59 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-9a077507-my-user-target will have desired state: Ready
2022-04-07 04:17:00 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-9a077507-my-user-target is in desired state: Ready
2022-04-07 04:17:00 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-9a077507-kafka-clients in namespace namespace-141
2022-04-07 04:17:00 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-141
2022-04-07 04:17:00 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-9a077507-kafka-clients will be ready
2022-04-07 04:17:02 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-9a077507-kafka-clients is ready
2022-04-07 04:17:02 [ForkJoinPool-4-worker-3] [1;31mERROR[m [TestExecutionWatcher:28] MirrorMaker2IsolatedST - Exception Index 0 out of bounds for length 0 has been thrown in @Test. Going to collect logs from components.
2022-04-07 04:17:02 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace infra-namespace
2022-04-07 04:17:02 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-04-07 04:17:02 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-04-07 04:17:03 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-04-07 04:17:03 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-04-07 04:17:03 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-04-07 04:17:03 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-04-07 04:17:03 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 04:17:03 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:252] Collecting events in Namespace namespace-141
2022-04-07 04:17:04 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:259] Collecting ConfigMaps in Namespace namespace-141
2022-04-07 04:17:04 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-141
2022-04-07 04:17:05 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:266] Collecting Deployments in Namespace namespace-141
2022-04-07 04:17:05 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:271] Collecting StatefulSets in Namespace namespace-141
2022-04-07 04:17:06 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:276] Collecting ReplicaSets in Namespace namespace-141
2022-04-07 04:17:06 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:281] Collecting Strimzi in Namespace namespace-141
2022-04-07 04:17:06 [ForkJoinPool-4-worker-3] [32mINFO [m [LogCollector:287] Collecting cluster status
2022-04-07 04:17:06 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:17:06 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 04:17:06 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-9a077507-my-user-source in namespace namespace-141
2022-04-07 04:17:12 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-c3100c55-target is in desired state: Ready
2022-04-07 04:17:12 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-a-1242848184 in namespace namespace-142
2022-04-07 04:17:12 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:12 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-a-1242848184 will have desired state: Ready
2022-04-07 04:17:13 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-a-1242848184 is in desired state: Ready
2022-04-07 04:17:13 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-b-991599297 in namespace namespace-142
2022-04-07 04:17:13 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:13 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-b-991599297 will have desired state: Ready
2022-04-07 04:17:14 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-b-991599297 is in desired state: Ready
2022-04-07 04:17:14 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-c3100c55-my-user-source in namespace namespace-142
2022-04-07 04:17:14 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:14 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-c3100c55-my-user-source will have desired state: Ready
2022-04-07 04:17:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-c3100c55-my-user-source is in desired state: Ready
2022-04-07 04:17:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-c3100c55-my-user-target in namespace namespace-142
2022-04-07 04:17:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-c3100c55-my-user-target will have desired state: Ready
2022-04-07 04:17:16 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-b-756184246 in namespace namespace-141
2022-04-07 04:17:16 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-c3100c55-my-user-target is in desired state: Ready
2022-04-07 04:17:16 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-e283759e-kafka-clients in namespace namespace-142
2022-04-07 04:17:16 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-9a077507-kafka-clients in namespace namespace-141
2022-04-07 04:17:26 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-875675068-225842393-test-1 in namespace namespace-142
2022-04-07 04:17:26 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:26 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-875675068-225842393-test-1 will have desired state: Ready
2022-04-07 04:17:27 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-875675068-225842393-test-1 is in desired state: Ready
2022-04-07 04:17:27 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-875675068-225842393-test-2 in namespace namespace-142
2022-04-07 04:17:27 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:27 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-875675068-225842393-test-2 will have desired state: Ready
2022-04-07 04:17:28 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-875675068-225842393-test-2 is in desired state: Ready
2022-04-07 04:17:28 [ForkJoinPool-4-worker-1] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 04:17:28 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@3190ed71, messages=[], arguments=[USER=my_cluster_c3100c55_my_user_target, --max-messages, 200, --bootstrap-server, my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093, --topic, my-topic-875675068-225842393-test-2], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093', topicName='my-topic-875675068-225842393-test-2', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-target', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@743ce471}
2022-04-07 04:17:28 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093:my-topic-875675068-225842393-test-2 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:17:28 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/producer.sh USER=my_cluster_c3100c55_my_user_target --max-messages 200 --bootstrap-server my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093 --topic my-topic-875675068-225842393-test-2
2022-04-07 04:17:32 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 04:17:32 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 04:17:32 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2b4db09e, messages=[], arguments=[--group-id, my-consumer-group-773692570, USER=my_cluster_c3100c55_my_user_target, --max-messages, 200, --group-instance-id, instance1005470379, --bootstrap-server, my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093, --topic, my-topic-875675068-225842393-test-2], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093', topicName='my-topic-875675068-225842393-test-2', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-target', consumerGroupName='my-consumer-group-773692570', consumerInstanceId='instance1005470379', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@30a16b89}
2022-04-07 04:17:32 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093:my-topic-875675068-225842393-test-2 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:17:32 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/consumer.sh --group-id my-consumer-group-773692570 USER=my_cluster_c3100c55_my_user_target --max-messages 200 --group-instance-id instance1005470379 --bootstrap-server my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093 --topic my-topic-875675068-225842393-test-2
2022-04-07 04:17:39 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 04:17:39 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 04:17:39 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-c3100c55 in namespace namespace-142
2022-04-07 04:17:39 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-142
2022-04-07 04:17:39 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-c3100c55 will have desired state: Ready
2022-04-07 04:18:06 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-9a077507-my-user-target in namespace namespace-141
2022-04-07 04:18:16 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9a077507-target in namespace namespace-141
2022-04-07 04:18:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-a-1809186724 in namespace namespace-141
2022-04-07 04:18:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-9a077507-source in namespace namespace-141
2022-04-07 04:18:44 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-c3100c55 is in desired state: Ready
2022-04-07 04:18:44 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@e0c5ff6, messages=[], arguments=[USER=my_cluster_c3100c55_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093, --topic, mirrormaker2-topic-example-a-1242848184], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093', topicName='mirrormaker2-topic-example-a-1242848184', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@3841c6ea}
2022-04-07 04:18:44 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093:mirrormaker2-topic-example-a-1242848184 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:18:44 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/producer.sh USER=my_cluster_c3100c55_my_user_source --max-messages 200 --bootstrap-server my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093 --topic mirrormaker2-topic-example-a-1242848184
2022-04-07 04:18:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:18:46 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-141 for test case:testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 04:18:47 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 04:18:47 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 04:18:47 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@4505f1ee, messages=[], arguments=[--group-id, my-consumer-group-773692570, USER=my_cluster_c3100c55_my_user_source, --max-messages, 200, --group-instance-id, instance1855680393, --bootstrap-server, my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093, --topic, mirrormaker2-topic-example-a-1242848184], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093', topicName='mirrormaker2-topic-example-a-1242848184', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-source', consumerGroupName='my-consumer-group-773692570', consumerInstanceId='instance1855680393', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@73450ffa}
2022-04-07 04:18:47 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093:mirrormaker2-topic-example-a-1242848184 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:18:47 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/consumer.sh --group-id my-consumer-group-773692570 USER=my_cluster_c3100c55_my_user_source --max-messages 200 --group-instance-id instance1855680393 --bootstrap-server my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093 --topic mirrormaker2-topic-example-a-1242848184
2022-04-07 04:18:55 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 04:18:55 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 04:18:55 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1551] Consumer in target cluster and topic should receive 200 messages
2022-04-07 04:18:55 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@48b5c691, messages=[], arguments=[--group-id, my-consumer-group-773692570, USER=my_cluster_c3100c55_my_user_target, --max-messages, 200, --group-instance-id, instance1118155656, --bootstrap-server, my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093, --topic, my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093', topicName='my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-target', consumerGroupName='my-consumer-group-773692570', consumerInstanceId='instance1118155656', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@291af557}
2022-04-07 04:18:55 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093:my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:18:55 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/consumer.sh --group-id my-consumer-group-773692570 USER=my_cluster_c3100c55_my_user_target --max-messages 200 --group-instance-id instance1118155656 --bootstrap-server my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093 --topic my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184
2022-04-07 04:18:57 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha-FINISHED
2022-04-07 04:18:57 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:19:02 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 04:19:02 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 04:19:02 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1553] Messages successfully mirrored
2022-04-07 04:19:02 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1567] Renew Clients CA secret for Source cluster via annotation
2022-04-07 04:19:02 [ForkJoinPool-4-worker-1] [32mINFO [m [SecretUtils:178] Annotating Secret:my-cluster-c3100c55-source-clients-ca-cert with annotation strimzi.io/force-renew=true
2022-04-07 04:19:02 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c3100c55-source-kafka rolling update
2022-04-07 04:21:57 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-c3100c55-source-kafka has been successfully rolled
2022-04-07 04:21:57 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-c3100c55-source-kafka to be ready
2022-04-07 04:22:24 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c3100c55-mirrormaker2 rolling update
2022-04-07 04:22:24 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c3100c55-mirrormaker2 will be ready
2022-04-07 04:22:24 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c3100c55-mirrormaker2 is ready
2022-04-07 04:22:34 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c3100c55-mirrormaker2 rolling update finished
2022-04-07 04:22:34 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1573] Renew Clients CA secret for Target cluster via annotation
2022-04-07 04:22:34 [ForkJoinPool-4-worker-1] [32mINFO [m [SecretUtils:178] Annotating Secret:my-cluster-c3100c55-target-clients-ca-cert with annotation strimzi.io/force-renew=true
2022-04-07 04:22:34 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c3100c55-target-kafka rolling update
2022-04-07 04:25:20 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-c3100c55-target-kafka has been successfully rolled
2022-04-07 04:25:20 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-c3100c55-target-kafka to be ready
2022-04-07 04:25:47 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c3100c55-mirrormaker2 rolling update
2022-04-07 04:26:27 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c3100c55-mirrormaker2 will be ready
2022-04-07 04:26:27 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c3100c55-mirrormaker2 is ready
2022-04-07 04:26:37 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c3100c55-mirrormaker2 rolling update finished
2022-04-07 04:26:37 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1579] Send and receive messages after clients certs were removed
2022-04-07 04:26:37 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4afe2407, messages=[], arguments=[USER=my_cluster_c3100c55_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093, --topic, mirrormaker2-topic-example-a-1242848184], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093', topicName='mirrormaker2-topic-example-a-1242848184', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@4968f34a}
2022-04-07 04:26:37 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093:mirrormaker2-topic-example-a-1242848184 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:26:37 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/producer.sh USER=my_cluster_c3100c55_my_user_source --max-messages 200 --bootstrap-server my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093 --topic mirrormaker2-topic-example-a-1242848184
2022-04-07 04:26:41 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 04:26:41 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 04:26:41 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1595] Consumer in target cluster and topic should receive 200 messages
2022-04-07 04:26:41 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@76c17afb, messages=[], arguments=[--group-id, my-consumer-group-1844505129, USER=my_cluster_c3100c55_my_user_target, --max-messages, 200, --group-instance-id, instance1120385611, --bootstrap-server, my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093, --topic, my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093', topicName='my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-target', consumerGroupName='my-consumer-group-1844505129', consumerInstanceId='instance1120385611', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@78bd2405}
2022-04-07 04:26:41 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093:my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:26:41 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1844505129 USER=my_cluster_c3100c55_my_user_target --max-messages 200 --group-instance-id instance1120385611 --bootstrap-server my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093 --topic my-cluster-c3100c55-source.mirrormaker2-topic-example-a-1242848184
2022-04-07 04:26:48 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 04:26:48 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 04:26:48 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1597] Messages successfully mirrored
2022-04-07 04:26:48 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1599] Renew Cluster CA secret for Source clusters via annotation
2022-04-07 04:26:48 [ForkJoinPool-4-worker-1] [32mINFO [m [SecretUtils:178] Annotating Secret:my-cluster-c3100c55-source-cluster-ca-cert with annotation strimzi.io/force-renew=true
2022-04-07 04:26:48 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c3100c55-source-zookeeper rolling update
2022-04-07 04:28:13 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-c3100c55-source-zookeeper has been successfully rolled
2022-04-07 04:28:13 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-c3100c55-source-zookeeper to be ready
2022-04-07 04:28:42 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c3100c55-source-kafka rolling update
2022-04-07 04:29:37 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-c3100c55-source-kafka has been successfully rolled
2022-04-07 04:29:37 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-c3100c55-source-kafka to be ready
2022-04-07 04:30:01 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c3100c55-source-entity-operator rolling update
2022-04-07 04:30:01 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c3100c55-source-entity-operator will be ready
2022-04-07 04:35:43 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c3100c55-source-entity-operator is ready
2022-04-07 04:35:54 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c3100c55-source-entity-operator rolling update finished
2022-04-07 04:35:54 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c3100c55-mirrormaker2 rolling update
2022-04-07 04:35:54 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c3100c55-mirrormaker2 will be ready
2022-04-07 04:35:54 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c3100c55-mirrormaker2 is ready
2022-04-07 04:36:04 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c3100c55-mirrormaker2 rolling update finished
2022-04-07 04:36:04 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1607] Renew Cluster CA secret for Target clusters via annotation
2022-04-07 04:36:04 [ForkJoinPool-4-worker-1] [32mINFO [m [SecretUtils:178] Annotating Secret:my-cluster-c3100c55-target-cluster-ca-cert with annotation strimzi.io/force-renew=true
2022-04-07 04:36:04 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c3100c55-target-zookeeper rolling update
2022-04-07 04:37:54 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-c3100c55-target-zookeeper has been successfully rolled
2022-04-07 04:37:54 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-c3100c55-target-zookeeper to be ready
2022-04-07 04:38:26 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c3100c55-target-kafka rolling update
2022-04-07 04:40:26 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:86] Component with name: my-cluster-c3100c55-target-kafka has been successfully rolled
2022-04-07 04:40:26 [ForkJoinPool-4-worker-1] [32mINFO [m [RollingUpdateUtils:101] Waiting for 3 Pod(s) of my-cluster-c3100c55-target-kafka to be ready
2022-04-07 04:40:58 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c3100c55-target-entity-operator rolling update
2022-04-07 04:41:03 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c3100c55-target-entity-operator will be ready
2022-04-07 04:41:36 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c3100c55-target-entity-operator is ready
2022-04-07 04:41:46 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c3100c55-target-entity-operator rolling update finished
2022-04-07 04:41:46 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-c3100c55-mirrormaker2 rolling update
2022-04-07 04:41:46 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-c3100c55-mirrormaker2 will be ready
2022-04-07 04:41:46 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-c3100c55-mirrormaker2 is ready
2022-04-07 04:41:56 [ForkJoinPool-4-worker-1] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-c3100c55-mirrormaker2 rolling update finished
2022-04-07 04:41:56 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1615] Send and receive messages after clients certs were removed
2022-04-07 04:41:56 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@774b1d66, messages=[], arguments=[USER=my_cluster_c3100c55_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093, --topic, mirrormaker2-topic-example-b-991599297], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093', topicName='mirrormaker2-topic-example-b-991599297', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@ead044a}
2022-04-07 04:41:56 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093:mirrormaker2-topic-example-b-991599297 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:41:56 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/producer.sh USER=my_cluster_c3100c55_my_user_source --max-messages 200 --bootstrap-server my-cluster-c3100c55-source-kafka-bootstrap.namespace-142.svc:9093 --topic mirrormaker2-topic-example-b-991599297
2022-04-07 04:41:59 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 04:41:59 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 04:41:59 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1631] Consumer in target cluster and topic should receive 200 messages
2022-04-07 04:41:59 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7217ecc5, messages=[], arguments=[--group-id, my-consumer-group-240040782, USER=my_cluster_c3100c55_my_user_target, --max-messages, 200, --group-instance-id, instance1348835854, --bootstrap-server, my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093, --topic, my-cluster-c3100c55-source.mirrormaker2-topic-example-b-991599297], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-e283759e-kafka-clients-cc56745cf-b686g', podNamespace='namespace-142', bootstrapServer='my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093', topicName='my-cluster-c3100c55-source.mirrormaker2-topic-example-b-991599297', maxMessages=200, kafkaUsername='my-cluster-c3100c55-my-user-target', consumerGroupName='my-consumer-group-240040782', consumerInstanceId='instance1348835854', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@14fe3544}
2022-04-07 04:41:59 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093:my-cluster-c3100c55-source.mirrormaker2-topic-example-b-991599297 from pod my-cluster-e283759e-kafka-clients-cc56745cf-b686g
2022-04-07 04:41:59 [ForkJoinPool-4-worker-1] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-e283759e-kafka-clients-cc56745cf-b686g -n namespace-142 -- /opt/kafka/consumer.sh --group-id my-consumer-group-240040782 USER=my_cluster_c3100c55_my_user_target --max-messages 200 --group-instance-id instance1348835854 --bootstrap-server my-cluster-c3100c55-target-kafka-bootstrap.namespace-142.svc:9093 --topic my-cluster-c3100c55-source.mirrormaker2-topic-example-b-991599297
2022-04-07 04:42:06 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 04:42:06 [ForkJoinPool-4-worker-1] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 04:42:06 [ForkJoinPool-4-worker-1] [32mINFO [m [MirrorMaker2IsolatedST:1633] Messages successfully mirrored
2022-04-07 04:42:06 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:42:06 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-07 04:42:06 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-e283759e-kafka-clients in namespace namespace-142
2022-04-07 04:42:06 [ForkJoinPool-4-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-a-1242848184 in namespace namespace-142
2022-04-07 04:42:46 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-c3100c55-my-user-target in namespace namespace-142
2022-04-07 04:42:57 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-875675068-225842393-test-2 in namespace namespace-142
2022-04-07 04:42:57 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-c3100c55 in namespace namespace-142
2022-04-07 04:42:57 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-875675068-225842393-test-1 in namespace namespace-142
2022-04-07 04:43:07 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c3100c55-target in namespace namespace-142
2022-04-07 04:43:17 [ForkJoinPool-4-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-c3100c55-my-user-source in namespace namespace-142
2022-04-07 04:43:17 [ForkJoinPool-4-worker-5] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-b-991599297 in namespace namespace-142
2022-04-07 04:43:17 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-c3100c55-source in namespace namespace-142
2022-04-07 04:43:27 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:43:27 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-142 for test case:testKMM2RollAfterSecretsCertsUpdateTLS
2022-04-07 04:44:11 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS-FINISHED
2022-04-07 04:44:11 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:44:11 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:44:11 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-04-07 04:44:11 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;31mERROR[m] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1,840.456 s <<< FAILURE! - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
[[1;31mERROR[m] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha(ExtensionContext)  Time elapsed: 264.966 s  <<< ERROR!
java.lang.IndexOutOfBoundsException: Index 0 out of bounds for length 0
	at java.base/jdk.internal.util.Preconditions.outOfBounds(Preconditions.java:64)
	at java.base/jdk.internal.util.Preconditions.outOfBoundsCheckIndex(Preconditions.java:70)
	at java.base/jdk.internal.util.Preconditions.checkIndex(Preconditions.java:248)
	at java.base/java.util.Objects.checkIndex(Objects.java:372)
	at java.base/java.util.ArrayList.get(ArrayList.java:459)
	at io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha(MirrorMaker2IsolatedST.java:1278)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:396)
	at java.base/java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:721)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.joinConcurrentTasksInReverseOrderToEnableWorkStealing(ForkJoinPoolHierarchicalTestExecutorService.java:162)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:136)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.executeNonConcurrentTasks(ForkJoinPoolHierarchicalTestExecutorService.java:155)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:135)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

[[1;34mINFO[m] Running io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
2022-04-07 04:44:11 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:44:36 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:44:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:44:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:44:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:44:46 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:44:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:44:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:44:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:44:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:44:56 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:45:06 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:45:06 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:06 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:45:07 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:45:07 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:45:07 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:45:17 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@c813e65
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:45:22 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 04:45:43 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 04:45:43 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 04:45:54 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 04:45:54 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 04:45:54 [ForkJoinPool-4-worker-3] [32mINFO [m [OauthAbstractST:125] Deploying keycloak...
2022-04-07 04:45:54 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakUtils:35] Prepare Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/prepare_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakUtils:48] Keycloak in namespace infra-namespace is ready
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:46] Waiting for Secret credential-example-keycloak
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:50] Secret credential-example-keycloak created
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-producer-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret hello-world-consumer-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-a-client-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret team-b-client-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-broker-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-connect-oauth
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-oauth
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-mirror-maker-2-oauth
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret my-bridge-oauth
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-audience-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [SecretUtils:70] Creating secret kafka-client-secret
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakInstance:50] Replacing validIssuerUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal to pointing to kafka-authz realm
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakInstance:51] Replacing jwksEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/certs to pointing to kafka-authz realm
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakInstance:52] Replacing oauthTokenEndpointUri: https://keycloak.infra-namespace.svc.cluster.local:8443/auth/realms/internal/protocol/openid-connect/token to pointing to kafka-authz realm
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakInstance:55] Using HTTPS endpoints
2022-04-07 04:47:42 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka oauth-cluster-authz-name in namespace infra-namespace
2022-04-07 04:47:43 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: oauth-cluster-authz-name will have desired state: Ready
2022-04-07 04:49:01 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] Kafka: oauth-cluster-authz-name is in desired state: Ready
2022-04-07 04:49:01 [ForkJoinPool-4-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:680] Setting producer and consumer properties
2022-04-07 04:49:01 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-a-client in namespace infra-namespace
2022-04-07 04:49:01 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-a-client will have desired state: Ready
2022-04-07 04:49:02 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: team-a-client is in desired state: Ready
2022-04-07 04:49:02 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser team-b-client in namespace infra-namespace
2022-04-07 04:49:02 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: team-b-client will have desired state: Ready
2022-04-07 04:49:03 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: team-b-client is in desired state: Ready
2022-04-07 04:49:03 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:49:03 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:49:03 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic-STARTED
2022-04-07 04:49:03 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic-STARTED
2022-04-07 04:49:03 [ForkJoinPool-4-worker-1] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:49:03 [ForkJoinPool-4-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:49:03 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1838927781-1330861301 in namespace infra-namespace
2022-04-07 04:49:03 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-627157728-2130367563 in namespace infra-namespace
2022-04-07 04:49:03 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1838927781-1330861301 will have desired state: Ready
2022-04-07 04:49:03 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-627157728-2130367563 will have desired state: Ready
2022-04-07 04:49:04 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1838927781-1330861301 is in desired state: Ready
2022-04-07 04:49:04 [ForkJoinPool-4-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:146] Sending 100 messages to broker with topic name my-topic-1838927781-1330861301
2022-04-07 04:49:04 [ForkJoinPool-4-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:147] Producer will not produce messages because authorization topic will failed. Team A can write only to topic starting with 'x-'
2022-04-07 04:49:04 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:49:04 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-627157728-2130367563 is in desired state: Ready
2022-04-07 04:49:04 [ForkJoinPool-4-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:259] Sending 100 messages to broker with topic name my-topic-2043299187-274155238
2022-04-07 04:49:04 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-25e0d628 in namespace infra-namespace
2022-04-07 04:49:04 [ForkJoinPool-4-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5e9c8976 will be in active state
2022-04-07 04:49:04 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-25e0d628 will be in active state
2022-04-07 04:49:04 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:70] Waiting for job: team-b-client-producer-my-cluster-25e0d628 will be in error state
2022-04-07 04:49:05 [ForkJoinPool-4-worker-1] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-producer-my-cluster-5e9c8976 will be in error state
2022-04-07 04:49:25 [ForkJoinPool-4-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:265] Sending 100 messages to broker with topic name b-topic
2022-04-07 04:49:25 [ForkJoinPool-4-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:154] Sending 100 messages to broker with topic name x-topic-my-cluster-5e9c8976
2022-04-07 04:49:25 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-producer-my-cluster-25e0d628 in namespace infra-namespace
2022-04-07 04:49:25 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:49:25 [ForkJoinPool-4-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5e9c8976 will be in active state
2022-04-07 04:49:25 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-producer-my-cluster-25e0d628 will be in active state
2022-04-07 04:49:26 [ForkJoinPool-4-worker-1] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-producer-my-cluster-5e9c8976 will be in error state
2022-04-07 04:49:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-b-client-consumer-my-cluster-25e0d628 in namespace infra-namespace
2022-04-07 04:49:26 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-b-client-consumer-my-cluster-25e0d628 will be in active state
2022-04-07 04:49:27 [ForkJoinPool-4-worker-3] [32mINFO [m [ClientUtils:61] Waiting till producer team-b-client-producer-my-cluster-25e0d628 and consumer team-b-client-consumer-my-cluster-25e0d628 finish
2022-04-07 04:49:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:49:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamBWriteToTopic
2022-04-07 04:49:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-25e0d628 in namespace infra-namespace
2022-04-07 04:49:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-consumer-my-cluster-25e0d628 in namespace infra-namespace
2022-04-07 04:49:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-b-client-producer-my-cluster-25e0d628 in namespace infra-namespace
2022-04-07 04:49:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-627157728-2130367563 in namespace infra-namespace
2022-04-07 04:49:45 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic x-topic-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:49:45 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: x-topic-my-cluster-5e9c8976 will have desired state: Ready
2022-04-07 04:49:46 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:444] KafkaTopic: x-topic-my-cluster-5e9c8976 is in desired state: Ready
2022-04-07 04:49:46 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:49:46 [ForkJoinPool-4-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5e9c8976 will be in active state
2022-04-07 04:49:47 [ForkJoinPool-4-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-5e9c8976 to finished
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic-FINISHED
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic-STARTED
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic a-topic-my-topic-41197230-1513411252 in namespace infra-namespace
2022-04-07 04:49:54 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: a-topic-my-topic-41197230-1513411252 will have desired state: Ready
2022-04-07 04:49:55 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: a-topic-my-topic-41197230-1513411252 is in desired state: Ready
2022-04-07 04:49:55 [ForkJoinPool-4-worker-3] [32mINFO [m [OauthAuthorizationIsolatedST:208] Sending 100 messages to broker with topic name a-topic-my-topic-41197230-1513411252
2022-04-07 04:49:55 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-0f07d10d in namespace infra-namespace
2022-04-07 04:49:55 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-0f07d10d will be in active state
2022-04-07 04:49:56 [ForkJoinPool-4-worker-1] [32mINFO [m [OauthAuthorizationIsolatedST:172] Sending 100 messages to broker with topic name a-topic-my-cluster-5e9c8976
2022-04-07 04:49:56 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:49:56 [ForkJoinPool-4-worker-1] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-5e9c8976 will be in active state
2022-04-07 04:49:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-0f07d10d to finished
2022-04-07 04:49:57 [ForkJoinPool-4-worker-1] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-5e9c8976 to finished
2022-04-07 04:50:04 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-consumer-my-cluster-0f07d10d in namespace infra-namespace
2022-04-07 04:50:04 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-consumer-my-cluster-0f07d10d will be in active state
2022-04-07 04:50:05 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:50:05 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAWriteToTopic
2022-04-07 04:50:05 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic x-topic-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:50:05 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:70] Waiting for job: team-a-client-consumer-my-cluster-0f07d10d will be in error state
2022-04-07 04:50:08 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Job team-a-client-producer-my-cluster-0f07d10d in namespace infra-namespace
2022-04-07 04:50:08 [ForkJoinPool-4-worker-3] [32mINFO [m [JobUtils:81] Waiting for job: team-a-client-producer-my-cluster-0f07d10d will be in active state
2022-04-07 04:50:09 [ForkJoinPool-4-worker-3] [32mINFO [m [ClientUtils:76] Waiting for producer/consumer:team-a-client-producer-my-cluster-0f07d10d to finished
2022-04-07 04:50:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:50:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:50:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:50:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-5e9c8976 in namespace infra-namespace
2022-04-07 04:50:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1838927781-1330861301 in namespace infra-namespace
2022-04-07 04:50:17 [ForkJoinPool-4-worker-3] [32mINFO [m [OauthAbstractST:153] Deleting team-a-client-consumer-my-cluster-0f07d10d job
2022-04-07 04:50:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:50:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testTeamAReadFromTopic
2022-04-07 04:50:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-consumer-my-cluster-0f07d10d in namespace infra-namespace
2022-04-07 04:50:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-0f07d10d in namespace infra-namespace
2022-04-07 04:50:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Job team-a-client-producer-my-cluster-0f07d10d in namespace infra-namespace
2022-04-07 04:50:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic a-topic-my-topic-41197230-1513411252 in namespace infra-namespace
2022-04-07 04:50:25 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:50:25 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic-FINISHED
2022-04-07 04:50:25 [ForkJoinPool-4-worker-1] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:50:32 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:50:32 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic-FINISHED
2022-04-07 04:50:32 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:50:32 [ForkJoinPool-4-worker-3] [32mINFO [m [KeycloakUtils:52] Teardown Keycloak Operator in namespace: infra-namespace with watching namespace: infra-namespace
2022-04-07 04:50:36 [ForkJoinPool-4-worker-3] [32mINFO [m [Exec:417] Command: /bin/bash ../systemtest/src/test/resources/oauth2/teardown_keycloak_operator.sh infra-namespace 11.0.1 infra-namespace
2022-04-07 04:50:36 [ForkJoinPool-4-worker-3] [32mINFO [m [Exec:417] Return code: 0
2022-04-07 04:50:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:50:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for OauthAuthorizationIsolatedST
2022-04-07 04:50:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-a-client in namespace infra-namespace
2022-04-07 04:50:36 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka oauth-cluster-authz-name in namespace infra-namespace
2022-04-07 04:50:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser team-b-client in namespace infra-namespace
2022-04-07 04:50:46 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy global-network-policy in namespace infra-namespace
2022-04-07 04:50:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:50:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:50:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:346] In context OauthAuthorizationIsolatedST is everything deleted.
2022-04-07 04:50:56 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 404.666 s - in io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.metrics.MetricsIsolatedST
2022-04-07 04:50:56 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:21 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:51:21 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:31 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:31 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:31 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:51:41 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:51:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:51:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 04:51:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:51:51 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:51:51 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:51:51 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:51:51 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:52:01 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:01 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:01 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@c813e65
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=infra-namespace,second-metrics-cluster-test
bindingsNamespaces=[infra-namespace, second-metrics-cluster-test]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: second-metrics-cluster-test
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: second-metrics-cluster-test
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 04:52:07 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 04:52:26 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 04:52:26 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka metrics-cluster-name in namespace infra-namespace
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-04-07 04:52:36 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: metrics-cluster-name will have desired state: Ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] Kafka: metrics-cluster-name is in desired state: Ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: second-kafka-cluster will have desired state: Ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] Kafka: second-kafka-cluster is in desired state: Ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: infra-namespace-kafka-clients will be ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: infra-namespace-kafka-clients is ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: second-metrics-cluster-test-kafka-clients will be ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: second-metrics-cluster-test-kafka-clients is ready
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaBridge my-bridge in namespace infra-namespace
2022-04-07 04:55:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaBridge: my-bridge will have desired state: Ready
2022-04-07 04:55:31 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaBridge: my-bridge is in desired state: Ready
2022-04-07 04:55:31 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-04-07 04:55:31 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: mm2-cluster will have desired state: Ready
2022-04-07 04:56:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: mm2-cluster is in desired state: Ready
2022-04-07 04:56:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-874230280-1101756945 in namespace infra-namespace
2022-04-07 04:56:41 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-874230280-1101756945 will have desired state: Ready
2022-04-07 04:56:42 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-874230280-1101756945 is in desired state: Ready
2022-04-07 04:56:42 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1490384347-1922106700 in namespace infra-namespace
2022-04-07 04:56:42 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1490384347-1922106700 will have desired state: Ready
2022-04-07 04:56:43 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1490384347-1922106700 is in desired state: Ready
2022-04-07 04:56:43 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic my-topic-1839374234-1356345584 in namespace infra-namespace
2022-04-07 04:56:43 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: my-topic-1839374234-1356345584 will have desired state: Ready
2022-04-07 04:56:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: my-topic-1839374234-1356345584 is in desired state: Ready
2022-04-07 04:56:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1638516100-652942292 in namespace infra-namespace
2022-04-07 04:56:44 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1638516100-652942292 will have desired state: Ready
2022-04-07 04:56:45 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1638516100-652942292 is in desired state: Ready
2022-04-07 04:56:45 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-user-1813228461-969788386 in namespace infra-namespace
2022-04-07 04:56:45 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-user-1813228461-969788386 will have desired state: Ready
2022-04-07 04:56:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-user-1813228461-969788386 is in desired state: Ready
2022-04-07 04:56:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-04-07 04:56:46 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: metrics-cluster-name will have desired state: Ready
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: metrics-cluster-name is in desired state: Ready
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:72] Apply NetworkPolicy access to cluster-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:90] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to metrics-cluster-name-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:104] Apply NetworkPolicy access to second-kafka-cluster-entity-operator from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:128] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to metrics-cluster-name-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:141] Apply NetworkPolicy access to second-kafka-cluster-kafka-exporter from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={})
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-04-07 04:57:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:161] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=kafka-clients}, additionalProperties={}) successfully created
2022-04-07 04:59:19 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.16 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:21 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.15 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:23 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.14 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:23 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.11 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:24 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.10 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:24 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.12 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:25 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.24 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-STARTED
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1d6e941f, messages=[], arguments=[--max-messages, 5000, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-1490384347-1922106700], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='infra-namespace-kafka-clients-748578f786-q4v9j', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1490384347-1922106700', maxMessages=5000, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7ee5dc91}
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:94] Producing 5000 messages to metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092:my-topic-1490384347-1922106700 from pod infra-namespace-kafka-clients-748578f786-q4v9j
2022-04-07 04:59:26 [ForkJoinPool-4-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-q4v9j -n infra-namespace -- /opt/kafka/producer.sh --max-messages 5000 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-1490384347-1922106700
2022-04-07 04:59:28 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:97] Producer finished correctly: true
2022-04-07 04:59:28 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:101] Producer produced 5000 messages
2022-04-07 04:59:28 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@2cd016ab, messages=[], arguments=[--group-id, my-consumer-group-682971175, --max-messages, 5000, --group-instance-id, instance603554050, --bootstrap-server, metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092, --topic, my-topic-1490384347-1922106700], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='infra-namespace-kafka-clients-748578f786-q4v9j', podNamespace='infra-namespace', bootstrapServer='metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092', topicName='my-topic-1490384347-1922106700', maxMessages=5000, kafkaUsername='null', consumerGroupName='my-consumer-group-682971175', consumerInstanceId='instance603554050', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@71f8166}
2022-04-07 04:59:28 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:157] Consuming 5000 messages from metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092#my-topic-1490384347-1922106700 from pod infra-namespace-kafka-clients-748578f786-q4v9j
2022-04-07 04:59:28 [ForkJoinPool-4-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec infra-namespace-kafka-clients-748578f786-q4v9j -n infra-namespace -- /opt/kafka/consumer.sh --group-id my-consumer-group-682971175 --max-messages 5000 --group-instance-id instance603554050 --bootstrap-server metrics-cluster-name-kafka-bootstrap.infra-namespace.svc:9092 --topic my-topic-1490384347-1922106700
2022-04-07 04:59:34 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:160] Consumer finished correctly: true
2022-04-07 04:59:34 [ForkJoinPool-4-worker-3] [32mINFO [m [InternalKafkaClient:163] Consumer consumed 5000 messages
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [MetricsCollector:239] Metrics collection for PodIp 172.17.0.21 from Pod infra-namespace-kafka-clients-748578f786-q4v9j finished with return code: 0
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:346] In context testKafkaExporterDataAfterExchange is everything deleted.
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange-FINISHED
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for MetricsIsolatedST
2022-04-07 04:59:35 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1813228461-969788386 in namespace infra-namespace
2022-04-07 04:59:35 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaBridge my-bridge in namespace infra-namespace
2022-04-07 04:59:45 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect metrics-cluster-name in namespace infra-namespace
2022-04-07 04:59:45 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 mm2-cluster in namespace infra-namespace
2022-04-07 04:59:55 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1839374234-1356345584 in namespace infra-namespace
2022-04-07 04:59:55 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-874230280-1101756945 in namespace infra-namespace
2022-04-07 05:00:05 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic my-topic-1490384347-1922106700 in namespace infra-namespace
2022-04-07 05:00:05 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-user-1638516100-652942292 in namespace infra-namespace
2022-04-07 05:00:15 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment infra-namespace-kafka-clients in namespace infra-namespace
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-entity-operator-allow in namespace second-metrics-cluster-test
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-kafka-exporter-allow in namespace infra-namespace
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy second-kafka-cluster-kafka-exporter-allow in namespace second-metrics-cluster-test
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy cluster-operator-allow in namespace infra-namespace
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of NetworkPolicy metrics-cluster-name-entity-operator-allow in namespace infra-namespace
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka metrics-cluster-name in namespace infra-namespace
2022-04-07 05:00:15 [ForkJoinPool-4-worker-3] [32mINFO [m [KafkaResource:64] Explicit deletion of KafkaTopics in namespace infra-namespace, for cruise control Kafka cluster metrics-cluster-name
2022-04-07 05:00:25 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Kafka second-kafka-cluster in namespace second-metrics-cluster-test
2022-04-07 05:01:05 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment second-metrics-cluster-test-kafka-clients in namespace second-metrics-cluster-test
2022-04-07 05:01:45 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 649.109 s - in io.strimzi.systemtest.metrics.MetricsIsolatedST
[[1;34mINFO[m] Running io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-07 05:01:45 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 05:02:10 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:02:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:20 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 05:02:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:02:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 05:02:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace second-metrics-cluster-test
2022-04-07 05:02:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace second-metrics-cluster-test
2022-04-07 05:02:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:02:30 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:30 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:40 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 05:02:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 05:02:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 05:02:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 05:02:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 05:02:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:50 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:02:50 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:02:50 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 05:02:50 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 05:02:50 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 05:02:50 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:00 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 05:03:21 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@c813e65
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 05:03:21 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 05:03:21 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:03:22 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 05:03:47 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 05:03:47 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero-STARTED
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-143 for test case:testScaleConnectWithoutConnectorToZero
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-143
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-143
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-143
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-629d8166 in namespace namespace-143
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-143
2022-04-07 05:03:57 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-629d8166 will have desired state: Ready
2022-04-07 05:05:18 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-629d8166 is in desired state: Ready
2022-04-07 05:05:18 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaConnect my-cluster-629d8166 in namespace namespace-143
2022-04-07 05:05:18 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-143
2022-04-07 05:05:18 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-629d8166 will have desired state: Ready
2022-04-07 05:06:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-629d8166 is in desired state: Ready
2022-04-07 05:06:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ConnectIsolatedST:891] Scaling KafkaConnect down to zero
2022-04-07 05:06:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaConnect: my-cluster-629d8166 will have desired state: Ready
2022-04-07 05:06:26 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:444] KafkaConnect: my-cluster-629d8166 is in desired state: Ready
2022-04-07 05:06:33 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:06:33 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testScaleConnectWithoutConnectorToZero
2022-04-07 05:06:33 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaConnect my-cluster-629d8166 in namespace namespace-143
2022-04-07 05:06:33 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-629d8166 in namespace namespace-143
2022-04-07 05:06:43 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 05:06:43 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-143 for test case:testScaleConnectWithoutConnectorToZero
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero-FINISHED
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:346] In context ConnectIsolatedST is everything deleted.
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 325.084 s - in io.strimzi.systemtest.connect.ConnectIsolatedST
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:10 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 05:07:10 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:20 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:20 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:30 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:31 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:07:31 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:07:31 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:07:31 [ForkJoinPool-4-worker-7] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 05:07:40 [ForkJoinPool-4-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:29] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:30] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:31]                         Test run started
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:32] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:33] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:48] Following testclasses are selected for run:
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:51] -> io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:52] =======================================================================
2022-04-07 05:07:51 [main] [32mINFO [m [TestExecutionListener:53] =======================================================================
[[1;34mINFO[m] Running io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-07 05:07:51 [ForkJoinPool-5-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:346] In context JUnit Jupiter is everything deleted.
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4254eabb
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=30000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/ec2-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:08:16 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-04-07 05:08:38 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-04-07 05:08:38 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [TestSeparator:23] ############################################################################
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [TestSeparator:24] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha-STARTED
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [AbstractST:597] Not first test we are gonna generate cluster name
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [TestSuiteNamespaceManager:163] Creating namespace:namespace-144 for test case:testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [KubeClusterResource:156] Creating Namespace: namespace-144
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [KubeClusterResource:82] Client use Namespace: namespace-144
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-144
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-010d9f38-source in namespace namespace-144
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:08:48 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-010d9f38-source will have desired state: Ready
2022-04-07 05:10:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-010d9f38-source is in desired state: Ready
2022-04-07 05:10:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Kafka my-cluster-010d9f38-target in namespace namespace-144
2022-04-07 05:10:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:10:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for Kafka: my-cluster-010d9f38-target will have desired state: Ready
2022-04-07 05:11:06 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] Kafka: my-cluster-010d9f38-target is in desired state: Ready
2022-04-07 05:11:06 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-a-1103661764 in namespace namespace-144
2022-04-07 05:11:06 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:11:06 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaTopic mirrormaker2-topic-example-b-1674639238 in namespace namespace-144
2022-04-07 05:11:06 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:11:06 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-a-1103661764 will have desired state: Ready
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-a-1103661764 is in desired state: Ready
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaTopic: mirrormaker2-topic-example-b-1674639238 will have desired state: Ready
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] KafkaTopic: mirrormaker2-topic-example-b-1674639238 is in desired state: Ready
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-010d9f38-my-user-source in namespace namespace-144
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaUser my-cluster-010d9f38-my-user-target in namespace namespace-144
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:11:07 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-010d9f38-my-user-source will have desired state: Ready
2022-04-07 05:11:08 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-010d9f38-my-user-source is in desired state: Ready
2022-04-07 05:11:08 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaUser: my-cluster-010d9f38-my-user-target will have desired state: Ready
2022-04-07 05:11:08 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] KafkaUser: my-cluster-010d9f38-my-user-target is in desired state: Ready
2022-04-07 05:11:08 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-010d9f38-kafka-clients in namespace namespace-144
2022-04-07 05:11:08 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:11:08 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-010d9f38-kafka-clients will be ready
2022-04-07 05:11:09 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-010d9f38-kafka-clients is ready
2022-04-07 05:11:09 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update KafkaMirrorMaker2 my-cluster-010d9f38 in namespace namespace-144
2022-04-07 05:11:09 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:11:09 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:433] Wait for KafkaMirrorMaker2: my-cluster-010d9f38 will have desired state: Ready
2022-04-07 05:12:12 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:444] KafkaMirrorMaker2: my-cluster-010d9f38 is in desired state: Ready
2022-04-07 05:12:12 [ForkJoinPool-5-worker-3] [32mINFO [m [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-04-07 05:12:12 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@2b1381b5, messages=[], arguments=[USER=my_cluster_010d9f38_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093, --topic, mirrormaker2-topic-example-a-1103661764], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb', podNamespace='namespace-144', bootstrapServer='my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093', topicName='mirrormaker2-topic-example-a-1103661764', maxMessages=200, kafkaUsername='my-cluster-010d9f38-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@66c26142}
2022-04-07 05:12:12 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093:mirrormaker2-topic-example-a-1103661764 from pod my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb
2022-04-07 05:12:12 [ForkJoinPool-5-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb -n namespace-144 -- /opt/kafka/producer.sh USER=my_cluster_010d9f38_my_user_source --max-messages 200 --bootstrap-server my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093 --topic mirrormaker2-topic-example-a-1103661764
2022-04-07 05:12:16 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 05:12:16 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 05:12:16 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5333fb5, messages=[], arguments=[--group-id, my-consumer-group-293324470, USER=my_cluster_010d9f38_my_user_source, --max-messages, 200, --group-instance-id, instance1471510808, --bootstrap-server, my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093, --topic, mirrormaker2-topic-example-a-1103661764], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb', podNamespace='namespace-144', bootstrapServer='my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093', topicName='mirrormaker2-topic-example-a-1103661764', maxMessages=200, kafkaUsername='my-cluster-010d9f38-my-user-source', consumerGroupName='my-consumer-group-293324470', consumerInstanceId='instance1471510808', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@700e9887}
2022-04-07 05:12:16 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093:mirrormaker2-topic-example-a-1103661764 from pod my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb
2022-04-07 05:12:16 [ForkJoinPool-5-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb -n namespace-144 -- /opt/kafka/consumer.sh --group-id my-consumer-group-293324470 USER=my_cluster_010d9f38_my_user_source --max-messages 200 --group-instance-id instance1471510808 --bootstrap-server my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093 --topic mirrormaker2-topic-example-a-1103661764
2022-04-07 05:12:23 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 05:12:23 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 05:12:23 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1338] Now messages should be mirrored to target topic and cluster
2022-04-07 05:12:23 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@3aaff601, messages=[], arguments=[--group-id, my-consumer-group-1809592911, USER=my_cluster_010d9f38_my_user_target, --max-messages, 200, --group-instance-id, instance1175029134, --bootstrap-server, my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093, --topic, my-cluster-010d9f38-source.mirrormaker2-topic-example-a-1103661764], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb', podNamespace='namespace-144', bootstrapServer='my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093', topicName='my-cluster-010d9f38-source.mirrormaker2-topic-example-a-1103661764', maxMessages=200, kafkaUsername='my-cluster-010d9f38-my-user-target', consumerGroupName='my-consumer-group-1809592911', consumerInstanceId='instance1175029134', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@321a088f}
2022-04-07 05:12:23 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093:my-cluster-010d9f38-source.mirrormaker2-topic-example-a-1103661764 from pod my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb
2022-04-07 05:12:23 [ForkJoinPool-5-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-010d9f38-kafka-clients-765b5bfdb8-dblpb -n namespace-144 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1809592911 USER=my_cluster_010d9f38_my_user_target --max-messages 200 --group-instance-id instance1175029134 --bootstrap-server my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093 --topic my-cluster-010d9f38-source.mirrormaker2-topic-example-a-1103661764
2022-04-07 05:12:30 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 05:12:30 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 05:12:30 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1340] Messages successfully mirrored
2022-04-07 05:12:30 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1344] Changing KafkaUser sha-password on KMM2 Source and make sure it rolled
2022-04-07 05:12:30 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-010d9f38-mirrormaker2 rolling update
2022-04-07 05:14:00 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-010d9f38-mirrormaker2 will be ready
2022-04-07 05:14:00 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-010d9f38-mirrormaker2 is ready
2022-04-07 05:14:10 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-010d9f38-mirrormaker2 rolling update finished
2022-04-07 05:14:10 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1354] Changing KafkaUser sha-password on KMM2 Target
2022-04-07 05:14:10 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:136] Waiting for Deployment my-cluster-010d9f38-mirrormaker2 rolling update
2022-04-07 05:16:11 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-010d9f38-mirrormaker2 will be ready
2022-04-07 05:16:11 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-010d9f38-mirrormaker2 is ready
2022-04-07 05:16:21 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:141] Deployment my-cluster-010d9f38-mirrormaker2 rolling update finished
2022-04-07 05:16:21 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1364] Recreate kafkaClients pod with new passwords.
2022-04-07 05:16:21 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-010d9f38-kafka-clients in namespace namespace-144
2022-04-07 05:17:01 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:155] Create/Update Deployment my-cluster-010d9f38-kafka-clients in namespace namespace-144
2022-04-07 05:17:01 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:164] Using Namespace: namespace-144
2022-04-07 05:17:01 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:161] Wait for Deployment: my-cluster-010d9f38-kafka-clients will be ready
2022-04-07 05:17:03 [ForkJoinPool-5-worker-3] [32mINFO [m [DeploymentUtils:168] Deployment: my-cluster-010d9f38-kafka-clients is ready
2022-04-07 05:17:03 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5c10fa, messages=[], arguments=[USER=my_cluster_010d9f38_my_user_source, --max-messages, 200, --bootstrap-server, my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093, --topic, mirrormaker2-topic-example-b-1674639238], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-010d9f38-kafka-clients-84ccf9b666-4h8h9', podNamespace='namespace-144', bootstrapServer='my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093', topicName='mirrormaker2-topic-example-b-1674639238', maxMessages=200, kafkaUsername='my-cluster-010d9f38-my-user-source', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@205e6220}
2022-04-07 05:17:03 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:124] Producing 200 messages to my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093:mirrormaker2-topic-example-b-1674639238 from pod my-cluster-010d9f38-kafka-clients-84ccf9b666-4h8h9
2022-04-07 05:17:03 [ForkJoinPool-5-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-010d9f38-kafka-clients-84ccf9b666-4h8h9 -n namespace-144 -- /opt/kafka/producer.sh USER=my_cluster_010d9f38_my_user_source --max-messages 200 --bootstrap-server my-cluster-010d9f38-source-kafka-bootstrap.namespace-144.svc:9093 --topic mirrormaker2-topic-example-b-1674639238
2022-04-07 05:17:06 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:127] Producer finished correctly: true
2022-04-07 05:17:06 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:131] Producer produced 200 messages
2022-04-07 05:17:06 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1385] Now messages should be mirrored to target topic and cluster
2022-04-07 05:17:06 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@448b8fce, messages=[], arguments=[--group-id, my-consumer-group-1866499420, USER=my_cluster_010d9f38_my_user_target, --max-messages, 200, --group-instance-id, instance936239803, --bootstrap-server, my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093, --topic, my-cluster-010d9f38-source.mirrormaker2-topic-example-b-1674639238], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-010d9f38-kafka-clients-84ccf9b666-4h8h9', podNamespace='namespace-144', bootstrapServer='my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093', topicName='my-cluster-010d9f38-source.mirrormaker2-topic-example-b-1674639238', maxMessages=200, kafkaUsername='my-cluster-010d9f38-my-user-target', consumerGroupName='my-consumer-group-1866499420', consumerInstanceId='instance936239803', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@1717a06e}
2022-04-07 05:17:06 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:192] Consuming 200 messages from my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093:my-cluster-010d9f38-source.mirrormaker2-topic-example-b-1674639238 from pod my-cluster-010d9f38-kafka-clients-84ccf9b666-4h8h9
2022-04-07 05:17:06 [ForkJoinPool-5-worker-3] [32mINFO [m [VerifiableClient:192] Client command: kubectl exec my-cluster-010d9f38-kafka-clients-84ccf9b666-4h8h9 -n namespace-144 -- /opt/kafka/consumer.sh --group-id my-consumer-group-1866499420 USER=my_cluster_010d9f38_my_user_target --max-messages 200 --group-instance-id instance936239803 --bootstrap-server my-cluster-010d9f38-target-kafka-bootstrap.namespace-144.svc:9093 --topic my-cluster-010d9f38-source.mirrormaker2-topic-example-b-1674639238
2022-04-07 05:17:14 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:195] Consumer finished correctly: true
2022-04-07 05:17:14 [ForkJoinPool-5-worker-3] [32mINFO [m [InternalKafkaClient:198] Consumer consumed 200 messages
2022-04-07 05:17:14 [ForkJoinPool-5-worker-3] [32mINFO [m [MirrorMaker2IsolatedST:1387] Messages successfully mirrored
2022-04-07 05:17:14 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:17:14 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 05:17:14 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-010d9f38-my-user-target in namespace namespace-144
2022-04-07 05:17:14 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-a-1103661764 in namespace namespace-144
2022-04-07 05:17:24 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of KafkaTopic mirrormaker2-topic-example-b-1674639238 in namespace namespace-144
2022-04-07 05:17:24 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaUser my-cluster-010d9f38-my-user-source in namespace namespace-144
2022-04-07 05:17:34 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of KafkaMirrorMaker2 my-cluster-010d9f38 in namespace namespace-144
2022-04-07 05:17:44 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-010d9f38-kafka-clients in namespace namespace-144
2022-04-07 05:17:54 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-010d9f38-target in namespace namespace-144
2022-04-07 05:18:04 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of Kafka my-cluster-010d9f38-source in namespace namespace-144
2022-04-07 05:18:14 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of Deployment my-cluster-010d9f38-kafka-clients in namespace namespace-144
2022-04-07 05:18:24 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 05:18:24 [ForkJoinPool-5-worker-3] [32mINFO [m [TestSuiteNamespaceManager:200] Deleting namespace:namespace-144 for test case:testKMM2RollAfterSecretsCertsUpdateScramsha
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [TestSeparator:29] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha-FINISHED
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [TestSeparator:30] ############################################################################
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:346] In context MirrorMaker2IsolatedST is everything deleted.
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
[[1;34mINFO[m] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 644.089 s - in io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:618] ============================================================================
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [SetupClusterOperator:620] ============================================================================
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:344] ############################################################################
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:35 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-04-07 05:18:35 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:45 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:45 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:45 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-04-07 05:18:55 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-04-07 05:18:55 [ForkJoinPool-5-worker-1] [32mINFO [m [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-04-07 05:19:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:19:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-04-07 05:19:05 [ForkJoinPool-5-worker-3] [32mINFO [m [ResourceManager:369] ############################################################################
2022-04-07 05:19:31 [main] [32mINFO [m [TestExecutionListener:40] =======================================================================
2022-04-07 05:19:31 [main] [32mINFO [m [TestExecutionListener:41] =======================================================================
2022-04-07 05:19:31 [main] [32mINFO [m [TestExecutionListener:42]                         Test run finished
2022-04-07 05:19:31 [main] [32mINFO [m [TestExecutionListener:43] =======================================================================
2022-04-07 05:19:31 [main] [32mINFO [m [TestExecutionListener:44] =======================================================================
[[1;34mINFO[m] 
[[1;34mINFO[m] Results:
[[1;34mINFO[m] 
[[1;33mWARNING[m] Flakes: 
[[1;33mWARNING[m] io.strimzi.systemtest.connect.ConnectIsolatedST.testScaleConnectWithoutConnectorToZero(ExtensionContext)
[[1;31mERROR[m]   Run 1: ConnectIsolatedST.testScaleConnectWithoutConnectorToZero:900 
Expected: is <0>
     but: was <2>
[[1;34mINFO[m]   Run 2: PASS
[[1;34mINFO[m] 
[[1;33mWARNING[m] io.strimzi.systemtest.metrics.MetricsIsolatedST.testKafkaExporterDataAfterExchange(ExtensionContext)
[[1;31mERROR[m]   Run 1: MetricsIsolatedST.testKafkaExporterDataAfterExchange:240->lambda$testKafkaExporterDataAfterExchange$10:243->lambda$testKafkaExporterDataAfterExchange$9:245 
Expected: a string containing "kafka_consumergroup_current_offset"
     but: was "# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
go_gc_duration_seconds{quantile="0"} 0
go_gc_duration_seconds{quantile="0.25"} 0
go_gc_duration_seconds{quantile="0.5"} 0
go_gc_duration_seconds{quantile="0.75"} 0
go_gc_duration_seconds{quantile="1"} 0
go_gc_duration_seconds_sum 0
go_gc_duration_seconds_count 0
# HELP go_goroutines Number of goroutines that currently exist.
# TYPE go_goroutines gauge
go_goroutines 18
# HELP go_info Information about the Go environment.
# TYPE go_info gauge
go_info{version="go1.17.1"} 1
# HELP go_memstats_alloc_bytes Number of bytes allocated and still in use.
# TYPE go_memstats_alloc_bytes gauge
go_memstats_alloc_bytes 1.904008e+06
# HELP go_memstats_alloc_bytes_total Total number of bytes allocated, even if freed.
# TYPE go_memstats_alloc_bytes_total counter
go_memstats_alloc_bytes_total 1.904008e+06
# HELP go_memstats_buck_hash_sys_bytes Number of bytes used by the profiling bucket hash table.
# TYPE go_memstats_buck_hash_sys_bytes gauge
go_memstats_buck_hash_sys_bytes 1.446551e+06
# HELP go_memstats_frees_total Total number of frees.
# TYPE go_memstats_frees_total counter
go_memstats_frees_total 2441
# HELP go_memstats_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started.
# TYPE go_memstats_gc_cpu_fraction gauge
go_memstats_gc_cpu_fraction 0
# HELP go_memstats_gc_sys_bytes Number of bytes used for garbage collection system metadata.
# TYPE go_memstats_gc_sys_bytes gauge
go_memstats_gc_sys_bytes 4.2212e+06
# HELP go_memstats_heap_alloc_bytes Number of heap bytes allocated and still in use.
# TYPE go_memstats_heap_alloc_bytes gauge
go_memstats_heap_alloc_bytes 1.904008e+06
# HELP go_memstats_heap_idle_bytes Number of heap bytes waiting to be used.
# TYPE go_memstats_heap_idle_bytes gauge
go_memstats_heap_idle_bytes 4.399104e+06
# HELP go_memstats_heap_inuse_bytes Number of heap bytes that are in use.
# TYPE go_memstats_heap_inuse_bytes gauge
go_memstats_heap_inuse_bytes 3.530752e+06
# HELP go_memstats_heap_objects Number of allocated objects.
# TYPE go_memstats_heap_objects gauge
go_memstats_heap_objects 10101
# HELP go_memstats_heap_released_bytes Number of heap bytes released to OS.
# TYPE go_memstats_heap_released_bytes gauge
go_memstats_heap_released_bytes 4.366336e+06
# HELP go_memstats_heap_sys_bytes Number of heap bytes obtained from system.
# TYPE go_memstats_heap_sys_bytes gauge
go_memstats_heap_sys_bytes 7.929856e+06
# HELP go_memstats_last_gc_time_seconds Number of seconds since 1970 of last garbage collection.
# TYPE go_memstats_last_gc_time_seconds gauge
go_memstats_last_gc_time_seconds 0
# HELP go_memstats_lookups_total Total number of pointer lookups.
# TYPE go_memstats_lookups_total counter
go_memstats_lookups_total 0
# HELP go_memstats_mallocs_total Total number of mallocs.
# TYPE go_memstats_mallocs_total counter
go_memstats_mallocs_total 12542
# HELP go_memstats_mcache_inuse_bytes Number of bytes in use by mcache structures.
# TYPE go_memstats_mcache_inuse_bytes gauge
go_memstats_mcache_inuse_bytes 9600
# HELP go_memstats_mcache_sys_bytes Number of bytes used for mcache structures obtained from system.
# TYPE go_memstats_mcache_sys_bytes gauge
go_memstats_mcache_sys_bytes 16384
# HELP go_memstats_mspan_inuse_bytes Number of bytes in use by mspan structures.
# TYPE go_memstats_mspan_inuse_bytes gauge
go_memstats_mspan_inuse_bytes 54944
# HELP go_memstats_mspan_sys_bytes Number of bytes used for mspan structures obtained from system.
# TYPE go_memstats_mspan_sys_bytes gauge
go_memstats_mspan_sys_bytes 65536
# HELP go_memstats_next_gc_bytes Number of heap bytes when next garbage collection will take place.
# TYPE go_memstats_next_gc_bytes gauge
go_memstats_next_gc_bytes 4.473924e+06
# HELP go_memstats_other_sys_bytes Number of bytes used for other system allocations.
# TYPE go_memstats_other_sys_bytes gauge
go_memstats_other_sys_bytes 1.018209e+06
# HELP go_memstats_stack_inuse_bytes Number of bytes in use by the stack allocator.
# TYPE go_memstats_stack_inuse_bytes gauge
go_memstats_stack_inuse_bytes 458752
# HELP go_memstats_stack_sys_bytes Number of bytes obtained from system for stack allocator.
# TYPE go_memstats_stack_sys_bytes gauge
go_memstats_stack_sys_bytes 458752
# HELP go_memstats_sys_bytes Number of bytes obtained from system.
# TYPE go_memstats_sys_bytes gauge
go_memstats_sys_bytes 1.5156488e+07
# HELP go_threads Number of OS threads created.
# TYPE go_threads gauge
go_threads 6
# HELP kafka_brokers Number of Brokers in the Kafka Cluster.
# TYPE kafka_brokers gauge
kafka_brokers 3
# HELP kafka_exporter_build_info A metric with a constant '1' value labeled by version, revision, branch, and goversion from which kafka_exporter was built.
# TYPE kafka_exporter_build_info gauge
kafka_exporter_build_info{branch="HEAD",goversion="go1.17.1",revision="0d5d4ac4ba63948748cc2c53b35ed95c310cd6f2",version="1.4.2"} 1
# HELP kafka_topic_partition_current_offset Current Offset of a Broker at Topic/Partition
# TYPE kafka_topic_partition_current_offset gauge
kafka_topic_partition_current_offset{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_current_offset{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partition_in_sync_replica Number of In-Sync Replicas for this Topic/Partition
# TYPE kafka_topic_partition_in_sync_replica gauge
kafka_topic_partition_in_sync_replica{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="1",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="2",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="4",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="5",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_in_sync_replica{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_leader Leader Broker ID of this Topic/Partition
# TYPE kafka_topic_partition_leader gauge
kafka_topic_partition_leader{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_leader{partition="1",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_leader{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_leader{partition="4",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_leader{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_leader_is_preferred 1 if Topic/Partition is using the Preferred Broker
# TYPE kafka_topic_partition_leader_is_preferred gauge
kafka_topic_partition_leader_is_preferred{partition="0",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="1",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="2",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="3",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="4",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="5",topic="my-topic-1438331180-1430761917"} 1
kafka_topic_partition_leader_is_preferred{partition="6",topic="my-topic-1438331180-1430761917"} 1
# HELP kafka_topic_partition_oldest_offset Oldest Offset of a Broker at Topic/Partition
# TYPE kafka_topic_partition_oldest_offset gauge
kafka_topic_partition_oldest_offset{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_oldest_offset{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partition_replicas Number of Replicas for this Topic/Partition
# TYPE kafka_topic_partition_replicas gauge
kafka_topic_partition_replicas{partition="0",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="1",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="2",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="3",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="4",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="5",topic="my-topic-1438331180-1430761917"} 2
kafka_topic_partition_replicas{partition="6",topic="my-topic-1438331180-1430761917"} 2
# HELP kafka_topic_partition_under_replicated_partition 1 if Topic/Partition is under Replicated
# TYPE kafka_topic_partition_under_replicated_partition gauge
kafka_topic_partition_under_replicated_partition{partition="0",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="1",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="2",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="3",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="4",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="5",topic="my-topic-1438331180-1430761917"} 0
kafka_topic_partition_under_replicated_partition{partition="6",topic="my-topic-1438331180-1430761917"} 0
# HELP kafka_topic_partitions Number of partitions for this Topic
# TYPE kafka_topic_partitions gauge
kafka_topic_partitions{topic="my-topic-1438331180-1430761917"} 7
# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
# TYPE process_cpu_seconds_total counter
process_cpu_seconds_total 0.02
# HELP process_max_fds Maximum number of open file descriptors.
# TYPE process_max_fds gauge
process_max_fds 1.048576e+06
# HELP process_open_fds Number of open file descriptors.
# TYPE process_open_fds gauge
process_open_fds 10
# HELP process_resident_memory_bytes Resident memory size in bytes.
# TYPE process_resident_memory_bytes gauge
process_resident_memory_bytes 1.355776e+07
# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.64930417096e+09
# HELP process_virtual_memory_bytes Virtual memory size in bytes.
# TYPE process_virtual_memory_bytes gauge
process_virtual_memory_bytes 7.33478912e+08
# HELP process_virtual_memory_max_bytes Maximum amount of virtual memory available in bytes.
# TYPE process_virtual_memory_max_bytes gauge
process_virtual_memory_max_bytes 1.8446744073709552e+19
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 0
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0
"
[[1;34mINFO[m]   Run 2: PASS
[[1;34mINFO[m] 
[[1;33mWARNING[m] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha(ExtensionContext)
[[1;31mERROR[m]   Run 1: MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha:1278 ? IndexOutOfBounds
[[1;31mERROR[m]   Run 2: MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateScramsha:1278 ? IndexOutOfBounds
[[1;34mINFO[m]   Run 3: PASS
[[1;34mINFO[m] 
[[1;33mWARNING[m] io.strimzi.systemtest.mirrormaker.MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS(ExtensionContext)
[[1;31mERROR[m]   Run 1: MirrorMaker2IsolatedST.testKMM2RollAfterSecretsCertsUpdateTLS:1562 ? NullPointer
[[1;34mINFO[m]   Run 2: PASS
[[1;34mINFO[m] 
[[1;33mWARNING[m] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAReadFromTopic(ExtensionContext)
[[1;31mERROR[m]   Run 1: OauthAuthorizationIsolatedST.testTeamAReadFromTopic:220 ? Wait Timeout after 3...
[[1;34mINFO[m]   Run 2: PASS
[[1;34mINFO[m] 
[[1;33mWARNING[m] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamAWriteToTopic(ExtensionContext)
[[1;31mERROR[m]   Run 1: OauthAuthorizationIsolatedST.testTeamAWriteToTopic:150 ? Wait Timeout after 30...
[[1;34mINFO[m]   Run 2: PASS
[[1;34mINFO[m] 
[[1;33mWARNING[m] io.strimzi.systemtest.security.oauth.OauthAuthorizationIsolatedST.testTeamBWriteToTopic(ExtensionContext)
[[1;31mERROR[m]   Run 1: OauthAuthorizationIsolatedST.testTeamBWriteToTopic:262 ? Wait Timeout after 30...
[[1;34mINFO[m]   Run 2: PASS
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;33mWARNING[m] Tests run: 304, Failures: 0, Errors: 0, Skipped: 13, Flakes: 7
[[1;34mINFO[m] 
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-failsafe-plugin:3.0.0-M5:verify[m [1m(default)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] 
[[1;34mINFO[m] [1m--- [0;32mmaven-dependency-plugin:3.1.1:analyze-only[m [1m(analyze)[m @ [36msystemtest[0;1m ---[m
[[1;34mINFO[m] No dependency problems found
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1mReactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:[m
[[1;34mINFO[m] 
[[1;34mINFO[m] Strimzi - Apache Kafka on Kubernetes and OpenShift . [1;32mSUCCESS[m [  2.575 s]
[[1;34mINFO[m] test ............................................... [1;32mSUCCESS[m [  1.001 s]
[[1;34mINFO[m] crd-annotations .................................... [1;32mSUCCESS[m [  1.056 s]
[[1;34mINFO[m] crd-generator ...................................... [1;32mSUCCESS[m [  2.558 s]
[[1;34mINFO[m] api ................................................ [1;32mSUCCESS[m [  6.776 s]
[[1;34mINFO[m] mockkube ........................................... [1;32mSUCCESS[m [  0.913 s]
[[1;34mINFO[m] config-model ....................................... [1;32mSUCCESS[m [  0.700 s]
[[1;34mINFO[m] certificate-manager ................................ [1;32mSUCCESS[m [  0.760 s]
[[1;34mINFO[m] operator-common .................................... [1;32mSUCCESS[m [  1.835 s]
[[1;34mINFO[m] systemtest ......................................... [1;32mSUCCESS[m [  14:24 h]
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] [1;32mBUILD SUCCESS[m
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
[[1;34mINFO[m] Total time:  14:25 h
[[1;34mINFO[m] Finished at: 2022-04-07T05:19:32Z
[[1;34mINFO[m] [1m------------------------------------------------------------------------[m
